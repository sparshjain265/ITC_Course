ELEMENTS OF
INFORMATION THEORY
Second Edition
THOMAS M. COVER
JOY A. THOMAS

A JOHN WILEY & SONS, INC., PUBLICATION

ELEMENTS OF
INFORMATION THEORY

ELEMENTS OF
INFORMATION THEORY
Second Edition
THOMAS M. COVER
JOY A. THOMAS

A JOHN WILEY & SONS, INC., PUBLICATION

Copyright ï 2006 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com.
Requests to the Publisher for permission should be addressed to the Permissions Department,
John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008,
or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciï¬cally disclaim any implied warranties of
merchantability or ï¬tness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proï¬t or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States at
(317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print
may not be available in electronic formats. For more information about Wiley products, visit our web
site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Cover, T. M., 1938â
Elements of information theory/by Thomas M. Cover, Joy A. Thomas.â2nd ed.
p. cm.
âA Wiley-Interscience publication.â
Includes bibliographical references and index.
ISBN-13 978-0-471-24195-9
ISBN-10 0-471-24195-4
1. Information theory. I. Thomas, Joy A. II. Title.
Q360.C68 2005
003 .54âdc22
2005047799
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1

CONTENTS

Contents

v

Preface to the Second Edition

xv

Preface to the First Edition

xvii

Acknowledgments for the Second Edition

xxi

Acknowledgments for the First Edition
1

2

Introduction and Preview
1.1
Preview of the Book

xxiii
1

5

Entropy, Relative Entropy, and Mutual Information
2.1
Entropy 13
2.2
Joint Entropy and Conditional Entropy 16
2.3
Relative Entropy and Mutual Information 19
2.4
Relationship Between Entropy and Mutual
Information 20
2.5
Chain Rules for Entropy, Relative Entropy,
and Mutual Information 22
2.6
Jensenâs Inequality and Its Consequences 25
2.7
Log Sum Inequality and Its Applications 30
2.8
Data-Processing Inequality 34
2.9
Sufï¬cient Statistics 35
2.10 Fanoâs Inequality 37
Summary 41
Problems 43
Historical Notes 54

13

v

vi

CONTENTS

3

Asymptotic Equipartition Property
3.1
Asymptotic Equipartition Property Theorem 58
3.2
Consequences of the AEP: Data Compression 60
3.3
High-Probability Sets and the Typical Set 62
Summary 64
Problems 64
Historical Notes 69

57

4

Entropy Rates of a Stochastic Process
4.1
Markov Chains 71
4.2
Entropy Rate 74
4.3
Example: Entropy Rate of a Random Walk on a
Weighted Graph 78
4.4
Second Law of Thermodynamics 81
4.5
Functions of Markov Chains 84
Summary 87
Problems 88
Historical Notes 100

71

5

Data Compression
5.1
Examples of Codes 103
5.2
Kraft Inequality 107
5.3
Optimal Codes 110
5.4
Bounds on the Optimal Code Length 112
5.5
Kraft Inequality for Uniquely Decodable
Codes 115
5.6
Huffman Codes 118
5.7
Some Comments on Huffman Codes 120
5.8
Optimality of Huffman Codes 123
5.9
ShannonâFanoâElias Coding 127
5.10 Competitive Optimality of the Shannon
Code 130
5.11 Generation of Discrete Distributions from
Fair Coins 134
Summary 141
Problems 142
Historical Notes 157

103

CONTENTS

vii

6

Gambling and Data Compression
6.1
The Horse Race 159
6.2
Gambling and Side Information 164
6.3
Dependent Horse Races and Entropy Rate 166
6.4
The Entropy of English 168
6.5
Data Compression and Gambling 171
6.6
Gambling Estimate of the Entropy of English 173
Summary 175
Problems 176
Historical Notes 182

159

7

Channel Capacity
7.1
Examples of Channel Capacity 184
7.1.1 Noiseless Binary Channel 184
7.1.2 Noisy Channel with Nonoverlapping
Outputs 185
7.1.3 Noisy Typewriter 186
7.1.4 Binary Symmetric Channel 187
7.1.5 Binary Erasure Channel 188
7.2
Symmetric Channels 189
7.3
Properties of Channel Capacity 191
7.4
Preview of the Channel Coding Theorem 191
7.5
Deï¬nitions 192
7.6
Jointly Typical Sequences 195
7.7
Channel Coding Theorem 199
7.8
Zero-Error Codes 205
7.9
Fanoâs Inequality and the Converse to the Coding
Theorem 206
7.10 Equality in the Converse to the Channel Coding
Theorem 208
7.11 Hamming Codes 210
7.12 Feedback Capacity 216
7.13 SourceâChannel Separation Theorem 218
Summary 222
Problems 223
Historical Notes 240

183

viii

CONTENTS

8

Differential Entropy
8.1
Deï¬nitions 243
8.2
AEP for Continuous Random Variables 245
8.3
Relation of Differential Entropy to Discrete
Entropy 247
8.4
Joint and Conditional Differential Entropy 249
8.5
Relative Entropy and Mutual Information 250
8.6
Properties of Differential Entropy, Relative Entropy,
and Mutual Information 252
Summary 256
Problems 256
Historical Notes 259

243

9

Gaussian Channel
9.1
Gaussian Channel: Deï¬nitions 263
9.2
Converse to the Coding Theorem for Gaussian
Channels 268
9.3
Bandlimited Channels 270
9.4
Parallel Gaussian Channels 274
9.5
Channels with Colored Gaussian Noise 277
9.6
Gaussian Channels with Feedback 280
Summary 289
Problems 290
Historical Notes 299

261

10 Rate Distortion Theory
301
10.1 Quantization 301
10.2 Deï¬nitions 303
10.3 Calculation of the Rate Distortion Function 307
10.3.1 Binary Source 307
10.3.2 Gaussian Source 310
10.3.3 Simultaneous Description of Independent
Gaussian Random Variables 312
10.4 Converse to the Rate Distortion Theorem 315
10.5 Achievability of the Rate Distortion Function 318
10.6 Strongly Typical Sequences and Rate Distortion 325
10.7 Characterization of the Rate Distortion Function 329

CONTENTS

ix

10.8

Computation of Channel Capacity and the Rate
Distortion Function 332
Summary 335
Problems 336
Historical Notes 345
11

Information Theory and Statistics
11.1 Method of Types 347
11.2 Law of Large Numbers 355
11.3 Universal Source Coding 357
11.4 Large Deviation Theory 360
11.5 Examples of Sanovâs Theorem 364
11.6 Conditional Limit Theorem 366
11.7 Hypothesis Testing 375
11.8 ChernoffâStein Lemma 380
11.9 Chernoff Information 384
11.10 Fisher Information and the CrameÌrâRao
Inequality 392
Summary 397
Problems 399
Historical Notes 408

347

12

Maximum Entropy
12.1 Maximum Entropy Distributions 409
12.2 Examples 411
12.3 Anomalous Maximum Entropy Problem 413
12.4 Spectrum Estimation 415
12.5 Entropy Rates of a Gaussian Process 416
12.6 Burgâs Maximum Entropy Theorem 417
Summary 420
Problems 421
Historical Notes 425

409

13

Universal Source Coding
13.1 Universal Codes and Channel Capacity 428
13.2 Universal Coding for Binary Sequences 433
13.3 Arithmetic Coding 436

427

x

CONTENTS

13.4

LempelâZiv Coding 440
13.4.1 Sliding Window LempelâZiv
Algorithm 441
13.4.2 Tree-Structured LempelâZiv
Algorithms 442
13.5 Optimality of LempelâZiv Algorithms 443
13.5.1 Sliding Window LempelâZiv
Algorithms 443
13.5.2 Optimality of Tree-Structured LempelâZiv
Compression 448
Summary 456
Problems 457
Historical Notes 461
14 Kolmogorov Complexity
14.1 Models of Computation 464
14.2 Kolmogorov Complexity: Deï¬nitions
and Examples 466
14.3 Kolmogorov Complexity and Entropy 473
14.4 Kolmogorov Complexity of Integers 475
14.5 Algorithmically Random and Incompressible
Sequences 476
14.6 Universal Probability 480
14.7 Kolmogorov complexity 482
14.8  484
14.9 Universal Gambling 487
14.10 Occamâs Razor 488
14.11 Kolmogorov Complexity and Universal
Probability 490
14.12 Kolmogorov Sufï¬cient Statistic 496
14.13 Minimum Description Length Principle 500
Summary 501
Problems 503
Historical Notes 507

463

15 Network Information Theory
15.1 Gaussian Multiple-User Channels

509
513

CONTENTS

15.2
15.3

15.4

15.5
15.6

15.7
15.8
15.9

15.1.1 Single-User Gaussian Channel 513
15.1.2 Gaussian Multiple-Access Channel
with m Users 514
15.1.3 Gaussian Broadcast Channel 515
15.1.4 Gaussian Relay Channel 516
15.1.5 Gaussian Interference Channel 518
15.1.6 Gaussian Two-Way Channel 519
Jointly Typical Sequences 520
Multiple-Access Channel 524
15.3.1 Achievability of the Capacity Region for the
Multiple-Access Channel 530
15.3.2 Comments on the Capacity Region for the
Multiple-Access Channel 532
15.3.3 Convexity of the Capacity Region of the
Multiple-Access Channel 534
15.3.4 Converse for the Multiple-Access
Channel 538
15.3.5 m-User Multiple-Access Channels 543
15.3.6 Gaussian Multiple-Access Channels 544
Encoding of Correlated Sources 549
15.4.1 Achievability of the SlepianâWolf
Theorem 551
15.4.2 Converse for the SlepianâWolf
Theorem 555
15.4.3 SlepianâWolf Theorem for Many
Sources 556
15.4.4 Interpretation of SlepianâWolf
Coding 557
Duality Between SlepianâWolf Encoding and
Multiple-Access Channels 558
Broadcast Channel 560
15.6.1 Deï¬nitions for a Broadcast Channel 563
15.6.2 Degraded Broadcast Channels 564
15.6.3 Capacity Region for the Degraded Broadcast
Channel 565
Relay Channel 571
Source Coding with Side Information 575
Rate Distortion with Side Information 580

xi

xii

CONTENTS

15.10 General Multiterminal Networks
Summary 594
Problems 596
Historical Notes 609

587

16 Information Theory and Portfolio Theory
16.1 The Stock Market: Some Deï¬nitions 613
16.2 KuhnâTucker Characterization of the Log-Optimal
Portfolio 617
16.3 Asymptotic Optimality of the Log-Optimal
Portfolio 619
16.4 Side Information and the Growth Rate 621
16.5 Investment in Stationary Markets 623
16.6 Competitive Optimality of the Log-Optimal
Portfolio 627
16.7 Universal Portfolios 629
16.7.1 Finite-Horizon Universal Portfolios 631
16.7.2 Horizon-Free Universal Portfolios 638
16.8 ShannonâMcMillanâBreiman Theorem
(General AEP) 644
Summary 650
Problems 652
Historical Notes 655

613

17 Inequalities in Information Theory
17.1 Basic Inequalities of Information Theory 657
17.2 Differential Entropy 660
17.3 Bounds on Entropy and Relative Entropy 663
17.4 Inequalities for Types 665
17.5 Combinatorial Bounds on Entropy 666
17.6 Entropy Rates of Subsets 667
17.7 Entropy and Fisher Information 671
17.8 Entropy Power Inequality and BrunnâMinkowski
Inequality 674
17.9 Inequalities for Determinants 679

657

CONTENTS

17.10 Inequalities for Ratios of Determinants
Summary 686
Problems 686
Historical Notes 687

xiii

683

Bibliography

689

List of Symbols

723

Index

727

PREFACE TO THE
SECOND EDITION

In the years since the publication of the ï¬rst edition, there were many
aspects of the book that we wished to improve, to rearrange, or to expand,
but the constraints of reprinting would not allow us to make those changes
between printings. In the new edition, we now get a chance to make some
of these changes, to add problems, and to discuss some topics that we had
omitted from the ï¬rst edition.
The key changes include a reorganization of the chapters to make
the book easier to teach, and the addition of more than two hundred
new problems. We have added material on universal portfolios, universal
source coding, Gaussian feedback capacity, network information theory,
and developed the duality of data compression and channel capacity. A
new chapter has been added and many proofs have been simpliï¬ed. We
have also updated the references and historical notes.
The material in this book can be taught in a two-quarter sequence. The
ï¬rst quarter might cover Chapters 1 to 9, which includes the asymptotic
equipartition property, data compression, and channel capacity, culminating in the capacity of the Gaussian channel. The second quarter could
cover the remaining chapters, including rate distortion, the method of
types, Kolmogorov complexity, network information theory, universal
source coding, and portfolio theory. If only one semester is available, we
would add rate distortion and a single lecture each on Kolmogorov complexity and network information theory to the ï¬rst semester. A web site,
http://www.elementsoï¬nformationtheory.com, provides links to additional
material and solutions to selected problems.
In the years since the ï¬rst edition of the book, information theory
celebrated its 50th birthday (the 50th anniversary of Shannonâs original
paper that started the ï¬eld), and ideas from information theory have been
applied to many problems of science and technology, including bioinformatics, web search, wireless communication, video compression, and
xv

xvi

PREFACE TO THE SECOND EDITION

others. The list of applications is endless, but it is the elegance of the
fundamental mathematics that is still the key attraction of this area. We
hope that this book will give some insight into why we believe that this
is one of the most interesting areas at the intersection of mathematics,
physics, statistics, and engineering.
Tom Cover
Joy Thomas
Palo Alto, California
January 2006

PREFACE TO THE
FIRST EDITION

This is intended to be a simple and accessible book on information theory.
As Einstein said, âEverything should be made as simple as possible, but no
simpler.â Although we have not veriï¬ed the quote (ï¬rst found in a fortune
cookie), this point of view drives our development throughout the book.
There are a few key ideas and techniques that, when mastered, make the
subject appear simple and provide great intuition on new questions.
This book has arisen from over ten years of lectures in a two-quarter
sequence of a senior and ï¬rst-year graduate-level course in information
theory, and is intended as an introduction to information theory for students of communication theory, computer science, and statistics.
There are two points to be made about the simplicities inherent in information theory. First, certain quantities like entropy and mutual information
arise as the answers to fundamental questions. For example, entropy is
the minimum descriptive complexity of a random variable, and mutual
information is the communication rate in the presence of noise. Also,
as we shall point out, mutual information corresponds to the increase in
the doubling rate of wealth given side information. Second, the answers
to information theoretic questions have a natural algebraic structure. For
example, there is a chain rule for entropies, and entropy and mutual information are related. Thus the answers to problems in data compression
and communication admit extensive interpretation. We all know the feeling that follows when one investigates a problem, goes through a large
amount of algebra, and ï¬nally investigates the answer to ï¬nd that the
entire problem is illuminated not by the analysis but by the inspection of
the answer. Perhaps the outstanding examples of this in physics are Newtonâs laws and SchroÌdingerâs wave equation. Who could have foreseen the
awesome philosophical interpretations of SchroÌdingerâs wave equation?
In the text we often investigate properties of the answer before we look
at the question. For example, in Chapter 2, we deï¬ne entropy, relative
entropy, and mutual information and study the relationships and a few
xvii

xviii

PREFACE TO THE FIRST EDITION

interpretations of them, showing how the answers ï¬t together in various
ways. Along the way we speculate on the meaning of the second law of
thermodynamics. Does entropy always increase? The answer is yes and
no. This is the sort of result that should please experts in the area but
might be overlooked as standard by the novice.
In fact, that brings up a point that often occurs in teaching. It is fun
to ï¬nd new proofs or slightly new results that no one else knows. When
one presents these ideas along with the established material in class, the
response is âsure, sure, sure.â But the excitement of teaching the material
is greatly enhanced. Thus we have derived great pleasure from investigating a number of new ideas in this textbook.
Examples of some of the new material in this text include the chapter
on the relationship of information theory to gambling, the work on the universality of the second law of thermodynamics in the context of Markov
chains, the joint typicality proofs of the channel capacity theorem, the
competitive optimality of Huffman codes, and the proof of Burgâs theorem
on maximum entropy spectral density estimation. Also, the chapter on
Kolmogorov complexity has no counterpart in other information theory
texts. We have also taken delight in relating Fisher information, mutual
information, the central limit theorem, and the BrunnâMinkowski and
entropy power inequalities. To our surprise, many of the classical results
on determinant inequalities are most easily proved using information theoretic inequalities.
Even though the ï¬eld of information theory has grown considerably
since Shannonâs original paper, we have strived to emphasize its coherence. While it is clear that Shannon was motivated by problems in communication theory when he developed information theory, we treat information theory as a ï¬eld of its own with applications to communication theory
and statistics. We were drawn to the ï¬eld of information theory from
backgrounds in communication theory, probability theory, and statistics,
because of the apparent impossibility of capturing the intangible concept
of information.
Since most of the results in the book are given as theorems and proofs,
we expect the elegance of the results to speak for themselves. In many
cases we actually describe the properties of the solutions before the problems. Again, the properties are interesting in themselves and provide a
natural rhythm for the proofs that follow.
One innovation in the presentation is our use of long chains of inequalities with no intervening text followed immediately by the explanations.
By the time the reader comes to many of these proofs, we expect that he
or she will be able to follow most of these steps without any explanation
and will be able to pick out the needed explanations. These chains of

PREFACE TO THE FIRST EDITION

xix

inequalities serve as pop quizzes in which the reader can be reassured
of having the knowledge needed to prove some important theorems. The
natural ï¬ow of these proofs is so compelling that it prompted us to ï¬out
one of the cardinal rules of technical writing; and the absence of verbiage
makes the logical necessity of the ideas evident and the key ideas perspicuous. We hope that by the end of the book the reader will share our
appreciation of the elegance, simplicity, and naturalness of information
theory.
Throughout the book we use the method of weakly typical sequences,
which has its origins in Shannonâs original 1948 work but was formally
developed in the early 1970s. The key idea here is the asymptotic equipartition property, which can be roughly paraphrased as âAlmost everything
is almost equally probable.â
Chapter 2 includes the basic algebraic relationships of entropy, relative
entropy, and mutual information. The asymptotic equipartition property
(AEP) is given central prominence in Chapter 3. This leads us to discuss the entropy rates of stochastic processes and data compression in
Chapters 4 and 5. A gambling sojourn is taken in Chapter 6, where the
duality of data compression and the growth rate of wealth is developed.
The sensational success of Kolmogorov complexity as an intellectual
foundation for information theory is explored in Chapter 14. Here we
replace the goal of ï¬nding a description that is good on the average with
the goal of ï¬nding the universally shortest description. There is indeed
a universal notion of the descriptive complexity of an object. Here also
the wonderful number  is investigated. This number, which is the binary
expansion of the probability that a Turing machine will halt, reveals many
of the secrets of mathematics.
Channel capacity is established in Chapter 7. The necessary material
on differential entropy is developed in Chapter 8, laying the groundwork
for the extension of previous capacity theorems to continuous noise channels. The capacity of the fundamental Gaussian channel is investigated in
Chapter 9.
The relationship between information theory and statistics, ï¬rst studied
by Kullback in the early 1950s and relatively neglected since, is developed
in Chapter 11. Rate distortion theory requires a little more background
than its noiseless data compression counterpart, which accounts for its
placement as late as Chapter 10 in the text.
The huge subject of network information theory, which is the study
of the simultaneously achievable ï¬ows of information in the presence of
noise and interference, is developed in Chapter 15. Many new ideas come
into play in network information theory. The primary new ingredients are
interference and feedback. Chapter 16 considers the stock market, which is

xx

PREFACE TO THE FIRST EDITION

the generalization of the gambling processes considered in Chapter 6, and
shows again the close correspondence of information theory and gambling.
Chapter 17, on inequalities in information theory, gives us a chance to
recapitulate the interesting inequalities strewn throughout the book, put
them in a new framework, and then add some interesting new inequalities
on the entropy rates of randomly drawn subsets. The beautiful relationship
of the BrunnâMinkowski inequality for volumes of set sums, the entropy
power inequality for the effective variance of the sum of independent
random variables, and the Fisher information inequalities are made explicit
here.
We have made an attempt to keep the theory at a consistent level.
The mathematical level is a reasonably high one, probably the senior or
ï¬rst-year graduate level, with a background of at least one good semester
course in probability and a solid background in mathematics. We have,
however, been able to avoid the use of measure theory. Measure theory
comes up only brieï¬y in the proof of the AEP for ergodic processes in
Chapter 16. This ï¬ts in with our belief that the fundamentals of information theory are orthogonal to the techniques required to bring them to
their full generalization.
The essential vitamins are contained in Chapters 2, 3, 4, 5, 7, 8, 9,
11, 10, and 15. This subset of chapters can be read without essential
reference to the others and makes a good core of understanding. In our
opinion, Chapter 14 on Kolmogorov complexity is also essential for a deep
understanding of information theory. The rest, ranging from gambling to
inequalities, is part of the terrain illuminated by this coherent and beautiful
subject.
Every course has its ï¬rst lecture, in which a sneak preview and overview
of ideas is presented. Chapter 1 plays this role.
Tom Cover
Joy Thomas
Palo Alto, California
June 1990

ACKNOWLEDGMENTS
FOR THE SECOND EDITION

Since the appearance of the ï¬rst edition, we have been fortunate to receive
feedback, suggestions, and corrections from a large number of readers. It
would be impossible to thank everyone who has helped us in our efforts,
but we would like to list some of them. In particular, we would like
to thank all the faculty who taught courses based on this book and the
students who took those courses; it is through them that we learned to
look at the same material from a different perspective.
In particular, we would like to thank Andrew Barron, Alon Orlitsky,
T. S. Han, Raymond Yeung, Nam Phamdo, Franz Willems, and Marty
Cohn for their comments and suggestions. Over the years, students at
Stanford have provided ideas and inspirations for the changesâthese
include George Gemelos, Navid Hassanpour, Young-Han Kim, Charles
Mathis, Styrmir Sigurjonsson, Jon Yard, Michael Baer, Mung Chiang,
Suhas Diggavi, Elza Erkip, Paul Fahn, Garud Iyengar, David Julian, Yiannis Kontoyiannis, Amos Lapidoth, Erik Ordentlich, Sandeep Pombra, Jim
Roche, Arak Sutivong, Joshua Sweetkind-Singer, and Assaf Zeevi. Denise
Murphy provided much support and help during the preparation of the
second edition.
Joy Thomas would like to acknowledge the support of colleagues
at IBM and Stratify who provided valuable comments and suggestions.
Particular thanks are due Peter Franaszek, C. S. Chang, Randy Nelson,
Ramesh Gopinath, Pandurang Nayak, John Lamping, Vineet Gupta, and
Ramana Venkata. In particular, many hours of dicussion with Brandon
Roy helped reï¬ne some of the arguments in the book. Above all, Joy
would like to acknowledge that the second edition would not have been
possible without the support and encouragement of his wife, Priya, who
makes all things worthwhile.
Tom Cover would like to thank his students and his wife, Karen.

xxi

ACKNOWLEDGMENTS
FOR THE FIRST EDITION

We wish to thank everyone who helped make this book what it is. In
particular, Aaron Wyner, Toby Berger, Masoud Salehi, Alon Orlitsky,
Jim Mazo and Andrew Barron have made detailed comments on various
drafts of the book which guided us in our ï¬nal choice of content. We
would like to thank Bob Gallager for an initial reading of the manuscript
and his encouragement to publish it. Aaron Wyner donated his new proof
with Ziv on the convergence of the Lempel-Ziv algorithm. We would
also like to thank Normam Abramson, Ed van der Meulen, Jack Salz and
Raymond Yeung for their suggested revisions.
Certain key visitors and research associates contributed as well, including Amir Dembo, Paul Algoet, Hirosuke Yamamoto, Ben Kawabata, M.
Shimizu and Yoichiro Watanabe. We beneï¬ted from the advice of John
Gill when he used this text in his class. Abbas El Gamal made invaluable
contributions, and helped begin this book years ago when we planned
to write a research monograph on multiple user information theory. We
would also like to thank the Ph.D. students in information theory as this
book was being written: Laura Ekroot, Will Equitz, Don Kimber, Mitchell
Trott, Andrew Nobel, Jim Roche, Erik Ordentlich, Elza Erkip and Vittorio Castelli. Also Mitchell Oslick, Chien-Wen Tseng and Michael Morrell
were among the most active students in contributing questions and suggestions to the text. Marc Goldberg and Anil Kaul helped us produce
some of the ï¬gures. Finally we would like to thank Kirsten Goodell and
Kathy Adams for their support and help in some of the aspects of the
preparation of the manuscript.
Joy Thomas would also like to thank Peter Franaszek, Steve Lavenberg,
Fred Jelinek, David Nahamoo and Lalit Bahl for their encouragment and
support during the ï¬nal stages of production of this book.

xxiii

CHAPTER 1

INTRODUCTION AND PREVIEW

Information theory answers two fundamental questions in communication
theory: What is the ultimate data compression (answer: the entropy H ),
and what is the ultimate transmission rate of communication (answer: the
channel capacity C). For this reason some consider information theory
to be a subset of communication theory. We argue that it is much more.
Indeed, it has fundamental contributions to make in statistical physics
(thermodynamics), computer science (Kolmogorov complexity or algorithmic complexity), statistical inference (Occamâs Razor: âThe simplest
explanation is bestâ), and to probability and statistics (error exponents for
optimal hypothesis testing and estimation).
This âï¬rst lectureâ chapter goes backward and forward through information theory and its naturally related ideas. The full deï¬nitions and study
of the subject begin in Chapter 2. Figure 1.1 illustrates the relationship
of information theory to other ï¬elds. As the ï¬gure suggests, information
theory intersects physics (statistical mechanics), mathematics (probability
theory), electrical engineering (communication theory), and computer science (algorithmic complexity). We now describe the areas of intersection
in greater detail.
Electrical Engineering (Communication Theory). In the early 1940s
it was thought to be impossible to send information at a positive rate
with negligible probability of error. Shannon surprised the communication theory community by proving that the probability of error could be
made nearly zero for all communication rates below channel capacity.
The capacity can be computed simply from the noise characteristics of
the channel. Shannon further argued that random processes such as music
and speech have an irreducible complexity below which the signal cannot
be compressed. This he named the entropy, in deference to the parallel
use of this word in thermodynamics, and argued that if the entropy of the

Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

1

INTRODUCTION AND PREVIEW

n
atio
nic
u
mm ory
Co The

Pro
b
Th ability
eor
y

Information
Theory

tics
Statis

m

at

ics

In

eq

ua

liti

es

ov
or y
og xit
lm ple
Ko om
C

r
te
pu
m nce
Co cie
S

Portfolio Theory
Kelly Gambling

he

Therm AEP
ody
Quan namics
Inform tum
ati
Theo on
ry

Th Limit
eo
L rem
De arge s
via
tion
s

thesis
Hypo g
Testin
r
Fishe on
ati
Inform

Physic
s

of
its tion
Lim unica
mm ory
Co The

M
at

2

Economics

FIGURE 1.1. Relationship of information theory to other ï¬elds.
Data compression
limit

^

min l (X; X )

Data transmission
limit

max l (X; Y )

FIGURE 1.2. Information theory as the extreme points of communication theory.

source is less than the capacity of the channel, asymptotically error-free
communication can be achieved.
Information theory today represents the extreme points of the set of
all possible communication schemes, as shown in the fanciful Figure 1.2.
The data compression minimum I (X; XÌ) lies at one extreme of the set of
communication ideas. All data compression schemes require description

INTRODUCTION AND PREVIEW

3

rates at least equal to this minimum. At the other extreme is the data
transmission maximum I (X; Y ), known as the channel capacity. Thus,
all modulation schemes and data compression schemes lie between these
limits.
Information theory also suggests means of achieving these ultimate
limits of communication. However, these theoretically optimal communication schemes, beautiful as they are, may turn out to be computationally
impractical. It is only because of the computational feasibility of simple modulation and demodulation schemes that we use them rather than
the random coding and nearest-neighbor decoding rule suggested by Shannonâs proof of the channel capacity theorem. Progress in integrated circuits
and code design has enabled us to reap some of the gains suggested by
Shannonâs theory. Computational practicality was ï¬nally achieved by the
advent of turbo codes. A good example of an application of the ideas of
information theory is the use of error-correcting codes on compact discs
and DVDs.
Recent work on the communication aspects of information theory has
concentrated on network information theory: the theory of the simultaneous rates of communication from many senders to many receivers in the
presence of interference and noise. Some of the trade-offs of rates between
senders and receivers are unexpected, and all have a certain mathematical
simplicity. A unifying theory, however, remains to be found.
Computer Science (Kolmogorov Complexity). Kolmogorov,
Chaitin, and Solomonoff put forth the idea that the complexity of a string
of data can be deï¬ned by the length of the shortest binary computer
program for computing the string. Thus, the complexity is the minimal
description length. This deï¬nition of complexity turns out to be universal,
that is, computer independent, and is of fundamental importance. Thus,
Kolmogorov complexity lays the foundation for the theory of descriptive
complexity. Gratifyingly, the Kolmogorov complexity K is approximately
equal to the Shannon entropy H if the sequence is drawn at random from
a distribution that has entropy H . So the tie-in between information theory
and Kolmogorov complexity is perfect. Indeed, we consider Kolmogorov
complexity to be more fundamental than Shannon entropy. It is the ultimate data compression and leads to a logically consistent procedure for
inference.
There is a pleasing complementary relationship between algorithmic
complexity and computational complexity. One can think about computational complexity (time complexity) and Kolmogorov complexity (program length or descriptive complexity) as two axes corresponding to

4

INTRODUCTION AND PREVIEW

program running time and program length. Kolmogorov complexity focuses on minimizing along the second axis, and computational complexity
focuses on minimizing along the ï¬rst axis. Little work has been done on
the simultaneous minimization of the two.
Physics (Thermodynamics). Statistical mechanics is the birthplace of
entropy and the second law of thermodynamics. Entropy always increases.
Among other things, the second law allows one to dismiss any claims to
perpetual motion machines. We discuss the second law brieï¬y in Chapter 4.
Mathematics (Probability Theory and Statistics). The fundamental
quantities of information theoryâentropy, relative entropy, and mutual
informationâare deï¬ned as functionals of probability distributions. In
turn, they characterize the behavior of long sequences of random variables
and allow us to estimate the probabilities of rare events (large deviation
theory) and to ï¬nd the best error exponent in hypothesis tests.
Philosophy of Science (Occamâs Razor). William of Occam said
âCauses shall not be multiplied beyond necessity,â or to paraphrase it,
âThe simplest explanation is best.â Solomonoff and Chaitin argued persuasively that one gets a universally good prediction procedure if one takes
a weighted combination of all programs that explain the data and observes
what they print next. Moreover, this inference will work in many problems
not handled by statistics. For example, this procedure will eventually predict the subsequent digits of Ï . When this procedure is applied to coin ï¬ips
that come up heads with probability 0.7, this too will be inferred. When
applied to the stock market, the procedure should essentially ï¬nd all the
âlawsâ of the stock market and extrapolate them optimally. In principle,
such a procedure would have found Newtonâs laws of physics. Of course,
such inference is highly impractical, because weeding out all computer
programs that fail to generate existing data will take impossibly long. We
would predict what happens tomorrow a hundred years from now.
Economics (Investment). Repeated investment in a stationary stock
market results in an exponential growth of wealth. The growth rate of
the wealth is a dual of the entropy rate of the stock market. The parallels between the theory of optimal investment in the stock market and
information theory are striking. We develop the theory of investment to
explore this duality.
Computation vs. Communication. As we build larger computers
out of smaller components, we encounter both a computation limit and
a communication limit. Computation is communication limited and communication is computation limited. These become intertwined, and thus

1.1

PREVIEW OF THE BOOK

5

all of the developments in communication theory via information theory
should have a direct impact on the theory of computation.
1.1

PREVIEW OF THE BOOK

The initial questions treated by information theory lay in the areas of
data compression and transmission. The answers are quantities such as
entropy and mutual information, which are functions of the probability
distributions that underlie the process of communication. A few deï¬nitions
will aid the initial discussion. We repeat these deï¬nitions in Chapter 2.
The entropy of a random variable X with a probability mass function
p(x) is deï¬ned by

H (X) = â
p(x) log2 p(x).
(1.1)
x

We use logarithms to base 2. The entropy will then be measured in bits.
The entropy is a measure of the average uncertainty in the random variable. It is the number of bits on average required to describe the random
variable.
Example 1.1.1 Consider a random variable that has a uniform distribution over 32 outcomes. To identify an outcome, we need a label that takes
on 32 different values. Thus, 5-bit strings sufï¬ce as labels.
The entropy of this random variable is
H (X) = â

32

i=1

p(i) log p(i) = â

32

1
1
log
= log 32 = 5 bits,
32
32
i=1

(1.2)
which agrees with the number of bits needed to describe X. In this case,
all the outcomes have representations of the same length.
Now consider an example with nonuniform distribution.
Example 1.1.2 Suppose that we have a horse race with eight horses
taking
Assume that the probabilities of winning for the eight horses
 1 part.
1 1 1 1 1 1 1
are 2 , 4 , 8 , 16 , 64 , 64 , 64 , 64 . We can calculate the entropy of the horse
race as
1
1 1
1 1
1
1
1
1
1
H (X) = â log â log â log â
log
â 4 log
2
2 4
4 8
8 16
16
64
64
= 2 bits.
(1.3)

6

INTRODUCTION AND PREVIEW

Suppose that we wish to send a message indicating which horse won
the race. One alternative is to send the index of the winning horse. This
description requires 3 bits for any of the horses. But the win probabilities
are not uniform. It therefore makes sense to use shorter descriptions for the
more probable horses and longer descriptions for the less probable ones,
so that we achieve a lower average description length. For example, we
could use the following set of bit strings to represent the eight horses: 0,
10, 110, 1110, 111100, 111101, 111110, 111111. The average description
length in this case is 2 bits, as opposed to 3 bits for the uniform code.
Notice that the average description length in this case is equal to the
entropy. In Chapter 5 we show that the entropy of a random variable is
a lower bound on the average number of bits required to represent the
random variable and also on the average number of questions needed to
identify the variable in a game of â20 questions.â We also show how to
construct representations that have an average length within 1 bit of the
entropy.
The concept of entropy in information theory is related to the concept of
entropy in statistical mechanics. If we draw a sequence of n independent
and identically distributed (i.i.d.) random variables, we will show that the
probability of a âtypicalâ sequence is about 2ânH (X) and that there are
about 2nH (X) such typical sequences. This property [known as the asymptotic equipartition property (AEP)] is the basis of many of the proofs in
information theory. We later present other problems for which entropy
arises as a natural answer (e.g., the number of fair coin ï¬ips needed to
generate a random variable).
The notion of descriptive complexity of a random variable can be
extended to deï¬ne the descriptive complexity of a single string. The Kolmogorov complexity of a binary string is deï¬ned as the length of the
shortest computer program that prints out the string. It will turn out that
if the string is indeed random, the Kolmogorov complexity is close to
the entropy. Kolmogorov complexity is a natural framework in which
to consider problems of statistical inference and modeling and leads to
a clearer understanding of Occamâs Razor: âThe simplest explanation is
best.â We describe some simple properties of Kolmogorov complexity in
Chapter 1.
Entropy is the uncertainty of a single random variable. We can deï¬ne
conditional entropy H (X|Y ), which is the entropy of a random variable
conditional on the knowledge of another random variable. The reduction
in uncertainty due to another random variable is called the mutual information. For two random variables X and Y this reduction is the mutual

1.1

PREVIEW OF THE BOOK

7

information
I (X; Y ) = H (X) â H (X|Y ) =



p(x, y) log

x,y

p(x, y)
.
p(x)p(y)

(1.4)

The mutual information I (X; Y ) is a measure of the dependence between
the two random variables. It is symmetric in X and Y and always nonnegative and is equal to zero if and only if X and Y are independent.
A communication channel is a system in which the output depends
probabilistically on its input. It is characterized by a probability transition
matrix p(y|x) that determines the conditional distribution of the output
given the input. For a communication channel with input X and output
Y , we can deï¬ne the capacity C by
C = max I (X; Y ).

(1.5)

p(x)

Later we show that the capacity is the maximum rate at which we can send
information over the channel and recover the information at the output
with a vanishingly low probability of error. We illustrate this with a few
examples.
Example 1.1.3 (Noiseless binary channel ) For this channel, the binary
input is reproduced exactly at the output. This channel is illustrated in
Figure 1.3. Here, any transmitted bit is received without error. Hence,
in each transmission, we can send 1 bit reliably to the receiver, and the
capacity is 1 bit. We can also calculate the information capacity C =
max I (X; Y ) = 1 bit.
Example 1.1.4 (Noisy four-symbol channel ) Consider the channel
shown in Figure 1.4. In this channel, each input letter is received either as
the same letter with probability 12 or as the next letter with probability 12 .
If we use all four input symbols, inspection of the output would not reveal
with certainty which input symbol was sent. If, on the other hand, we use
0

0

1

1

FIGURE 1.3. Noiseless binary channel. C = 1 bit.

8

INTRODUCTION AND PREVIEW

1

1

2

2

3

3

4

4

FIGURE 1.4. Noisy channel.

only two of the inputs (1 and 3, say), we can tell immediately from the
output which input symbol was sent. This channel then acts like the noiseless channel of Example 1.1.3, and we can send 1 bit per transmission
over this channel with no errors. We can calculate the channel capacity
C = max I (X; Y ) in this case, and it is equal to 1 bit per transmission,
in agreement with the analysis above.
In general, communication channels do not have the simple structure of
this example, so we cannot always identify a subset of the inputs to send
information without error. But if we consider a sequence of transmissions,
all channels look like this example and we can then identify a subset of the
input sequences (the codewords) that can be used to transmit information
over the channel in such a way that the sets of possible output sequences
associated with each of the codewords are approximately disjoint. We can
then look at the output sequence and identify the input sequence with a
vanishingly low probability of error.
Example 1.1.5 (Binary symmetric channel ) This is the basic example
of a noisy communication system. The channel is illustrated in Figure 1.5.

0

1âp

0

p
p

1

1
1âp

FIGURE 1.5. Binary symmetric channel.

1.1

PREVIEW OF THE BOOK

9

The channel has a binary input, and its output is equal to the input with
probability 1 â p. With probability p, on the other hand, a 0 is received
as a 1, and vice versa. In this case, the capacity of the channel can be calculated to be C = 1 + p log p + (1 â p) log(1 â p) bits per transmission.
However, it is no longer obvious how one can achieve this capacity. If we
use the channel many times, however, the channel begins to look like the
noisy four-symbol channel of Example 1.1.4, and we can send information at a rate C bits per transmission with an arbitrarily low probability
of error.
The ultimate limit on the rate of communication of information over
a channel is given by the channel capacity. The channel coding theorem
shows that this limit can be achieved by using codes with a long block
length. In practical communication systems, there are limitations on the
complexity of the codes that we can use, and therefore we may not be
able to achieve capacity.
Mutual information turns out to be a special case of a more general
quantity called relative entropy D(p||q), which is a measure of the âdistanceâ between two probability mass functions p and q. It is deï¬ned
as

p(x)
D(p||q) =
.
(1.6)
p(x) log
q(x)
x
Although relative entropy is not a true metric, it has some of the properties
of a metric. In particular, it is always nonnegative and is zero if and only
if p = q. Relative entropy arises as the exponent in the probability of
error in a hypothesis test between distributions p and q. Relative entropy
can be used to deï¬ne a geometry for probability distributions that allows
us to interpret many of the results of large deviation theory.
There are a number of parallels between information theory and the
theory of investment in a stock market. A stock market is deï¬ned by a
random vector X whose elements are nonnegative numbers equal to the
ratio of the price of a stock at the end of a day to the price at the beginning
of the day. For a stock market with distribution F (x), we can deï¬ne the
doubling rate W as

log bt x dF (x).
(1.7)
W =
max

b:bi â¥0,

bi =1

The doubling rate is the maximum asymptotic exponent in the growth
of wealth. The doubling rate has a number of properties that parallel the
properties of entropy. We explore some of these properties in Chapter 16.

10

INTRODUCTION AND PREVIEW

The quantities H, I, C, D, K, W arise naturally in the following areas:
â¢

â¢

â¢

Data compression. The entropy H of a random variable is a lower
bound on the average length of the shortest description of the random
variable. We can construct descriptions with average length within 1
bit of the entropy. If we relax the constraint of recovering the source
perfectly, we can then ask what communication rates are required to
describe the source up to distortion D? And what channel capacities
are sufï¬cient to enable the transmission of this source over the channel and its reconstruction with distortion less than or equal to D?
This is the subject of rate distortion theory.
When we try to formalize the notion of the shortest description
for nonrandom objects, we are led to the deï¬nition of Kolmogorov
complexity K. Later, we show that Kolmogorov complexity is universal and satisï¬es many of the intuitive requirements for the theory
of shortest descriptions.
Data transmission. We consider the problem of transmitting information so that the receiver can decode the message with a small probability of error. Essentially, we wish to ï¬nd codewords (sequences
of input symbols to a channel) that are mutually far apart in the
sense that their noisy versions (available at the output of the channel)
are distinguishable. This is equivalent to sphere packing in highdimensional space. For any set of codewords it is possible to calculate
the probability that the receiver will make an error (i.e., make an
incorrect decision as to which codeword was sent). However, in most
cases, this calculation is tedious.
Using a randomly generated code, Shannon showed that one can
send information at any rate below the capacity C of the channel
with an arbitrarily low probability of error. The idea of a randomly
generated code is very unusual. It provides the basis for a simple
analysis of a very difï¬cult problem. One of the key ideas in the proof
is the concept of typical sequences. The capacity C is the logarithm
of the number of distinguishable input signals.
Network information theory. Each of the topics mentioned previously
involves a single source or a single channel. What if one wishes to compress each of many sources and then put the compressed descriptions
together into a joint reconstruction of the sources? This problem is
solved by the SlepianâWolf theorem. Or what if one has many senders
sending information independently to a common receiver? What is the
channel capacity of this channel? This is the multiple-access channel
solved by Liao and Ahlswede. Or what if one has one sender and many

1.1

â¢

â¢

â¢

â¢

â¢

â¢

â¢

PREVIEW OF THE BOOK

11

receivers and wishes to communicate (perhaps different) information
simultaneously to each of the receivers? This is the broadcast channel.
Finally, what if one has an arbitrary number of senders and receivers in
an environment of interference and noise. What is the capacity region
of achievable rates from the various senders to the receivers? This is
the general network information theory problem. All of the preceding
problems fall into the general area of multiple-user or network information theory. Although hopes for a comprehensive theory for networks
may be beyond current research techniques, there is still some hope that
all the answers involve only elaborate forms of mutual information and
relative entropy.
Ergodic theory. The asymptotic equipartition theorem states that most
sample n-sequences of an ergodic process have probability about 2ânH
and that there are about 2nH such typical sequences.
Hypothesis testing. The relative entropy D arises as the exponent in
the probability of error in a hypothesis test between two distributions.
It is a natural measure of distance between distributions.
Statistical mechanics. The entropy H arises in statistical mechanics
as a measure of uncertainty or disorganization in a physical system.
Roughly speaking, the entropy is the logarithm of the number of
ways in which the physical system can be conï¬gured. The second law
of thermodynamics says that the entropy of a closed system cannot
decrease. Later we provide some interpretations of the second law.
Quantum mechanics. Here, von Neumann entropy S = tr(Ï ln Ï) =

i Î»i log
Î»i plays the role of classical ShannonâBoltzmann entropy
H = â i pi log pi . Quantum mechanical versions of data compression and channel capacity can then be found.
Inference. We can use the notion of Kolmogorov complexity K to
ï¬nd the shortest description of the data and use that as a model to
predict what comes next. A model that maximizes the uncertainty or
entropy yields the maximum entropy approach to inference.
Gambling and investment. The optimal exponent in the growth rate
of wealth is given by the doubling rate W . For a horse race with
uniform odds, the sum of the doubling rate W and the entropy H is
constant. The increase in the doubling rate due to side information is
equal to the mutual information I between a horse race and the side
information. Similar results hold for investment in the stock market.
Probability theory. The asymptotic equipartition property (AEP)
shows that most sequences are typical in that they have a sample entropy close to H . So attention can be restricted to these
approximately 2nH typical sequences. In large deviation theory, the

12

INTRODUCTION AND PREVIEW

â¢

probability of a set is approximately 2ânD , where D is the relative
entropy distance between the closest element in the set and the true
distribution.
Complexity theory. The Kolmogorov complexity K is a measure of
the descriptive complexity of an object. It is related to, but different
from, computational complexity, which measures the time or space
required for a computation.

Information-theoretic quantities such as entropy and relative entropy
arise again and again as the answers to the fundamental questions in
communication and statistics. Before studying these questions, we shall
study some of the properties of the answers. We begin in Chapter 2 with
the deï¬nitions and basic properties of entropy, relative entropy, and mutual
information.

CHAPTER 2

ENTROPY, RELATIVE ENTROPY,
AND MUTUAL INFORMATION

In this chapter we introduce most of the basic deï¬nitions required for
subsequent development of the theory. It is irresistible to play with their
relationships and interpretations, taking faith in their later utility. After
deï¬ning entropy and mutual information, we establish chain rules, the
nonnegativity of mutual information, the data-processing inequality, and
illustrate these deï¬nitions by examining sufï¬cient statistics and Fanoâs
inequality.
The concept of information is too broad to be captured completely by
a single deï¬nition. However, for any probability distribution, we deï¬ne a
quantity called the entropy, which has many properties that agree with the
intuitive notion of what a measure of information should be. This notion is
extended to deï¬ne mutual information, which is a measure of the amount
of information one random variable contains about another. Entropy then
becomes the self-information of a random variable. Mutual information is
a special case of a more general quantity called relative entropy, which is
a measure of the distance between two probability distributions. All these
quantities are closely related and share a number of simple properties,
some of which we derive in this chapter.
In later chapters we show how these quantities arise as natural answers
to a number of questions in communication, statistics, complexity, and
gambling. That will be the ultimate test of the value of these deï¬nitions.
2.1

ENTROPY

We ï¬rst introduce the concept of entropy, which is a measure of the
uncertainty of a random variable. Let X be a discrete random variable
with alphabet X and probability mass function p(x) = Pr{X = x}, x â X.
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

13

14

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

We denote the probability mass function by p(x) rather than pX (x), for
convenience. Thus, p(x) and p(y) refer to two different random variables
and are in fact different probability mass functions, pX (x) and pY (y),
respectively.
Deï¬nition The entropy H (X) of a discrete random variable X is
deï¬ned by

p(x) log p(x).
(2.1)
H (X) = â
xâX

We also write H (p) for the above quantity. The log is to the base 2
and entropy is expressed in bits. For example, the entropy of a fair coin
toss is 1 bit. We will use the convention that 0 log 0 = 0, which is easily
justiï¬ed by continuity since x log x â 0 as x â 0. Adding terms of zero
probability does not change the entropy.
If the base of the logarithm is b, we denote the entropy as Hb (X). If
the base of the logarithm is e, the entropy is measured in nats. Unless
otherwise speciï¬ed, we will take all logarithms to base 2, and hence all
the entropies will be measured in bits. Note that entropy is a functional
of the distribution of X. It does not depend on the actual values taken by
the random variable X, but only on the probabilities.
We denote expectation by E. Thus, if X â¼ p(x), the expected value of
the random variable g(X) is written

g(x)p(x),
(2.2)
Ep g(X) =
xâX

or more simply as Eg(X) when the probability mass function is understood from the context. We shall take a peculiar interest in the eerily
1
self-referential expectation of g(X) under p(x) when g(X) = log p(X)
.
Remark The entropy of X can also be interpreted as the expected value
1
, where X is drawn according to probability
of the random variable log p(X)
mass function p(x). Thus,
H (X) = Ep log

1
.
p(X)

(2.3)

This deï¬nition of entropy is related to the deï¬nition of entropy in thermodynamics; some of the connections are explored later. It is possible
to derive the deï¬nition of entropy axiomatically by deï¬ning certain properties that the entropy of a random variable must satisfy. This approach
is illustrated in Problem 2.46. We do not use the axiomatic approach to

2.1

ENTROPY

15

justify the deï¬nition of entropy; instead, we show that it arises as the
answer to a number of natural questions, such as âWhat is the average
length of the shortest description of the random variable?â First, we derive
some immediate consequences of the deï¬nition.
Lemma 2.1.1

H (X) â¥ 0.

1
â¥ 0.
Proof: 0 â¤ p(x) â¤ 1 implies that log p(x)

Lemma 2.1.2



Hb (X) = (logb a)Ha (X).

Proof: logb p = logb a loga p.



The second property of entropy enables us to change the base of the
logarithm in the deï¬nition. Entropy can be changed from one base to
another by multiplying by the appropriate factor.
Example 2.1.1

Let

X=

Then

1
0

with probability p,
with probability 1 â p.

def
H (X) = âp log p â (1 â p) log(1 â p) == H (p).

(2.4)

(2.5)

In particular, H (X) = 1 bit when p = 12 . The graph of the function H (p)
is shown in Figure 2.1. The ï¬gure illustrates some of the basic properties
of entropy: It is a concave function of the distribution and equals 0 when
p = 0 or 1. This makes sense, because when p = 0 or 1, the variable
is not random and there is no uncertainty. Similarly, the uncertainty is
maximum when p = 12 , which also corresponds to the maximum value of
the entropy.
Example 2.1.2

Let
ï£±
a
ï£´
ï£´
ï£´
ï£² b
X=
ï£´
c
ï£´
ï£´
ï£³
d

with
with
with
with

probability 12 ,
probability 14 ,
probability 18 ,
probability 18 .

(2.6)

The entropy of X is
1
1 1
1 1
1 1
1
7
H (X) = â log â log â log â log = bits.
2
2 4
4 8
8 8
8
4

(2.7)

16

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

1
0.9
0.8
0.7

H(p)

0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5
p

0.6

0.7

0.8

0.9

1

FIGURE 2.1. H (p) vs. p.

Suppose that we wish to determine the value of X with the minimum
number of binary questions. An efï¬cient ï¬rst question is âIs X = a?â
This splits the probability in half. If the answer to the ï¬rst question is
no, the second question can be âIs X = b?â The third question can be
âIs X = c?â The resulting expected number of binary questions required
is 1.75. This turns out to be the minimum expected number of binary
questions required to determine the value of X. In Chapter 5 we show that
the minimum expected number of binary questions required to determine
X lies between H (X) and H (X) + 1.
2.2

JOINT ENTROPY AND CONDITIONAL ENTROPY

We deï¬ned the entropy of a single random variable in Section 2.1. We
now extend the deï¬nition to a pair of random variables. There is nothing
really new in this deï¬nition because (X, Y ) can be considered to be a
single vector-valued random variable.
Deï¬nition The joint entropy H (X, Y ) of a pair of discrete random
variables (X, Y ) with a joint distribution p(x, y) is deï¬ned as

H (X, Y ) = â
p(x, y) log p(x, y),
(2.8)
xâX yâY

2.2

17

JOINT ENTROPY AND CONDITIONAL ENTROPY

which can also be expressed as
H (X, Y ) = âE log p(X, Y ).

(2.9)

We also deï¬ne the conditional entropy of a random variable given
another as the expected value of the entropies of the conditional distributions, averaged over the conditioning random variable.
Deï¬nition If (X, Y ) â¼ p(x, y), the conditional entropy H (Y |X) is
deï¬ned as

p(x)H (Y |X = x)
(2.10)
H (Y |X) =
xâX

=â



p(x)

xâX

=â





p(y|x) log p(y|x)

(2.11)

yâY

p(x, y) log p(y|x)

(2.12)

xâX yâY

= âE log p(Y |X).

(2.13)

The naturalness of the deï¬nition of joint entropy and conditional entropy
is exhibited by the fact that the entropy of a pair of random variables is
the entropy of one plus the conditional entropy of the other. This is proved
in the following theorem.
Theorem 2.2.1

(Chain rule)
H (X, Y ) = H (X) + H (Y |X).

Proof
H (X, Y ) = â



(2.14)

p(x, y) log p(x, y)

(2.15)

p(x, y) log p(x)p(y|x)

(2.16)

xâX yâY

=â



xâX yâY

=â



p(x, y) log p(x) â

xâX yâY

=â



p(x) log p(x) â

xâX

= H (X) + H (Y |X).



p(x, y) log p(y|x)

xâX yâY



p(x, y) log p(y|x)

(2.17)
(2.18)

xâX yâY

(2.19)

18

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Equivalently, we can write
log p(X, Y ) = log p(X) + log p(Y |X)

(2.20)

and take the expectation of both sides of the equation to obtain the

theorem.
Corollary

H (X, Y |Z) = H (X|Z) + H (Y |X, Z).

Proof: The proof follows along the same lines as the theorem.
Example 2.2.1

(2.21)


Let (X, Y ) have the following joint distribution:

The marginal distribution of X is ( 12 , 14 , 18 , 18 ) and the marginal distribution
of Y is ( 14 , 14 , 14 , 14 ), and hence H (X) = 74 bits and H (Y ) = 2 bits. Also,
H (X|Y ) =

4


p(Y = i)H (X|Y = i)

i=1

(2.22)




1 1 1 1
1 1 1 1
1
, , ,
+ H
, , ,
2 4 8 8
4
4 2 8 8


1 1 1 1
1
1
+ H
, , ,
+ H (1, 0, 0, 0)
4
4 4 4 4
4

1
= H
4



1 7 1 7 1
1
Ã + Ã + Ã2+ Ã0
4 4 4 4 4
4
11
=
bits.
8
=

Similarly, H (Y |X) =

13
8

bits and H (X, Y ) =

27
8

(2.23)
(2.24)
(2.25)

bits.

Remark Note that H (Y |X) = H (X|Y ). However, H (X) â H (X|Y ) =
H (Y )â H (Y |X), a property that we exploit later.

2.3

2.3

RELATIVE ENTROPY AND MUTUAL INFORMATION

19

RELATIVE ENTROPY AND MUTUAL INFORMATION

The entropy of a random variable is a measure of the uncertainty of the
random variable; it is a measure of the amount of information required on
the average to describe the random variable. In this section we introduce
two related concepts: relative entropy and mutual information.
The relative entropy is a measure of the distance between two distributions. In statistics, it arises as an expected logarithm of the likelihood ratio.
The relative entropy D(p||q) is a measure of the inefï¬ciency of assuming
that the distribution is q when the true distribution is p. For example, if
we knew the true distribution p of the random variable, we could construct a code with average description length H (p). If, instead, we used
the code for a distribution q, we would need H (p) + D(p||q) bits on the
average to describe the random variable.
Deï¬nition The relative entropy or KullbackâLeibler distance between
two probability mass functions p(x) and q(x) is deï¬ned as
D(p||q) =



p(x) log

xâX

p(x)
q(x)

(2.26)

p(X)
.
(2.27)
q(X)
In the above deï¬nition, we use the convention that 0 log 00 = 0 and the
convention (based on continuity arguments) that 0 log q0 = 0 and p log p0 =
â. Thus, if there is any symbol x â X such that p(x) > 0 and q(x) = 0,
then D(p||q) = â.
We will soon show that relative entropy is always nonnegative and is
zero if and only if p = q. However, it is not a true distance between
distributions since it is not symmetric and does not satisfy the triangle
inequality. Nonetheless, it is often useful to think of relative entropy as a
âdistanceâ between distributions.
We now introduce mutual information, which is a measure of the
amount of information that one random variable contains about another
random variable. It is the reduction in the uncertainty of one random
variable due to the knowledge of the other.
= Ep log

Deï¬nition Consider two random variables X and Y with a joint probability mass function p(x, y) and marginal probability mass functions p(x)
and p(y). The mutual information I (X; Y ) is the relative entropy between

20

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

the joint distribution and the product distribution p(x)p(y):
I (X; Y ) =



p(x, y) log

xâX yâY

p(x, y)
p(x)p(y)

(2.28)

= D(p(x, y)||p(x)p(y))

(2.29)

p(X, Y )
.
p(X)p(Y )

(2.30)

= Ep(x,y) log

In Chapter 8 we generalize this deï¬nition to continuous random variables, and in (8.54) to general random variables that could be a mixture
of discrete and continuous random variables.
Example 2.3.1 Let X = {0, 1} and consider two distributions p and q
on X. Let p(0) = 1 â r, p(1) = r, and let q(0) = 1 â s, q(1) = s. Then
D(p||q) = (1 â r) log

r
1âr
+ r log
1âs
s

(2.31)

D(q||p) = (1 â s) log

1âs
s
+ s log .
1âr
r

(2.32)

and

If r = s, then D(p||q) = D(q||p) = 0. If r = 12 , s = 14 , we can calculate
1
D(p||q) = log
2

1
2
3
4

1
+ log
2

1
2
1
4

=1â

3
4
1
2

+

1
log
4

1
4
1
2

=

1
log 3 = 0.2075 bit,
2

(2.33)

3
log 3 â 1 = 0.1887 bit.
4

(2.34)

whereas
D(q||p) =

3
log
4

Note that D(p||q) = D(q||p) in general.
2.4 RELATIONSHIP BETWEEN ENTROPY AND MUTUAL
INFORMATION
We can rewrite the deï¬nition of mutual information I (X; Y ) as
I (X; Y ) =


x,y

p(x, y) log

p(x, y)
p(x)p(y)

(2.35)

2.4

RELATIONSHIP BETWEEN ENTROPY AND MUTUAL INFORMATION

=



p(x, y) log

x,y

=â



p(x|y)
p(x)

=â



(2.36)

p(x, y) log p(x) +

x,y

	



p(x, y) log p(x|y)

x,y

p(x) log p(x) â â

x

21



(2.37)



p(x, y) log p(x|y) (2.38)

x,y

= H (X) â H (X|Y ).

(2.39)

Thus, the mutual information I (X; Y ) is the reduction in the uncertainty
of X due to the knowledge of Y .
By symmetry, it also follows that
I (X; Y ) = H (Y ) â H (Y |X).

(2.40)

Thus, X says as much about Y as Y says about X.
Since H (X, Y ) = H (X) + H (Y |X), as shown in Section 2.2, we have
I (X; Y ) = H (X) + H (Y ) â H (X, Y ).

(2.41)

Finally, we note that
I (X; X) = H (X) â H (X|X) = H (X).

(2.42)

Thus, the mutual information of a random variable with itself is the
entropy of the random variable. This is the reason that entropy is sometimes referred to as self-information.
Collecting these results, we have the following theorem.
Theorem 2.4.1

(Mutual information and entropy)
I (X; Y ) = H (X) â H (X|Y )

(2.43)

I (X; Y ) = H (Y ) â H (Y |X)

(2.44)

I (X; Y ) = H (X) + H (Y ) â H (X, Y )

(2.45)

I (X; Y ) = I (Y ; X)

(2.46)

I (X; X) = H (X).

(2.47)

22

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION
H(X,Y )

H(X |Y )

I(X;Y )

H(Y |X )

H(X )

H(Y )

FIGURE 2.2. Relationship between entropy and mutual information.

The relationship between H (X), H (Y ), H (X, Y ), H (X|Y ), H (Y |X),
and I (X; Y ) is expressed in a Venn diagram (Figure 2.2). Notice that
the mutual information I (X; Y ) corresponds to the intersection of the
information in X with the information in Y .
Example 2.4.1 For the joint distribution of Example 2.2.1, it is easy to
calculate the mutual information I (X; Y ) = H (X) â H (X|Y ) = H (Y ) â
H (Y |X) = 0.375 bit.
2.5 CHAIN RULES FOR ENTROPY, RELATIVE ENTROPY,
AND MUTUAL INFORMATION
We now show that the entropy of a collection of random variables is the
sum of the conditional entropies.
Theorem 2.5.1 (Chain rule for entropy)
according to p(x1 , x2 , . . . , xn ). Then
H (X1 , X2 , . . . , Xn ) =

n


Let X1 , X2 , . . . , Xn be drawn

H (Xi |Xiâ1 , . . . , X1 ).

(2.48)

i=1

Proof: By repeated application of the two-variable expansion rule for
entropies, we have
H (X1 , X2 ) = H (X1 ) + H (X2 |X1 ),

(2.49)

H (X1 , X2 , X3 ) = H (X1 ) + H (X2 , X3 |X1 )

(2.50)

2.5

CHAIN RULES FOR ENTROPY, RELATIVE ENTROPY, MUTUAL INFORMATION

= H (X1 ) + H (X2 |X1 ) + H (X3 |X2 , X1 ),

23

(2.51)

..
.
H (X1 , X2 , . . . , Xn ) = H (X1 ) + H (X2 |X1 ) + Â· Â· Â· + H (Xn |Xnâ1 , . . . , X1 )
=

n


(2.52)
H (Xi |Xiâ1 , . . . , X1 ). 

i=1

Alternative Proof: We write p(x1 , . . . , xn ) =
and evaluate

(2.53)

n

i=1 p(xi |xiâ1 , . . . , x1 )

H (X1 , X2 , . . . , Xn )

=â
p(x1 , x2 , . . . , xn ) log p(x1 , x2 , . . . , xn )

(2.54)

x1 ,x2 ,...,xn



=â

(2.55)

p(x1 , x2 , . . . , xn ) log p(xi |xiâ1 , . . . , x1 )

(2.56)

p(x1 , x2 , . . . , xn ) log p(xi |xiâ1 , . . . , x1 )

(2.57)

p(x1 , x2 , . . . , xi ) log p(xi |xiâ1 , . . . , x1 )

(2.58)

x1 ,x2 ,...,xn



=â
=â

n


p(xi |xiâ1 , . . . , x1 )

p(x1 , x2 , . . . , xn ) log

i=1
n


x1 ,x2 ,...,xn i=1
n


i=1 x1 ,x2 ,...,xn

=â

n




i=1 x1 ,x2 ,...,xi

=

n


H (Xi |Xiâ1 , . . . , X1 ). 

(2.59)

i=1

We now deï¬ne the conditional mutual information as the reduction in
the uncertainty of X due to knowledge of Y when Z is given.
Deï¬nition The conditional mutual information of random variables X
and Y given Z is deï¬ned by
I (X; Y |Z) = H (X|Z) â H (X|Y, Z)
p(X, Y |Z)
.
= Ep(x,y,z) log
p(X|Z)p(Y |Z)
Mutual information also satisï¬es a chain rule.

(2.60)
(2.61)

24

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Theorem 2.5.2

(Chain rule for information)

I (X1 , X2 , . . . , Xn ; Y ) =

n


I (Xi ; Y |Xiâ1 , Xiâ2 , . . . , X1 ).

(2.62)

i=1

Proof
I (X1 , X2 , . . . , Xn ; Y )
= H (X1 , X2 , . . . , Xn ) â H (X1 , X2 , . . . , Xn |Y )
=

n


H (Xi |Xiâ1 , . . . , X1 ) â

i=1

=

n


n


(2.63)

H (Xi |Xiâ1 , . . . , X1 , Y )

i=1

I (Xi ; Y |X1 , X2 , . . . , Xiâ1 ). 

(2.64)

i=1

We deï¬ne a conditional version of the relative entropy.
Deï¬nition For joint probability mass functions p(x, y) and q(x, y), the
conditional relative entropy D(p(y|x)||q(y|x)) is the average of the relative entropies between the conditional probability mass functions p(y|x)
and q(y|x) averaged over the probability mass function p(x). More precisely,
D(p(y|x)||q(y|x)) =


x

p(x)



p(y|x) log

y

= Ep(x,y) log

p(y|x)
q(y|x)

p(Y |X)
.
q(Y |X)

(2.65)
(2.66)

The notation for conditional relative entropy is not explicit since it omits
mention of the distribution p(x) of the conditioning random variable.
However, it is normally understood from the context.
The relative entropy between two joint distributions on a pair of random variables can be expanded as the sum of a relative entropy and a
conditional relative entropy. The chain rule for relative entropy is used in
Section 4.4 to prove a version of the second law of thermodynamics.
Theorem 2.5.3

(Chain rule for relative entropy)

D(p(x, y)||q(x, y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x)).

(2.67)

2.6 JENSENâS INEQUALITY AND ITS CONSEQUENCES

25

Proof
D(p(x, y)||q(x, y))

p(x, y)
p(x, y) log
=
q(x, y)
x
y
=


x

=

x

p(x, y) log

p(x)p(y|x)
q(x)q(y|x)

(2.69)

p(x, y) log

p(x)  
p(y|x)
+
p(x, y) log
q(x)
q(y|x)
x
y

(2.70)

y


y

= D(p(x)||q(x)) + D(p(y|x)||q(y|x)).
2.6

(2.68)



(2.71)

JENSENâS INEQUALITY AND ITS CONSEQUENCES

In this section we prove some simple properties of the quantities deï¬ned
earlier. We begin with the properties of convex functions.
Deï¬nition A function f (x) is said to be convex over an interval (a, b)
if for every x1 , x2 â (a, b) and 0 â¤ Î» â¤ 1,
f (Î»x1 + (1 â Î»)x2 ) â¤ Î»f (x1 ) + (1 â Î»)f (x2 ).

(2.72)

A function f is said to be strictly convex if equality holds only if Î» = 0
or Î» = 1.
Deï¬nition A function f is concave if âf is convex. A function is
convex if it always lies below any chord. A function is concave if it
always lies above any chord.
Examples of convex functions include x 2 , |x|, ex , x log x (for
â xâ¥
0), and so on. Examples of concave functions include log x and x for
x â¥ 0. Figure 2.3 shows some examples of convex and concave functions.
Note that linear functions ax + b are both convex and concave. Convexity
underlies many of the basic properties of information-theoretic quantities
such as entropy and mutual information. Before we prove some of these
properties, we derive some simple results for convex functions.
Theorem 2.6.1 If the function f has a second derivative that is nonnegative (positive) over an interval, the function is convex (strictly convex)
over that interval.

26

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

(a)

(b)

FIGURE 2.3. Examples of (a) convex and (b) concave functions.

Proof: We use the Taylor series expansion of the function around x0 :
f (x) = f (x0 ) + f 	 (x0 )(x â x0 ) +

f 		 (x â )
(x â x0 )2 ,
2

(2.73)

where x â lies between x0 and x. By hypothesis, f 		 (x â ) â¥ 0, and thus
the last term is nonnegative for all x.
We let x0 = Î»x1 + (1 â Î»)x2 and take x = x1 , to obtain
f (x1 ) â¥ f (x0 ) + f 	 (x0 )((1 â Î»)(x1 â x2 )).

(2.74)

Similarly, taking x = x2 , we obtain
f (x2 ) â¥ f (x0 ) + f 	 (x0 )(Î»(x2 â x1 )).

(2.75)

Multiplying (2.74) by Î» and (2.75) by 1 â Î» and adding, we obtain (2.72).
The proof for strict convexity proceeds along the same lines.

Theorem 2.6.1 allows us immediately to verify the strict convexity
â of
x 2 , ex , and x log x for x â¥ 0, and the strict concavity of log x and x for
x â¥ 0.

Let E denoteexpectation. Thus, EX = xâX p(x)x in the discrete
case and EX = xf (x) dx in the continuous case.

2.6 JENSENâS INEQUALITY AND ITS CONSEQUENCES

27

The next inequality is one of the most widely used in mathematics and
one that underlies many of the basic results in information theory.
Theorem 2.6.2 (Jensenâs inequality)
X is a random variable,

If f is a convex function and

Ef (X) â¥ f (EX).

(2.76)

Moreover, if f is strictly convex, the equality in (2.76) implies that
X = EX with probability 1 (i.e., X is a constant).
Proof: We prove this for discrete distributions by induction on the number of mass points. The proof of conditions for equality when f is strictly
convex is left to the reader.
For a two-mass-point distribution, the inequality becomes
p1 f (x1 ) + p2 f (x2 ) â¥ f (p1 x1 + p2 x2 ),

(2.77)

which follows directly from the deï¬nition of convex functions. Suppose
that the theorem is true for distributions with k â 1 mass points. Then
writing pi	 = pi /(1 â pk ) for i = 1, 2, . . . , k â 1, we have
k


pi f (xi ) = pk f (xk ) + (1 â pk )

i=1

kâ1


pi	 f (xi )

i=1

â¥ pk f (xk ) + (1 â pk )f

=f



pi	 xi

i=1

	
â¥f

	 kâ1


pk xk + (1 â pk )
	 k




pi xi ,

kâ1


(2.78)

pi	 xi

(2.79)



(2.80)

i=1

(2.81)

i=1

where the ï¬rst inequality follows from the induction hypothesis and the
second follows from the deï¬nition of convexity.
The proof can be extended to continuous distributions by continuity

arguments.
We now use these results to prove some of the properties of entropy and
relative entropy. The following theorem is of fundamental importance.

28

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Theorem 2.6.3 (Information inequality)
two probability mass functions. Then

Let p(x), q(x), x â X, be

D(p||q) â¥ 0

(2.82)

with equality if and only if p(x) = q(x) for all x.
Proof: Let A = {x : p(x) > 0} be the support set of p(x). Then
âD(p||q) = â



p(x) log

xâA

=



p(x) log

xâA

â¤ log



p(x)

xâA

= log



p(x)
q(x)

(2.83)

q(x)
p(x)

(2.84)

q(x)
p(x)

(2.85)

q(x)

(2.86)

q(x)

(2.87)

xâA

â¤ log



xâX

= log 1

(2.88)

= 0,

(2.89)

where (2.85) follows from Jensenâs inequality. Since log t is a strictly
concave function of t, we have equality in (2.85) if and only
if q(x)/p(x)
is
constant everywhere [i.e., q(x) = cp(x) for all x].
Thus, xâA q(x)

 =
c xâA p(x) = c. We have equality in (2.87) only if xâA q(x) = xâX
q(x) = 1, which implies that c = 1. Hence, we have D(p||q) = 0 if and

only if p(x) = q(x) for all x.
Corollary (Nonnegativity of mutual information)
variables, X, Y ,
I (X; Y ) â¥ 0,

For any two random
(2.90)

with equality if and only if X and Y are independent.
Proof: I (X; Y ) = D(p(x, y)||p(x)p(y)) â¥ 0, with equality if and only

if p(x, y) = p(x)p(y) (i.e., X and Y are independent).

2.6 JENSENâS INEQUALITY AND ITS CONSEQUENCES

Corollary

D(p(y|x)||q(y|x)) â¥ 0,

29

(2.91)

with equality if and only if p(y|x) = q(y|x) for all y and x such that
p(x) > 0.
Corollary

I (X; Y |Z) â¥ 0,

(2.92)

with equality if and only if X and Y are conditionally independent given Z.
We now show that the uniform distribution over the range X is the
maximum entropy distribution over this range. It follows that any random
variable with this range has an entropy no greater than log |X|.
Theorem 2.6.4 H (X) â¤ log |X|, where |X| denotes the number of elements in the range of X, with equality if and only X has a uniform distribution over X.
Proof: Let u(x) = |X1 | be the uniform probability mass function over X,
and let p(x) be the probability mass function for X. Then
D(p  u) =



p(x) log

p(x)
= log |X| â H (X).
u(x)

(2.93)

Hence by the nonnegativity of relative entropy,
0 â¤ D(p  u) = log |X| â H (X). 
Theorem 2.6.5

(2.94)

(Conditioning reduces entropy)(Information canât hurt)
H (X|Y ) â¤ H (X)

(2.95)

with equality if and only if X and Y are independent.
Proof: 0 â¤ I (X; Y ) = H (X) â H (X|Y ).



Intuitively, the theorem says that knowing another random variable Y
can only reduce the uncertainty in X. Note that this is true only on the
average. Speciï¬cally, H (X|Y = y) may be greater

 than or less than or
equal to H (X), but on the average H (X|Y ) = y p(y)H (X|Y = y) â¤
H (X). For example, in a court case, speciï¬c new evidence might increase
uncertainty, but on the average evidence decreases uncertainty.

30

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Example 2.6.1

Let (X, Y ) have the following joint distribution:

Then H (X) = H ( 18 , 78 ) = 0.544 bit, H (X|Y = 1) = 0 bits, and
H (X|Y = 2) = 1 bit. We calculate H (X|Y ) = 34 H (X|Y = 1) + 14
H (X|Y = 2) = 0.25 bit. Thus, the uncertainty in X is increased if Y = 2
is observed and decreased if Y = 1 is observed, but uncertainty decreases
on the average.
Theorem 2.6.6 (Independence bound on entropy)
X1 , X2 , . . . , Xn be drawn according to p(x1 , x2 , . . . , xn ). Then
H (X1 , X2 , . . . , Xn ) â¤

n


H (Xi )

Let

(2.96)

i=1

with equality if and only if the Xi are independent.
Proof: By the chain rule for entropies,
H (X1 , X2 , . . . , Xn ) =

n


H (Xi |Xiâ1 , . . . , X1 )

(2.97)

H (Xi ),

(2.98)

i=1

â¤

n

i=1

where the inequality follows directly from Theorem 2.6.5. We have equality if and only if Xi is independent of Xiâ1 , . . . , X1 for all i (i.e., if and
only if the Xi âs are independent).

2.7

LOG SUM INEQUALITY AND ITS APPLICATIONS

We now prove a simple consequence of the concavity of the logarithm,
which will be used to prove some concavity results for the entropy.

2.7

LOG SUM INEQUALITY AND ITS APPLICATIONS

31

Theorem 2.7.1 (Log sum inequality)
For nonnegative numbers,
a1 , a2 , . . . , an and b1 , b2 , . . . , bn ,
	 n 


n
n


ai
ai

ai log â¥
ai log i=1
(2.99)
n
bi
i=1 bi
i=1

i=1

with equality if and only if

ai
bi

= const.

We again use the convention that 0 log 0 = 0, a log a0 = â if a > 0 and
0 log 00 = 0. These follow easily from continuity.
Proof: Assume without loss of generality that ai > 0 and bi > 0. The
function f (t) = t log t is strictly convex, since f 		 (t) = 1t log e > 0 for all
positive t. Hence by Jensenâs inequality, we have


for Î±i â¥ 0,


i

Î±i f (ti ) â¥ f

Î±i = 1. Setting Î±i =




nbi

j =1 bj


Î± i ti
and ti =

(2.100)
ai
bi ,

we obtain

 ai
 ai
 ai
ai

 log â¥

 log

 ,
bi
bj
bj
bj
which is the log sum inequality.

(2.101)


We now use the log sum inequality to prove various convexity results.
We begin by reproving Theorem 2.6.3, which states that D(p||q) â¥ 0 with
equality if and only if p(x) = q(x). By the log sum inequality,


p(x)
q(x)




q(x)
p(x) log
p(x)
â¥

D(p||q) =

p(x) log

= 1 log

1
=0
1

(2.102)
(2.103)
(2.104)

with equality if and only if p(x)
q(x) = c. Since both p and q are probability
mass functions, c = 1, and hence we have D(p||q) = 0 if and only if
p(x) = q(x) for all x.

32

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Theorem 2.7.2 (Convexity of relative entropy)
D(p||q) is convex in
the pair (p, q); that is, if (p1 , q1 ) and (p2 , q2 ) are two pairs of probability
mass functions, then
D(Î»p1 + (1 â Î»)p2 ||Î»q1 + (1 â Î»)q2 ) â¤ Î»D(p1 ||q1 ) + (1 â Î»)D(p2 ||q2 )
(2.105)
for all 0 â¤ Î» â¤ 1.
Proof: We apply the log sum inequality to a term on the left-hand side
of (2.105):
(Î»p1 (x) + (1 â Î»)p2 (x)) log
â¤ Î»p1 (x) log

Î»p1 (x) + (1 â Î»)p2 (x)
Î»q1 (x) + (1 â Î»)q2 (x)

Î»p1 (x)
(1 â Î»)p2 (x)
+ (1 â Î»)p2 (x) log
.
Î»q1 (x)
(1 â Î»)q2 (x)

Summing this over all x, we obtain the desired property.
Theorem 2.7.3
of p.

(Concavity of entropy)

(2.106)


H (p) is a concave function

Proof
H (p) = log |X| â D(p||u),

(2.107)

where u is the uniform distribution on |X| outcomes. The concavity of H
then follows directly from the convexity of D.

Alternative Proof: Let X1 be a random variable with distribution p1 ,
taking on values in a set A. Let X2 be another random variable with
distribution p2 on the same set. Let

1
with probability Î»,
Î¸=
(2.108)
2
with probability 1 â Î».
Let Z = XÎ¸ . Then the distribution of Z is Î»p1 + (1 â Î»)p2 . Now since
conditioning reduces entropy, we have
H (Z) â¥ H (Z|Î¸ ),

(2.109)

or equivalently,
H (Î»p1 + (1 â Î»)p2 ) â¥ Î»H (p1 ) + (1 â Î»)H (p2 ),

(2.110)

which proves the concavity of the entropy as a function of the distribution.


2.7

LOG SUM INEQUALITY AND ITS APPLICATIONS

33

One of the consequences of the concavity of entropy is that mixing two
gases of equal entropy results in a gas with higher entropy.
Theorem 2.7.4 Let (X, Y ) â¼ p(x, y) = p(x)p(y|x). The mutual information I (X; Y ) is a concave function of p(x) for ï¬xed p(y|x) and a convex
function of p(y|x) for ï¬xed p(x).
Proof: To prove the ï¬rst part, we expand the mutual information

p(x)H (Y |X = x). (2.111)
I (X; Y ) = H (Y ) â H (Y |X) = H (Y ) â
x

If p(y|x) is ï¬xed, then p(y) is a linear function of p(x). Hence H (Y ),
which is a concave function of p(y), is a concave function of p(x). The
second term is a linear function of p(x). Hence, the difference is a concave
function of p(x).
To prove the second part, we ï¬x p(x) and consider two different conditional distributions p1 (y|x) and p2 (y|x). The corresponding joint distributions are p1 (x, y) = p(x)p1 (y|x) and p2 (x, y) = p(x)p2 (y|x), and
their respective marginals are p(x), p1 (y) and p(x), p2 (y). Consider a
conditional distribution
pÎ» (y|x) = Î»p1 (y|x) + (1 â Î»)p2 (y|x),

(2.112)

which is a mixture of p1 (y|x) and p2 (y|x) where 0 â¤ Î» â¤ 1. The corresponding joint distribution is also a mixture of the corresponding joint
distributions,
pÎ» (x, y) = Î»p1 (x, y) + (1 â Î»)p2 (x, y),

(2.113)

and the distribution of Y is also a mixture,
pÎ» (y) = Î»p1 (y) + (1 â Î»)p2 (y).

(2.114)

Hence if we let qÎ» (x, y) = p(x)pÎ» (y) be the product of the marginal
distributions, we have
qÎ» (x, y) = Î»q1 (x, y) + (1 â Î»)q2 (x, y).

(2.115)

Since the mutual information is the relative entropy between the joint
distribution and the product of the marginals,
I (X; Y ) = D(pÎ» (x, y)||qÎ» (x, y)),

(2.116)

and relative entropy D(p||q) is a convex function of (p, q), it follows that
the mutual information is a convex function of the conditional distribution.


34

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

2.8

DATA-PROCESSING INEQUALITY

The data-processing inequality can be used to show that no clever manipulation of the data can improve the inferences that can be made from
the data.
Deï¬nition Random variables X, Y, Z are said to form a Markov chain
in that order (denoted by X â Y â Z) if the conditional distribution of
Z depends only on Y and is conditionally independent of X. Speciï¬cally,
X, Y , and Z form a Markov chain X â Y â Z if the joint probability
mass function can be written as
p(x, y, z) = p(x)p(y|x)p(z|y).

(2.117)

Some simple consequences are as follows:
â¢

X â Y â Z if and only if X and Z are conditionally independent
given Y . Markovity implies conditional independence because
p(x, z|y) =

â¢
â¢

p(x, y)p(z|y)
p(x, y, z)
=
= p(x|y)p(z|y). (2.118)
p(y)
p(y)

This is the characterization of Markov chains that can be extended
to deï¬ne Markov ï¬elds, which are n-dimensional random processes
in which the interior and exterior are independent given the values
on the boundary.
X â Y â Z implies that Z â Y â X. Thus, the condition is sometimes written X â Y â Z.
If Z = f (Y ), then X â Y â Z.

We can now prove an important and useful theorem demonstrating that
no processing of Y , deterministic or random, can increase the information
that Y contains about X.
Theorem 2.8.1 (Data-processing inequality)
I (X; Y ) â¥ I (X; Z).

If X â Y â Z, then

Proof: By the chain rule, we can expand mutual information in two
different ways:
I (X; Y, Z) = I (X; Z) + I (X; Y |Z)

(2.119)

= I (X; Y ) + I (X; Z|Y ).

(2.120)

2.9 SUFFICIENT STATISTICS

35

Since X and Z are conditionally independent given Y , we have
I (X; Z|Y ) = 0. Since I (X; Y |Z) â¥ 0, we have
I (X; Y ) â¥ I (X; Z).

(2.121)

We have equality if and only if I (X; Y |Z) = 0 (i.e., X â Z â Y forms
a Markov chain). Similarly, one can prove that I (Y ; Z) â¥ I (X; Z). 
Corollary

In particular, if Z = g(Y ), we have I (X; Y ) â¥ I (X; g(Y )).

Proof: X â Y â g(Y ) forms a Markov chain.



Thus functions of the data Y cannot increase the information about X.
Corollary

If X â Y â Z, then I (X; Y |Z) â¤ I (X; Y ).

Proof: We note in (2.119) and (2.120) that I (X; Z|Y ) = 0, by
Markovity, and I (X; Z) â¥ 0. Thus,
I (X; Y |Z) â¤ I (X; Y ). 

(2.122)

Thus, the dependence of X and Y is decreased (or remains unchanged)
by the observation of a âdownstreamâ random variable Z. Note that it is
also possible that I (X; Y |Z) > I (X; Y ) when X, Y , and Z do not form a
Markov chain. For example, let X and Y be independent fair binary random variables, and let Z = X + Y . Then I (X; Y ) = 0, but I (X; Y |Z) =
H (X|Z) â H (X|Y, Z) = H (X|Z) = P (Z = 1)H (X|Z = 1) = 12 bit.
2.9

SUFFICIENT STATISTICS

This section is a sidelight showing the power of the data-processing
inequality in clarifying an important idea in statistics. Suppose that we
have a family of probability mass functions {fÎ¸ (x)} indexed by Î¸ , and let
X be a sample from a distribution in this family. Let T (X) be any statistic
(function of the sample) like the sample mean or sample variance. Then
Î¸ â X â T (X), and by the data-processing inequality, we have
I (Î¸ ; T (X)) â¤ I (Î¸ ; X)

(2.123)

for any distribution on Î¸ . However, if equality holds, no information
is lost.
A statistic T (X) is called sufï¬cient for Î¸ if it contains all the information in X about Î¸ .

36

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Deï¬nition A function T (X) is said to be a sufï¬cient statistic relative to
the family {fÎ¸ (x)} if X is independent of Î¸ given T (X) for any distribution
on Î¸ [i.e., Î¸ â T (X) â X forms a Markov chain].
This is the same as the condition for equality in the data-processing
inequality,
I (Î¸ ; X) = I (Î¸ ; T (X))

(2.124)

for all distributions on Î¸ . Hence sufï¬cient statistics preserve mutual information and conversely.
Here are some examples of sufï¬cient statistics:
1. Let X1 , X2 , . . . , Xn , Xi â {0, 1}, be an independent and identically
distributed (i.i.d.) sequence of coin tosses of a coin with unknown
parameter Î¸ = Pr(Xi = 1). Given n, the number

 of 1âs is a sufï¬cient
statistic for Î¸ . Here T (X1 , X2 , . . . , Xn ) = ni=1 Xi . In fact, we can
show that given T , all sequences having that many 1âs are equally
likely and independent of the parameter Î¸ . Speciï¬cally,
 n



Xi = k
Pr (X1 , X2 , . . . , Xn ) = (x1 , x2 , . . . , xn ) 

i=1


1
if
xi = k,
(nk)
=
(2.125)
0
otherwise.



Thus, Î¸ â Xi â (X1 , X2 , . . . , Xn ) forms a Markov chain, and
T is a sufï¬cient statistic for Î¸ .
The next two examples involve probability densities instead of
probability mass functions, but the theory still applies. We deï¬ne
entropy and mutual information for continuous random variables in
Chapter 8.
2. If X is normally distributed with mean Î¸ and variance 1; that is, if
1
2
fÎ¸ (x) = â eâ(xâÎ¸ ) /2 = N(Î¸, 1),
2Ï

(2.126)

and X1 , X2 , . . . , Xn are drawn independently according to this

 distribution, a sufï¬cient statistic for Î¸ is the sample mean X n = n1 ni=1 Xi .
It can be veriï¬ed that the conditional distribution of X1 , X2 , . . . , Xn ,
conditioned on Xn and n does not depend on Î¸ .

2.10

FANOâS INEQUALITY

37

3. If fÎ¸ = Uniform(Î¸, Î¸ + 1), a sufï¬cient statistic for Î¸ is
T (X1 , X2 , . . . , Xn )
= (max{X1 , X2 , . . . , Xn }, min{X1 , X2 , . . . , Xn }).

(2.127)

The proof of this is slightly more complicated, but again one can
show that the distribution of the data is independent of the parameter
given the statistic T .
The minimal sufï¬cient statistic is a sufï¬cient statistic that is a function
of all other sufï¬cient statistics.
Deï¬nition A statistic T (X) is a minimal sufï¬cient statistic relative to
{fÎ¸ (x)} if it is a function of every other sufï¬cient statistic U . Interpreting
this in terms of the data-processing inequality, this implies that
Î¸ â T (X) â U (X) â X.

(2.128)

Hence, a minimal sufï¬cient statistic maximally compresses the information about Î¸ in the sample. Other sufï¬cient statistics may contain
additional irrelevant information. For example, for a normal distribution
with mean Î¸ , the pair of functions giving the mean of all odd samples and
the mean of all even samples is a sufï¬cient statistic, but not a minimal
sufï¬cient statistic. In the preceding examples, the sufï¬cient statistics are
also minimal.
2.10

FANOâS INEQUALITY

Suppose that we know a random variable Y and we wish to guess the value
of a correlated random variable X. Fanoâs inequality relates the probability of error in guessing the random variable X to its conditional entropy
H (X|Y ). It will be crucial in proving the converse to Shannonâs channel
capacity theorem in Chapter 7. From Problem 2.5 we know that the conditional entropy of a random variable X given another random variable
Y is zero if and only if X is a function of Y . Hence we can estimate X
from Y with zero probability of error if and only if H (X|Y ) = 0.
Extending this argument, we expect to be able to estimate X with a
low probability of error only if the conditional entropy H (X|Y ) is small.
Fanoâs inequality quantiï¬es this idea. Suppose that we wish to estimate a
random variable X with a distribution p(x). We observe a random variable
Y that is related to X by the conditional distribution p(y|x). From Y , we

38

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

calculate a function g(Y ) = XÌ, where XÌ is an estimate of X and takes on
values in XÌ. We will not restrict the alphabet XÌ to be equal to X, and we
will also allow the function g(Y ) to be random. We wish to bound the
probability that XÌ = X. We observe that X â Y â XÌ forms a Markov
chain. Deï¬ne the probability of error


(2.129)
Pe = Pr XÌ = X .
Theorem 2.10.1 (Fanoâs Inequality)
For any estimator XÌ such that
X â Y â XÌ, with Pe = Pr(X = XÌ), we have
H (Pe ) + Pe log |X| â¥ H (X|XÌ) â¥ H (X|Y ).

(2.130)

This inequality can be weakened to
1 + Pe log |X| â¥ H (X|Y )
or
Pe â¥

H (X|Y ) â 1
.
log |X|

(2.131)

(2.132)

Remark Note from (2.130) that Pe = 0 implies that H (X|Y ) = 0, as
intuition suggests.
Proof: We ï¬rst ignore the role of Y and prove the ï¬rst inequality in
(2.130). We will then use the data-processing inequality to prove the more
traditional form of Fanoâs inequality, given by the second inequality in
(2.130). Deï¬ne an error random variable,

1 if XÌ = X,
(2.133)
E=
0 if XÌ = X.
Then, using the chain rule for entropies to expand H (E, X|XÌ) in two
different ways, we have
H (E, X|XÌ) = H (X|XÌ) + H (E|X, XÌ)
  

(2.134)

=0

= H (E|XÌ) + H (X|E, XÌ) .
     
â¤H (Pe )

(2.135)

â¤Pe log |X |

Since conditioning reduces entropy, H (E|XÌ) â¤ H (E) = H (Pe ). Now
since E is a function of X and XÌ, the conditional entropy H (E|X, XÌ) is

2.10

FANOâS INEQUALITY

39

equal to 0. Also, since E is a binary-valued random variable, H (E) =
H (Pe ). The remaining term, H (X|E, XÌ), can be bounded as follows:
H (X|E, XÌ) = Pr(E = 0)H (X|XÌ, E = 0) + Pr(E = 1)H (X|XÌ, E = 1)
â¤ (1 â Pe )0 + Pe log |X|,

(2.136)

since given E = 0, X = XÌ, and given E = 1, we can upper bound the
conditional entropy by the log of the number of possible outcomes. Combining these results, we obtain
H (Pe ) + Pe log |X| â¥ H (X|XÌ).

(2.137)

By the data-processing inequality, we have I (X; XÌ) â¤ I (X; Y ) since
X â Y â XÌ is a Markov chain, and therefore H (X|XÌ) â¥ H (X|Y ). Thus,
we have
H (Pe ) + Pe log |X| â¥ H (X|XÌ) â¥ H (X|Y ). 
Corollary

(2.138)

For any two random variables X and Y , let p = Pr(X = Y ).
H (p) + p log |X| â¥ H (X|Y ).

Proof: Let XÌ = Y in Fanoâs inequality.

(2.139)


For any two random variables X and Y , if the estimator g(Y ) takes
values in the set X, we can strengthen the inequality slightly by replacing
log |X| with log(|X| â 1).
Corollary

Let Pe = Pr(X = XÌ), and let XÌ : Y â X; then
H (Pe ) + Pe log(|X| â 1) â¥ H (X|Y ).

(2.140)

Proof: The proof of the theorem goes through without change, except
that
H (X|E, XÌ) = Pr(E = 0)H (X|XÌ, E = 0) + Pr(E = 1)H (X|XÌ, E = 1)
(2.141)
(2.142)
â¤ (1 â Pe )0 + Pe log(|X| â 1),
since given E = 0, X = XÌ, and given E = 1, the range of possible X
outcomes is |X| â 1, we can upper bound the conditional entropy by the
log(|X| â 1), the logarithm of the number of possible outcomes. Substi
tuting this provides us with the stronger inequality.

40

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Remark Suppose that there is no knowledge of Y . Thus, X must be
guessed without any information. Let X â {1, 2, . . . , m} and p1 â¥ p2 â¥
Â· Â· Â· â¥ pm . Then the best guess of X is XÌ = 1 and the resulting probability
of error is Pe = 1 â p1 . Fanoâs inequality becomes
H (Pe ) + Pe log(m â 1) â¥ H (X).
The probability mass function

(p1 , p2 , . . . , pm ) = 1 â Pe ,

Pe
Pe
,...,
mâ1
mâ1

(2.143)

(2.144)

achieves this bound with equality. Thus, Fanoâs inequality is sharp.
While we are at it, let us introduce a new inequality relating probability
of error and entropy. Let X and X 	 by two independent identically distributed random variables with entropy H (X). The probability at X = X 	
is given by

Pr(X = X 	 ) =
p 2 (x).
(2.145)
x

We have the following inequality:
Lemma 2.10.1

If X and X 	 are i.i.d. with entropy H (X),
Pr(X = X 	 ) â¥ 2âH (X) ,

(2.146)

with equality if and only if X has a uniform distribution.
Proof: Suppose that X â¼ p(x). By Jensenâs inequality, we have
2E log p(X) â¤ E2log p(X) ,

(2.147)

which implies that


2âH (X) = 2
Corollary
X. Then

p(x) log p(x)

â¤



p(x)2log p(x) =



p 2 (x). 

(2.148)

Let X, X 	 be independent with X â¼ p(x), X 	 â¼ r(x), x, x 	 â
Pr(X = X 	 ) â¥ 2âH (p)âD(p||r) ,

(2.149)

Pr(X = X 	 ) â¥ 2âH (r)âD(r||p).

(2.150)

SUMMARY

41

Proof: We have


2âH (p)âD(p||r) = 2

p(x) log p(x)+





r(x)
p(x) log p(x)

(2.151)

= 2 p(x) log r(x)

â¤
p(x)2log r(x)

=
p(x)r(x)

(2.152)

= Pr(X = X 	 ),

(2.155)

(2.153)
(2.154)

where the inequality follows from Jensenâs inequality and the convexity
of the function f (y) = 2y .

The following telegraphic summary omits qualifying conditions.

SUMMARY
Deï¬nition The entropy H (X) of a discrete random variable X is
deï¬ned by

H (X) = â
p(x) log p(x).
(2.156)
xâX

Properties of H
1. H (X) â¥ 0.
2. Hb (X) = (logb a)Ha (X).
3. (Conditioning reduces entropy) For any two random variables, X
and Y , we have
H (X|Y ) â¤ H (X)

(2.157)

with equality if and only if X and Y are independent.

4. H (X1 , X2 , . . . , Xn ) â¤ ni=1 H (Xi ), with equality if and only if the
Xi are independent.
5. H (X) â¤ log | X |, with equality if and only if X is distributed uniformly over X.
6. H (p) is concave in p.

42

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Deï¬nition The relative entropy D(p  q) of the probability mass
function p with respect to the probability mass function q is deï¬ned by


p(x)
.
(2.158)
x
q(x)
Deï¬nition The mutual information between two random variables X
and Y is deï¬ned as

p(x, y)
.
(2.159)
p(x, y) log
I (X; Y ) =
p(x)p(y)
D(p  q) =

p(x) log

xâX yâY

Alternative expressions
1
,
p(X)

(2.160)

H (X, Y ) = Ep log

1
,
p(X, Y )

(2.161)

H (X|Y ) = Ep log

1
,
p(X|Y )

(2.162)

I (X; Y ) = Ep log

p(X, Y )
,
p(X)p(Y )

(2.163)

D(p||q) = Ep log

p(X)
.
q(X)

(2.164)

H (X) = Ep log

Properties of D and I
1. I (X; Y ) = H (X) â H (X|Y ) = H (Y ) â H (Y |X) = H (X) +
H (Y ) â H (X, Y ).
2. D(p  q) â¥ 0 with equality if and only if p(x) = q(x), for all x â
X.
3. I (X; Y ) = D(p(x, y)||p(x)p(y)) â¥ 0, with equality if and only if
p(x, y) = p(x)p(y) (i.e., X and Y are independent).
4. If | X |= m, and u is the uniform distribution over X, then D(p 
u) = log m â H (p).
5. D(p||q) is convex in the pair (p, q).
Chain rules

Entropy: H (X1 , X2 , . . . , Xn ) = ni=1 H (Xi |Xiâ1 , . . . , X1 ).
Mutual information:

I (X1 , X2 , . . . , Xn ; Y ) = ni=1 I (Xi ; Y |X1 , X2 , . . . , Xiâ1 ).

PROBLEMS

43

Relative entropy:
D(p(x, y)||q(x, y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x)).
Jensenâs inequality. If f is a convex function, then Ef (X) â¥ f (EX).
Log sum inequality. For n positive numbers, a1 , a2 , . . . , an and
b1 , b2 , . . . , bn ,
	 n 


n
n


ai
ai
ai log â¥
ai log 
i=1
(2.165)
n
bi
i=1 bi
i=1

i=1

with equality if and only if

ai
bi

= constant.

Data-processing inequality. If X â Y â Z forms a Markov chain,
I (X; Y ) â¥ I (X; Z).
Sufï¬cient statistic. T (X) is sufï¬cient relative to {fÎ¸ (x)} if and only
if I (Î¸ ; X) = I (Î¸ ; T (X)) for all distributions on Î¸ .
Fanoâs inequality. Let Pe = Pr{XÌ(Y ) = X}. Then
H (Pe ) + Pe log |X| â¥ H (X|Y ).

(2.166)

Inequality. If X and X 	 are independent and identically distributed,
then
Pr(X = X 	 ) â¥ 2âH (X) ,

(2.167)

PROBLEMS
2.1

Coin ï¬ips. A fair coin is ï¬ipped until the ï¬rst head occurs. Let
X denote the number of ï¬ips required.
(a) Find the entropy H (X) in bits. The following expressions may
be useful:
â

n=0

1
r =
,
1âr
n

â

n=0

nr n =

r
.
(1 â r)2

(b) A random variable X is drawn according to this distribution.
Find an âefï¬cientâ sequence of yesâno questions of the form,

44

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

âIs X contained in the set S?â Compare H (X) to the expected
number of questions required to determine X.
2.2

Entropy of functions. Let X be a random variable taking on a
ï¬nite number of values. What is the (general) inequality relationship of H (X) and H (Y ) if
(a) Y = 2X ?
(b) Y = cos X?

2.3

Minimum entropy. What is the minimum value of
H (p1 , . . . , pn ) = H (p) as p ranges over the set of n-dimensional
probability vectors? Find all pâs that achieve this minimum.

2.4

Entropy of functions of a random variable. Let X be a discrete
random variable. Show that the entropy of a function of X is less
than or equal to the entropy of X by justifying the following steps:
(a)

H (X, g(X)) = H (X) + H (g(X) | X)
(b)

= H (X),

(c)

H (X, g(X)) = H (g(X)) + H (X | g(X))
(d)

â¥ H (g(X)).

(2.168)
(2.169)
(2.170)
(2.171)

Thus, H (g(X)) â¤ H (X).
2.5

Zero conditional entropy. Show that if H (Y |X) = 0, then Y is
a function of X [i.e., for all x with p(x) > 0, there is only one
possible value of y with p(x, y) > 0].

2.6

Conditional mutual information vs. unconditional mutual information. Give examples of joint random variables X, Y , and Z
such that
(a) I (X; Y | Z) < I (X; Y ).
(b) I (X; Y | Z) > I (X; Y ).

2.7

Coin weighing. Suppose that one has n coins, among which there
may or may not be one counterfeit coin. If there is a counterfeit
coin, it may be either heavier or lighter than the other coins. The
coins are to be weighed by a balance.
(a) Find an upper bound on the number of coins n so that k
weighings will ï¬nd the counterfeit coin (if any) and correctly
declare it to be heavier or lighter.

PROBLEMS

45

(b) (Difï¬cult) What is the coin- weighing strategy for k = 3 weighings and 12 coins?
2.8

Drawing with and without replacement. An urn contains r red, w
white, and b black balls. Which has higher entropy, drawing k â¥ 2
balls from the urn with replacement or without replacement? Set it
up and show why. (There is both a difï¬cult way and a relatively
simple way to do this.)

2.9

Metric. A function Ï(x, y) is a metric if for all x, y,
â¢ Ï(x, y) â¥ 0.
â¢ Ï(x, y) = Ï(y, x).
â¢ Ï(x, y) = 0 if and only if x = y.
â¢ Ï(x, y) + Ï(y, z) â¥ Ï(x, z).
(a) Show that Ï(X, Y ) = H (X|Y ) + H (Y |X) satisï¬es the ï¬rst,
second, and fourth properties above. If we say that X = Y if
there is a one-to-one function mapping from X to Y , the third
property is also satisï¬ed, and Ï(X, Y ) is a metric.
(b) Verify that Ï(X, Y ) can also be expressed as
Ï(X, Y ) = H (X) + H (Y ) â 2I (X; Y )

2.10

(2.172)

= H (X, Y ) â I (X; Y )

(2.173)

= 2H (X, Y ) â H (X) â H (Y ).

(2.174)

Entropy of a disjoint mixture. Let X1 and X2 be discrete random
variables drawn according to probability mass functions p1 (Â·) and
p2 (Â·) over the respective alphabets X1 = {1, 2, . . . , m} and X2 =
{m + 1, . . . , n}. Let

X1 with probability Î±,
X=
X2 with probability 1 â Î±.
(a) Find H (X) in terms of H (X1 ), H (X2 ), and Î±.
(b) Maximize over Î± to show that 2H (X) â¤ 2H (X1 ) + 2H (X2 ) and
interpret using the notion that 2H (X) is the effective alphabet size.

2.11

Measure of correlation. Let X1 and X2 be identically distributed
but not necessarily independent. Let
Ï =1â

H (X2 | X1 )
.
H (X1 )

46

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

(a)
(b)
(c)
(d)
2.12

1 ;X2 )
Show that Ï = I (X
H (X1 ) .
Show that 0 â¤ Ï â¤ 1.
When is Ï = 0?
When is Ï = 1?

Example of joint entropy.

Let p(x, y) be given by

Find:
(a) H (X), H (Y ).
(b) H (X | Y ), H (Y | X).
(c) H (X, Y ).
(d) H (Y ) â H (Y | X).
(e) I (X; Y ).
(f) Draw a Venn diagram for the quantities in parts (a) through (e).
Show that ln x â¥ 1 â

1
x

for x > 0.

2.13

Inequality.

2.14

Entropy of a sum. Let X and Y be random variables that take
on values x1 , x2 , . . . , xr and y1 , y2 , . . . , ys , respectively. Let Z =
X + Y.
(a) Show that H (Z|X) = H (Y |X). Argue that if X, Y are independent, then H (Y ) â¤ H (Z) and H (X) â¤ H (Z). Thus, the
addition of independent random variables adds uncertainty.
(b) Give an example of (necessarily dependent) random variables
in which H (X) > H (Z) and H (Y ) > H (Z).
(c) Under what conditions does H (Z) = H (X) + H (Y )?

2.15

Data processing. Let X1 â X2 â X3 â Â· Â· Â· â Xn form a
Markov chain in this order; that is, let
p(x1 , x2 , . . . , xn ) = p(x1 )p(x2 |x1 ) Â· Â· Â· p(xn |xnâ1 ).
Reduce I (X1 ; X2 , . . . , Xn ) to its simplest form.

2.16

Bottleneck . Suppose that a (nonstationary) Markov chain starts
in one of n states, necks down to k < n states, and then
fans back to m > k states. Thus, X1 â X2 â X3 , that is,

PROBLEMS

47

p(x1 , x2 , x3 ) = p(x1 )p(x2 |x1 )p(x3 |x2 ), for all x1 â {1, 2, . . . , n},
x2 â {1, 2, . . . , k}, x3 â {1, 2, . . . , m}.
(a) Show that the dependence of X1 and X3 is limited by the
bottleneck by proving that I (X1 ; X3 ) â¤ log k.
(b) Evaluate I (X1 ; X3 ) for k = 1, and conclude that no dependence can survive such a bottleneck.
2.17

Pure randomness and bent coins. Let X1 , X2 , . . . , Xn denote the
outcomes of independent ï¬ips of a bent coin. Thus, Pr {Xi =
1} = p, Pr {Xi = 0} = 1 â p, where p is unknown. We wish
to obtain a sequence Z1 , Z2 , . . . , ZK of fair coin ï¬ips from
X1 , X2 , . . . , Xn . Toward this end, let f : X n â {0, 1}â (where
{0, 1}â = {, 0, 1, 00, 01, . . .} is the set of all ï¬nite-length binary
sequences) be a mapping f (X1 , X2 , . . . , Xn ) = (Z1 , Z2 , . . . , ZK ),
where Zi â¼ Bernoulli ( 12 ), and K may depend on (X1 , . . . , Xn ).
In order that the sequence Z1 , Z2 , . . . appear to be fair coin ï¬ips,
the map f from bent coin ï¬ips to fair ï¬ips must have the property that all 2k sequences (Z1 , Z2 , . . . , Zk ) of a given length k
have equal probability (possibly 0), for k = 1, 2, . . .. For example,
for n = 2, the map f (01) = 0, f (10) = 1, f (00) = f (11) = 
(the null string) has the property that Pr{Z1 = 1|K = 1} = Pr{Z1 =
0|K = 1} = 12 . Give reasons for the following inequalities:
(a)

nH (p) = H (X1 , . . . , Xn )
(b)

â¥ H (Z1 , Z2 , . . . , ZK , K)

(c)

= H (K) + H (Z1 , . . . , ZK |K)

(d)

= H (K) + E(K)

(e)

â¥ EK.

Thus, no more than nH (p) fair coin tosses can be derived from
(X1 , . . . , Xn ), on the average. Exhibit a good map f on sequences
of length 4.
2.18

World Series. The World Series is a seven-game series that terminates as soon as either team wins four games. Let X be the random
variable that represents the outcome of a World Series between
teams A and B; possible values of X are AAAA, BABABAB, and
BBBAAAA. Let Y be the number of games played, which ranges
from 4 to 7. Assuming that A and B are equally matched and that

48

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

the games are independent, calculate H (X), H (Y ), H (Y |X), and
H (X|Y ).
2.19

Inï¬nite entropy. This problem shows that 
the entropy of a discrete
2
â1
random variable can be inï¬nite. Let A = â
n=2 (n log n) . [It is
easy to show that A is ï¬nite by bounding the inï¬nite sum by the
integral of (x log2 x)â1 .] Show that the integer-valued random variable X deï¬ned by Pr(X = n) = (An log2 n)â1 for n = 2, 3, . . .,
has H (X) = +â.

2.20

Run-length coding. Let X1 , X2 , . . . , Xn be (possibly dependent)
binary random variables. Suppose that one calculates the run
lengths R = (R1 , R2 , . . .) of this sequence (in order as they
occur). For example, the sequence X = 0001100100 yields run
lengths R = (3, 2, 2, 1, 2). Compare H (X1 , X2 , . . . , Xn ), H (R),
and H (Xn , R). Show all equalities and inequalities, and bound all
the differences.

2.21

Markovâs inequality for probabilities. Let p(x) be a probability
mass function. Prove, for all d â¥ 0, that
Pr {p(X) â¤ d} log

1
â¤ H (X).
d

(2.175)

2.22

Logical order of ideas. Ideas have been developed in order of
need and then generalized if necessary. Reorder the following ideas,
strongest ï¬rst, implications following:
(a) Chain rule for I (X1 , . . . , Xn ; Y ), chain rule for D(p(x1 , . . . ,
xn )||q(x1 , x2 , . . . , xn )), and chain rule for H (X1 , X2 , . . . , Xn ).
(b) D(f ||g) â¥ 0, Jensenâs inequality, I (X; Y ) â¥ 0.

2.23

Conditional mutual information. Consider a sequence of n binary
random variables X1 , X2 , . . . , Xn . Each sequence with an even
number of 1âs has probability 2â(nâ1) , and each sequence with an
odd number of 1âs has probability 0. Find the mutual informations
I (X1 ; X2 ),

2.24

I (X2 ; X3 |X1 ), . . . , I (Xnâ1 ; Xn |X1 , . . . , Xnâ2 ).

Average entropy. Let H (p) = âp log2 p â (1 â p) log2 (1 â p)
be the binary entropy function.
(a) Evaluate H ( 14 ) using the fact that log2 3 â 1.584. (Hint: You
may wish to consider an experiment with four equally likely
outcomes, one of which is more interesting than the others.)

PROBLEMS

49

(b) Calculate the average entropy H (p) when the probability p is
chosen uniformly in the range 0 â¤ p â¤ 1.
(c) (Optional ) Calculate the average entropy H (p1 , p2 , p3 ), where
(p1 , p2 , p3 ) is a uniformly distributed probability vector. Generalize to dimension n.
2.25

Venn diagrams. There isnât really a notion of mutual information
common to three random variables. Here is one attempt at a deï¬nition: Using Venn diagrams, we can see that the mutual information
common to three random variables X, Y , and Z can be deï¬ned by
I (X; Y ; Z) = I (X; Y ) â I (X; Y |Z) .
This quantity is symmetric in X, Y , and Z, despite the preceding
asymmetric deï¬nition. Unfortunately, I (X; Y ; Z) is not necessarily nonnegative. Find X, Y , and Z such that I (X; Y ; Z) < 0, and
prove the following two identities:
(a) I (X; Y ; Z) = H (X, Y, Z) â H (X) â H (Y ) â H (Z) +
I (X; Y ) + I (Y ; Z) + I (Z; X).
(b) I (X; Y ; Z) = H (X, Y, Z) â H (X, Y ) â H (Y, Z) â
H (Z, X) + H (X) + H (Y ) + H (Z).
The ï¬rst identity can be understood using the Venn diagram analogy
for entropy and mutual information. The second identity follows
easily from the ï¬rst.

2.26

Another proof of nonnegativity of relative entropy. In view of the
fundamental nature of the result D(p||q) â¥ 0, we will give another
proof.
(a) Show that ln x â¤ x â 1 for 0 < x < â.
(b) Justify the following steps:
âD(p||q) =


x

â¤


x

p(x) ln

q(x)
p(x)


q(x)
â1
p(x)
p(x)

(2.176)



â¤ 0.

(2.177)
(2.178)

(c) What are the conditions for equality?
2.27

Grouping rule for entropy. Let p = (p1 , p2 , . . . , p
m ) be a probability distribution on m elements (i.e., pi â¥ 0 and m
i=1 pi = 1).

50

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

Deï¬ne a new distribution q on m â 1 elements as q1 = p1 , q2 = p2 ,
. . . , qmâ2 = pmâ2 , and qmâ1 = pmâ1 + pm [i.e., the distribution q
is the same as p on {1, 2, . . . , m â 2}, and the probability of the
last element in q is the sum of the last two probabilities of p].
Show that


pmâ1
pm
H (p) = H (q) + (pmâ1 + pm )H
,
.
pmâ1 + pm pmâ1 + pm
(2.179)
2.28

Mixing increases entropy. Show that the entropy of the probability distribution, (p1 , . . . , pi , . . . , pj , . . . , pm ), is less than the
p +p
p +p
entropy
of
the
distribution
(p1 , . . . , i 2 j , . . . , i 2 j ,
. . . , pm ). Show that in general any transfer of probability that
makes the distribution more uniform increases the entropy.

2.29

Inequalities. Let X, Y , and Z be joint random variables. Prove
the following inequalities and ï¬nd conditions for equality.
(a) H (X, Y |Z) â¥ H (X|Z).
(b) I (X, Y ; Z) â¥ I (X; Z).
(c) H (X, Y, Z) â H (X, Y ) â¤ H (X, Z) â H (X).
(d) I (X; Z|Y ) â¥ I (Z; Y |X) â I (Z; Y ) + I (X; Z).

2.30

Maximum entropy. Find the probability mass function p(x) that
maximizes the entropy H (X) of a nonnegative integer-valued random variable X subject to the constraint
EX =

â


np(n) = A

n=0

for a ï¬xed value A > 0. Evaluate this maximum H (X).
2.31

Conditional entropy.
H (X|Y )?

2.32

Fano.

Under what conditions does H (X|g(Y )) =

We are given the following joint distribution on (X, Y ):

PROBLEMS

51

Let XÌ(Y ) be an estimator for X (based on Y ) and let Pe =
Pr{XÌ(Y ) = X}.
(a) Find the minimum probability of error estimator XÌ(Y ) and the
associated Pe .
(b) Evaluate Fanoâs inequality for this problem and compare.
2.33

Fanoâs inequality. Let Pr(X = i) = pi , i = 1, 2, . . . , m, and let
p1 â¥ p2 â¥ p3 â¥ Â· Â· Â· â¥ pm . The minimal probability of error predictor of X is XÌ = 1, with resulting probability of error Pe =
1 â p1 . Maximize H (p) subject to the constraint 1 â p1 = Pe to
ï¬nd a bound on Pe in terms of H . This is Fanoâs inequality in the
absence of conditioning.

2.34

Entropy of initial conditions. Prove that H (X0 |Xn ) is nondecreasing with n for any Markov chain.

2.35

Relative entropy is not symmetric.
Let the random variable X have three possible outcomes {a, b, c}.
Consider two distributions on this random variable:
Symbol

p(x)

q(x)

a

1
2
1
4
1
4

1
3
1
3
1
3

b
c

Calculate H (p), H (q), D(p||q), and D(q||p). Verify that in this
case, D(p||q) = D(q||p).
2.36

Symmetric relative entropy. Although, as Problem 2.35 shows,
D(p||q) = D(q||p) in general, there could be distributions for
which equality holds. Give an example of two distributions p and
q on a binary alphabet such that D(p||q) = D(q||p) (other than
the trivial case p = q).

2.37

Relative entropy. Let X, Y, Z be three random variables with a
joint probability mass function p(x, y, z). The relative entropy
between the joint distribution and the product of the marginals is



p(x, y, z)
D(p(x, y, z)||p(x)p(y)p(z)) = E log
. (2.180)
p(x)p(y)p(z)
Expand this in terms of entropies. When is this quantity zero?

52

2.38

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

The value of a question. Let X â¼ p(x), x = 1, 2, . . . , m. We are
given a set S â {1, 2, . . . , m}. We ask whether X â S and receive
the answer

1 if X â S
Y =
0 if X â S.
Suppose that Pr{X â S} = Î±. Find the decrease in uncertainty
H (X) â H (X|Y ).
Apparently, any set S with a given Î± is as good as any other.

2.39

Entropy and pairwise independence. Let X, Y, Z be three binary
Bernoulli( 12 ) random variables that are pairwise independent; that
is, I (X; Y ) = I (X; Z) = I (Y ; Z) = 0.
(a) Under this constraint, what is the minimum value for
H (X, Y, Z)?
(b) Give an example achieving this minimum.

2.40

Discrete entropies. Let X and Y be two independent integervalued random variables. Let X be uniformly distributed over {1, 2,
. . . , 8}, and let Pr{Y = k} = 2âk , k = 1, 2, 3, . . ..
(a) Find H (X).
(b) Find H (Y ).
(c) Find H (X + Y, X â Y ).

2.41

Random questions. One wishes to identify a random object X â¼
p(x). A question Q â¼ r(q) is asked at random according to r(q).
This results in a deterministic answer A = A(x, q) â {a1 , a2 , . . .}.
Suppose that X and Q are independent. Then I (X; Q, A) is the
uncertainty in X removed by the questionâanswer (Q, A).
(a) Show that I (X; Q, A) = H (A|Q). Interpret.
(b) Now suppose that two i.i.d. questions Q1 , Q2 , â¼ r(q) are
asked, eliciting answers A1 and A2 . Show that two questions
are less valuable than twice a single question in the sense that
I (X; Q1 , A1 , Q2 , A2 ) â¤ 2I (X; Q1 , A1 ).

2.42

Inequalities. Which of the following inequalities are generally
â¥, =, â¤? Label each with â¥, =, or â¤.
(a) H (5X) vs. H (X)
(b) I (g(X); Y ) vs. I (X; Y )
(c) H (X0 |Xâ1 ) vs. H (X0 |Xâ1 , X1 )
(d) H (X, Y )/(H (X) + H (Y )) vs. 1

PROBLEMS

53

2.43

Mutual information of heads and tails
(a) Consider a fair coin ï¬ip. What is the mutual information
between the top and bottom sides of the coin?
(b) A six-sided fair die is rolled. What is the mutual information
between the top side and the front face (the side most facing
you)?

2.44

Pure randomness. We wish to use a three-sided coin to generate
a fair coin toss. Let the coin X have probability mass function
ï£±
pA
ï£² A,
B,
pB
X=
ï£³ C,
pC ,
where pA , pB , pC are unknown.
(a) How would you use two independent ï¬ips X1 , X2 to generate
(if possible) a Bernoulli( 12 ) random variable Z?
(b) What is the resulting maximum expected number of fair bits
generated?

2.45

Finite entropy. Show that for a discrete random variable X â
{1, 2, . . .}, if E log X < â, then H (X) < â.

2.46

Axiomatic deï¬nition of entropy (Difï¬cult). If we assume certain
axioms for our measure of information, we will be forced to use a
logarithmic measure such as entropy. Shannon used this to justify
his initial deï¬nition of entropy. In this book we rely more on the
other properties of entropy rather than its axiomatic derivation to
justify its use. The following problem is considerably more difï¬cult
than the other problems in this section.
If a sequence of symmetric functions Hm (p1 , p2 , . . . , pm ) satisï¬es
the following properties:


â¢ Normalization: H2 1 , 1 = 1,
2 2
â¢ Continuity: H2 (p, 1 â p) is a continuous function of p,
â¢ Grouping: Hm (p1 , p2 , . . . , pm ) = Hmâ1 (p1 + p2 , p3 , . . . , pm ) +


p2
1
,
(p1 + p2 )H2 p1p+p
,
2 p1 +p2
prove that Hm must be of the form
Hm (p1 , p2 , . . . , pm ) = â

m


pi log pi ,

m = 2, 3, . . . .

i=1

(2.181)

54

ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

There are various other axiomatic formulations which result in the
same deï¬nition of entropy. See, for example, the book by CsiszaÌr
and KoÌrner [149].
2.47

Entropy of a missorted ï¬le. A deck of n cards in order 1, 2, . . . , n
is provided. One card is removed at random, then replaced at random. What is the entropy of the resulting deck?

2.48

Sequence length. How much information does the length of a
sequence give about the content of a sequence? Suppose that we
consider a Bernoulli ( 12 ) process {Xi }. Stop the process when the
ï¬rst 1 appears. Let N designate this stopping time. Thus, X N is an
element of the set of all ï¬nite-length binary sequences {0, 1}â =
{0, 1, 00, 01, 10, 11, 000, . . . }.
(a) Find I (N ; X N ).
(b) Find H (X N |N ).
(c) Find H (X N ).
Letâs now consider a different stopping time. For this part, again
assume that Xi â¼ Bernoulli( 12 ) but stop at time N = 6, with probability 13 and stop at time N = 12 with probability 23 . Let this
stopping time be independent of the sequence X1 X2 Â· Â· Â· X12 .
(d) Find I (N ; X N ).
(e) Find H (X N |N ).
(f) Find H (X N ).

HISTORICAL NOTES
The concept of entropy was introduced in thermodynamics, where it
was used to provide a statement of the second law of thermodynamics. Later, statistical mechanics provided a connection between thermodynamic entropy and the logarithm of the number of microstates in a
macrostate of the system. This work was the crowning achievement of
Boltzmann, who had the equation S = k ln W inscribed as the epitaph on
his gravestone [361].
In the 1930s, Hartley introduced a logarithmic measure of information for communication. His measure was essentially the logarithm of the
alphabet size. Shannon [472] was the ï¬rst to deï¬ne entropy and mutual
information as deï¬ned in this chapter. Relative entropy was ï¬rst deï¬ned
by Kullback and Leibler [339]. It is known under a variety of names,
including the KullbackâLeibler distance, cross entropy, information divergence, and information for discrimination, and has been studied in detail
by CsiszaÌr [138] and Amari [22].

HISTORICAL NOTES

55

Many of the simple properties of these quantities were developed by
Shannon. Fanoâs inequality was proved in Fano [201]. The notion of
sufï¬cient statistic was deï¬ned by Fisher [209], and the notion of the
minimal sufï¬cient statistic was introduced by Lehmann and ScheffeÌ [350].
The relationship of mutual information and sufï¬ciency is due to Kullback
[335]. The relationship between information theory and thermodynamics
has been discussed extensively by Brillouin [77] and Jaynes [294].
The physics of information is a vast new subject of inquiry spawned
from statistical mechanics, quantum mechanics, and information theory.
The key question is how information is represented physically. Quantum channel capacity (the logarithm of the number of distinguishable
preparations of a physical system) and quantum data compression [299]
are well-deï¬ned problems with nice answers involving the von Neumann
entropy. A new element of quantum information arises from the existence of quantum entanglement and the consequences (exhibited in Bellâs
inequality) that the observed marginal distribution of physical events are
not consistent with any joint distribution (no local realism). The fundamental text by Nielsen and Chuang [395] develops the theory of quantum
information and the quantum counterparts to many of the results in this
book. There have also been attempts to determine whether there are
any fundamental physical limits to computation, including work by Bennett [47] and Bennett and Landauer [48].

CHAPTER 3

ASYMPTOTIC EQUIPARTITION
PROPERTY

In information theory, the analog of the law of large numbers is the
asymptotic equipartition property (AEP). It is a direct consequence
of the weak law of large numbers. The law of large numbers states
that
for independent, identically distributed (i.i.d.) random variables,
1 n
i=1 Xi is close to its expected value EX for large values of n.
n
The AEP states that n1 log p(X1 ,X12 ,...,Xn ) is close to the entropy H , where
X1 , X2 , . . . , Xn are i.i.d. random variables and p(X1 , X2 , . . . , Xn ) is the
probability of observing the sequence X1 , X2 , . . . , Xn . Thus, the probability p(X1 , X2 , . . . , Xn ) assigned to an observed sequence will be close
to 2ânH .
This enables us to divide the set of all sequences into two sets, the
typical set, where the sample entropy is close to the true entropy, and the
nontypical set, which contains the other sequences. Most of our attention
will be on the typical sequences. Any property that is proved for the typical
sequences will then be true with high probability and will determine the
average behavior of a large sample.
First, an example. Let the random variable X â {0, 1} have a probability
mass function deï¬ned by p(1) = p and p(0) = q. If X1 , X2 , . . . , Xn are
i.i.d. according to p(x), the probability of a sequence x1 , x2 , . . . , xn is

n
For example, the probability of the sequence (1, 0, 1, 1, 0, 1)
i=1
p(xi ). 
X
nâ
Xi = p 4 q 2 . Clearly, it is not true that all 2n sequences of
i
is p
q
length n have the same probability.
However, we might be able to predict the probability of the sequence
that we actually observe. We ask for the probability p(X1 , X2 , . . . , Xn ) of
the outcomes X1 , X2 , . . . , Xn , where X1 , X2 , . . . are i.i.d. â¼ p(x). This is
insidiously self-referential, but well deï¬ned nonetheless. Apparently, we
are asking for the probability of an event drawn according to the same
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

57

58

ASYMPTOTIC EQUIPARTITION PROPERTY

probability distribution. Here it turns out that p(X1 , X2 , . . . , Xn ) is close
to 2ânH with high probability.
We summarize this by saying, âAlmost all events are almost equally
surprising.â This is a way of saying that


Pr (X1 , X2 , . . . , Xn ) : p(X1 , X2 , . . . , Xn ) = 2ân(H Â±) â 1
(3.1)
if X1 , X2 , . . . , Xn are i.i.d. â¼ p(x).


In the example just given, where p(X1 , X2 , . . . , Xn ) = p Xi q nâ Xi ,
we are simply saying that the number of 1âs in the sequence is close
to np (with high probability), and all such sequences have (roughly) the
same probability 2ânH (p) . We use the idea of convergence in probability,
deï¬ned as follows:
Deï¬nition (Convergence of random variables). Given a sequence of
random variables, X1 , X2 , . . . , we say that the sequence X1 , X2 , . . . converges to a random variable X:
1. In probability if for every  > 0, Pr{|Xn â X| > } â 0
2. In mean square if E(Xn â X)2 â 0
3. With probability 1 (also called almost surely) if Pr{limnââ Xn =
X} = 1
3.1

ASYMPTOTIC EQUIPARTITION PROPERTY THEOREM

The asymptotic equipartition property is formalized in the following
theorem.
Theorem 3.1.1

(AEP )

If X1 , X2 , . . . are i.i.d. â¼ p(x), then

1
â log p(X1 , X2 , . . . , Xn ) â H (X)
n

in probability.

(3.2)

Proof: Functions of independent random variables are also independent
random variables. Thus, since the Xi are i.i.d., so are log p(Xi ). Hence,
by the weak law of large numbers,
1
1
log p(Xi )
â log p(X1 , X2 , . . . , Xn ) = â
n
n

(3.3)

i

â âE log p(X)
= H (X),
which proves the theorem.

in probability

(3.4)
(3.5)


3.1

ASYMPTOTIC EQUIPARTITION PROPERTY THEOREM

59

Deï¬nition The typical set A(n)
 with respect to p(x) is the set of sequences (x1 , x2 , . . . , xn ) â X n with the property
2ân(H (X)+) â¤ p(x1 , x2 , . . . , xn ) â¤ 2ân(H (X)â) .

(3.6)

has the
As a consequence of the AEP, we can show that the set A(n)

following properties:
Theorem 3.1.2
1
1. If (x1 , x2 , . . . , xn ) â A(n)
 , then H (X) â  â¤ â n log p(x1 , x2 , . . . ,
xn ) â¤ H (X) + .


> 1 â  for n sufï¬ciently large.
2. Pr A(n)

 (n) 


3. A â¤ 2n(H (X)+) , where |A| denotes the number of elements in the
set A.
n(H (X)â)
for n sufï¬ciently large.
4. |A(n)
 | â¥ (1 â )2

Thus, the typical set has probability nearly 1, all elements of the typical
set are nearly equiprobable, and the number of elements in the typical set
is nearly 2nH .
Proof: The proof of property (1) is immediate from the deï¬nition of
A(n)
 . The second property follows directly from Theorem 3.1.1, since the
probability of the event (X1 , X2 , . . . , Xn ) â A(n)
 tends to 1 as n â â.
Thus, for any Î´ > 0, there exists an n0 such that for all n â¥ n0 , we have



 1

Pr â log p(X1 , X2 , . . . , Xn ) â H (X) <  > 1 â Î´.
(3.7)
n
Setting Î´ = , we obtain the second part of the theorem. The identiï¬cation
of Î´ =  will conveniently simplify notation later.
To prove property (3), we write

p(x)
(3.8)
1=
xâX n

â¥



p(x)

(3.9)

(n)
xâA

â¥



2ân(H (X)+)

(3.10)

(n)
xâA

= 2ân(H (X)+) |A(n)
 |,

(3.11)

60

ASYMPTOTIC EQUIPARTITION PROPERTY

where the second inequality follows from (3.6). Hence
n(H (X)+)
|A(n)
.
 |â¤2

(3.12)

Finally, for sufï¬ciently large n, Pr{A(n)
 } > 1 â , so that
1 â  < Pr{A(n)
 }

â¤
2ân(H (X)â)

(3.13)
(3.14)

(n)
xâA

= 2ân(H (X)â) |A(n)
 |,

(3.15)

where the second inequality follows from (3.6). Hence,
n(H (X)â)
|A(n)
,
 | â¥ (1 â )2

(3.16)


which completes the proof of the properties of A(n)
 .
3.2

CONSEQUENCES OF THE AEP: DATA COMPRESSION

Let X1 , X2 , . . . , Xn be independent, identically distributed random variables drawn from the probability mass function p(x). We wish to ï¬nd
short descriptions for such sequences of random variables. We divide all
sequences in X n into two sets: the typical set A(n)
 and its complement,
as shown in Figure 3.1.

n:|

|n elements

Non-typical set

Typical set
â

FIGURE 3.1. Typical sets and source coding.

â

A(n) : 2n(H +

)

elements

3.2 CONSEQUENCES OF THE AEP: DATA COMPRESSION

61

Non-typical set
Description: n log | | + 2 bits

Typical set
Description: n(H + ) + 2 bits
â

FIGURE 3.2. Source code using the typical set.

We order all elements in each set according to some order (e.g., lexicographic order). Then we can represent each sequence of A(n)
 by giving
n(H +)
sequences
the index of the sequence in the set. Since there are â¤ 2
in A(n)
,
the
indexing
requires
no
more
than
n(H
+
)
+
1
bits.
[The
extra

bit may be necessary because n(H + ) may not be an integer.] We preï¬x all these sequences by a 0, giving a total length of â¤ n(H + ) + 2
bits to represent each sequence in A(n)
 (see Figure 3.2). Similarly, we can
by
using
not more than n log |X| + 1 bits.
index each sequence not in A(n)

Preï¬xing these indices by 1, we have a code for all the sequences in X n .
Note the following features of the above coding scheme:
â¢
â¢

â¢

The code is one-to-one and easily decodable. The initial bit acts as
a ï¬ag bit to indicate the length of the codeword that follows.
c
We have used a brute-force enumeration of the atypical set A(n)

without taking into account the fact that the number of elements in
c
is less than the number of elements in X n . Surprisingly, this is
A(n)

good enough to yield an efï¬cient description.
The typical sequences have short descriptions of length â nH .

We use the notation x n to denote a sequence x1 , x2 , . . . , xn . Let l(x n )
be the length of the codeword corresponding to x n . If n is sufï¬ciently
large so that Pr{A(n)
 } â¥ 1 â , the expected length of the codeword is

E(l(X n )) =
p(x n )l(x n )
(3.17)
xn

62

ASYMPTOTIC EQUIPARTITION PROPERTY

=



p(x n )l(x n ) +

(n)

â¤

p(x n )l(x n )

(3.18)

(n) c
x n âA

x n âA





p(x n )(n(H + ) + 2)

(n)

x n âA

+



(n)
x n âA

p(x n )(n log |X| + 2)

(3.19)

c



	


c
(n(H + ) + 2) + Pr A(n)
(n log |X| + 2)
= Pr A(n)


(3.20)
â¤ n(H + ) + n(log |X|) + 2

(3.21)

= n(H +   ),

(3.22)

where   =  +  log |X| + n2 can be made arbitrarily small by an appropriate choice of  followed by an appropriate choice of n. Hence we have
proved the following theorem.
Theorem 3.2.1 Let X n be i.i.d. â¼ p(x). Let  > 0. Then there exists a
code that maps sequences x n of length n into binary strings such that the
mapping is one-to-one (and therefore invertible) and


1
n
E l(X ) â¤ H (X) + 
(3.23)
n
for n sufï¬ciently large.
Thus, we can represent sequences X n using nH (X) bits on the average.
3.3

HIGH-PROBABILITY SETS AND THE TYPICAL SET

(n)
From the deï¬nition of A(n)
 , it is clear that A is a fairly small set that
contains most of the probability. But from the deï¬nition, it is not clear
whether it is the smallest such set. We will prove that the typical set has
essentially the same number of elements as the smallest set, to ï¬rst order
in the exponent.

Deï¬nition For each n = 1, 2, . . . , let BÎ´(n) â X n be the smallest set
with
Pr{BÎ´(n) } â¥ 1 â Î´.

(3.24)

63

3.3 HIGH-PROBABILITY SETS AND THE TYPICAL SET

We argue that BÎ´(n) must have signiï¬cant intersection with A(n)
 and therefore must have about as many elements. In Problem 3.3.11, we outline
the proof of the following theorem.
Theorem 3.3.1 Let X1 , X2 , . . . , Xn be i.i.d. â¼ p(x). For Î´ <
any Î´  > 0, if Pr{BÎ´(n) } > 1 â Î´, then
1
log |BÎ´(n) | > H â Î´ 
n

for n sufï¬ciently large.

1
2

and

(3.25)

Thus, BÎ´(n) must have at least 2nH elements, to ï¬rst order in the expon(H Â±)
nent. But A(n)
elements. Therefore, A(n)
 has 2
 is about the same
size as the smallest high-probability set.
We will now deï¬ne some new notation to express equality to ï¬rst order
in the exponent.
.

Deï¬nition The notation an = bn means
lim

nââ

1
an
= 0.
log
n
bn

(3.26)

.

Thus, an = bn implies that an and bn are equal to the ï¬rst order in the
exponent.
We can now restate the above results: If Î´n â 0 and n â 0, then
.

.

nH
|=|A(n)
.
|BÎ´(n)
n |=2
n

(3.27)

and BÎ´(n) , let us conTo illustrate the difference between A(n)

sider a Bernoulli sequence X1 , X2 , . . . , Xn with parameter p = 0.9. [A
Bernoulli(Î¸ ) random variable is a binary random variable that takes on
the value 1 with probability Î¸ .] The typical sequences in this case are the
sequences in which the proportion of 1âs is close to 0.9. However, this
does not include the most likely single sequence, which is the sequence of
all 1âs. The set BÎ´(n) includes all the most probable sequences and therefore includes the sequence of all 1âs. Theorem 3.3.1 implies that A(n)
 and
BÎ´(n) must both contain the sequences that have about 90% 1âs, and the
two sets are almost equal in size.

64

ASYMPTOTIC EQUIPARTITION PROPERTY

SUMMARY
AEP. âAlmost all events are almost equally surprising.â Speciï¬cally,
if X1 , X2 , . . . are i.i.d. â¼ p(x), then
1
â log p(X1 , X2 , . . . , Xn ) â H (X) in probability.
n

(3.28)

Deï¬nition. The typical set A(n)
 is the set of sequences x1 , x2 , . . . , xn
satisfying
2ân(H (X)+) â¤ p(x1 , x2 , . . . , xn ) â¤ 2ân(H (X)â) .

(3.29)

Properties of the typical set
1.
2.
3.

ân(H Â±) .
If (x1 , x2 , . . . , xn ) â A(n)
 , then p(x1 , x2 , . . . , xn ) = 2
 (n) 
> 1 â  for n sufï¬ciently large.
Pr A
 (n) 
A  â¤ 2n(H (X)+) , where |A| denotes the number of elements in

set A.
.

Deï¬nition. an =bn means that

1
n

log abnn â 0 as n â â.

Smallest probable set. Let X1 , X2 , . . . , Xn be i.i.d. â¼ p(x), and for
Î´ < 12 , let BÎ´(n) â X n be the smallest set such that Pr{BÎ´(n) } â¥ 1 â Î´.
Then
.

|BÎ´(n) |=2nH .

(3.30)

PROBLEMS
3.1

Markovâs inequality and Chebyshevâs inequality
(a) (Markovâs inequality) For any nonnegative random variable X
and any t > 0, show that
EX
.
(3.31)
Pr {X â¥ t} â¤
t
Exhibit a random variable that achieves this inequality with
equality.
(b) (Chebyshevâs inequality) Let Y be a random variable with
mean Âµ and variance Ï 2 . By letting X = (Y â Âµ)2 , show that

PROBLEMS

65

for any  > 0,
Pr {|Y â Âµ| > } â¤

Ï2
.
2

(3.32)

(c) (Weak law of large numbers) Let Z1 , Z2 , . . . , Zn be a sequence
of i.i.d. random variables with mean Âµ and variance Ï 2 . Let
n

Z n = n1
Zi be the sample mean. Show that
i=1




Ï2
Pr Z n â Âµ >  â¤ 2 .
(3.33)
n



Thus, Pr Z n â Âµ >  â 0 as n â â. This is known as the
weak law of large numbers.
3.2

AEP and mutual information. Let (Xi , Yi ) be i.i.d. â¼ p(x, y). We
form the log likelihood ratio of the hypothesis that X and Y are
independent vs. the hypothesis that X and Y are dependent. What
is the limit of
p(X n )p(Y n )
1
log
?
n
p(X n , Y n )

3.3

Piece of cake.
A cake is sliced roughly in half, the largest piece being chosen each
time, the other pieces discarded. We will assume that a random cut
creates pieces of proportions

P =

( 23 , 13 ) with probability
( 25 , 35 ) with probability

3
4
1
4

Thus, for example, the ï¬rst cut (and choice of largest piece) may
and choosing from this piece
result in a piece of size 35 . Cutting

might reduce it to size 35 23 at time 2, and so on. How large, to
ï¬rst order in the exponent, is the piece of cake after n cuts?
3.4

AEP . Let
 Xi be iid â¼ p(x), x â {1, 2, . . . , m}. Let Âµ = EX and
H = â p(x) log p(x). Let An = {x n â X n : | â n1 log p(x n ) â

H | â¤ }. Let B n = {x n â X n : | n1 ni=1 Xi â Âµ| â¤ }.
(a) Does Pr{X n â An } ââ 1?
(b) Does Pr{X n â An â© B n } ââ 1?

66

ASYMPTOTIC EQUIPARTITION PROPERTY

(c) Show that |An â© B n | â¤ 2n(H +) for all n.
 
(d) Show that |An â© B n | â¥ 12 2n(H â) for n sufï¬ciently large.
3.5

Sets deï¬ned by probabilities. Let X1 , X2 , . . . be an i.i.d. sequence
of discrete random variables with entropy H (X). Let
Cn (t) = {x n â X n : p(x n ) â¥ 2ânt }
denote the subset of n-sequences with probabilities â¥ 2ânt .
(a) Show that |Cn (t)| â¤ 2nt .
(b) For what values of t does P ({X n â Cn (t)}) â 1?

3.6

AEP-like limit. Let X1 , X2 , . . . be i.i.d. drawn according to probability mass function p(x). Find
1

lim (p(X1 , X2 , . . . , Xn )) n .

nââ

3.7

AEP and source coding. A discrete memoryless source emits a
sequence of statistically independent binary digits with probabilities
p(1) = 0.005 and p(0) = 0.995. The digits are taken 100 at a time
and a binary codeword is provided for every sequence of 100 digits
containing three or fewer 1âs.
(a) Assuming that all codewords are the same length, ï¬nd the minimum length required to provide codewords for all sequences
with three or fewer 1âs.
(b) Calculate the probability of observing a source sequence for
which no codeword has been assigned.
(c) Use Chebyshevâs inequality to bound the probability of observing a source sequence for which no codeword has been assigned. Compare this bound with the actual probability computed
in part (b).

3.8

Products.
Let

ï£±
ï£´
ï£² 1, with probability
X=
2, with probability
ï£´
ï£³
3, with probability

1
2
1
4
1
4

Let X1 , X2 , . . . be drawn i.i.d. according to this distribution. Find
the limiting behavior of the product
1

(X1 X2 Â· Â· Â· Xn ) n .

PROBLEMS

67

3.9

AEP . Let X1 , X2 , . . . be independent, identically distributed random variables drawn according to the probabilitymass function
p(x), x â {1, 2, . . . , m}. Thus, p(x1 , x2 , . . . , xn ) = ni=1 p(xi ). We
know that â n1 log p(X
n1 , X2 , . . . , Xn ) â H (X) in probability. Let
q(x1 , x2 , . . . , xn ) = i=1 q(xi ), where q is another probability
mass function on {1, 2, . . . , m}.
(a) Evaluate lim â n1 log q(X1 , X2 , . . . , Xn ), where X1 , X2 , . . . are
i.i.d. â¼ p(x).
(b) Now evaluate the limit of the log likelihood ratio
q(X1 ,...,Xn )
1
n log p(X1 ,...,Xn ) when X1 , X2 , . . . are i.i.d. â¼ p(x). Thus, the
odds favoring q are exponentially small when p is true.

3.10

Random box size.
An n-dimensional rectangular box with sides
 X1 , X2 , X3 , . . . , Xn is
to be constructed. The volume is Vn = ni=1 Xi . The edge length l
1/n
of a n-cube with the same volume as the random box is l = Vn .
Let X1 , X2 , . . . be i.i.d. uniform random variables over the unit
1
1/n
interval [0, 1]. Find limnââ Vn and compare to (EVn ) n . Clearly,
the expected edge length does not capture the idea of the volume
of the box. The geometric mean, rather than the arithmetic mean,
characterizes the behavior of products.

3.11

Proof of Theorem 3.3.1. This problem shows that the size of the
smallest âprobableâ set is about 2nH . Let X1 , X2 , . . . , Xn be i.i.d.
â¼ p(x). Let BÎ´(n) â X n such that Pr(BÎ´(n) ) > 1 â Î´. Fix  < 12 .
(a) Given any two sets A, B such that Pr(A) > 1 â 1 and Pr(B) >
1 â 2 , show that Pr(A â© B) > 1 â 1 â 2 . Hence, Pr(A(n)
 â©
(n)
BÎ´ ) â¥ 1 â  â Î´.
(b) Justify the steps in the chain of inequalities
(n)
1 â  â Î´ â¤ Pr(A(n)
 â© BÎ´ )

=
p(x n )
(n)

(3.34)
(3.35)

(n)

A â©BÎ´

â¤



(n)

2ân(H â)

(3.36)

(n)

A â©BÎ´

(n) ân(H â)
= |A(n)
 â© BÎ´ |2

(3.37)

â¤ |BÎ´(n) |2ân(H â) .

(3.38)

(c) Complete the proof of the theorem.

68

3.12

ASYMPTOTIC EQUIPARTITION PROPERTY

Monotonic convergence of the empirical distribution.
Let pÌn denote the empirical probability mass function corresponding to X1 , X2 , . . . , Xn i.i.d. â¼ p(x), x â X. Speciï¬cally,
1
pÌn (x) =
I (Xi = x)
n
n

i=1

is the proportion of times that Xi = x in the ï¬rst n samples, where
I is the indicator function.
(a) Show for X binary that
ED(pÌ2n  p) â¤ ED(pÌn  p).
Thus, the expected relative entropy âdistanceâ from the empirical distribution to the true distribution decreases with sample
size. (Hint: Write pÌ2n = 12 pÌn + 12 pÌn and use the convexity
of D.)
(b) Show for an arbitrary discrete X that
ED(pÌn  p) â¤ ED(pÌnâ1  p).
(Hint: Write pÌn as the average of n empirical mass functions
with each of the n samples deleted in turn.)
3.13

Calculation of typical set . To clarify the notion of a typical set
(n)
A(n)
 and the smallest set of high probability BÎ´ , we will calculate
the set for a simple example. Consider a sequence of i.i.d. binary
random variables, X1 , X2 , . . . , Xn , where the probability that Xi =
1 is 0.6 (and therefore the probability that Xi = 0 is 0.4).
(a) Calculate H (X).
(b) With n = 25 and  = 0.1, which sequences fall in the typical set A(n)
 ? What is the probability of the typical set? How
many elements are there in the typical set? (This involves computation of a table of probabilities for sequences with k 1âs,
0 â¤ k â¤ 25, and ï¬nding those sequences that are in the typical set.)
(c) How many elements are there in the smallest set that has probability 0.9?
(d) How many elements are there in the intersection of the sets in
parts (b) and (c)? What is the probability of this intersection?

HISTORICAL NOTES

k
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

 
n
k
1
25
300
2300
12650
53130
177100
480700
1081575
2042975
3268760
4457400
5200300
5200300
4457400
3268760
2042975
1081575
480700
177100
53130
12650
2300
300
25
1

 
n k
p (1 â p)nâk
k
0.000000
0.000000
0.000000
0.000001
0.000007
0.000054
0.000227
0.001205
0.003121
0.013169
0.021222
0.077801
0.075967
0.267718
0.146507
0.575383
0.151086
0.846448
0.079986
0.970638
0.019891
0.997633
0.001937
0.999950
0.000047
0.000003

69

1
â log p(x n )
n
1.321928
1.298530
1.275131
1.251733
1.228334
1.204936
1.181537
1.158139
1.134740
1.111342
1.087943
1.064545
1.041146
1.017748
0.994349
0.970951
0.947552
0.924154
0.900755
0.877357
0.853958
0.830560
0.807161
0.783763
0.760364
0.736966

HISTORICAL NOTES
The asymptotic equipartition property (AEP) was ï¬rst stated by Shannon in his original 1948 paper [472], where he proved the result for
i.i.d. processes and stated the result for stationary ergodic processes.
McMillan [384] and Breiman [74] proved the AEP for ergodic ï¬nite
alphabet sources. The result is now referred to as the AEP or the ShannonâMcMillanâBreiman theorem. Chung [101] extended the theorem to
the case of countable alphabets and Moy [392], Perez [417], and Kieffer
[312] proved the L1 convergence when {Xi } is continuous valued and
ergodic. Barron [34] and Orey [402] proved almost sure convergence for
real-valued ergodic processes; a simple sandwich argument (Algoet and
Cover [20]) will be used in Section 16.8 to prove the general AEP.

CHAPTER 4

ENTROPY RATES
OF A STOCHASTIC PROCESS

The asymptotic equipartition property in Chapter 3 establishes that
nH (X) bits sufï¬ce on the average to describe n independent and identically distributed random variables. But what if the random variables
are dependent? In particular, what if the random variables form a stationary process? We will show, just as in the i.i.d. case, that the entropy
H (X1 , X2 , . . . , Xn ) grows (asymptotically) linearly with n at a rate H (X),
which we will call the entropy rate of the process. The interpretation of
H (X) as the best achievable data compression will await the analysis in
Chapter 5.
4.1

MARKOV CHAINS

A stochastic process {Xi } is an indexed sequence of random variables.
In general, there can be an arbitrary dependence among the random variables. The process is characterized by the joint probability mass functions
Pr{(X1 , X2 , . . . , Xn ) = (x1 , x2 , . . . , xn )} = p(x1 , x2 , . . . , xn ), (x1 , x2 , . . . ,
xn ) â Xn for n = 1, 2, . . . .
Deï¬nition A stochastic process is said to be stationary if the joint
distribution of any subset of the sequence of random variables is invariant
with respect to shifts in the time index; that is,
Pr{X1 = x1 , X2 = x2 , . . . , Xn = xn }
= Pr{X1+l = x1 , X2+l = x2 , . . . , Xn+l = xn } (4.1)
for every n and every shift l and for all x1 , x2 , . . . , xn â X.
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

71

72

ENTROPY RATES OF A STOCHASTIC PROCESS

A simple example of a stochastic process with dependence is one in
which each random variable depends only on the one preceding it and
is conditionally independent of all the other preceding random variables.
Such a process is said to be Markov.
Deï¬nition A discrete stochastic process X1 , X2 , . . . is said to be a
Markov chain or a Markov process if for n = 1, 2, . . . ,
Pr(Xn+1 = xn+1 |Xn = xn , Xnâ1 = xnâ1 , . . . , X1 = x1 )
= Pr (Xn+1 = xn+1 |Xn = xn )

(4.2)

for all x1 , x2 , . . . , xn , xn+1 â X.
In this case, the joint probability mass function of the random variables
can be written as
p(x1 , x2 , . . . , xn ) = p(x1 )p(x2 |x1 )p(x3 |x2 ) Â· Â· Â· p(xn |xnâ1 ).

(4.3)

Deï¬nition The Markov chain is said to be time invariant if the conditional probability p(xn+1 |xn ) does not depend on n; that is, for n =
1, 2, . . . ,
Pr{Xn+1 = b|Xn = a} = Pr{X2 = b|X1 = a}

for all a, b â X. (4.4)

We will assume that the Markov chain is time invariant unless otherwise
stated.
If {Xi } is a Markov chain, Xn is called the state at time n. A timeinvariant Markov chain is characterized by its initial state and a probability
transition matrix P = [Pij ], i, j â {1, 2, . . . , m}, where Pij = Pr{Xn+1 =
j |Xn = i}.
If it is possible to go with positive probability from any state of the
Markov chain to any other state in a ï¬nite number of steps, the Markov
chain is said to be irreducible. If the largest common factor of the lengths
of different paths from a state to itself is 1, the Markov chain is said to
aperiodic.
If the probability mass function of the random variable at time n is
p(xn ), the probability mass function at time n + 1 is

p(xn )Pxn xn+1 .
(4.5)
p(xn+1 ) =
xn

A distribution on the states such that the distribution at time n + 1 is the
same as the distribution at time n is called a stationary distribution. The

4.1 MARKOV CHAINS

73

stationary distribution is so called because if the initial state of a Markov
chain is drawn according to a stationary distribution, the Markov chain
forms a stationary process.
If the ï¬nite-state Markov chain is irreducible and aperiodic, the stationary distribution is unique, and from any starting distribution, the
distribution of Xn tends to the stationary distribution as n â â.
Example 4.1.1 Consider a two-state Markov chain with a probability
transition matrix


1âÎ±
Î±
(4.6)
P =
Î²
1âÎ²
as shown in Figure 4.1.
Let the stationary distribution be represented by a vector Âµ whose components are the stationary probabilities of states 1 and 2, respectively. Then
the stationary probability can be found by solving the equation ÂµP = Âµ
or, more simply, by balancing probabilities. For the stationary distribution,
the net probability ï¬ow across any cut set in the state transition graph is
zero. Applying this to Figure 4.1, we obtain
Âµ1 Î± = Âµ2 Î².

(4.7)

Since Âµ1 + Âµ2 = 1, the stationary distribution is
Âµ1 =

Î²
,
Î±+Î²

Âµ2 =

Î±
.
Î±+Î²

(4.8)

If the Markov chain has an initial state drawn according to the stationary
distribution, the resulting process will be stationary. The entropy of the
1âÎ±

Î±

State 1

Î²

FIGURE 4.1. Two-state Markov chain.

1âÎ²

State 2

74

ENTROPY RATES OF A STOCHASTIC PROCESS

state Xn at time n is

H (Xn ) = H

Î±
Î²
,
Î±+Î² Î±+Î²


.

(4.9)

However, this is not the rate at which entropy grows for H (X1 , X2 , . . . ,
Xn ). The dependence among the Xi âs will take a steady toll.
4.2

ENTROPY RATE

If we have a sequence of n random variables, a natural question to ask
is: How does the entropy of the sequence grow with n? We deï¬ne the
entropy rate as this rate of growth as follows.
Deï¬nition The entropy of a stochastic process {Xi } is deï¬ned by
1
H (X1 , X2 , . . . , Xn )
nââ n

H (X) = lim

(4.10)

when the limit exists.
We now consider some simple examples of stochastic processes and
their corresponding entropy rates.
1. Typewriter.
Consider the case of a typewriter that has m equally likely output
letters. The typewriter can produce mn sequences of length n, all
of them equally likely. Hence H (X1 , X2 , . . . , Xn ) = log mn and the
entropy rate is H (X) = log m bits per symbol.
2. X1 , X2 , . . . are i.i.d. random variables. Then
H (X) = lim

H (X1 , X2 , . . . , Xn )
nH (X1 )
= lim
= H (X1 ),
n
n
(4.11)

which is what one would expect for the entropy rate per symbol.
3. Sequence of independent but not identically distributed random variables. In this case,
n

H (X1 , X2 , . . . , Xn ) =
H (Xi ),
(4.12)
i=1

but the H (Xi )âs are all not equal. We can choose
a sequence of disH (Xi ) does not
tributions on X1 , X2 , . . . such that the limit of n1
exist. An example of such a sequence is a random binary sequence

4.2

ENTROPY RATE

75

where pi = P (Xi = 1) is not constant but a function of i, chosen
carefully so that the limit in (4.10) does not exist. For example, let

0.5 if 2k < log log i â¤ 2k + 1,
(4.13)
pi =
0
if 2k + 1 < log log i â¤ 2k + 2
for k = 0, 1, 2, . . . .
Then there are arbitrarily long stretches where H (Xi ) = 1, followed
by exponentially longer segments where H (Xi ) = 0. Hence, the running average of the H (Xi ) will oscillate between 0 and 1 and will
not have a limit. Thus, H (X) is not deï¬ned for this process.
We can also deï¬ne a related quantity for entropy rate:
H  (X) = lim H (Xn |Xnâ1 , Xnâ2 , . . . , X1 )
nââ

(4.14)

when the limit exists.
The two quantities H (X) and H  (X) correspond to two different notions
of entropy rate. The ï¬rst is the per symbol entropy of the n random variables, and the second is the conditional entropy of the last random variable
given the past. We now prove the important result that for stationary processes both limits exist and are equal.
Theorem 4.2.1 For a stationary stochastic process, the limits in (4.10)
and (4.14) exist and are equal:
H (X) = H  (X).

(4.15)

We ï¬rst prove that lim H (Xn |Xnâ1 , . . . , X1 ) exists.
Theorem 4.2.2 For a stationary stochastic process, H (Xn |Xnâ1 , . . . ,
X1 ) is nonincreasing in n and has a limit H  (X).
Proof
H (Xn+1 |X1 , X2 , . . . , Xn ) â¤ H (Xn+1 |Xn , . . . , X2 )

(4.16)

= H (Xn |Xnâ1 , . . . , X1 ),

(4.17)

where the inequality follows from the fact that conditioning reduces entropy and the equality follows from the stationarity of the process. Since
H (Xn |Xnâ1 , . . . , X1 ) is a decreasing sequence of nonnegative numbers,

it has a limit, H  (X).

76

ENTROPY RATES OF A STOCHASTIC PROCESS

We now use the following simple result from analysis.
Theorem 4.2.3
bn â a.

If an â a and bn =

(CesaÌro mean)

1
n

n

i=1 ai ,

then

Proof: (Informal outline). Since most of the terms in the sequence {ak }
are eventually close to a, then bn , which is the average of the ï¬rst n terms,
is also eventually close to a.
Formal Proof: Let  > 0. Since an â a, there exists a number N ()
such that |an â a| â¤  for all n â¥ N (). Hence,
 n

1 



(ai â a)
|bn â a| = 
(4.18)
n

i=1

1
|(ai â a)|
â¤
n

(4.19)

N ()
1
n â N ()
â¤
|ai â a| +

n
n

(4.20)

n

i=1

i=1

â¤

N ()
1
|ai â a| + 
n

(4.21)

i=1

for all n â¥ N (). Since the ï¬rst term goes to 0 as n â â, we can make
|bn â a| â¤ 2 by taking n large enough. Hence, bn â a as n â â. 
Proof of Theorem 4.2.1: By the chain rule,
1
H (X1 , X2 , . . . , Xn )
=
H (Xi |Xiâ1 , . . . , X1 ),
n
n
n

(4.22)

i=1

that is, the entropy rate is the time average of the conditional entropies.
But we know that the conditional entropies tend to a limit H  . Hence, by
Theorem 4.2.3, their running average has a limit, which is equal to the
limit H  of the terms. Thus, by Theorem 4.2.2,
H (X1 , X2 , . . . , Xn )
= lim H (Xn |Xnâ1 , . . . , X1 )
n
 (4.23)
= H  (X).

H (X) = lim

4.2

77

ENTROPY RATE

The signiï¬cance of the entropy rate of a stochastic process arises from
the AEP for a stationary ergodic process. We prove the general AEP in
Section 16.8, where we show that for any stationary ergodic process,
1
â log p(X1 , X2 , . . . , Xn ) â H (X)
n

(4.24)

with probability 1. Using this, the theorems of Chapter 3 can easily be
extended to a general stationary ergodic process. We can deï¬ne a typical
set in the same way as we did for the i.i.d. case in Chapter 3. By the
same arguments, we can show that the typical set has a probability close
to 1 and that there are about 2nH (X ) typical sequences of length n, each
with probability about 2ânH (X ) . We can therefore represent the typical
sequences of length n using approximately nH (X) bits. This shows the
signiï¬cance of the entropy rate as the average description length for a
stationary ergodic process.
The entropy rate is well deï¬ned for all stationary processes. The entropy
rate is particularly easy to calculate for Markov chains.
Markov Chains. For a stationary Markov chain, the entropy rate is
given by
H (X) = H  (X) = lim H (Xn |Xnâ1 , . . . , X1 ) = lim H (Xn |Xnâ1 )
= H (X2 |X1 ),

(4.25)

where the conditional entropy is calculated using the given stationary
distribution. Recall that the stationary distribution Âµ is the solution of the
equations

Âµj =
Âµi Pij for all j.
(4.26)
i

We express the conditional entropy explicitly in the following theorem.
Theorem 4.2.4 Let {Xi } be a stationary Markov chain with stationary distribution Âµ and transition matrix P . Let X1 â¼ Âµ. Then the entropy
rate is

Âµi Pij log Pij .
(4.27)
H (X) = â
ij

Proof: H (X) = H (X2 |X1 ) =



i Âµi

	



âP
log
P
ij
ij .
j



78

ENTROPY RATES OF A STOCHASTIC PROCESS

Example 4.2.1 (Two-state Markov chain)
state Markov chain in Figure 4.1 is
H (X) = H (X2 |X1 ) =

The entropy rate of the two-

Î±
Î²
H (Î±) +
H (Î²).
Î±+Î²
Î±+Î²

(4.28)

Remark If the Markov chain is irreducible and aperiodic, it has a unique
stationary distribution on the states, and any initial distribution tends to
the stationary distribution as n â â. In this case, even though the initial
distribution is not the stationary distribution, the entropy rate, which is
deï¬ned in terms of long-term behavior, is H (X), as deï¬ned in (4.25) and
(4.27).
4.3 EXAMPLE: ENTROPY RATE OF A RANDOM WALK
ON A WEIGHTED GRAPH
As an example of a stochastic process, let us consider a random walk on
a connected graph (Figure 4.2). Consider a graph with m nodes labeled
{1, 2, . . . , m}, with weight Wij â¥ 0 on the edge joining node i to node
j . (The graph is assumed to be undirected, so that Wij = Wj i . We set
Wij = 0 if there is no edge joining nodes i and j .)
A particle walks randomly from node to node in this graph. The random walk {Xn }, Xn â {1, 2, . . . , m}, is a sequence of vertices of the
graph. Given Xn = i, the next vertex j is chosen from among the nodes
connected to node i with a probability proportional
to the weight of the

edge connecting i to j . Thus, Pij = Wij / k Wik .
2

1

3

4

5

FIGURE 4.2. Random walk on a graph.

4.3

EXAMPLE: ENTROPY RATE OF A RANDOM WALK ON A WEIGHTED GRAPH

79

In this case, the stationary distribution has a surprisingly simple form,
which we will guess and verify. The stationary distribution for this Markov
chain assigns probability to node i proportional to the total weight of the
edges emanating from node i. Let

Wij
(4.29)
Wi =
j

be the total weight of edges emanating from node i, and let

Wij
W =

(4.30)

i,j :j >i


be the sum of the weights of all the edges. Then i Wi = 2W .
We now guess that the stationary distribution is
Âµi =

Wi
.
2W

(4.31)

We verify that this is the stationary distribution by checking that ÂµP = Âµ.
Here


Âµi Pij =

i

 Wi Wij
2W Wi

(4.32)

 1
Wij
2W

(4.33)

i

=

i

Wj
2W
= Âµj .

=

(4.34)
(4.35)

Thus, the stationary probability of state i is proportional to the weight of
edges emanating from node i. This stationary distribution has an interesting property of locality: It depends only on the total weight and the
weight of edges connected to the node and hence does not change if the
weights in some other part of the graph are changed while keeping the
total weight constant. We can now calculate the entropy rate as
H (X) = H (X2 |X1 )
 
=â
Âµi
Pij log Pij
i

j

(4.36)
(4.37)

80

ENTROPY RATES OF A STOCHASTIC PROCESS

=â

 Wi  Wij
Wij
log
2W
Wi
Wi
i

=â

j

  Wij
i

j

2W

  Wij

log

Wij
Wi

Wij   Wij
Wi
+
log
2W
2W
2W
2W
i
j
i
j




Wij
Wi
,... â H ...,
,... .
= H ...,
2W
2W

=â

(4.38)

log

(4.39)

(4.40)

(4.41)

If all the edges have equal weight, the stationary distribution puts
weight Ei /2E on node i, where Ei is the number of edges emanating
from node i and E is the total number of edges in the graph. In this case,
the entropy rate of the random walk is


Em
E1 E2
H (X) = log(2E) â H
,
,...,
.
(4.42)
2E 2E
2E
This answer for the entropy rate is so simple that it is almost misleading. Apparently, the entropy rate, which is the average transition entropy,
depends only on the entropy of the stationary distribution and the total
number of edges.
Example 4.3.1 (Random walk on a chessboard ) Let a king move at
random on an 8 Ã 8 chessboard. The king has eight moves in the interior,
ï¬ve moves at the edges, and three moves at the corners. Using this and
8
the preceding results, the stationary probabilities are, respectively, 420
,
5
3
,
and
,
and
the
entropy
rate
is
0.92
log
8.
The
factor
of
0.92
is
due
420
420
to edge effects; we would have an entropy rate of log 8 on an inï¬nite
chessboard.
Similarly, we can ï¬nd the entropy rate of rooks (log 14 bits, since the
rook always has 14 possible moves), bishops, and queens. The queen
combines the moves of a rook and a bishop. Does the queen have more
or less freedom than the pair?
Remark It is easy to see that a stationary random walk on a graph is
time-reversible; that is, the probability of any sequence of states is the

81

4.4 SECOND LAW OF THERMODYNAMICS

same forward or backward:
Pr(X1 = x1 , X2 = x2 , . . . , Xn = xn )
= Pr(Xn = x1 , Xnâ1 = x2 , . . . , X1 = xn ).

(4.43)

Rather surprisingly, the converse is also true; that is, any time-reversible
Markov chain can be represented as a random walk on an undirected
weighted graph.
4.4

SECOND LAW OF THERMODYNAMICS

One of the basic laws of physics, the second law of thermodynamics,
states that the entropy of an isolated system is nondecreasing. We now
explore the relationship between the second law and the entropy function
that we deï¬ned earlier in this chapter.
In statistical thermodynamics, entropy is often deï¬ned as the log of
the number of microstates in the system. This corresponds exactly to our
notion of entropy if all the states are equally likely. But why does entropy
increase?
We model the isolated system as a Markov chain with transitions obeying the physical laws governing the system. Implicit in this assumption is
the notion of an overall state of the system and the fact that knowing the
present state, the future of the system is independent of the past. In such
a system we can ï¬nd four different interpretations of the second law. It
may come as a shock to ï¬nd that the entropy does not always increase.
However, relative entropy always decreases.
1. Relative entropy D(Âµn ||Âµn ) decreases with n. Let Âµn and Âµn be two
probability distributions on the state space of a Markov chain at time
n, and let Âµn+1 and Âµn+1 be the corresponding distributions at time
n + 1. Let the corresponding joint mass functions be denoted by
p and q. Thus, p(xn , xn+1 ) = p(xn )r(xn+1 |xn ) and q(xn , xn+1 ) =
q(xn )r(xn+1 |xn ), where r(Â·|Â·) is the probability transition function
for the Markov chain. Then by the chain rule for relative entropy,
we have two expansions:
D(p(xn , xn+1 )||q(xn , xn+1 )) = D(p(xn )||q(xn ))
+ D(p(xn+1 |xn )||q(xn+1 |xn ))

82

ENTROPY RATES OF A STOCHASTIC PROCESS

= D(p(xn+1 )||q(xn+1 ))
+ D(p(xn |xn+1 )||q(xn |xn+1 )).
Since both p and q are derived from the Markov chain, the conditional probability mass functions p(xn+1 |xn ) and q(xn+1 |xn ) are
both equal to r(xn+1 |xn ), and hence D(p(xn+1 |xn )||q(xn+1 |xn )) = 0.
Now using the nonnegativity of D(p(xn |xn+1 )||q(xn |xn+1 )) (Corollary to Theorem 2.6.3), we have
D(p(xn )||q(xn )) â¥ D(p(xn+1 )||q(xn+1 ))

(4.44)

D(Âµn ||Âµn ) â¥ D(Âµn+1 ||Âµn+1 ).

(4.45)

or

Consequently, the distance between the probability mass functions
is decreasing with time n for any Markov chain.
An example of one interpretation of the preceding inequality is
to suppose that the tax system for the redistribution of wealth is
the same in Canada and in England. Then if Âµn and Âµn represent
the distributions of wealth among people in the two countries, this
inequality shows that the relative entropy distance between the two
distributions decreases with time. The wealth distributions in Canada
and England become more similar.
2. Relative entropy D(Âµn ||Âµ) between a distribution Âµn on the states at
time n and a stationary distribution Âµ decreases with n. In (4.45),
Âµn is any distribution on the states at time n. If we let Âµn be any
stationary distribution Âµ, the distribution Âµn+1 at the next time is
also equal to Âµ. Hence,
D(Âµn ||Âµ) â¥ D(Âµn+1 ||Âµ),

(4.46)

which implies that any state distribution gets closer and closer to
each stationary distribution as time passes. The sequence D(Âµn ||Âµ)
is a monotonically nonincreasing nonnegative sequence and must
therefore have a limit. The limit is zero if the stationary distribution
is unique, but this is more difï¬cult to prove.
3. Entropy increases if the stationary distribution is uniform. In general, the fact that the relative entropy decreases does not imply that
the entropy increases. A simple counterexample is provided by any
Markov chain with a nonuniform stationary distribution. If we start

83

4.4 SECOND LAW OF THERMODYNAMICS

this Markov chain from the uniform distribution, which already is
the maximum entropy distribution, the distribution will tend to the
stationary distribution, which has a lower entropy than the uniform.
Here, the entropy decreases with time.
If, however, the stationary distribution is the uniform distribution,
we can express the relative entropy as
D(Âµn ||Âµ) = log |X| â H (Âµn ) = log |X| â H (Xn ).

(4.47)

In this case the monotonic decrease in relative entropy implies a
monotonic increase in entropy. This is the explanation that ties in
most closely with statistical thermodynamics, where all the microstates are equally likely. We now characterize processes having a
uniform stationary distribution.
Deï¬nition A probability transition matrix [Pij ], Pij = Pr{Xn+1 =
j |Xn = i}, is called doubly stochastic if


Pij = 1,

j = 1, 2, . . .

(4.48)

Pij = 1,

i = 1, 2, . . . .

(4.49)

i

and


j

Remark The uniform distribution is a stationary distribution of P if
and only if the probability transition matrix is doubly stochastic (see
Problem 4.1).
4. The conditional entropy H (Xn |X1 ) increases with n for a stationary Markov process. If the Markov process is stationary, H (Xn ) is
constant. So the entropy is nonincreasing. However, we will prove
that H (Xn |X1 ) increases with n. Thus, the conditional uncertainty
of the future increases. We give two alternative proofs of this result.
First, we use the properties of entropy,
H (Xn |X1 ) â¥ H (Xn |X1 , X2 ) (conditioning reduces entropy)
(4.50)
= H (Xn |X2 ) (by Markovity)

(4.51)

= H (Xnâ1 |X1 ) (by stationarity).

(4.52)

84

ENTROPY RATES OF A STOCHASTIC PROCESS

Alternatively, by an application of the data-processing inequality to
the Markov chain X1 â Xnâ1 â Xn , we have
I (X1 ; Xnâ1 ) â¥ I (X1 ; Xn ).

(4.53)

Expanding the mutual informations in terms of entropies, we have
H (Xnâ1 ) â H (Xnâ1 |X1 ) â¥ H (Xn ) â H (Xn |X1 ).

(4.54)

By stationarity, H (Xnâ1 ) = H (Xn ), and hence we have
H (Xnâ1 |X1 ) â¤ H (Xn |X1 ).

(4.55)

[These techniques can also be used to show that H (X0 |Xn ) is
increasing in n for any Markov chain.]
5. Shufï¬es increase entropy. If T is a shufï¬e (permutation) of a deck
of cards and X is the initial (random) position of the cards in the
deck, and if the choice of the shufï¬e T is independent of X, then
H (T X) â¥ H (X),

(4.56)

where T X is the permutation of the deck induced by the shufï¬e T
on the initial permutation X. Problem 4.3 outlines a proof.
4.5

FUNCTIONS OF MARKOV CHAINS

Here is an example that can be very difï¬cult if done the wrong
way. It illustrates the power of the techniques developed so far. Let
X1 , X2 , . . . , Xn , . . . be a stationary Markov chain, and let Yi = Ï(Xi ) be
a process each term of which is a function of the corresponding state
in the Markov chain. What is the entropy rate H (Y)? Such functions of
Markov chains occur often in practice. In many situations, one has only
partial information about the state of the system. It would simplify matters
greatly if Y1 , Y2 , . . . , Yn also formed a Markov chain, but in many cases,
this is not true. Since the Markov chain is stationary, so is Y1 , Y2 , . . . , Yn ,
and the entropy rate is well deï¬ned. However, if we wish to compute
H (Y), we might compute H (Yn |Ynâ1 , . . . , Y1 ) for each n and ï¬nd the
limit. Since the convergence can be arbitrarily slow, we will never know
how close we are to the limit. (We canât look at the change between the
values at n and n + 1, since this difference may be small
 1 even when we
are far away from the limitâconsider, for example,
n .)

4.5 FUNCTIONS OF MARKOV CHAINS

85

It would be useful computationally to have upper and lower bounds converging to the limit from above and below. We can halt the computation
when the difference between upper and lower bounds is small, and we
will then have a good estimate of the limit.
We already know that H (Yn |Ynâ1 , . . . , Y1 ) converges monotonically to H (Y) from above. For a lower bound, we will use
H (Yn |Ynâ1 , . . . , Y1 , X1 ). This is a neat trick based on the idea that X1
contains as much information about Yn as Y1 , Y0 , Yâ1 , . . . .
Lemma 4.5.1
H (Yn |Ynâ1 , . . . , Y2 , X1 ) â¤ H (Y).
Proof: We have for k = 1, 2, . . . ,
( a)
H (Yn |Ynâ1 , . . . , Y2 , X1 ) = H (Yn |Ynâ1 , . . . , Y2 , Y1 , X1 )

(4.57)

(4.58)

(b)

= H (Yn |Ynâ1 , . . . , Y1 , X1 , X0 , Xâ1 , . . . , Xâk )
(4.59)

( c)

= H (Yn |Ynâ1 , . . . , Y1 , X1 , X0 , Xâ1 , . . . ,
Xâk , Y0 , . . . , Yâk )

(d)

â¤ H (Yn |Ynâ1 , . . . , Y1 , Y0 , . . . , Yâk )

( e)

= H (Yn+k+1 |Yn+k , . . . , Y1 ),

(4.60)
(4.61)
(4.62)

where (a) follows from that fact that Y1 is a function of X1 , and (b) follows
from the Markovity of X, (c) follows from the fact that Yi is a function
of Xi , (d) follows from the fact that conditioning reduces entropy, and (e)
follows by stationarity. Since the inequality is true for all k, it is true in
the limit. Thus,
H (Yn |Ynâ1 , . . . , Y1 , X1 ) â¤ lim H (Yn+k+1 |Yn+k , . . . , Y1 )
k

= H (Y).



(4.63)
(4.64)

The next lemma shows that the interval between the upper and the
lower bounds decreases in length.
Lemma 4.5.2
H (Yn |Ynâ1 , . . . , Y1 ) â H (Yn |Ynâ1 , . . . , Y1 , X1 ) â 0.

(4.65)

86

ENTROPY RATES OF A STOCHASTIC PROCESS

Proof: The interval length can be rewritten as
H (Yn |Ynâ1 , . . . , Y1 ) â H (Yn |Ynâ1 , . . . , Y1 , X1 )
= I (X1 ; Yn |Ynâ1 , . . . , Y1 ).

(4.66)

By the properties of mutual information,
I (X1 ; Y1 , Y2 , . . . , Yn ) â¤ H (X1 ),

(4.67)

and I (X1 ; Y1 , Y2 , . . . , Yn ) increases with n. Thus, lim I (X1 ; Y1 , Y2 , . . . ,
Yn ) exists and
lim I (X1 ; Y1 , Y2 , . . . , Yn ) â¤ H (X1 ).

nââ

(4.68)

By the chain rule,
H (X) â¥ lim I (X1 ; Y1 , Y2 , . . . , Yn )
nââ

= lim

nââ

=

â


n


I (X1 ; Yi |Yiâ1 , . . . , Y1 )

(4.69)
(4.70)

i=1

I (X1 ; Yi |Yiâ1 , . . . , Y1 ).

(4.71)

i=1

Since this inï¬nite sum is ï¬nite and the terms are nonnegative, the terms
must tend to 0; that is,
lim I (X1 ; Yn |Ynâ1 , . . . , Y1 ) = 0,
which proves the lemma.

(4.72)


Combining Lemmas 4.5.1 and 4.5.2, we have the following theorem.
Theorem 4.5.1 If X1 , X2 , . . . , Xn form a stationary Markov chain, and
Yi = Ï(Xi ), then
H (Yn |Ynâ1 , . . . , Y1 , X1 ) â¤ H (Y) â¤ H (Yn |Ynâ1 , . . . , Y1 )

(4.73)

and
lim H (Yn |Ynâ1 , . . . , Y1 , X1 ) = H (Y) = lim H (Yn |Ynâ1 , . . . , Y1 ). (4.74)
In general, we could also consider the case where Yi is a stochastic
function (as opposed to a deterministic function) of Xi . Consider a Markov

SUMMARY

87

process X1 , X2 , . . . , Xn , and deï¬ne a new process Y1 , Y2 , . . . , Yn , where
each Yi is drawn according to p(yi |xi ), conditionally independent of all
the other Xj , j = i; that is,
p(x , y ) = p(x1 )
n

n

nâ1


p(xi+1 |xi )

i=1

n


p(yi |xi ).

(4.75)

i=1

Such a process, called a hidden Markov model (HMM), is used extensively
in speech recognition, handwriting recognition, and so on. The same argument as that used above for functions of a Markov chain carry over to
hidden Markov models, and we can lower bound the entropy rate of a
hidden Markov model by conditioning it on the underlying Markov state.
The details of the argument are left to the reader.

SUMMARY
Entropy rate. Two deï¬nitions of entropy rate for a stochastic process
are
1
H (X1 , X2 , . . . , Xn ),
nââ n
H  (X) = lim H (Xn |Xnâ1 , Xnâ2 , . . . , X1 ).
H (X) = lim

nââ

(4.76)
(4.77)

For a stationary stochastic process,
H (X) = H  (X).
Entropy rate of a stationary Markov chain

Âµi Pij log Pij .
H (X) = â

(4.78)

(4.79)

ij

Second law of thermodynamics. For a Markov chain:
1. Relative entropy D(Âµn ||Âµn ) decreases with time
2. Relative entropy D(Âµn ||Âµ) between a distribution and the stationary
distribution decreases with time.
3. Entropy H (Xn ) increases if the stationary distribution is uniform.

88

ENTROPY RATES OF A STOCHASTIC PROCESS

4. The conditional entropy H (Xn |X1 ) increases with time for a stationary Markov chain.
5. The conditional entropy H (X0 |Xn ) of the initial condition X0 increases for any Markov chain.
Functions of a Markov chain. If X1 , X2 , . . . , Xn form a stationary
Markov chain and Yi = Ï(Xi ), then
H (Yn |Ynâ1 , . . . , Y1 , X1 ) â¤ H (Y) â¤ H (Yn |Ynâ1 , . . . , Y1 )

(4.80)

and
lim H (Yn |Ynâ1 , . . . , Y1 , X1 ) = H (Y) = lim H (Yn |Ynâ1 , . . . , Y1 ).

nââ

nââ

(4.81)

PROBLEMS
4.1

Doubly stochastic matrices. An n Ã n 
matrix P = [Pij ] is said
â¥
0
and
to
be
doubly
stochastic
if
P
ij
j Pij = 1 for all i and

i Pij = 1 for all j . An n Ã n matrix P is said to be a permutation matrix if it is doubly stochastic and there is precisely one
Pij = 1 in each row and each column. It can be shown that every
doubly stochastic matrix can be written as the convex combination
of permutation matrices.

(a) Let at = (a1 , a2 , . . . , an ), ai â¥ 0,
ai = 1, be a probability
vector. Let b = aP , where P is doubly stochastic. Show that b
is a probability vector and that H (b1 , b2 , . . . , bn ) â¥ H (a1 , a2 ,
. . . , an ). Thus, stochastic mixing increases entropy.
(b) Show that a stationary distribution Âµ for a doubly stochastic
matrix P is the uniform distribution.
(c) Conversely, prove that if the uniform distribution is a stationary
distribution for a Markov transition matrix P , then P is doubly
stochastic.

4.2

Timeâs arrow .
Prove that

Let {Xi }â
i=ââ be a stationary stochastic process.

H (X0 |Xâ1 , Xâ2 , . . . , Xân ) = H (X0 |X1 , X2 , . . . , Xn ).

PROBLEMS

89

In other words, the present has a conditional entropy given the past
equal to the conditional entropy given the future. This is true even
though it is quite easy to concoct stationary random processes for
which the ï¬ow into the future looks quite different from the ï¬ow
into the past. That is, one can determine the direction of time by
looking at a sample function of the process. Nonetheless, given
the present state, the conditional uncertainty of the next symbol in
the future is equal to the conditional uncertainty of the previous
symbol in the past.
4.3

Shufï¬es increase entropy. Argue that for any distribution on shufï¬es T and any distribution on card positions X that
H (T X) â¥ H (T X|T )

(4.82)

= H (T â1 T X|T )

(4.83)

= H (X|T )

(4.84)

= H (X)

(4.85)

if X and T are independent.
4.4

Second law of thermodynamics. Let X1 , X2 , X3 , . . . be a stationary ï¬rst-order Markov chain. In Section 4.4 it was shown that
H (Xn | X1 ) â¥ H (Xnâ1 | X1 ) for n = 2, 3, . . . . Thus, conditional
uncertainty about the future grows with time. This is true although
the unconditional uncertainty H (Xn ) remains constant. However,
show by example that H (Xn |X1 = x1 ) does not necessarily grow
with n for every x1 .

4.5

Entropy of a random tree. Consider the following method of generating a random tree with n nodes. First expand the root node:

Then expand one of the two terminal nodes at random:

At time k, choose one of the k â 1 terminal nodes according to a
uniform distribution and expand it. Continue until n terminal nodes

90

ENTROPY RATES OF A STOCHASTIC PROCESS

have been generated. Thus, a sequence leading to a ï¬ve-node tree
might look like this:

Surprisingly, the following method of generating random trees
yields the same probability distribution on trees with n terminal nodes. First choose an integer N1 uniformly distributed on
{1, 2, . . . , n â 1}. We then have the picture

N1

n â N1

Then choose an integer N2 uniformly distributed over
{1, 2, . . . , N1 â 1}, and independently choose another integer N3
uniformly over {1, 2, . . . , (n â N1 ) â 1}. The picture is now

N2

N1 â N2

N3

n â N1 â N3

Continue the process until no further subdivision can be made.
(The equivalence of these two tree generation schemes follows, for
example, from Polyaâs urn model.)
Now let Tn denote a random n-node tree generated as described. The
probability distribution on such trees seems difï¬cult to describe, but
we can ï¬nd the entropy of this distribution in recursive form.
First some examples. For n = 2, we have only one tree. Thus,
H (T2 ) = 0. For n = 3, we have two equally probable trees:

PROBLEMS

91

Thus, H (T3 ) = log 2. For n = 4, we have ï¬ve possible trees, with
probabilities 13 , 16 , 16 , 16 , 16 .
Now for the recurrence relation. Let N1 (Tn ) denote the number of
terminal nodes of Tn in the right half of the tree. Justify each of
the steps in the following:
(a)

H (Tn ) = H (N1 , Tn )

(4.86)

(b)

= H (N1 ) + H (Tn |N1 )

(4.87)

(c)

= log(n â 1) + H (Tn |N1 )

(4.88)

1 
= log(n â 1) +
(H (Tk ) + H (Tnâk ))
nâ1

(4.89)

nâ1

(d)

k=1

(e)

2 
H (Tk )
nâ1
nâ1

= log(n â 1) +

(4.90)

k=1

= log(n â 1) +

2
nâ1

nâ1


Hk .

(4.91)

k=1

(f) Use this to show that
(n â 1)Hn = nHnâ1 + (n â 1) log(n â 1) â (n â 2) log(n â 2)
(4.92)
or

Hn
Hnâ1
=
+ cn
(4.93)
n
nâ1

for appropriately deï¬ned cn . Since cn = c < â, you have proved
that n1 H (Tn ) converges to a constant. Thus, the expected number of
bits necessary to describe the random tree Tn grows linearly with n.
4.6

Monotonicity of entropy per element . For a stationary stochastic
process X1 , X2 , . . . , Xn , show that
(a)
H (X1 , X2 , . . . , Xnâ1 )
H (X1 , X2 , . . . , Xn )
â¤
.
(4.94)
n
nâ1
(b)

H (X1 , X2 , . . . , Xn )
â¥ H (Xn |Xnâ1 , . . . , X1 ).
n

(4.95)

92

4.7

ENTROPY RATES OF A STOCHASTIC PROCESS

Entropy rates of Markov chains
(a) Find the entropy rate of the two-state Markov chain with transition matrix


1 â p01
p01
P =
.
p10
1 â p10
(b) What values of p01 , p10 maximize the entropy rate?
(c) Find the entropy rate of the two-state Markov chain with transition matrix


1âp p
P =
.
1
0
(d) Find the maximum value of the entropy rate of the Markov
chain of part (c). We expect that the maximizing value of p
should be less than 12 , since the 0 state permits more information to be generated than the 1 state.
(e) Let N (t) be the number of allowable state sequences of length t
for the Markov chain of part (c). Find N (t) and calculate
H0 = lim

tââ

1
log N (t).
t

[Hint: Find a linear recurrence that expresses N (t) in terms
of N (t â 1) and N (t â 2). Why is H0 an upper bound on the
entropy rate of the Markov chain? Compare H0 with the maximum entropy found in part (d).]
4.8

Maximum entropy process. A discrete memoryless source has the
alphabet {1, 2}, where the symbol 1 has duration 1 and the symbol 2 has duration 2. The probabilities of 1 and 2 are p1 and p2 ,
respectively. Find the value of p1 that maximizes the source entropy
(X)
. What is the maximum value H (X)?
per unit time H (X) = HET

4.9

Initial conditions.

Show, for a Markov chain, that
H (X0 |Xn ) â¥ H (X0 |Xnâ1 ).

Thus, initial conditions X0 become more difï¬cult to recover as the
future Xn unfolds.
4.10

Pairwise independence. Let X1 , X2 , . . . , Xnâ1 be i.i.d. random
variables taking values in {0, 1}, with Pr{Xi = 1} = 12 . Let Xn = 1

if nâ1
i=1 Xi is odd and Xn = 0 otherwise. Let n â¥ 3.

93

PROBLEMS

(a) Show that Xi and Xj are independent for i = j , i, j â {1, 2,
. . . , n}.
(b) Find H (Xi , Xj ) for i = j .
(c) Find H (X1 , X2 , . . . , Xn ). Is this equal to nH (X1 )?
4.11

Stationary processes. Let . . . , Xâ1 , X0 , X1 , . . . be a stationary
(not necessarily Markov) stochastic process. Which of the following statements are true? Prove or provide a counterexample.
(a) H (Xn |X0 ) = H (Xân |X0 ) .
(b) H (Xn |X0 ) â¥ H (Xnâ1 |X0 ) .
(c) H (Xn |X1 , X2 , . . . , Xnâ1 , Xn+1 ) is nonincreasing in n.
(d) H (Xn |X1 , . . . , Xnâ1 , Xn+1 , . . . , X2n ) is nonincreasing in n.

4.12

Entropy rate of a dog looking for a bone. A dog walks on the
integers, possibly reversing direction at each step with probability
p = 0.1. Let X0 = 0. The ï¬rst step is equally likely to be positive
or negative. A typical walk might look like this:
(X0 , X1 , . . .) = (0, â1, â2, â3, â4, â3, â2, â1, 0, 1, . . .).
(a) Find H (X1 , X2 , . . . , Xn ).
(b) Find the entropy rate of the dog.
(c) What is the expected number of steps that the dog takes before
reversing direction?

4.13

The past has little to say about the future. For a stationary stochastic process X1 , X2 , . . . , Xn , . . . , show that
lim

nââ

1
I (X1 , X2 , . . . , Xn ; Xn+1 , Xn+2 , . . . , X2n ) = 0.
2n

(4.96)

Thus, the dependence between adjacent n-blocks of a stationary
process does not grow linearly with n.
4.14

Functions of a stochastic process
(a) Consider a stationary stochastic process X1 , X2 , . . . , Xn , and
let Y1 , Y2 , . . . , Yn be deï¬ned by
Yi = Ï(Xi ),

i = 1, 2, . . .

(4.97)

for some function Ï. Prove that
H (Y) â¤ H (X).

(4.98)

94

ENTROPY RATES OF A STOCHASTIC PROCESS

(b) What is the relationship between the entropy rates H (Z) and
H (X) if
(4.99)
Zi = Ï(Xi , Xi+1 ), i = 1, 2, . . .
for some function Ï?
4.15

Entropy rate. Let {Xi } be a discrete stationary stochastic process
with entropy rate H (X). Show that
1
H (Xn , . . . , X1 | X0 , Xâ1 , . . . , Xâk ) â H (X)
n

(4.100)

for k = 1, 2, . . ..
4.16

Entropy rate of constrained sequences. In magnetic recording, the
mechanism of recording and reading the bits imposes constraints
on the sequences of bits that can be recorded. For example, to
ensure proper synchronization, it is often necessary to limit the
length of runs of 0âs between two 1âs. Also, to reduce intersymbol
interference, it may be necessary to require at least one 0 between
any two 1âs. We consider a simple example of such a constraint.
Suppose that we are required to have at least one 0 and at most
two 0âs between any pair of 1âs in a sequences. Thus, sequences
like 101001 and 0101001 are valid sequences, but 0110010 and
0000101 are not. We wish to calculate the number of valid sequences of length n.
(a) Show that the set of constrained sequences is the same as the
set of allowed paths on the following state diagram:

(b) Let Xi (n) be the number of valid paths of length n ending at
state i. Argue that X(n) = [X1 (n) X2 (n) X3 (n)]t satisï¬es the

PROBLEMS

following recursion:
ï£¹ï£®
ï£¹ ï£®
ï£¹
ï£®
X1 (n â 1)
0 1 1
X1 (n)
ï£° X2 (n) ï£» = ï£° 1 0 0 ï£»ï£° X2 (n â 1) ï£»,
0 1 0
X3 (n)
X3 (n â 1)
with initial conditions X(1) = [1 1 0]t .
(c) Let
ï£®
ï£¹
0 1 1
A = ï£° 1 0 0 ï£».
0 1 0

95

(4.101)

(4.102)

Then we have by induction
X(n) = AX(n â 1) = A2 X(n â 2) = Â· Â· Â· = Anâ1 X(1).
(4.103)
Using the eigenvalue decomposition of A for the case of distinct
eigenvalues, we can write A = U â1 U , where  is the diagonal matrix of eigenvalues. Then Anâ1 = U â1 nâ1 U . Show
that we can write
nâ1
nâ1
X(n) = Î»nâ1
1 Y1 + Î»2 Y2 + Î»3 Y3 ,

(4.104)

where Y1 , Y2 , Y3 do not depend on n. For large n, this sum
is dominated by the largest term. Therefore, argue that for i =
1, 2, 3, we have
1
log Xi (n) â log Î»,
(4.105)
n
where Î» is the largest (positive) eigenvalue. Thus, the number
of sequences of length n grows as Î»n for large n. Calculate Î»
for the matrix A above. (The case when the eigenvalues are
not distinct can be handled in a similar manner.)
(d) We now take a different approach. Consider a Markov chain
whose state diagram is the one given in part (a), but with
arbitrary transition probabilities. Therefore, the probability transition matrix of this Markov chain is
ï£¹
ï£®
0 1
0
(4.106)
P = ï£° Î± 0 1 â Î± ï£».
1 0
0
Show that the stationary distribution of this Markov chain is


1
1
1âÎ±
Âµ=
,
,
.
(4.107)
3âÎ± 3âÎ± 3âÎ±

96

ENTROPY RATES OF A STOCHASTIC PROCESS

(e) Maximize the entropy rate of the Markov chain over choices
of Î±. What is the maximum entropy rate of the chain?
(f) Compare the maximum entropy rate in part (e) with log Î» in
part (c). Why are the two answers the same?
4.17

Recurrence times are insensitive to distributions. Let X0 , X1 , X2 ,
. . . be drawn i.i.d. â¼ p(x), x â X = {1, 2, . . . , m}, and let N be the
waiting time to the next occurrence of X0 . Thus N = minn {Xn =
X0 }.
(a) Show that EN = m.
(b) Show that E log N â¤ H (X).
(c) (Optional ) Prove part (a) for {Xi } stationary and ergodic.

4.18

Stationary but not ergodic process. A bin has two biased coins,
one with probability of heads p and the other with probability of
heads 1 â p. One of these coins is chosen at random (i.e., with
probability 12 ) and is then tossed n times. Let X denote the identity
of the coin that is picked, and let Y1 and Y2 denote the results of
the ï¬rst two tosses.
(a) Calculate I (Y1 ; Y2 |X).
(b) Calculate I (X; Y1 , Y2 ).
(c) Let H(Y) be the entropy rate of the Y process (the sequence of coin tosses). Calculate H(Y). [Hint: Relate this to
lim n1 H (X, Y1 , Y2 , . . . , Yn ).]
You can check the answer by considering the behavior as p â 12 .

4.19

Random walk on graph.
graph:

Consider a random walk on the following
2

3

1
4

5

(a) Calculate the stationary distribution.

PROBLEMS

97

(b) What is the entropy rate?
(c) Find the mutual information I (Xn+1 ; Xn ) assuming that the
process is stationary.
4.20

Random walk on chessboard . Find the entropy rate of the Markov
chain associated with a random walk of a king on the 3 Ã 3 chessboard
1
4
7

2
5
8

3
6
9

What about the entropy rate of rooks, bishops, and queens? There
are two types of bishops.
4.21

Maximal entropy graphs. Consider a random walk on a connected
graph with four edges.
(a) Which graph has the highest entropy rate?
(b) Which graph has the lowest?

4.22

Three-dimensional maze. A bird is lost in a 3 Ã 3 Ã 3 cubical
maze. The bird ï¬ies from room to room going to adjoining rooms
with equal probability through each of the walls. For example, the
corner rooms have three exits.
(a) What is the stationary distribution?
(b) What is the entropy rate of this random walk?

4.23

Entropy rate. Let {Xi } be a stationary stochastic process with
entropy rate H (X).
(a) Argue that H (X) â¤ H (X1 ).
(b) What are the conditions for equality?

4.24

Entropy rates. Let {Xi } be a stationary process. Let Yi = (Xi ,
Xi+1 ). Let Zi = (X2i , X2i+1 ). Let Vi = X2i . Consider the entropy
rates H (X), H (Y), H (Z), and H (V) of the processes {Xi },{Yi },
{Zi }, and {Vi }. What is the inequality relationship â¤, =, or â¥
between each of the pairs listed below?
(a) H (X)
<H (Y).
(b) H (X)
<H (Z).
(c) H (X)
<H (V).
(d) H (Z)
<H (X).

4.25

Monotonicity
(a) Show that I (X; Y1 , Y2 , . . . , Yn ) is nondecreasing in n.

98

ENTROPY RATES OF A STOCHASTIC PROCESS

(b) Under what conditions is the mutual information constant for
all n?
4.26

Transitions in Markov chains. Suppose that {Xi } forms an irreducible Markov chain with transition matrix P and stationary distribution Âµ. Form the associated âedge processâ {Yi } by keeping track
only of the transitions. Thus, the new process {Yi } takes values in
X Ã X, and Yi = (Xiâ1 , Xi ). For example,
X n = 3, 2, 8, 5, 7, . . .
becomes

Y n = (â, 3), (3, 2), (2, 8), (8, 5), (5, 7), . . . .

Find the entropy rate of the edge process {Yi }.
4.27

Entropy rate. Let {Xi } be a stationary {0, 1}-valued stochastic
process obeying
Xk+1 = Xk â Xkâ1 â Zk+1 ,
where {Zi } is Bernoulli(p)and â denotes mod 2 addition. What is
the entropy rate H (X)?

4.28

Mixture of processes. Suppose that we observe one of two
stochastic processes but donât know which. What is the entropy
rate? Speciï¬cally, let X11 , X12 , X13 , . . . be a Bernoulli process with
parameter p1 , and let X21 , X22 , X23 , . . . be Bernoulli(p2 ). Let

1 with probability 12
Î¸=
2 with probability 12
and let Yi = XÎ¸ i , i = 1, 2, . . . , be the stochastic process observed.
Thus, Y observes the process {X1i } or {X2i }. Eventually, Y will
know which.
(a) Is {Yi } stationary?
(b) Is {Yi } an i.i.d. process?
(c) What is the entropy rate H of {Yi }?
(d) Does
1
â log p(Y1 , Y2 , . . . Yn ) ââ H ?
n
(e) Is there a code that achieves an expected per-symbol description
length n1 ELn ââ H ?

PROBLEMS

99

Now let Î¸i be Bern( 12 ). Observe that
Z i = XÎ¸ i i ,

i = 1, 2, . . . .

Thus, Î¸ is not ï¬xed for all time, as it was in the ï¬rst part, but is
chosen i.i.d. each time. Answer parts (a), (b), (c), (d), (e) for the
process {Zi }, labeling the answers (a ), (b ), (c ), (d ), (e ).
4.29

Waiting times. Let X be the waiting time for the ï¬rst heads to
appear in successive ï¬ips of a fair coin. For example, Pr{X = 3} =
( 12 )3 . Let Sn be the waiting time for the nth head to appear. Thus,
S0 = 0
Sn+1 = Sn + Xn+1 ,
where X1 , X2 , X3 , . . . are i.i.d according to the distribution above.
(a) Is the process {Sn } stationary?
(b) Calculate H (S1 , S2 , . . . , Sn ).
(c) Does the process {Sn } have an entropy rate? If so, what is it?
If not, why not?
(d) What is the expected number of fair coin ï¬ips required to
generate a random variable having the same distribution as Sn ?

4.30

Markov chain transitions

ï£®

ï£¯
P = [Pij ] = ï£°

1
2
1
4
1
4

1
4
1
2
1
4

1
4
1
4
1
2

ï£¹
ï£º
ï£».

Let X1 be distributed uniformly over the states {0, 1, 2}. Let {Xi }â
1
be a Markov chain with transition matrix P ; thus, P (Xn+1 =
j |Xn = i) = Pij , i, j â {0, 1, 2}.
(a) Is {Xn } stationary?
(b) Find limnââ n1 H (X1 , . . . , Xn ).
Now consider the derived process Z1 , Z2 , . . . , Zn , where
Z 1 = X1
Zi = Xi â Xiâ1

(mod 3),

i = 2, . . . , n.

Thus, Z n encodes the transitions, not the states.
(c) Find H (Z1 , Z2 , . . . , Zn ).
(d) Find H (Zn ) and H (Xn ) for n â¥ 2.

100

ENTROPY RATES OF A STOCHASTIC PROCESS

(e) Find H (Zn |Znâ1 ) for n â¥ 2.
(f) Are Znâ1 and Zn independent for n â¥ 2?
4.31

Markov . Let {Xi } â¼ Bernoulli(p). Consider the associated
Markov chain {Yi }ni=1 , where
Yi = (the number of 1âs in the current run of 1âs). For example, if
X n = 101110 . . . , we have Y n = 101230 . . . .
(a) Find the entropy rate of X n .
(b) Find the entropy rate of Y n .

4.32

Time symmetry. Let {Xn } be a stationary Markov process. We
condition on (X0 , X1 ) and look into the past and future. For what
index k is
H (Xân |X0 , X1 ) = H (Xk |X0 , X1 )?
Give the argument.

4.33

Chain inequality.
chain. Show that

Let X1 â X2 â X3 â X4 form a Markov

I (X1 ; X3 ) + I (X2 ; X4 ) â¤ I (X1 ; X4 ) + I (X2 ; X3 ).
4.34

Broadcast channel . Let X â Y â (Z, W ) form a Markov chain
[i.e., p(x, y, z, w) = p(x)p(y|x)p(z, w|y) for all x, y, z, w]. Show
that
I (X; Z) + I (X; W ) â¤ I (X; Y ) + I (Z; W ).

4.35

(4.108)

(4.109)

Concavity of second law . Let {Xn }â
ââ be a stationary Markov
process. Show that H (Xn |X0 ) is concave in n. Speciï¬cally, show
that
H (Xn |X0 ) â H (Xnâ1 |X0 ) â (H (Xnâ1 |X0 ) â H (Xnâ2 |X0 ))
= âI (X1 ; Xnâ1 |X0 , Xn ) â¤ 0.

(4.110)

Thus, the second difference is negative, establishing that H (Xn |X0 )
is a concave function of n.
HISTORICAL NOTES
The entropy rate of a stochastic process was introduced by Shannon [472],
who also explored some of the connections between the entropy rate of the
process and the number of possible sequences generated by the process.
Since Shannon, there have been a number of results extending the basic

HISTORICAL NOTES

101

theorems of information theory to general stochastic processes. The AEP
for a general stationary stochastic process is proved in Chapter 16.
Hidden Markov models are used for a number of applications, such
as speech recognition [432]. The calculation of the entropy rate for constrained sequences was introduced by Shannon [472]. These sequences
are used for coding for magnetic and optical channels [288].

CHAPTER 5

DATA COMPRESSION

We now put content in the deï¬nition of entropy by establishing the fundamental limit for the compression of information. Data compression can be
achieved by assigning short descriptions to the most frequent outcomes
of the data source, and necessarily longer descriptions to the less frequent outcomes. For example, in Morse code, the most frequent symbol
is represented by a single dot. In this chapter we ï¬nd the shortest average
description length of a random variable.
We ï¬rst deï¬ne the notion of an instantaneous code and then prove the
important Kraft inequality, which asserts that the exponentiated codeword
length assignments must look like a probability mass function. Elementary calculus then shows that the expected description length must be
greater than or equal to the entropy, the ï¬rst main result. Then Shannonâs simple construction shows that the expected description length can
achieve this bound asymptotically for repeated descriptions. This establishes the entropy as a natural measure of efï¬cient description length.
The famous Huffman coding procedure for ï¬nding minimum expected
description length assignments is provided. Finally, we show that Huffman codes are competitively optimal and that it requires roughly H fair
coin ï¬ips to generate a sample of a random variable having entropy H .
Thus, the entropy is the data compression limit as well as the number of
bits needed in random number generation, and codes achieving H turn
out to be optimal from many points of view.
5.1

EXAMPLES OF CODES

Deï¬nition A source code C for a random variable X is a mapping from
X, the range of X, to Dâ , the set of ï¬nite-length strings of symbols from
a D-ary alphabet. Let C(x) denote the codeword corresponding to x and
let l(x) denote the length of C(x).
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

103

104

DATA COMPRESSION

For example, C(red) = 00, C(blue) = 11 is a source code for X = {red,
blue} with alphabet D = {0, 1}.
Deï¬nition The expected length L(C) of a source code C(x) for a random variable X with probability mass function p(x) is given by

L(C) =
p(x)l(x),
(5.1)
xâX

where l(x) is the length of the codeword associated with x.
Without loss of generality, we can assume that the D-ary alphabet is
D = {0, 1, . . . , D â 1}.
Some examples of codes follow.
Example 5.1.1 Let X be a random variable with the following distribution and codeword assignment:
Pr(X = 1) = 12 , codeword C(1) = 0
Pr(X = 2) = 14 , codeword C(2) = 10
Pr(X = 3) = 18 , codeword C(3) = 110

(5.2)

Pr(X = 4) = 18 , codeword C(4) = 111.
The entropy H (X) of X is 1.75 bits, and the expected length L(C) =
El(X) of this code is also 1.75 bits. Here we have a code that has the
same average length as the entropy. We note that any sequence of bits
can be uniquely decoded into a sequence of symbols of X. For example,
the bit string 0110111100110 is decoded as 134213.
Example 5.1.2
variable:

Consider another simple example of a code for a random
Pr(X = 1) = 13 , codeword C(1) = 0
Pr(X = 2) = 13 , codeword C(2) = 10
Pr(X = 3) =

1
3,

(5.3)

codeword C(3) = 11.

Just as in Example 5.1.1, the code is uniquely decodable. However, in
this case the entropy is log 3 = 1.58 bits and the average length of the
encoding is 1.66 bits. Here El(X) > H (X).
Example 5.1.3 (Morse code) The Morse code is a reasonably efï¬cient
code for the English alphabet using an alphabet of four symbols: a dot,

5.1 EXAMPLES OF CODES

105

a dash, a letter space, and a word space. Short sequences represent frequent letters (e.g., a single dot represents E) and long sequences represent
infrequent letters (e.g., Q is represented by âdash,dash,dot,dashâ). This is
not the optimal representation for the alphabet in four symbolsâin fact,
many possible codewords are not utilized because the codewords for letters do not contain spaces except for a letter space at the end of every
codeword, and no space can follow another space. It is an interesting problem to calculate the number of sequences that can be constructed under
these constraints. The problem was solved by Shannon in his original
1948 paper. The problem is also related to coding for magnetic recording,
where long strings of 0âs are prohibited [5], [370].
We now deï¬ne increasingly more stringent conditions on codes. Let x n
denote (x1 , x2 , . . . , xn ).
Deï¬nition A code is said to be nonsingular if every element of the
range of X maps into a different string in Dâ ; that is,
x = x  â C(x) = C(x  ).

(5.4)

Nonsingularity sufï¬ces for an unambiguous description of a single
value of X. But we usually wish to send a sequence of values of X.
In such cases we can ensure decodability by adding a special symbol
(a âcommaâ) between any two codewords. But this is an inefï¬cient use
of the special symbol; we can do better by developing the idea of selfpunctuating or instantaneous codes. Motivated by the necessity to send
sequences of symbols X, we deï¬ne the extension of a code as follows:
Deï¬nition The extension C â of a code C is the mapping from ï¬nitelength strings of X to ï¬nite-length strings of D, deï¬ned by
C(x1 x2 Â· Â· Â· xn ) = C(x1 )C(x2 ) Â· Â· Â· C(xn ),

(5.5)

where C(x1 )C(x2 ) Â· Â· Â· C(xn ) indicates concatenation of the corresponding
codewords.
Example 5.1.4

If C(x1 ) = 00 and C(x2 ) = 11, then C(x1 x2 ) = 0011.

Deï¬nition A code is called uniquely decodable if its extension is nonsingular.
In other words, any encoded string in a uniquely decodable code has
only one possible source string producing it. However, one may have
to look at the entire string to determine even the ï¬rst symbol in the
corresponding source string.

106

DATA COMPRESSION

Deï¬nition A code is called a preï¬x code or an instantaneous code if
no codeword is a preï¬x of any other codeword.
An instantaneous code can be decoded without reference to future codewords since the end of a codeword is immediately recognizable. Hence,
for an instantaneous code, the symbol xi can be decoded as soon as we
come to the end of the codeword corresponding to it. We need not wait
to see the codewords that come later. An instantaneous code is a selfpunctuating code; we can look down the sequence of code symbols and
add the commas to separate the codewords without looking at later symbols. For example, the binary string 01011111010 produced by the code
of Example 5.1.1 is parsed as 0,10,111,110,10.
The nesting of these deï¬nitions is shown in Figure 5.1. To illustrate the
differences between the various kinds of codes, consider the examples of
codeword assignments C(x) to x â X in Table 5.1. For the nonsingular
code, the code string 010 has three possible source sequences: 2 or 14 or
31, and hence the code is not uniquely decodable. The uniquely decodable
code is not preï¬x-free and hence is not instantaneous. To see that it is
uniquely decodable, take any code string and start from the beginning.
If the ï¬rst two bits are 00 or 10, they can be decoded immediately. If

All
codes

Nonsingular
codes

Uniquely
decodable
codes

Instantaneous
codes

FIGURE 5.1. Classes of codes.

5.2

KRAFT INEQUALITY

107

TABLE 5.1 Classes of Codes
X

Singular

Nonsingular, But Not
Uniquely Decodable

Uniquely Decodable,
But Not Instantaneous

Instantaneous

1
2
3
4

0
0
0
0

0
010
01
10

10
00
11
110

0
10
110
111

the ï¬rst two bits are 11, we must look at the following bits. If the next
bit is a 1, the ï¬rst source symbol is a 3. If the length of the string of
0âs immediately following the 11 is odd, the ï¬rst codeword must be 110
and the ï¬rst source symbol must be 4; if the length of the string of 0âs is
even, the ï¬rst source symbol is a 3. By repeating this argument, we can see
that this code is uniquely decodable. Sardinas and Patterson [455] have
devised a ï¬nite test for unique decodability, which involves forming sets
of possible sufï¬xes to the codewords and eliminating them systematically.
The test is described more fully in Problem 5.5.27. The fact that the last
code in Table 5.1 is instantaneous is obvious since no codeword is a preï¬x
of any other.
5.2

KRAFT INEQUALITY

We wish to construct instantaneous codes of minimum expected length to
describe a given source. It is clear that we cannot assign short codewords
to all source symbols and still be preï¬x-free. The set of codeword lengths
possible for instantaneous codes is limited by the following inequality.
Theorem 5.2.1 (Kraft inequality) For any instantaneous code (preï¬x
code) over an alphabet of size D, the codeword lengths l1 , l2 , . . . , lm must
satisfy the inequality

D âli â¤ 1.
(5.6)
i

Conversely, given a set of codeword lengths that satisfy this inequality,
there exists an instantaneous code with these word lengths.
Proof: Consider a D-ary tree in which each node has D children. Let the
branches of the tree represent the symbols of the codeword. For example,
the D branches arising from the root node represent the D possible values
of the ï¬rst symbol of the codeword. Then each codeword is represented

108

DATA COMPRESSION

0

Root

10

110

111

FIGURE 5.2. Code tree for the Kraft inequality.

by a leaf on the tree. The path from the root traces out the symbols of the
codeword. A binary example of such a tree is shown in Figure 5.2. The
preï¬x condition on the codewords implies that no codeword is an ancestor
of any other codeword on the tree. Hence, each codeword eliminates its
descendants as possible codewords.
Let lmax be the length of the longest codeword of the set of codewords.
Consider all nodes of the tree at level lmax . Some of them are codewords,
some are descendants of codewords, and some are neither. A codeword
at level li has D lmax âli descendants at level lmax . Each of these descendant
sets must be disjoint. Also, the total number of nodes in these sets must
be less than or equal to D lmax . Hence, summing over all the codewords,
we have

D lmax âli â¤ D lmax
(5.7)
or


D âli â¤ 1,

(5.8)

which is the Kraft inequality.
Conversely, given any set of codeword lengths l1 , l2 , . . . , lm that satisfy the Kraft inequality, we can always construct a tree like the one in

5.2

KRAFT INEQUALITY

109

Figure 5.2. Label the ï¬rst node (lexicographically) of depth l1 as codeword 1, and remove its descendants from the tree. Then label the ï¬rst
remaining node of depth l2 as codeword 2, and so on. Proceeding this

way, we construct a preï¬x code with the speciï¬ed l1 , l2 , . . . , lm .
We now show that an inï¬nite preï¬x code also satisï¬es the Kraft inequality.
Theorem 5.2.2 (Extended Kraft Inequality) For any countably inï¬nite set of codewords that form a preï¬x code, the codeword lengths satisfy
the extended Kraft inequality,
â


D âli â¤ 1.

(5.9)

i=1

Conversely, given any l1 , l2 , . . . satisfying the extended Kraft inequality,
we can construct a preï¬x code with these codeword lengths.
Proof: Let the D-ary alphabet be {0, 1, . . . , D â 1}. Consider the ith
codeword y1 y2 Â· Â· Â· yli . Let 0.y1 y2 Â· Â· Â· yli be the real number given by the
D-ary expansion
0.y1 y2 Â· Â· Â· yli =

li


yj D âj .

(5.10)

j =1

This codeword corresponds to the interval


1
0.y1 y2 Â· Â· Â· yli , 0.y1 y2 Â· Â· Â· yli + l ,
Di

(5.11)

the set of all real numbers whose D-ary expansion begins with
0.y1 y2 Â· Â· Â· yli . This is a subinterval of the unit interval [0, 1]. By the preï¬x
condition, these intervals are disjoint. Hence, the sum of their lengths has
to be less than or equal to 1. This proves that
â


D âli â¤ 1.

(5.12)

i=1

Just as in the ï¬nite case, we can reverse the proof to construct the
code for a given l1 , l2 , . . . that satisï¬es the Kraft inequality. First, reorder
the indexing so that l1 â¤ l2 â¤ . . . . Then simply assign the intervals in

110

DATA COMPRESSION

order from the low end of the unit interval. For example, if we wish to
construct a binary code with l1 = 1, l2 = 2, . . . , we assign the intervals
[0, 12 ), [ 12 , 14 ), . . . to the symbols, with corresponding codewords 0, 10,

....
In Section 5.5 we show that the lengths of codewords for a uniquely
decodable code also satisfy the Kraft inequality. Before we do that, we
consider the problem of ï¬nding the shortest instantaneous code.
5.3

OPTIMAL CODES

In Section 5.2 we proved that any codeword set that satisï¬es the preï¬x
condition has to satisfy the Kraft inequality and that the Kraft inequality
is a sufï¬cient condition for the existence of a codeword set with the
speciï¬ed set of codeword lengths. We now consider the problem of ï¬nding
the preï¬x code with the minimum expected length. From the results of
Section 5.2, this is equivalent to ï¬nding the set of lengths l1 , l
2 , . . . , lm
satisfying the Kraft inequality and whose expected length L = pi li is
less than the expected length of any other preï¬x code. This is a standard
optimization problem: Minimize

(5.13)
L=
pi li
over all integers l1 , l2 , . . . , lm satisfying

D âli â¤ 1.

(5.14)

A simple analysis by calculus suggests the form of the minimizing liâ .
We neglect the integer constraint on li and assume equality in the constraint. Hence, we can write the constrained minimization using Lagrange
multipliers as the minimization of



D âli .
J =
pi li + Î»
(5.15)
Differentiating with respect to li , we obtain
âJ
= pi â Î»D âli loge D.
âli

(5.16)

Setting the derivative to 0, we obtain
D âli =

pi
.
Î» loge D

(5.17)

5.3 OPTIMAL CODES

111

Substituting this in the constraint to ï¬nd Î», we ï¬nd Î» = 1/ loge D, and
hence
pi = D âli ,

(5.18)

liâ = â logD pi .

(5.19)

yielding optimal code lengths,

This noninteger choice of codeword lengths yields expected codeword
length


Lâ =
pi liâ = â
pi logD pi = HD (X).
(5.20)
But since the li must be integers, we will not always be able to set
the codeword lengths as in (5.19). Instead, we should choose a set of
codeword lengths li âcloseâ to the optimal set. Rather than demonstrate
by calculus that liâ = â logD pi is a global minimum, we verify optimality
directly in the proof of the following theorem.
Theorem 5.3.1 The expected length L of any instantaneous D-ary code
for a random variable X is greater than or equal to the entropy HD (X);
that is,
L â¥ HD (X),

(5.21)

with equality if and only if D âli = pi .
Proof: We can write the difference between the expected length and the
entropy as
L â HD (X) =



=â
Letting ri = D âli /


j

pi li â





pi logD

pi logD D âli +

D âlj and c =




1
pi


(5.22)
pi logD pi .

D âli , we obtain

pi
â logD c
ri
1
= D(p||r) + logD
c
â¥0

LâH =

(5.23)

pi logD

(5.24)
(5.25)
(5.26)

112

DATA COMPRESSION

by the nonnegativity of relative entropy and the fact (Kraft inequality)
that c â¤ 1. Hence, L â¥ H with equality if and only if pi = D âli (i.e., if
and only if â logD pi is an integer for all i).

Deï¬nition A probability distribution is called D-adic if each of the
probabilities is equal to D ân for some n. Thus, we have equality in the
theorem if and only if the distribution of X is D-adic.
The preceding proof also indicates a procedure for ï¬nding an optimal
code: Find the D-adic distribution that is closest (in the relative entropy
sense) to the distribution of X. This distribution provides the set of codeword lengths. Construct the code by choosing the ï¬rst available node as
in the proof of the Kraft inequality. We then have an optimal code for X.
However, this procedure is not easy, since the search for the closest
D-adic distribution is not obvious. In the next section we give a good
suboptimal procedure (ShannonâFano coding). In Section 5.6 we describe
a simple procedure (Huffman coding) for actually ï¬nding the optimal
code.

5.4

BOUNDS ON THE OPTIMAL CODE LENGTH

We now demonstrate a code that achieves an expected description length
L within 1 bit of the lower bound; that is,
H (X) â¤ L < H (X) + 1.

(5.27)


Recall the setup of Section 5.3: We wish to minimize 
L = pi li subject to the constraint that l1 , l2 , . . . , lm are integers and
D âli â¤ 1. We
proved that the optimal codeword lengths can be found by ï¬nding the
D-adic probability distribution closest to the distribution
 of X in relative
entropy, that is, by ï¬nding the D-adic r (ri = D âli / j D âlj ) minimizing
L â HD = D(p||r) â log




D âli â¥ 0.

(5.28)

The choice of word lengths li = logD p1i yields L = H . Since logD p1i
may not equal an integer, we round it up to give integer word-length
assignments,



1
li = logD
,
pi

(5.29)

5.4 BOUNDS ON THE OPTIMAL CODE LENGTH

113

where 	x
 is the smallest integer â¥ x. These lengths satisfy the Kraft
inequality since


D

â	log p1 

i

â¤



D

â log p1

i

=



pi = 1.

(5.30)

This choice of codeword lengths satisï¬es
logD

1
1
â¤ li < logD
+ 1.
pi
pi

(5.31)

Multiplying by pi and summing over i, we obtain
HD (X) â¤ L < HD (X) + 1.

(5.32)

Since an optimal code can only be better than this code, we have the
following theorem.
â
be optimal codeword lengths for a
Theorem 5.4.1 Let l1â , l2â , . . . , lm
â
source distribution p and a D-ary alphabet,
andâlet L be the associated
â
expected length of an optimal code (L = pi li ). Then

HD (X) â¤ Lâ < HD (X) + 1.
Proof: Let li = 	logD
(5.32) we have

1
pi 
.

(5.33)

Then li satisï¬es the Kraft inequality and from

HD (X) â¤ L =



pi li < HD (X) + 1.

(5.34)

But since Lâ , the expected length of the optimal code, is less than L =

pi li , and since Lâ â¥ HD from Theorem 5.3.1, we have the

theorem.
In Theorem 5.4.1 there is an overhead that is at most 1 bit, due to the
fact that log p1i is not always an integer. We can reduce the overhead per
symbol by spreading it out over many symbols. With this in mind, let us
consider a system in which we send a sequence of n symbols from X.
The symbols are assumed to be drawn i.i.d. according to p(x). We can
consider these n symbols to be a supersymbol from the alphabet X n .
Deï¬ne Ln to be the expected codeword length per input symbol, that
is, if l(x1 , x2 , . . . , xn ) is the length of the binary codeword associated

114

DATA COMPRESSION

with (x1 , x2 , . . . , xn ) (for the rest of this section, we assume that D = 2,
for simplicity), then
1
1
p(x1 , x2 , . . . , xn )l(x1 , x2 , . . . , xn ) = El(X1 , X2 , . . . , Xn ).
n
n
(5.35)
We can now apply the bounds derived above to the code:
Ln =

H (X1 , X2 , . . . , Xn ) â¤ El(X1 , X2 , . . . , Xn ) < H (X1 , X2 , . . . , Xn ) + 1.
(5.36)

Since X1 , X2 , . . . , Xn are i.i.d., H (X1 , X2 , . . . , Xn ) = H (Xi ) =
nH (X). Dividing (5.36) by n, we obtain
1
H (X) â¤ Ln < H (X) + .
n

(5.37)

Hence, by using large block lengths we can achieve an expected codelength per symbol arbitrarily close to the entropy.
We can use the same argument for a sequence of symbols from a
stochastic process that is not necessarily i.i.d. In this case, we still have
the bound
H (X1 , X2 , . . . , Xn ) â¤ El(X1 , X2 , . . . , Xn ) < H (X1 , X2 , . . . , Xn ) + 1.
(5.38)
Dividing by n again and deï¬ning Ln to be the expected description length
per symbol, we obtain
H (X1 , X2 , . . . , Xn )
H (X1 , X2 , . . . , Xn ) 1
â¤ Ln <
+ .
n
n
n

(5.39)

If the stochastic process is stationary, then H (X1 , X2 , . . . , Xn )/n â
H (X), and the expected description length tends to the entropy rate as
n â â. Thus, we have the following theorem:
Theorem 5.4.2
isï¬es

The minimum expected codeword length per symbol sat-

H (X1 , X2 , . . . , Xn )
H (X1 , X2 , . . . , Xn ) 1
â¤ Lân <
+ .
n
n
n

(5.40)

Moreover, if X1 , X2 , . . . , Xn is a stationary stochastic process,
Lân â H (X),
where H (X) is the entropy rate of the process.

(5.41)

5.5 KRAFT INEQUALITY FOR UNIQUELY DECODABLE CODES

115

This theorem provides another justiï¬cation for the deï¬nition of entropy
rateâit is the expected number of bits per symbol required to describe
the process.
Finally, we ask what happens to the expected description length if the
code is designed for the wrong distribution. For example, the wrong distribution may be the best estimate that we can make of the unknown
true
	


1
distribution. We consider the Shannon code assignment l(x) = log q(x)
designed for the probability mass function q(x). Suppose that the true
probability mass function
 is p(x). Thus, we will not achieve expected
length L â H (p) = â p(x) log p(x). We now show that the increase
in expected description length due to the incorrect distribution is the relative entropy D(p||q). Thus, D(p||q) has a concrete interpretation as the
increase in descriptive complexity due to incorrect information.
Theorem 5.4.3 (Wrong
	 code) 
 The expected length under p(x) of the
1
code assignment l(x) = log q(x)
satisï¬es
H (p) + D(p||q) â¤ Ep l(X) < H (p) + D(p||q) + 1.
Proof: The expected codelength is



1
El(X) =
p(x) log
q(x)
x



1
p(x) log
<
+1
q(x)
x
=



x

(5.43)
(5.44)

p(x) log

p(x) 1
+1
q(x) p(x)

(5.45)

p(x) log

p(x) 
1
p(x) log
+
+1
q(x)
p(x)
x

(5.46)

x

=

(5.42)

= D(p||q) + H (p) + 1.
The lower bound can be derived similarly.

(5.47)


Thus, believing that the distribution is q(x) when the true distribution
is p(x) incurs a penalty of D(p||q) in the average description length.
5.5

KRAFT INEQUALITY FOR UNIQUELY DECODABLE CODES

We have proved that any instantaneous code must satisfy the Kraft inequality. The class of uniquely decodable codes is larger than the class of

116

DATA COMPRESSION

instantaneous codes, so one expects to achieve a lower expected codeword
length if L is minimized over all uniquely decodable codes. In this section
we prove that the class of uniquely decodable codes does not offer any
further possibilities for the set of codeword lengths than do instantaneous
codes. We now give Karushâs elegant proof of the following theorem.
Theorem 5.5.1 (McMillan) The codeword lengths of any uniquely
decodable D-ary code must satisfy the Kraft inequality

D âli â¤ 1.
(5.48)
Conversely, given a set of codeword lengths that satisfy this inequality, it
is possible to construct a uniquely decodable code with these codeword
lengths.
Proof: Consider C k , the kth extension of the code (i.e., the code formed
by the concatenation of k repetitions of the given uniquely decodable code
C). By the deï¬nition of unique decodability, the kth extension of the code
is nonsingular. Since there are only D n different D-ary strings of length n,
unique decodability implies that the number of code sequences of length
n in the kth extension of the code must be no greater than D n . We now
use this observation to prove the Kraft inequality.
Let the codeword lengths of the symbols x â X be denoted by l(x).
For the extension code, the length of the code sequence is
l(x1 , x2 , . . . , xk ) =

k


l(xi ).

(5.49)

i=1

The inequality that we wish to prove is

D âl(x) â¤ 1.

(5.50)

xâX

The trick is to consider the kth power of this quantity. Thus,


k

 

D âl(x) =
Â·Â·Â·
D âl(x1 ) D âl(x2 ) Â· Â· Â· D âl(xk )
x1 âX x2 âX

xâX

=

(5.51)

xk âX



D âl(x1 ) D âl(x2 ) Â· Â· Â· D âl(xk )

(5.52)

x1 ,x2 ,...,xk âX k

=



x k âX k

k

D âl(x ) ,

(5.53)

5.5 KRAFT INEQUALITY FOR UNIQUELY DECODABLE CODES

117

by (5.49). We now gather the terms by word lengths to obtain


k

D âl(x ) =

x k âX k

kl
max


a(m)D âm ,

(5.54)

m=1

where lmax is the maximum codeword length and a(m) is the number
of source sequences x k mapping into codewords of length m. But the
code is uniquely decodable, so there is at most one sequence mapping
into each code m-sequence and there are at most D m code m-sequences.
Thus, a(m) â¤ D m , and we have





k
D âl(x)

=

xâX

kl
max


a(m)D âm

(5.55)

D m D âm

(5.56)

m=1

â¤

kl
max

m=1

= klmax

(5.57)

and hence


D âlj â¤ (klmax )1/k .

(5.58)

j

Since this inequality is true for all k, it is true in the limit as k â â.
Since (klmax )1/k â 1, we have

D âlj â¤ 1,
(5.59)
j

which is the Kraft inequality.
Conversely, given any set of l1 , l2 , . . . , lm satisfying the Kraft inequality, we can construct an instantaneous code as proved in Section 5.2. Since
every instantaneous code is uniquely decodable, we have also constructed
a uniquely decodable code.

Corollary A uniquely decodable code for an inï¬nite source alphabet X
also satisï¬es the Kraft inequality.
Proof: The point at which the preceding proof breaks down for inï¬nite
|X| is at (5.58), since for an inï¬nite code lmax is inï¬nite. But there is a

118

DATA COMPRESSION

simple ï¬x to the proof. Any subset of a uniquely decodable code is also
uniquely decodable; thus, any ï¬nite subset of the inï¬nite set of codewords
satisï¬es the Kraft inequality. Hence,
â


D âli = lim

i=1

N ââ

N


D âli â¤ 1.

(5.60)

i=1

Given a set of word lengths l1 , l2 , . . . that satisfy the Kraft inequality, we
can construct an instantaneous code as in Section 5.4. Since instantaneous
codes are uniquely decodable, we have constructed a uniquely decodable
code with an inï¬nite number of codewords. So the McMillan theorem
also applies to inï¬nite alphabets.

The theorem implies a rather surprising resultâthat the class of
uniquely decodable codes does not offer any further choices for the set
of codeword lengths than the class of preï¬x codes. The set of achievable
codeword lengths is the same for uniquely decodable and instantaneous
codes. Hence, the bounds derived on the optimal codeword lengths continue to hold even when we expand the class of allowed codes to the class
of all uniquely decodable codes.
5.6

HUFFMAN CODES

An optimal (shortest expected length) preï¬x code for a given distribution
can be constructed by a simple algorithm discovered by Huffman [283].
We will prove that any other code for the same alphabet cannot have a
lower expected length than the code constructed by the algorithm. Before
we give any formal proofs, let us introduce Huffman codes with some
examples.
Example 5.6.1 Consider a random variable X taking values in the set
X = {1, 2, 3, 4, 5} with probabilities 0.25, 0.25, 0.2, 0.15, 0.15, respectively. We expect the optimal binary code for X to have the longest
codewords assigned to the symbols 4 and 5. These two lengths must be
equal, since otherwise we can delete a bit from the longer codeword and
still have a preï¬x code, but with a shorter expected length. In general,
we can construct a code in which the two longest codewords differ only
in the last bit. For this code, we can combine the symbols 4 and 5 into
a single source symbol, with a probability assignment 0.30. Proceeding
this way, combining the two least likely symbols into one symbol until
we are ï¬nally left with only one symbol, and then assigning codewords
to the symbols, we obtain the following table:

5.6 HUFFMAN CODES

Codeword
Length

Codeword

X

01
10
11
000
001

1
2
3
4
5

2
2
2
3
3

119

Probability
0.25
0.25
0.2
0.15
0.15

0.3
0.25
0.25
0.2

0.45
0.3
0.25

0.55
0.45

1

This code has average length 2.3 bits.
Example 5.6.2 Consider a ternary code for the same random variable.
Now we combine the three least likely symbols into one supersymbol and
obtain the following table:
Codeword

X

1
2
00
01
02

1
2
3
4
5

Probability
0.25
0.25
0.2
0.15
0.15

0.5
0.25
0.25

1

This code has an average length of 1.5 ternary digits.
Example 5.6.3 If D â¥ 3, we may not have a sufï¬cient number of symbols so that we can combine them D at a time. In such a case, we add
dummy symbols to the end of the set of symbols. The dummy symbols
have probability 0 and are inserted to ï¬ll the tree. Since at each stage of
the reduction, the number of symbols is reduced by D â 1, we want the
total number of symbols to be 1 + k(D â 1), where k is the number of
merges. Hence, we add enough dummy symbols so that the total number
of symbols is of this form. For example:

This code has an average length of 1.7 ternary digits.

120

DATA COMPRESSION

A proof of the optimality of Huffman coding is given in Section 5.8.

5.7

SOME COMMENTS ON HUFFMAN CODES

1. Equivalence of source coding and 20 questions. We now digress
to show the equivalence of coding and the game â20 questionsâ.
Suppose that we wish to ï¬nd the most efï¬cient series of yesâno
questions to determine an object from a class of objects. Assuming
that we know the probability distribution on the objects, can we ï¬nd
the most efï¬cient sequence of questions? (To determine an object,
we need to ensure that the responses to the sequence of questions
uniquely identiï¬es the object from the set of possible objects; it is
not necessary that the last question have a âyesâ answer.)
We ï¬rst show that a sequence of questions is equivalent to a code
for the object. Any question depends only on the answers to the
questions before it. Since the sequence of answers uniquely determines the object, each object has a different sequence of answers,
and if we represent the yesâno answers by 0âs and 1âs, we have a
binary code for the set of objects. The average length of this code
is the average number of questions for the questioning scheme.
Also, from a binary code for the set of objects, we can ï¬nd a
sequence of questions that correspond to the code, with the average
number of questions equal to the expected codeword length of the
code. The ï¬rst question in this scheme becomes: Is the ï¬rst bit equal
to 1 in the objectâs codeword?
Since the Huffman code is the best source code for a random
variable, the optimal series of questions is that determined by the
Huffman code. In Example 5.6.1 the optimal ï¬rst question is: Is
X equal to 2 or 3? The answer to this determines the ï¬rst bit of
the Huffman code. Assuming that the answer to the ï¬rst question
is âyes,â the next question should be âIs X equal to 3?â, which
determines the second bit. However, we need not wait for the answer
to the ï¬rst question to ask the second. We can ask as our second
question âIs X equal to 1 or 3?â, determining the second bit of the
Huffman code independent of the ï¬rst.
The expected number of questions EQ in this optimal scheme
satisï¬es
H (X) â¤ EQ < H (X) + 1.

(5.61)

5.7 SOME COMMENTS ON HUFFMAN CODES

121

2. Huffman coding
 for weighted codewords. Huffmanâs algorithm for
minimizing pi li can be applied to any set of numbers pi â¥ 0,
regardless of
pi . In this case,
 the Huffman code minimizes the
sum of weighted code lengths
wi li rather than the average code
length.
Example 5.7.1 We perform the weighted minimization using the
same algorithm.

In this case the code minimizes the weighted sum of the codeword
lengths, and the minimum weighted sum is 36.
3. Huffman coding and âsliceâ questions (Alphabetic codes). We have
described the equivalence of source coding with the game of 20
questions. The optimal sequence of questions corresponds to an
optimal source code for the random variable. However, Huffman
codes ask arbitrary questions of the form âIs X â A?â for any set
A â {1, 2, . . . , m}.
Now we consider the game â20 questionsâ with a restricted set
of questions. Speciï¬cally, we assume that the elements of X =
{1, 2, . . . , m} are ordered so that p1 â¥ p2 â¥ Â· Â· Â· â¥ pm and that the
only questions allowed are of the form âIs X > a?â for some a. The
Huffman code constructed by the Huffman algorithm may not correspond to slices (sets of the form {x : x < a}). If we take the codeword lengths (l1 â¤ l2 â¤ Â· Â· Â· â¤ lm , by Lemma 5.8.1) derived from the
Huffman code and use them to assign the symbols to the code tree
by taking the ï¬rst available node at the corresponding level, we
will construct another optimal code. However, unlike the Huffman
code itself, this code is a slice code, since each question (each bit
of the code) splits the tree into sets of the form {x : x > a} and
{x : x < a}.
We illustrate this with an example.
Example 5.7.2 Consider the ï¬rst example of Section 5.6. The
code that was constructed by the Huffman coding procedure is not a

122

DATA COMPRESSION

slice code. But using the codeword lengths from the Huffman procedure, namely, {2, 2, 2, 3, 3}, and assigning the symbols to the ï¬rst
available node on the tree, we obtain the following code for this
random variable:
1 â 00,

2 â 01,

3 â 10,

4 â 110,

5 â 111

It can be veriï¬ed that this code is a slice code, codes known as
alphabetic codes because the codewords are ordered alphabetically.
4. Huffman codes and Shannon codes. Using codeword lengths of
	log p1i 
 (which is called Shannon coding) may be much worse than
the optimal code for some particular symbol. For example, consider two symbols, one of which occurs with probability 0.9999 and
the other with probability 0.0001. Then using codeword lengths of
	log p1i 
 gives codeword lengths of 1 bit and 14 bits, respectively.
The optimal codeword length is obviously 1 bit for both symbols.
Hence, the codeword for the infrequent symbol is much longer in
the Shannon code than in the optimal code.
Is it true that the codeword lengths for an optimal code are always
less than 	log p1i 
? The following example illustrates that this is not
always true.
5.7.3
Consider a random variable X with a distribution
Example

1 1 1 1
,
,
,
.
The
Huffman coding procedure results in codeword
3 3 4 12
lengths of (2, 2, 2, 2) or (1, 2, 3, 3) [depending on where one puts
the merged probabilities, as the reader can verify (Problem 5.5.12)].
Both these codes achieve the same expected codeword length. In the
second code, the third symbol has length 3, which is greater than
	log p13 
. Thus, the codeword length for a Shannon code could be
less than the codeword length of the corresponding symbol of an
optimal (Huffman) code. This example also illustrates the fact that
the set of codeword lengths for an optimal code is not unique (there
may be more than one set of lengths with the same expected value).
Although either the Shannon code or the Huffman code can be
shorter for individual symbols, the Huffman code is shorter on average. Also, the Shannon code and the Huffman code differ by less
than 1 bit in expected codelength (since both lie between H and
H + 1.)

5.8 OPTIMALITY OF HUFFMAN CODES

123

5. Fano codes. Fano proposed a suboptimal procedure for constructing
a source code, which is similar to the idea of slice codes. In his
method we ï¬rst order
in decreasing
order. Then we

the probabilities
m

 k
choose k such that  i=1 pi â i=k+1 pi  is minimized. This point
divides the source symbols into two sets of almost equal probability.
Assign 0 for the ï¬rst bit of the upper set and 1 for the lower set.
Repeat this process for each subset. By this recursive procedure, we
obtain a code for each source symbol. This scheme, although not
optimal in general, achieves L(C) â¤ H (X) + 2. (See [282].)
5.8

OPTIMALITY OF HUFFMAN CODES

We prove by induction that the binary Huffman code is optimal. It is
important to remember that there are many optimal codes: inverting all
the bits or exchanging two codewords of the same length will give another
optimal code. The Huffman procedure constructs one such optimal code.
To prove the optimality of Huffman codes, we ï¬rst prove some properties
of a particular optimal code.
Without loss of generality, we will assume that the probability masses
are
 ordered, so that p1 â¥ p2 â¥ Â· Â· Â· â¥ pm . Recall that a code is optimal if
pi li is minimal.
Lemma 5.8.1 For any distribution, there exists an optimal instantaneous
code (with minimum expected length) that satisï¬es the following properties:
1. The lengths are ordered inversely with the probabilities (i.e., if pj >
pk , then lj â¤ lk ).
2. The two longest codewords have the same length.
3. Two of the longest codewords differ only in the last bit and correspond to the two least likely symbols.
Proof: The proof amounts to swapping, trimming, and rearranging, as
shown in Figure 5.3. Consider an optimal code Cm :
â¢

If pj > pk , then lj â¤ lk . Here we swap codewords.

, with the codewords j and k of Cm interchanged. Then
Consider Cm



) â L(Cm ) =
pi li  â
pi li
(5.62)
L(Cm
= pj lk + pk lj â pj lj â pk lk

(5.63)

= (pj â pk )(lk â lj ).

(5.64)

124

DATA COMPRESSION
p5

0

0

p5

0

1

p1

1

0

p3

1

p4

0
0

1

p1

1

0

p3

1

p4

0

1
1

0

1

p2

p2
(b)

( a)

0

p2

0
1
1

(c)

0

p5

1

p1

0

p3

1

p4

0

p2

0
1
1

0

p2

1

p3

0

p4

1

p5

(d )

FIGURE 5.3. Properties of optimal codes. We assume that p1 â¥ p2 â¥ Â· Â· Â· â¥ pm . A possible
instantaneous code is given in (a). By trimming branches without siblings, we improve the
code to (b). We now rearrange the tree as shown in (c), so that the word lengths are ordered
by increasing length from top to bottom. Finally, we swap probability assignments to improve
the expected depth of the tree, as shown in (d ). Every optimal code can be rearranged and
swapped into canonical form as in (d ), where l1 â¤ l2 â¤ Â· Â· Â· â¤ lm and lmâ1 = lm , and the last
two codewords differ only in the last bit.

â¢

â¢


But pj â pk > 0, and since Cm is optimal, L(Cm
) â L(Cm ) â¥ 0.
Hence, we must have lk â¥ lj . Thus, Cm itself satisï¬es property 1.
The two longest codewords are of the same length. Here we trim the
codewords. If the two longest codewords are not of the same length,
one can delete the last bit of the longer one, preserving the preï¬x
property and achieving lower expected codeword length. Hence, the
two longest codewords must have the same length. By property 1, the
longest codewords must belong to the least probable source symbols.
The two longest codewords differ only in the last bit and correspond
to the two least likely symbols. Not all optimal codes satisfy this
property, but by rearranging, we can ï¬nd an optimal code that does.
If there is a maximal-length codeword without a sibling, we can delete
the last bit of the codeword and still satisfy the preï¬x property. This
reduces the average codeword length and contradicts the optimality

5.8 OPTIMALITY OF HUFFMAN CODES

125

of the code. Hence, every maximal-length codeword in any optimal
code has a sibling. Now we can exchange the longest codewords so
that the two lowest-probability source symbols are associated with
two
 siblings on the tree. This does not change the expected length,
pi li . Thus, the codewords for the two lowest-probability source
symbols have maximal length and agree in all but the last bit.
Summarizing, we have shown that if p1 â¥ p2 â¥ Â· Â· Â· â¥ pm , there exists
an optimal code with l1 â¤ l2 â¤ Â· Â· Â· â¤ lmâ1 = lm , and codewords C(xmâ1 )

and C(xm ) that differ only in the last bit.
Thus, we have shown that there exists an optimal code satisfying the properties of the lemma. We call such codes canonical codes.
For any probability mass function for an alphabet of size m, p =
(p1 , p2 , . . . , pm ) with p1 â¥ p2 â¥ Â· Â· Â· â¥ pm , we deï¬ne the Huffman reduction p = (p1 , p2 , . . . , pmâ2 , pmâ1 + pm ) over an alphabet of size m â 1
â
â
(p ) be an optimal code for p , and let Cm
(p) be
(Figure 5.4). Let Cmâ1
the canonical optimal code for p.
The proof of optimality will follow from two constructions: First, we
expand an optimal code for p to construct a code for p, and then we

0

p1
0

1
1

0

p2

1

p3

0

p4

1

p5

0

p1
0

1
1

(a)

0

p2

1

p3

p4 + p5
(b)

0

p1
p4 + p5

0
1
1

0

p2

1

p3

(c)

FIGURE 5.4. Induction step for Huffman coding. Let p1 â¥ p2 â¥ Â· Â· Â· â¥ p5 . A canonical
optimal code is illustrated in (a). Combining the two lowest probabilities, we obtain the
code in (b). Rearranging the probabilities in decreasing order, we obtain the canonical code
in (c) for m â 1 symbols.

126

DATA COMPRESSION

condense an optimal canonical code for p to construct a code for the
Huffman reduction p . Comparing the average codeword lengths for the
two codes establishes that the optimal code for p can be obtained by
extending the optimal code for p .
From the optimal code for p , we construct an extension code for m
â
corresponding to weight
elements as follows: Take the codeword in Cmâ1
pmâ1 + pm and extend it by adding a 0 to form a codeword for symbol
m â 1 and by adding 1 to form a codeword for symbol m. The code
construction is illustrated as follows:
â
(p )
Cmâ1
w1
w2
..
.

p1
p2
..
.


wmâ2

wmâ1

pmâ2
pmâ1 + pm

Cm (p)
w1 = w1
w2 = w2
..
.

l1
l2
..
.


lmâ2

lmâ1

Calculation of the average length


wmâ2 = wmâ2

wmâ1 = wmâ1 0

wm = wmâ1
1


i

pi li shows that

l1 = l1
l2 = l2
..
.


lmâ2 = lmâ2

lmâ1 = lmâ1 + 1

lm = lmâ1
+1
(5.65)

L(p) = Lâ (p ) + pmâ1 + pm .

(5.66)

Similarly, from the canonical code for p, we construct a code for p by
merging the codewords for the two lowest-probability symbols m â 1 and
m with probabilities pmâ1 and pm , which are siblings by the properties
of the canonical code. The new code for p has average length


L(p ) =

mâ2


pi li + pmâ1 (lmâ1 â 1) + pm (lm â 1)

(5.67)

pi li â pmâ1 â pm

(5.68)

i=1

=

m

i=1

= Lâ (p) â pmâ1 â pm .

(5.69)

Adding (5.66) and (5.69) together, we obtain
L(p ) + L(p) = Lâ (p ) + Lâ (p)

(5.70)

(L(p ) â Lâ (p )) + (L(p) â Lâ (p)) = 0.

(5.71)

or

5.9

SHANNONâFANOâELIAS CODING

127

Now examine the two terms in (5.71). By assumption, since Lâ (p ) is the
optimal length for p , we have L(p ) â Lâ (p ) â¥ 0. Similarly, the length
of the extension of the optimal code for p has to have an average length
at least as large as the optimal code for p [i.e., L(p) â Lâ (p) â¥ 0]. But
the sum of two nonnegative terms can only be 0 if both of them are 0,
which implies that L(p) = Lâ (p) (i.e., the extension of the optimal code
for p is optimal for p).
Consequently, if we start with an optimal code for p with m â 1 symbols and construct a code for m symbols by extending the codeword
corresponding to pmâ1 + pm , the new code is also optimal. Starting with
a code for two elements, in which case the optimal code is obvious, we
can by induction extend this result to prove the following theorem.
Theorem 5.8.1 Huffman coding is optimal; that is, if C â is a Huffman
code and C  is any other uniquely decodable code, L(C â ) â¤ L(C  ).
Although we have proved the theorem for a binary alphabet, the proof
can be extended to establishing optimality of the Huffman coding algorithm for a D-ary alphabet as well. Incidentally, we should remark that
Huffman coding is a âgreedyâ algorithm in that it coalesces the two least
likely symbols at each stage. The proof above shows that this local optimality ensures global optimality of the ï¬nal code.
5.9

SHANNONâFANOâELIAS CODING

	


1
In Section 5.4 we showed that the codeword lengths l(x) = log p(x)
satisfy the Kraft inequality and can therefore be used to construct a uniquely
decodable code for the source. In this section we describe a simple constructive procedure that uses the cumulative distribution function to allot
codewords.
Without loss of generality, we can take X = {1, 2, . . . , m}. Assume that
p(x) > 0 for all x. The cumulative distribution function F (x) is deï¬ned
as

F (x) =
p(a).
(5.72)
aâ¤x

This function is illustrated in Figure 5.5. Consider the modiï¬ed cumulative
distribution function

1
(5.73)
F (x) =
p(a) + p(x),
2
a<x

128

DATA COMPRESSION
F(x)

F(x)
F(x)
F(x â 1)

p(x)

1

2

x

x

FIGURE 5.5. Cumulative distribution function and ShannonâFanoâElias coding.

where F (x) denotes the sum of the probabilities of all symbols less than
x plus half the probability of the symbol x. Since the random variable is
discrete, the cumulative distribution function consists of steps of size p(x).
The value of the function F (x) is the midpoint of the step corresponding
to x.
Since all the probabilities are positive, F (a) = F (b) if a = b, and hence
we can determine x if we know F (x). Merely look at the graph of the
cumulative distribution function and ï¬nd the corresponding x. Thus, the
value of F (x) can be used as a code for x.
But, in general, F (x) is a real number expressible only by an inï¬nite
number of bits. So it is not efï¬cient to use the exact value of F (x) as a
code for x. If we use an approximate value, what is the required accuracy?
Assume that we truncate F (x) to l(x) bits (denoted by F (x)l(x) ).
Thus, we use the ï¬rst l(x) bits of F (x) as a code for x. By deï¬nition of
rounding off, we have
F (x) â F (x)l(x) <


	
1
+ 1, then
If l(x) = log p(x)
1
2l(x)

<

1
2l(x)

.

p(x)
= F (x) â F (x â 1),
2

(5.74)

(5.75)

and therefore F (x)l(x) lies within the step corresponding to x. Thus,
l(x) bits sufï¬ce to describe x.

5.9

SHANNONâFANOâELIAS CODING

129

In addition to requiring that the codeword identify the corresponding
symbol, we also require the set of codewords to be preï¬x-free. To check
whether the code is preï¬x-free, we consider
each codeword z1 z2 Â· Â·Â· zl to

represent not a point but the interval 0.z1 z2 Â· Â· Â· zl , 0.z1 z2 Â· Â· Â· zl + 21l . The
code is preï¬x-free if and only if the intervals corresponding to codewords
are disjoint.
We now verify that the code above is preï¬x-free. The interval corresponding to any codeword has length 2âl(x) , which is less than half the
height of the step corresponding to x by (5.75). The lower end of the
interval is in the lower half of the step. Thus, the upper end of the interval lies below the top of the step, and the interval corresponding to any
codeword lies entirely within the step corresponding to that symbol in the
cumulative distribution function. Therefore, the intervals corresponding to
different codewords are disjoint and the code is preï¬x-free. Note that this
procedure does not require the symbols to be ordered in terms of probability. Another procedure that uses the ordered probabilities is described
in Problem 5.5.28.


	
1
+ 1 bits to represent x, the expected
Since we use l(x) = log p(x)
length of this code is





1
L=
p(x)l(x) =
p(x) log
+ 1 < H (X) + 2. (5.76)
p(x)
x
x
Thus, this coding scheme achieves an average codeword length that is
within 2 bits of the entropy.
Example 5.9.1 We ï¬rst consider an example where all the probabilities
are dyadic. We construct the code in the following table:
x p(x)

F (x)

F (x)

1
2
3
4

0.25
0.75
0.875
1.0

0.125
0.5
0.8125
0.9375

0.25
0.5
0.125
0.125


F (x) in Binary l(x) = log
0.001
0.10
0.1101
0.1111

3
2
4
4


1
+ 1 Codeword
p(x)
001
10
1101
1111

In this case, the average codeword length is 2.75 bits and the entropy
is 1.75 bits. The Huffman code for this case achieves the entropy
bound. Looking at the codewords, it is obvious that there is some inefï¬ciencyâfor example, the last bit of the last two codewords can be
omitted. But if we remove the last bit from all the codewords, the code
is no longer preï¬x-free.

130

DATA COMPRESSION

Example 5.9.2 We now give another example for construction of the
ShannonâFanoâElias code. In this case, since the distribution is not
dyadic, the representation of F (x) in binary may have an inï¬nite number
of bits. We denote 0.01010101 . . . by 0.01. We construct the code in the
following table:
x

p(x)

F (x)

F (x)

F (x) in Binary

1
2
3
4
5

0.25
0.25
0.2
0.15
0.15

0.25
0.5
0.7
0.85
1.0

0.125
0.375
0.6
0.775
0.925

0.001
0.011
0.10011
0.1100011
0.1110110


l(x) = log
3
3
4
4
4


1
+1
p(x)

Codeword
001
011
1001
1100
1110

The above code is 1.2 bits longer on the average than the Huffman
code for this source (Example 5.6.1).
The ShannonâFanoâElias coding procedure can also be applied to
sequences of random variables. The key idea is to use the cumulative
distribution function of the sequence, expressed to the appropriate accuracy, as a code for the sequence. Direct application of the method to blocks
of length n would require calculation of the probabilities and cumulative
distribution function for all sequences of length n, a calculation that would
grow exponentially with the block length. But a simple trick ensures that
we can calculate both the probability and the cumulative density function sequentially as we see each symbol in the block, ensuring that the
calculation grows only linearly with the block length. Direct application
of ShannonâFanoâElias coding would also need arithmetic whose precision grows with the block size, which is not practical when we deal with
long blocks. In Chapter 13 we describe arithmetic coding, which is an
extension of the ShannonâFanoâElias method to sequences of random
variables that encodes using ï¬xed-precision arithmetic with a complexity
that is linear in the length of the sequence. This method is the basis of
many practical compression schemes such as those used in the JPEG and
FAX compression standards.
5.10

COMPETITIVE OPTIMALITY OF THE SHANNON CODE

We have shown that Huffman coding is optimal in that it has minimum
expected length. But what does that say about its performance on any
particular sequence? For example, is it always better than any other code
for all sequences? Obviously not, since there are codes that assign short

5.10 COMPETITIVE OPTIMALITY OF THE SHANNON CODE

131

codewords to infrequent source symbols. Such codes will be better than
the Huffman code on those source symbols.
To formalize the question of competitive optimality, consider the following two-person zero-sum game: Two people are given a probability
distribution and are asked to design an instantaneous code for the distribution. Then a source symbol is drawn from this distribution, and the
payoff to player A is 1 or â1, depending on whether the codeword of
player A is shorter or longer than the codeword of player B. The payoff
is 0 for ties.
Dealing with Huffman code lengths is difï¬cult, since there is no explicit
expression for the codeword lengths.
	 Instead,
 we consider the Shannon
1
. In this case, we have the
code with codeword lengths l(x) = log p(x)
following theorem.
Theorem 5.10.1 Let l(x) be the codeword lengths associated with the
Shannon code, and let l  (x) be the codeword lengths associated with any
other uniquely decodable code. Then


1
Pr l(X) â¥ l  (X) + c â¤ câ1 .
(5.77)
2
For example, the probability that l  (X) is 5 or more bits shorter than
1
l(X) is less than 16
.
Proof



1

â¥ l (X) + c
log
p(X)


1

â¤ Pr log
â¥ l (X) + c â 1
p(X)



= Pr p(X) â¤ 2âl (X)âc+1

=
p(x)



Pr l(X) â¥ l  (X) + c = Pr



(5.78)
(5.79)
(5.80)
(5.81)


x: p(x)â¤2âl (x)âc+1



â¤

x: p(x)â¤2

â¤





2âl (x)â(câ1)

(5.82)

âl  (x)âc+1



2âl (x) 2â(câ1)

(5.83)

x

since



â¤ 2â(câ1)
âl  (x)

2

â¤ 1 by the Kraft inequality.

(5.84)


132

DATA COMPRESSION

Hence, no other code can do much better than the Shannon code most
of the time. We now strengthen this result. In a game-theoretic setting,
one would like to ensure that l(x) < l  (x) more often than l(x) > l  (x).
The fact that l(x) â¤ l  (x) + 1 with probability â¥ 12 does not ensure this.
We now show that even under this stricter criterion, Shannon coding is
1
optimal. Recall that the probability mass function p(x) is dyadic if log p(x)
is an integer for all x.
Theorem 5.10.2
For a dyadic probability mass function p(x), let
1
be the word lengths of the binary Shannon code for the
l(x) = log p(x)
source, and let l  (x) be the lengths of any other uniquely decodable binary
code for the source. Then
Pr(l(X) < l  (X)) â¥ Pr(l(X) > l  (X)),

(5.85)

l  (x)

with equality if and only if
= l(x) for all x. Thus, the code length
1
assignment l(x) = log p(x) is uniquely competitively optimal.
Proof: Deï¬ne the function sgn(t) as follows:
ï£±
if t > 0
ï£² 1
0
if t = 0
sgn(t) =
ï£³ â1 if t < 0

(5.86)

Then it is easy to see from Figure 5.6 that
sgn(t) â¤ 2t â 1 for t = 0, Â±1, Â±2, . . . .

(5.87)

sgn(x)
2t â 1
1

â1

1

FIGURE 5.6. Sgn function and a bound.

x

133

5.10 COMPETITIVE OPTIMALITY OF THE SHANNON CODE

Note that though this inequality is not satisï¬ed for all t, it is satisï¬ed
at all integer values of t. We can now write


p(x) â
p(x)
Pr(l  (X) < l(X)) â Pr(l  (X) > l(X)) =
x:l  (x)<l(x)

=



x:l  (x)>l(x)

(5.88)
p(x)sgn(l(x) â l  (x))

x

(5.89)


(5.90)
= E sgn l(X) â l  (X)


(a) 

â¤
p(x) 2l(x)âl (x) â 1
x





=
2âl(x) 2l(x)âl (x) â 1

(5.91)

x

=





2âl (x) â



x

=



(5.92)
2âl(x)

(5.93)

x


2âl (x) â 1

(5.94)

x
(b)

â¤ 1â1

(5.95)

= 0,

(5.96)

where (a) follows from the bound on sgn(x) and (b) follows from the fact
that l  (x) satisï¬es the Kraft inequality.
We have equality in the above chain only if we have equality in (a)
and (b). We have equality in the bound for sgn(t) only if t is 0 or 1 [i.e.,
l(x) = l  (x) or l(x) = l  (x) + 1]. Equality in (b) implies that l  (x) satisï¬es
the Kraft inequality with equality. Combining these two facts implies that

l  (x) = l(x) for all x.
Corollary

For nondyadic probability mass functions,
E sgn(l(X) â l  (X) â 1) â¤ 0,

	


1
where l(x) = log p(x)
and l  (x) is any other code for the source.

(5.97)

134

DATA COMPRESSION

Proof: Along the same lines as the preceding proof.



	


1
Hence we have shown that Shannon coding l(x) = log p(x)
is optimal under a variety of criteria; it is robust with respect to the payoff
function. In particular, for dyadic p, E(l â l  ) â¤ 0, E sgn(l â l  ) â¤ 0, and
by use of inequality (5.87), Ef (l â l  ) â¤ 0 for any function f satisfying
f (t) â¤ 2t â 1, t = 0, Â±1, Â±2, . . ..
5.11 GENERATION OF DISCRETE DISTRIBUTIONS FROM FAIR
COINS
In the early sections of this chapter we considered the problem of representing a random variable by a sequence of bits such that the expected
length of the representation was minimized. It can be argued (Problem 5.5.29) that the encoded sequence is essentially incompressible and
therefore has an entropy rate close to 1 bit per symbol. Therefore, the bits
of the encoded sequence are essentially fair coin ï¬ips.
In this section we take a slight detour from our discussion of source
coding and consider the dual question. How many fair coin ï¬ips does
it take to generate a random variable X drawn according to a speciï¬ed
probability mass function p? We ï¬rst consider a simple example.
Example 5.11.1 Given a sequence of fair coin tosses (fair bits), suppose
that we wish to generate a random variable X with distribution
ï£±
ï£² a with probability 12 ,
X=
b with probability 14 ,
ï£³
c with probability 14 .

(5.98)

It is easy to guess the answer. If the ï¬rst bit is 0, we let X = a. If the
ï¬rst two bits are 10, we let X = b. If we see 11, we let X = c. It is clear
that X has the desired distribution.
We calculate the average number of fair bits required for generating
the random variable, in this case as 12 (1) + 14 (2) + 14 (2) = 1.5 bits. This
is also the entropy of the distribution. Is this unusual? No, as the results
of this section indicate.
The general problem can now be formulated as follows. We are given a
sequence of fair coin tosses Z1 , Z2 , . . . , and we wish to generate a discrete
random variable X â X = {1, 2, . . . , m} with probability mass function

5.11 GENERATION OF DISCRETE DISTRIBUTIONS FROM FAIR COINS

135

a

b

c

FIGURE 5.7. Tree for generation of the distribution ( 12 , 14 , 14 ).

p = (p1 , p2 , . . . , pm ). Let the random variable T denote the number of
coin ï¬ips used in the algorithm.
We can describe the algorithm mapping strings of bits Z1 , Z2 , . . . , to
possible outcomes X by a binary tree. The leaves of the tree are marked
by output symbols X, and the path to the leaves is given by the sequence
of bits produced by the fair coin. For example, the tree for the distribution
( 12 , 14 , 14 ) is shown in Figure 5.7.
The tree representing the algorithm must satisfy certain properties:
1. The tree should be complete (i.e., every node is either a leaf or has
two descendants in the tree). The tree may be inï¬nite, as we will
see in some examples.
2. The probability of a leaf at depth k is 2âk . Many leaves may be
labeled with the same output symbolâthe total probability of all
these leaves should equal the desired probability of the output symbol.
3. The expected number of fair bits ET required to generate X is equal
to the expected depth of this tree.
There are many possible algorithms that generate the same output distribution. For example, the mapping 00 â a, 01 â b, 10 â c, 11 â a
also yields the distribution ( 12 , 14 , 14 ). However, this algorithm uses two
fair bits to generate each sample and is therefore not as efï¬cient as the
mapping given earlier, which used only 1.5 bits per sample. This brings
up the question: What is the most efï¬cient algorithm to generate a given
distribution, and how is this related to the entropy of the distribution?
We expect that we need at least as much randomness in the fair bits as
we produce in the output samples. Since entropy is a measure of randomness, and each fair bit has an entropy of 1 bit, we expect that the number
of fair bits used will be at least equal to the entropy of the output. This
is proved in the following theorem. We will need a simple lemma about
trees in the proof of the theorem. Let Y denote the set of leaves of a complete tree. Consider a distribution on the leaves such that the probability

136

DATA COMPRESSION

of a leaf at depth k on the tree is 2âk . Let Y be a random variable with
this distribution. Then we have the following lemma.
Lemma 5.11.1 For any complete tree, consider a probability distribution on the leaves such that the probability of a leaf at depth k is 2âk . Then
the expected depth of the tree is equal to the entropy of this distribution.
Proof: The expected depth of the tree

ET =
k(y)2âk(y)

(5.99)

yâY

and the entropy of the distribution of Y is

1
1
H (Y ) = â yâY 2k(y)
log 2k(y)

= yâY k(y)2âk(y) ,

(5.100)
(5.101)

where k(y) denotes the depth of leaf y. Thus,
H (Y ) = ET .



(5.102)

Theorem 5.11.1 For any algorithm generating X, the expected number
of fair bits used is greater than the entropy H (X), that is,
ET â¥ H (X).

(5.103)

Proof: Any algorithm generating X from fair bits can be represented by
a complete binary tree. Label all the leaves of this tree by distinct symbols
y â Y = {1, 2, . . .}. If the tree is inï¬nite, the alphabet Y is also inï¬nite.
Now consider the random variable Y deï¬ned on the leaves of the tree,
such that for any leaf y at depth k, the probability that Y = y is 2âk .
By Lemma 5.11.1, the expected depth of this tree is equal to the entropy
of Y :
ET = H (Y ).

(5.104)

Now the random variable X is a function of Y (one or more leaves
map onto an output symbol), and hence by the result of Problem 2.4, we
have
H (X) â¤ H (Y ).

(5.105)

137

5.11 GENERATION OF DISCRETE DISTRIBUTIONS FROM FAIR COINS

Thus, for any algorithm generating the random variable X, we have
H (X) â¤ ET .



(5.106)

The same argument answers the question of optimality for a dyadic distribution.
Theorem 5.11.2 Let the random variable X have a dyadic distribution. The optimal algorithm to generate X from fair coin ï¬ips requires an
expected number of coin tosses precisely equal to the entropy:
ET = H (X).

(5.107)

Proof: Theorem 5.11.1 shows that we need at least H (X) bits to generate
X. For the constructive part, we use the Huffman code tree for X as
the tree to generate the random variable. For a dyadic distribution, the
Huffman code is the same as the Shannon code and achieves the entropy
bound. For any x â X, the depth of the leaf in the code tree corresponding
1
. Hence,
to x is the length of the corresponding codeword, which is log p(x)
when this code tree is used to generate X, the leaf will have a probability
1
â log p(x)
= p(x). The expected number of coin ï¬ips is the expected depth
2
of the tree, which is equal to the entropy (because the distribution is
dyadic). Hence, for a dyadic distribution, the optimal generating algorithm
achieves
ET = H (X). 

(5.108)

What if the distribution is not dyadic? In this case we cannot use the
same idea, since the code tree for the Huffman code will generate a dyadic
distribution on the leaves, not the distribution with which we started. Since
all the leaves of the tree have probabilities of the form 2âk , it follows that
we should split any probability pi that is not of this form into atoms of this
form. We can then allot these atoms to leaves on the tree. For example, if
one of the outcomes x has probability p(x) = 14 , we need only one atom
(leaf of the tree at level 2), but if p(x) = 78 = 12 + 14 + 18 , we need three
atoms, one each at levels 1, 2, and 3 of the tree.
To minimize the expected depth of the tree, we should use atoms with
as large a probability as possible. So given a probability pi , we ï¬nd the
largest atom of the form 2âk that is less than pi , and allot this atom to
the tree. Then we calculate the remainder and ï¬nd that largest atom that
will ï¬t in the remainder. Continuing this process, we can split all the

138

DATA COMPRESSION

probabilities into dyadic atoms. This process is equivalent to ï¬nding the
binary expansions of the probabilities. Let the binary expansion of the
probability pi be
pi =



(j )

pi ,

(5.109)

j â¥1

where pi = 2âj or 0. Then the atoms of the expansion are the {pi :
i = 1, 2, .
. . , m, j â¥ 1}.
Since
i pi = 1, the sum of the probabilities of these atoms is 1.
We will allot an atom of probability 2âj to a leaf at depth j on the
tree. The depths of the atoms satisfy the Kraft inequality, and hence by
Theorem 5.2.1, we can always construct such a tree with all the atoms at
the right depths. We illustrate this procedure with an example.
(j )

Example 5.11.2

(j )

Let X have the distribution

X=

a with probability 23 ,
b with probability 13 .

(5.110)

We ï¬nd the binary expansions of these probabilities:
2
= 0.10101010 . . .2
3
1
= 0.01010101 . . .2 .
3

(5.111)
(5.112)

Hence, the atoms for the expansion are
2
â
3
1
â
3




1 1 1
, , ,...
2 8 32




1 1 1
, , ,... .
4 16 64

(5.113)
(5.114)

These can be allotted to a tree as shown in Figure 5.8.
This procedure yields a tree that generates the random variable X.
We have argued that this procedure is optimal (gives a tree of minimum
expected depth), but we will not give a formal proof. Instead, we bound
the expected depth of the tree generated by this procedure.

5.11 GENERATION OF DISCRETE DISTRIBUTIONS FROM FAIR COINS

139

a

b
a

b

FIGURE 5.8. Tree to generate a ( 32 , 13 ) distribution.

Theorem 5.11.3 The expected number of fair bits required by the optimal algorithm to generate a random variable X lies between H (X) and
H (X) + 2:
H (X) â¤ ET < H (X) + 2.

(5.115)

Proof: The lower bound on the expected number of coin tosses is proved
in Theorem 5.11.1. For the upper bound, we write down an explicit
expression for the expected number of coin tosses required for the procedure described above. We split all the probabilities (p1 , p2 , . . . , pm ) into
dyadic atoms, for example,


(1)
(2)
(5.116)
p1 â p1 , p1 , . . . ,
and so on. Using these atoms (which form a dyadic distribution), we
construct a tree with leaves corresponding to each of these atoms. The
number of coin tosses required to generate each atom is its depth in the
tree, and therefore the expected number of coin tosses is the expected
depth of the tree, which is equal to the entropy of the dyadic distribution
of the atoms. Hence,
ET = H (Y ),

(5.117)

(1)
(2)
, pm
, . . .).
where Y has the distribution, (p1(1), p1(2), . . . , p2(1), p2(2), . . . , pm
Now since X is a function of Y , we have

H (Y ) = H (Y, X) = H (X) + H (Y |X),

(5.118)

140

DATA COMPRESSION

and our objective is to show that H (Y |X) < 2. We now give an algebraic
proof of this result. Expanding the entropy of Y , we have
H (Y ) = â
=

m 
i=1

(j )
(j )
j â¥1 pi log pi

i=1

j :pi >0

m 

(j )

(5.119)

j 2âj ,

(5.120)

since each of the atoms is either 0 or 2âk for some k. Now consider the
term in the expansion corresponding to each i, which we shall call Ti :

j 2âj .
(5.121)
Ti =
(j )

j :pi >0

We can ï¬nd an n such that 2â(nâ1) > pi â¥ 2ân , or
n â 1 < â log pi â¤ n.
(j )

Then it follows that pi
as

(5.122)

> 0 only if j â¥ n, so that we can rewrite (5.121)


j 2âj .

(5.123)

We use the deï¬nition of the atom to write pi as

pi =
2âj .

(5.124)

Ti =

(j )
j :j â¥n,pi >0

(j )
j :j â¥n,pi >0

To prove the upper bound, we ï¬rst show that Ti < âpi log pi + 2pi .
Consider the difference
(a)
(5.125)
Ti + pi log pi â 2pi < Ti â pi (n â 1) â 2pi
= Ti â (n â 1 + 2)pi

=
j 2âj â (n + 1)
(j )

=

(j )
j :j â¥n,pi >0

(5.126)
2âj

(j )

j :j â¥n,pi >0




j :j â¥n,pi >0

(5.127)
(j â n â 1)2âj

(5.128)

141

SUMMARY



= â2ân + 0 +

(j â n â 1)2âj
(j )

j :j â¥n+2,pi >0

(5.129)


(b)

= â2ân +

k2â(k+n+1)

(5.130)

(k+n+1)
k:kâ¥1,pi
>0

( c)

â¤ â2ân +



k2â(k+n+1)

(5.131)

k:kâ¥1

= â2ân + 2â(n+1) 2

(5.132)

= 0,

(5.133)

where (a) follows from (5.122), (b) follows from a change of variables
for the summation, and (c) follows from increasing the range of the summation. Hence, we have shown that
Ti < âpi log pi + 2pi .
Since ET =


i

(5.134)

Ti , it follows immediately that

ET < â



pi log pi + 2



i

pi = H (X) + 2,

(5.135)

i

completing the proof of the theorem.


Thus, an average of H (X) + 2 coin ï¬ips sufï¬ce to simulate a random
variable X.

SUMMARY



D âli â¤ 1.

McMillan inequality. Uniquely decodable codes â D âli â¤ 1.

Kraft inequality. Instantaneous codes â

Entropy bound on data compression
 
pi li â¥ HD (X).
L=

(5.136)

142

DATA COMPRESSION

Shannon code



1
li = logD
pi

(5.137)

HD (X) â¤ L < HD (X) + 1.

(5.138)

Huffman code
Lâ =  min



D âli â¤1

pi li

(5.139)

HD (X) â¤ Lâ < HD (X) + 1.
(5.140)


	

1
, L = p(x)l(x):
Wrong code. X â¼ p(x), l(x) = log q(x)
H (p) + D(p||q) â¤ L < H (p) + D(p||q) + 1.

(5.141)

Stochastic processes
H (X1 , X2 , . . . , Xn ) 1
H (X1 , X2 , . . . , Xn )
â¤ Ln <
+ .
n
n
n

(5.142)

Stationary processes
Ln â H (X).

(5.143)

	


1
Competitive optimality. Shannon code l(x) = log p(x)
versus any

other code l (x):


Pr l(X) â¥ l  (X) + c â¤

1
2câ1

.

(5.144)

PROBLEMS
5.1

Uniquely
decodable
and
instantaneous
codes. Let

100
p
l
be
the
expected
value
of
the
100th
power
L= m
i
i=1
i
of the word lengths associated with an encoding of the random
variable X. Let L1 = min L over all instantaneous codes; and let
L2 = min L over all uniquely decodable codes. What inequality
relationship exists between L1 and L2 ?

PROBLEMS

5.2

143

How many ï¬ngers has a Martian? Let


S1 , . . . , Sm
S=
.
p1 , . . . , pm
The Si âs are encoded into strings from a D-symbol output alphabet
in a uniquely decodable manner. If m = 6 and the codeword lengths
are (l1 , l2 , . . . , l6 ) = (1, 1, 2, 3, 2, 3), ï¬nd a good lower bound on
D. You may wish to explain the title of the problem.

5.3

Slackness in the Kraft inequality. An instantaneous code has word
lengths l1 , l2 , . . . , lm , which satisfy the strict inequality
m


D âli < 1.

i=1

The code alphabet is D = {0, 1, 2, . . . , D â 1}. Show that there
exist arbitrarily long sequences of code symbols in Dâ which cannot
be decoded into sequences of codewords.
5.4

Huffman coding. Consider the random variable


x1
x2
x3
x4
x5
x6
x7
X=
.
0.49 0.26 0.12 0.04 0.04 0.03 0.02
(a) Find a binary Huffman code for X.
(b) Find the expected code length for this encoding.
(c) Find a ternary Huffman code for X.

5.5

More Huffman codes. Find the binary Huffman code for the
2 2
, 15 ). Argue that this code is
source with probabilities ( 13 , 15 , 15 , 15
also optimal for the source with probabilities ( 15 , 15 , 15 , 15 , 15 ).

5.6

Bad codes. Which of these codes cannot be Huffman codes for
any probability assignment?
(a) {0, 10, 11}
(b) {00, 01, 10, 110}
(c) {01, 10}

5.7

Huffman 20 questions. Consider a set of n objects. Let Xi =
1 or 0 accordingly as the ith object is good or defective. Let
X1 , X2 , . . . , Xn be independent with Pr{Xi = 1} = pi ; and p1 >
p2 > Â· Â· Â· > pn > 12 . We are asked to determine the set of all defective objects. Any yesâno question you can think of is admissible.

144

DATA COMPRESSION

(a) Give a good lower bound on the minimum average number of
questions required.
(b) If the longest sequence of questions is required by natureâs
answers to our questions, what (in words) is the last question we should ask? What two sets are we distinguishing with
this question? Assume a compact (minimum average length)
sequence of questions.
(c) Give an upper bound (within one question) on the minimum
average number of questions required.
5.8

Simple optimum compression of a Markov source. Consider the
three-state Markov process U1 , U2 , . . . having transition matrix
Un
Unâ1

S1

S2

S3

S1
S2

1
2
1
4

S3

0

1
4
1
2
1
2

1
4
1
4
1
2

Thus, the probability that S1 follows S3 is equal to zero. Design
three codes C1 , C2 , C3 (one for each state 1, 2 and 3, each code
mapping elements of the set of Si âs into sequences of 0âs and 1âs,
such that this Markov process can be sent with maximal compression by the following scheme:
(a) Note the present symbol Xn = i.
(b) Select code Ci .
(c) Note the next symbol Xn+1 = j and send the codeword in Ci
corresponding to j .
(d) Repeat for the next symbol. What is the average message length
of the next symbol conditioned on the previous state Xn = i
using this coding scheme? What is the unconditional average
number of bits per source symbol? Relate this to the entropy
rate H (U) of the Markov chain.
5.9

Optimal code lengths that require one bit above entropy. The
source coding theorem shows that the optimal code for a random
variable X has an expected length less than H (X) + 1. Give an
example of a random variable for which the expected length of the
optimal code is close to H (X) + 1 [i.e., for any  > 0, construct a
distribution for which the optimal code has L > H (X) + 1 â ].

PROBLEMS

5.10

145

Ternary codes that achieve the entropy bound . A random variable
X takes on m values and has entropy H (X). An instantaneous
ternary code is found for this source, with average length
L=

H (X)
= H3 (X).
log2 3

(5.145)

(a) Show that each symbol of X has a probability of the form 3âi
for some i.
(b) Show that m is odd.
5.11

Sufï¬x condition. Consider codes that satisfy the sufï¬x condition,
which says that no codeword is a sufï¬x of any other codeword.
Show that a sufï¬x condition code is uniquely decodable, and show
that the minimum average length over all codes satisfying the sufï¬x
condition is the same as the average length of the Huffman code
for that random variable.

5.12

Shannon codes and Huffman codes. Consider a random variable
1
X that takes on four values with probabilities ( 13 , 13 , 14 , 12
).
(a) Construct a Huffman code for this random variable.
(b) Show that there exist two different sets of optimal lengths
for the codewords; namely, show that codeword length assignments (1, 2, 3, 3) and (2, 2, 2, 2) are both optimal.
(c) Conclude that there are optimal codes with codeword lengths
for
	 some
 symbols that exceed the Shannon code length
1
.
log p(x)

5.13

Twenty questions. Player A chooses some object in the universe,
and player B attempts to identify the object with a series of yesâno
questions. Suppose that player B is clever enough to use the code
achieving the minimal expected length with respect to player Aâs
distribution. We observe that player B requires an average of 38.5
questions to determine the object. Find a rough lower bound to the
number of objects in the universe.

5.14

Huffman code. Find the (a) binary and (b) ternary Huffman codes
for the random variable X with probabilities

p=
(c) Calculate L =




1 2 3 4 5 6
.
, , , , ,
21 21 21 21 21 21

pi li in each case.

146

DATA COMPRESSION

5.15

Huffman codes
(a) Construct a binary Huffman code for the following distribution on ï¬ve symbols: p = (0.3, 0.3, 0.2, 0.1, 0.1). What is the
average length of this code?
(b) Construct a probability distribution p on ï¬ve symbols for
which the code that you constructed in part (a) has an average
length (under p ) equal to its entropy H (p ).

5.16

Huffman codes. Consider a random variable X that takes six values {A, B, C, D, E, F } with probabilities 0.5, 0.25, 0.1, 0.05, 0.05,
and 0.05, respectively.
(a) Construct a binary Huffman code for this random variable.
What is its average length?
(b) Construct a quaternary Huffman code for this random variable
[i.e., a code over an alphabet of four symbols (call them a, b, c
and d)]. What is the average length of this code?
(c) One way to construct a binary code for the random variable
is to start with a quaternary code and convert the symbols into
binary using the mapping a â 00, b â 01, c â 10, and d â
11. What is the average length of the binary code for the random
variable above constructed by this process?
(d) For any random variable X, let LH be the average length of
the binary Huffman code for the random variable, and let LQB
be the average length code constructed by ï¬rst building a quaternary Huffman code and converting it to binary. Show that
LH â¤ LQB < LH + 2.

(5.146)

(e) The lower bound in the example is tight. Give an example
where the code constructed by converting an optimal quaternary
code is also the optimal binary code.
(f) The upper bound (i.e., LQB < LH + 2) is not tight. In fact, a
better bound is LQB â¤ LH + 1. Prove this bound, and provide
an example where this bound is tight.
5.17

Data compression. Find an 
optimal set of binary codeword
pi li ) for an instantaneous code
lengths l1 , l2 , . . . (minimizing
for each of the following probability mass functions:
9 8 7 7
(a) p = ( 10
41 , 41 , 41 , 41 , 41 )
9
9
1
9
1 2
9
1 3
, ( 10
)( 10
), ( 10
)( 10
) , ( 10
)( 10
) , . . .)
(b) p = ( 10

PROBLEMS

5.18

Classes of codes.

147

Consider the code {0, 01}.

(a) Is it instantaneous?
(b) Is it uniquely decodable?
(c) Is it nonsingular?
5.19

The game of Hi-Lo
(a) A computer generates a number X according to a known probability mass function p(x), x â {1, 2, . . . , 100}. The player asks
a question, âIs X = i?â and is told âYes,â âYouâre too high,â
or âYouâre too low.â He continues for a total of six questions.
If he is right (i.e., he receives the answer âYesâ) during this
sequence, he receives a prize of value v(X). How should the
player proceed to maximize his expected winnings?
(b) Part (a) doesnât have much to do with information theory. Consider the following variation: X â¼ p(x), prize = v(x), p(x)
known, as before. But arbitrary yesâno questions are asked
sequentially until X is determined. (âDeterminedâ doesnât mean
that a âYesâ answer is received.) Questions cost 1 unit each.
How should the player proceed? What is the expected payoff?
(c) Continuing part (b), what if v(x) is ï¬xed but p(x) can be
chosen by the computer (and then announced to the player)?
The computer wishes to minimize the playerâs expected return.
What should p(x) be? What is the expected return to the
player?

5.20

Huffman codes with costs. Words such as âRun!â, âHelp!â, and
âFire!â are short, not because they are used frequently, but perhaps
because time is precious in the situations in which these words are
required. Suppose that X = i with probability pi , i = 1, 2, . . . , m.
Let li be the number of binary symbols in the codeword associated
with X = i, and let ci denote the cost per letter of the codeword
when
X = i. Thus, the average cost C of the description of X is
C= m
i=1 pi ci li .

(a) Minimize C over all l1 , l2 , . . . , lm such that 2âli â¤ 1. Ignore
any implied integer constraints on li . Exhibit the minimizing
â
l1â , l2â , . . . , lm
and the associated minimum value C â .
(b) How would you use the Huffman code procedure to minimize
C over all uniquely decodable codes? Let CHuffman denote this
minimum.

148

DATA COMPRESSION

(c) Can you show that
â

â

C â¤ CHuffman â¤ C +

m


pi ci ?

i=1

5.21

Conditions for unique decodability. Prove that a code C is
uniquely decodable if (and only if) the extension
C k (x1 , x2 , . . . , xk ) = C(x1 )C(x2 ) Â· Â· Â· C(xk )
is a one-to-one mapping from Xk to D â for every k â¥ 1. (The âonly
ifâ part is obvious.)

5.22

Average length of an optimal code. Prove that L(p1 , . . . , pm ),
the average codeword length for an optimal D-ary preï¬x code for
probabilities {p1 , . . . , pm }, is a continuous function of p1 , . . . , pm .
This is true even though the optimal code changes discontinuously
as the probabilities vary.

5.23

Unused code sequences. Let C be a variable-length code that
satisï¬es the Kraft inequality with an equality but does not satisfy
the preï¬x condition.
(a) Prove that some ï¬nite sequence of code alphabet symbols is
not the preï¬x of any sequence of codewords.
(b) (Optional ) Prove or disprove: C has inï¬nite decoding delay.

5.24

Optimal codes for uniform distributions. Consider a random variable with m equiprobable outcomes. The entropy of this information source is obviously log2 m bits.
(a) Describe the optimal instantaneous binary code for this source
and compute the average codeword length Lm .
(b) For what values of m does the average codeword length Lm
equal the entropy H = log2 m?
(c) We know that L < H + 1 for any probability distribution. The
redundancy of a variable-length code is deï¬ned to be Ï =
L â H . For what value(s) of m, where 2k â¤ m â¤ 2k+1 , is the
redundancy of the code maximized? What is the limiting value
of this worst-case redundancy as m â â?

5.25

Optimal codeword lengths. Although the codeword lengths of an
optimal variable-length code are complicated functions of the message probabilities {p1 , p2 , . . . , pm }, it can be said that less probable

PROBLEMS

149

symbols are encoded into longer codewords. Suppose that the message probabilities are given in decreasing order, p1 > p2 â¥ Â· Â· Â· â¥
pm .
(a) Prove that for any binary Huffman code, if the most probable
message symbol has probability p1 > 25 , that symbol must be
assigned a codeword of length 1.
(b) Prove that for any binary Huffman code, if the most probable
message symbol has probability p1 < 13 , that symbol must be
assigned a codeword of length â¥ 2.
5.26

Merges. Companies with values W1 , W2 , . . . , Wm are merged as
follows. The two least valuable companies are merged, thus forming a list of m â 1 companies. The value of the merge is the
sum of the values of the two merged companies. This continues until one supercompany remains. Let V equal the sum of
the values of the merges. Thus, V represents the total reported
dollar volume of the merges. For example, if W = (3, 3, 2, 2),
the merges yield (3, 3, 2, 2) â (4, 3, 3) â (6, 4) â (10) and V =
4 + 6 + 10 = 20.
(a) Argue that V is the minimum volume achievable by sequences
of pairwise merges terminating in one supercompany. (Hint:
Compare to Huffman coding.)

(b) Let W = Wi , WÌi = Wi /W , and show that the minimum
merge volume V satisï¬es
W H (WÌ) â¤ V â¤ W H (WÌ) + W.

5.27

(5.147)

SardinasâPatterson test for unique decodability. A code is not
uniquely decodable if and only if there exists a ï¬nite sequence of
code symbols which can be resolved into sequences of codewords
in two different ways. That is, a situation such as
A1
B1

B2

A2
B3

...

A3

...

Am
Bn

must occur where each Ai and each Bi is a codeword. Note that
B1 must be a preï¬x of A1 with some resulting âdangling sufï¬x.â
Each dangling sufï¬x must in turn be either a preï¬x of a codeword
or have another codeword as its preï¬x, resulting in another dangling sufï¬x. Finally, the last dangling sufï¬x in the sequence must
also be a codeword. Thus, one can set up a test for unique decodability (which is essentially the SardinasâPatterson test [456]) in

150

DATA COMPRESSION

the following way: Construct a set S of all possible dangling sufï¬xes. The code is uniquely decodable if and only if S contains no
codeword.
(a) State the precise rules for building the set S.
(b) Suppose that the codeword lengths are li , i = 1, 2, . . . , m. Find
a good upper bound on the number of elements in the set S.
(c) Determine which of the following codes is uniquely decodable:
(i) {0, 10, 11}
(ii) {0, 01, 11}
(iii) {0, 01, 10}
(iv) {0, 01}
(v) {00, 01, 10, 11}
(vi) {110, 11, 10}
(vii) {110, 11, 100, 00, 10}
(d) For each uniquely decodable code in part (c), construct, if possible, an inï¬nite encoded sequence with a known starting point
such that it can be resolved into codewords in two different
ways. (This illustrates that unique decodability does not imply
ï¬nite decodability.) Prove that such a sequence cannot arise in
a preï¬x code.
5.28

Shannon code. Consider the following method for generating a
code for a random variable X that takes on m values {1, 2, . . . , m}
with probabilities p1 , p2 , . . . , pm . Assume that the probabilities are
ordered so that p1 â¥ p2 â¥ Â· Â· Â· â¥ pm . Deï¬ne
Fi =

iâ1


pk ,

(5.148)

k=1

the sum of the probabilities of all symbols less than i. Then the
codeword for i is the number Fi â [0, 1] rounded off to li bits,
where li = 	log p1i 
.
(a) Show that the code constructed by this process is preï¬x-free
and that the average length satisï¬es
H (X) â¤ L < H (X) + 1.

(5.149)

(b) Construct the code for the probability distribution (0.5, 0.25,
0.125, 0.125).

PROBLEMS

151

5.29

Optimal codes for dyadic distributions. For a Huffman code tree,
deï¬ne the probability of a node as the sum of the probabilities of
all the leaves under that node. Let the random variable X be drawn
from a dyadic distribution [i.e., p(x) = 2âi , for some i, for all
x â X]. Now consider a binary Huffman code for this distribution.
(a) Argue that for any node in the tree, the probability of the left
child is equal to the probability of the right child.
(b) Let X1 , X2 , . . . , Xn be drawn i.i.d. â¼ p(x). Using the Huffman code for p(x), we map X1 , X2 , . . . , Xn to a sequence
of bits Y1 , Y2 , . . . , Yk(X1 ,X2 ,...,Xn ) . (The length of this sequence
will depend on the outcome X1 , X2 , . . . , Xn .) Use part (a) to
argue that the sequence Y1 , Y2 , . . . forms a sequence of fair coin
ï¬ips [i.e., that Pr{Yi = 0} = Pr{Yi = 1} = 12 , independent of
Y1 , Y2 , . . . , Yiâ1 ]. Thus, the entropy rate of the coded sequence
is 1 bit per symbol.
(c) Give a heuristic argument why the encoded sequence of bits
for any code that achieves the entropy bound cannot be compressible and therefore should have an entropy rate of 1 bit per
symbol.

5.30

Relative entropy is cost of miscoding. Let the random variable X
have ï¬ve possible outcomes {1, 2, 3, 4, 5}. Consider two distributions p(x) and q(x) on this random variable.
Symbol

p(x)

q(x)

C1 (x)

C2 (x)

1
2
3
4
5

1
2
1
4
1
8
1
16
1
16

1
2
1
8
1
8
1
8
1
8

0
10
110
1110
1111

0
100
101
110
111

(a) Calculate H (p), H (q), D(p||q), and D(q||p).
(b) The last two columns represent codes for the random variable.
Verify that the average length of C1 under p is equal to the
entropy H (p). Thus, C1 is optimal for p. Verify that C2 is
optimal for q.
(c) Now assume that we use code C2 when the distribution is p.
What is the average length of the codewords. By how much
does it exceed the entropy p?
(d) What is the loss if we use code C1 when the distribution is q?

152

DATA COMPRESSION

5.31

Nonsingular codes. The discussion in the text focused on instantaneous codes, with extensions to uniquely decodable codes. Both
these are required in cases when the code is to be used repeatedly
to encode a sequence of outcomes of a random variable. But if
we need to encode only one outcome and we know when we have
reached the end of a codeword, we do not need unique decodabilityâthe fact that the code is nonsingular would sufï¬ce. For
example, if a random variable X takes on three values, a, b, and c,
we could encode them by 0, 1, and 00. Such a code is nonsingular
but not uniquely decodable.
In the following, assume that we have a random variable X which
takes on m values with probabilities p1 , p2 , . . . , pm and that the
probabilities are ordered so that p1 â¥ p2 â¥ Â· Â· Â· â¥ pm .
(a) By viewing the nonsingular binary code as a ternary code with
three symbols, 0, 1, and âSTOP,â show that the expected length
of a nonsingular code L1:1 for a random variable X satisï¬es the
following inequality:
L1:1 â¥

H2 (X)
â 1,
log2 3

(5.150)

where H2 (X) is the entropy of X in bits. Thus, the average
length of a nonsingular code is at least a constant fraction of
the average length of an instantaneous code.
(b) Let LINST be the expected length of the best instantaneous code
and Lâ1:1 be the expected length of the best nonsingular code
for X. Argue that Lâ1:1 â¤ LâINST â¤ H (X) + 1.
(c) Give a simple example where the average length of the nonsingular code is less than the entropy.
(d) The set of codewords available for a nonsingular
code is {0, 1,

p
l
,
show
that this
00, 01, 10, 11, 000, . . .}. Since L1:1 = m
i
i
i=1
is minimized if we allot the shortest codewords to the most
probable symbols. Thus, l1 = l2 = 1, l3= l4 = l5 = l6 = 2, etc.
Show that
li = log 2i + 1 , and therefore Lâ1:1 =

 in general
m
i
i=1 pi log 2 + 1 .
(e) Part (d) shows that it is easy to ï¬nd the optimal nonsingular code for a distribution. However, it is a little more
tricky to deal with the average length of this code. We now
bound this average length. It follows from part (d) that Lâ1:1 â¥

PROBLEMS



LÌ =

m

i=1 pi

log

i
2

153


+ 1 . Consider the difference

F (p) = H (X) â LÌ = â

m


pi log pi â

i=1

m

i=1




i
pi log
+1 .
2

(5.151)
Prove by the method of Lagrange multipliers that the maximum
of F (p) occurs when pi = c/(i + 2), where c = 1/(Hm+2 â
H2 ) and Hk is the sum of the harmonic series:


Hk =

k

1
i=1

i

.

(5.152)

(This can also be done using the nonnegativity of relative
entropy.)
(f) Complete the arguments for
H (X) â Lâ1:1 â¤ H (X) â LÌ
â¤ log(2(Hm+2 â H2 )).

(5.153)
(5.154)

Now it is well known (see, e.g., Knuth [315]) that Hk â ln k
1
1
1
â 12k
(more precisely, Hk = ln k + Î³ + 2k
2 + 120k 4 â , where
0 <  < 1/252n6 , and Î³ = Eulerâs constant = 0.577 . . .).
Using either this or a simple approximation that Hk â¤ ln k + 1,
which can be proved by integration of x1 , it can be shown that
H (X) â Lâ1:1 < log log m + 2. Thus, we have
H (X) â log log |X| â 2 â¤ Lâ1:1 â¤ H (X) + 1.

(5.155)

A nonsingular code cannot do much better than an instantaneous
code!
5.32

Bad wine. One is given six bottles of wine. It is known that
precisely one bottle has gone bad (tastes terrible). From inspection
of the bottles it is determined that the probability pi that the ith
8 6 4 2 2 1
, 23 , 23 , 23 , 23 , 23 ).
bottle is bad is given by (p1 , p2 , . . . , p6 ) = ( 23
Tasting will determine the bad wine. Suppose that you taste the
wines one at a time. Choose the order of tasting to minimize the

154

DATA COMPRESSION

expected number of tastings required to determine the bad bottle.
Remember, if the ï¬rst ï¬ve wines pass the test, you donât have to
taste the last.
(a) What is the expected number of tastings required?
(b) Which bottle should be tasted ï¬rst?
Now you get smart. For the ï¬rst sample, you mix some of the wines
in a fresh glass and sample the mixture. You proceed, mixing and
tasting, stopping when the bad bottle has been determined.
(a) What is the minimum expected number of tastings required to
determine the bad wine?
(b) What mixture should be tasted ï¬rst?
5.33

Huffman vs. Shannon. A random variable X takes on three values
with probabilities 0.6, 0.3, and 0.1.
(a) What are the lengths of the binary Huffman codewords for
X?
lengths of the binary Shannon codewords
 What	 arethe 

1
l(x) = log p(x)
for X?
(b) What is the smallest integer D such that the expected Shannon
codeword length with a D-ary alphabet equals the expected
Huffman codeword length with a D-ary alphabet?

5.34

Huffman algorithm for tree construction. Consider the following
problem: m binary signals S1 , S2 , . . . , Sm are available at times
T1 â¤ T2 â¤ Â· Â· Â· â¤ Tm , and we would like to ï¬nd their sum S1 â S2 â
Â· Â· Â· â Sm using two-input gates, each gate with one time unit delay,
so that the ï¬nal result is available as quickly as possible. A simple
greedy algorithm is to combine the earliest two results, forming
the partial result at time max(T1 , T2 ) + 1. We now have a new
problem with S1 â S2 , S3 , . . . , Sm , available at times max(T1 , T2 ) +
1, T3 , . . . , Tm . We can now sort this list of T âs and apply the same
merging step again, repeating this until we have the ï¬nal result.
(a) Argue that the foregoing procedure is optimal, in that it constructs a circuit for which the ï¬nal result is available as quickly
as possible.
(b) Show that this procedure ï¬nds the tree that minimizes
C(T ) = max(Ti + li ),
i

(5.156)

where Ti is the time at which the result allotted to the ith leaf
is available and li is the length of the path from the ith leaf to
the root.

PROBLEMS

155

(c) Show that

C(T ) â¥ log2




2Ti

(5.157)

i

for any tree T .
(d) Show that there exists a tree such that



C(T ) â¤ log2
2Ti + 1.

(5.158)

i

Thus, log2
5.35


i



2Ti is the analog of entropy for this problem.

Generating random variables. One wishes to generate a random
variable X

1 with probability p
X=
(5.159)
0 with probability 1 â p.
You are given fair coin ï¬ips Z1 , Z2 , . . . . Let N be the (random)
number of ï¬ips needed to generate X. Find a good way to use
Z1 , Z2 , . . . to generate X. Show that EN â¤ 2.

5.36

Optimal word lengths.
(a) Can l = (1, 2, 2) be the word lengths of a binary Huffman
code. What about (2,2,3,3)?
(b) What word lengths l = (l1 , l2 , . . .) can arise from binary Huffman codes?

5.37

Codes. Which of the following codes are
(a) Uniquely decodable?
(b) Instantaneous?
C1
C2
C3
C4

5.38

= {00, 01, 0}
= {00, 01, 100, 101, 11}
= {0, 10, 110, 1110, . . .}
= {0, 00, 000, 0000}

Huffman. Find the Huffman D-ary code for (p1 , p2 , p3 , p4 , p5 ,
6 6 4 4 3 2
, 25 , 25 , 25 , 25 , 25 ) and the expected word length
p6 ) = ( 25
(a) For D = 2.
(b) For D = 4.

156

DATA COMPRESSION

5.39

Entropy of encoded bits. Let C : X ââ {0, 1}â be a nonsingular
but nonuniquely decodable code. Let X have entropy H (X).
(a) Compare H (C(X)) to H (X).
(b) Compare H (C(X n )) to H (X n ).

5.40

Code rate. Let X be a random
and distribution
ï£±
ï£´
ï£² 1 with
X=
2 with
ï£´
ï£³
3 with

variable with alphabet {1, 2, 3}
probability
probability
probability

1
2
1
4
1
4.

The data compression code for X assigns codewords
ï£±
if x = 1
ï£² 0
10 if x = 2
C(x) =
ï£³ 11 if x = 3.
Let X1 , X2 , . . . be independent, identically distributed according
to this distribution and let Z1 Z2 Z3 Â· Â· Â· = C(X1 )C(X2 ) Â· Â· Â· be the
string of binary symbols resulting from concatenating the corresponding codewords. For example, 122 becomes 01010.
(a) Find the entropy rate H (X) and the entropy rate H (Z) in bits
per symbol. Note that Z is not compressible further.
(b) Now let the code be
ï£±
ï£² 00 if x = 1
10 if x = 2
C(x) =
ï£³ 01 if x = 3
and ï¬nd the entropy rate H (Z).
(c) Finally, let the code be
ï£±
ï£² 00
1
C(x) =
ï£³ 01

if x = 1
if x = 2
if x = 3

and ï¬nd the entropy rate H (Z).
5.41

Optimal codes. Let l1 , l2 , . . . , l10 be the binary Huffman codeword lengths for the probabilities p1 â¥ p2 â¥ Â· Â· Â· â¥ p10 . Suppose
that we get a new distribution by splitting the last probability

HISTORICAL NOTES

157

mass. What can you say about the optimal binary codeword lengths
Ë for the probabilities p1 , p2 , . . . , p9 , Î±p10 , (1 â Î±)p10 ,
lË1 , lË2 , . . . , l11
where 0 â¤ Î± â¤ 1.
5.42

Ternary codes. Which of the following codeword lengths can be
the word lengths of a 3-ary Huffman code, and which cannot?
(a) (1, 2, 2, 2, 2)
(b) (2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3)

5.43

Piecewise Huffman. Suppose the codeword that we use to
describe a random variable X â¼ p(x) always starts with a symbol
chosen from the set {A, B, C}, followed by binary digits {0, 1}.
Thus, we have a ternary code for the ï¬rst symbol and binary
thereafter. Give the optimal uniquely decodable code (minimum
expected number of symbols) for the probability distribution


16 15 12 10 8 8
p=
, , , , ,
.
(5.160)
69 69 69 69 69 69

5.44

Huffman.
the word lengths of the optimal binary encoding
 1 Find
1
1
of p = 100
, 100
, . . . , 100
.

5.45

Random 20 questions. Let X be uniformly distributed over {1, 2,
. . . , m}. Assume that m = 2n . We ask random questions: Is X â S1 ?
Is X â S2 ?... until only one integer remains. All 2m subsets S of
{1, 2, . . . , m} are equally likely to be asked.
(a) Without loss of generality, suppose that X = 1 is the random
object. What is the probability that object 2 yields the same
answers for k questions as does object 1?
(b) What is the expected number of objects in {2, 3, . . . , m} that
have the same answers to the questions as does the correct
object 1?
â
(c) Suppose that we ask n + n random questions. What is the
expected number of wrong objects agreeing with the answers?
(d) Use Markovâs inequality Pr{X â¥ tÂµ} â¤ 1t , to show that the
probability of error (one or more wrong object remaining) goes
to zero as n ââ â.

HISTORICAL NOTES
The foundations for the material in this chapter can be found in Shannonâs original paper [469], in which Shannon stated the source coding

158

DATA COMPRESSION

theorem and gave simple examples of codes. He described a simple code
construction procedure (described in Problem 5.5.28), which he attributed
to Fano. This method is now called the ShannonâFano code construction
procedure.
The Kraft inequality for uniquely decodable codes was ï¬rst proved
by McMillan [385]; the proof given here is due to Karush [306]. The
Huffman coding procedure was ï¬rst exhibited and proved to be optimal
by Huffman [283].
In recent years, there has been considerable interest in designing source
codes that are matched to particular applications, such as magnetic recording. In these cases, the objective is to design codes so that the output
sequences satisfy certain properties. Some of the results for this problem
are described by Franaszek [219], Adler et al. [5] and Marcus [370].
The arithmetic coding procedure has its roots in the ShannonâFano
code developed by Elias (unpublished), which was analyzed by Jelinek
[297]. The procedure for the construction of a preï¬x-free code described
in the text is due to Gilbert and Moore [249]. The extension of the
ShannonâFanoâElias method to sequences is based on the enumerative
methods in Cover [120] and was described with ï¬nite-precision arithmetic
by Pasco [414] and Rissanen [441]. The competitive optimality of Shannon codes was proved in Cover [125] and extended to Huffman codes by
Feder [203]. Section 5.11 on the generation of discrete distributions from
fair coin ï¬ips follows the work of Knuth and Yao[317].

CHAPTER 6

GAMBLING AND DATA
COMPRESSION

At ï¬rst sight, information theory and gambling seem to be unrelated.
But as we shall see, there is strong duality between the growth rate of
investment in a horse race and the entropy rate of the horse race. Indeed,
the sum of the growth rate and the entropy rate is a constant. In the process
of proving this, we shall argue that the ï¬nancial value of side information
is equal to the mutual information between the horse race and the side
information. The horse race is a special case of investment in the stock
market, studied in Chapter 16.
We also show how to use a pair of identical gamblers to compress a
sequence of random variables by an amount equal to the growth rate of
wealth on that sequence. Finally, we use these gambling techniques to
estimate the entropy rate of English.
6.1

THE HORSE RACE

Assume that m horses run in a race. Let the ith horse win with probability
pi . If horse i wins, the payoff is oi for 1 (i.e., an investment of 1 dollar
on horse i results in oi dollars if horse i wins and 0 dollars if horse i
loses).
There are two ways of describing odds: a-for-1 and b-to-1. The ï¬rst
refers to an exchange that takes place before the raceâthe gambler puts
down 1 dollar before the race and at a-for-1 odds will receive a dollars
after the race if his horse wins, and will receive nothing otherwise. The
second refers to an exchange after the raceâat b-to-1 odds, the gambler
will pay 1 dollar after the race if his horse loses and will pick up b dollars
after the race if his horse wins. Thus, a bet at b-to-1 odds is equivalent to
a bet at a-for-1 odds if b = a â 1. For example, fair odds on a coin ï¬ip
would be 2-for-1 or 1-to-1, otherwise known as even odds.
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

159

160

GAMBLING AND DATA COMPRESSION

We assume that the gambler distributes all of his wealth across the
horses. Let bi be the
fraction of the gamblerâs wealth invested in horse i,
where bi â¥ 0 and
bi = 1. Then if horse i wins the race, the gambler
will receive oi times the amount of wealth bet on horse i. All the other
bets are lost. Thus, at the end of the race, the gambler will have multiplied
his wealth by a factor bi oi if horse i wins, and this will happen with probability pi . For notational convenience, we use b(i) and bi interchangeably
throughout this chapter.
The wealth at the end of the race is a random variable, and the gambler
wishes to âmaximizeâ the value of this random variable. It is tempting to
bet everything on the horse that has the maximum expected return (i.e.,
the one with the maximum pi oi ). But this is clearly risky, since all the
money could be lost.
Some clarity results from considering repeated gambles on this race.
Now since the gambler can reinvest his money, his wealth is the product
of the gains for each race. Let Sn be the gamblerâs wealth after n races.
Then
Sn =

n


S(Xi ),

(6.1)

i=1

where S(X) = b(X)o(X) is the factor by which the gamblerâs wealth is
multiplied when horse X wins.
Deï¬nition The wealth relative S(X) = b(X)o(X) is the factor by which
the gamblerâs wealth grows if horse X wins the race.
Deï¬nition The doubling rate of a horse race is
W (b, p) = E(log S(X)) =

m


pk log bk ok .

(6.2)

k=1

The deï¬nition of doubling rate is justiï¬ed by the following theorem.
Theorem 6.1.1 Let the race outcomes X1 , X2 , . . . be i.i.d. â¼ p(x).
Then the wealth of the gambler using betting strategy b grows exponentially at rate W (b, p); that is,
.

Sn = 2nW (b,p) .

(6.3)

6.1

THE HORSE RACE

161

Proof: Functions of independent random variables are also independent,
and hence log S(X1 ), log S(X2 ), . . . are i.i.d. Then, by the weak law of
large numbers,
1
1
log S(Xi ) â E(log S(X))
log Sn =
n
n
n

in probability.

(6.4)

i=1

Thus,
.

Sn = 2nW (b,p) .



(6.5)

Now since the gamblerâs wealth grows as 2nW (b,p) , we seek to maximize
the exponent W (b, p) over all choices of the portfolio b.
Deï¬nition The optimum doubling rate W â (p) is the maximum doubling
rate over all choices of the portfolio b:
â

W (p) = max W (b, p) =
b

max


b:bi â¥0,

i bi =1

m


pi log bi oi .

(6.6)

i=1

We maximize W (b, p) as a function of b subject to the constraint
bi = 1. Writing the functional with a Lagrange multiplier and changing
the base of the logarithm (which does not affect the maximizing b), we
have


J (b) =
pi ln bi oi + Î»
bi .
(6.7)
Differentiating this with respect to bi yields
pi
âJ
=
+ Î»,
âbi
bi

i = 1, 2, . . . , m.

(6.8)

Setting the partial derivative equal to 0 for a maximum, we have
bi = â

pi
.
Î»

(6.9)


Substituting this in the constraint
bi = 1 yields Î» = â1 and bi = pi .
Hence, we can conclude that b = p is a stationary point of the function
J (b). To prove that this is actually a maximum is tedious if we take

162

GAMBLING AND DATA COMPRESSION

second derivatives. Instead, we use a method that works for many such
problems: Guess and verify. We verify that proportional gambling b = p
is optimal in the following theorem. Proportional gambling is known as
Kelly gambling [308].
Theorem 6.1.2 (Proportional gambling is log-optimal )
doubling rate is given by

W â (p) =
pi log oi â H (p)

The optimum

(6.10)

and is achieved by the proportional gambling scheme bâ = p.
Proof: We rewrite the function W (b, p) in a form in which the maximum
is obvious:

W (b, p) =
pi log bi oi
(6.11)



bi
pi oi
=
pi log
(6.12)
pi

(6.13)
=
pi log oi â H (p) â D(p||b)

â¤
pi log oi â H (p),
(6.14)
with equality iff p = b (i.e., the gambler bets on each horse in proportion
to its probability of winning).

Example 6.1.1 Consider a case with two horses, where horse 1 wins
with probability p1 and horse 2 wins with probability p2 . Assume even
odds (2-for-1 on both horses). Then the optimal bet is proportional betting (i.e., b1 = p1 , b2 = p2 ). The optimal doubling rate is W â (p) =

pi log oi â H (p) = 1 â H (p), and the resulting wealth grows to inï¬nity at this rate:
.

Sn = 2n(1âH (p)) .

(6.15)

Thus, we have shown that proportional betting is growth rate optimal
for a sequence of i.i.d. horse races if the gambler can reinvest his wealth
and if there is no alternative of keeping some of the wealth in cash.
We now consider a special case when the odds
are fair with respect to
1
some distribution (i.e., there is no track take and
oi = 1). In this case,
we write ri = o1i , where ri can be interpreted as a probability mass function

6.1

163

THE HORSE RACE

over the horses. (This is the bookieâs estimate of the win probabilities.)
With this deï¬nition, we can write the doubling rate as

(6.16)
W (b, p) =
pi log bi oi



bi pi
=
pi log
(6.17)
pi ri
= D(p||r) â D(p||b).

(6.18)

This equation gives another interpretation for the relative entropy distance: The doubling rate is the difference between the distance of the
bookieâs estimate from the true distribution and the distance of the gamblerâs estimate from the true distribution. Hence, the gambler can make
money only if his estimate (as expressed by b) is better than the bookieâs.
An even more special case is when the odds are m-for-1 on each horse.
In this case, the odds are fair with respect to the uniform distribution and
the optimum doubling rate is


1
â
= log m â H (p).
(6.19)
W (p) = D p||
m
In this case we can clearly see the duality between data compression and
the doubling rate.
Theorem 6.1.3

(Conservation theorem)

For uniform fair odds,

W â (p) + H (p) = log m.

(6.20)

Thus, the sum of the doubling rate and the entropy rate is a constant.
Every bit of entropy decrease doubles the gamblerâs wealth. Low entropy
races are the most proï¬table.
In the analysis above, we assumed that the gambler was fully invested.
In general, we should allow the gambler the option of retaining some of
his wealth as cash. Let b(0) be the proportion of wealth held out as cash,
and b(1), b(2), . . . , b(m) be the proportions bet on the various horses.
Then at the end of a race, the ratio of ï¬nal wealth to initial wealth (the
wealth relative) is
S(X) = b(0) + b(X)o(X).

(6.21)

Now the optimum strategy may depend on the odds and will not necessarily have the simple form of proportional gambling. We distinguish three
subcases:

164

GAMBLING AND DATA COMPRESSION

 1
1. Fair odds with respect to some distribution:
oi = 1. For fair odds,
the option of withholding cash does not change the analysis. This is
because we can get the effect of withholding cash by betting bi = o1i
on the ith horse, i = 1, 2, . . . , m. Then S(X) = 1 irrespective of
which horse wins. Thus, whatever money the gambler keeps aside
as cash can equally well be distributed over the horses, and the
assumption that the gambler must invest all his money does not
change the analysis. Proportional betting is optimal.
 1
2. Superfair odds:
oi < 1. In this case, the odds are even better than
fair odds, so one would always want to put all oneâs wealth into the
race rather than leave it as cash. In this race, too, the optimum
strategy is proportional betting. However, it is possible to choose
b so as to form a Dutch book by choosing bi = c o1i , where c =

1/ c1i , to get oi bi = c, irrespective of which horse wins. With

this allotment, one has wealth S(X) = 1/ o1i > 1 with probability
1 (i.e., no risk). Needless to say, one seldom ï¬nds such odds in
real life. Incidentally, a Dutch book, although risk-free, does not
optimize the doubling rate.
 1
3. Subfair odds:
oi > 1. This is more representative of real life. The
organizers of the race track take a cut of all the bets. In this case it
is optimal to bet only some of the money and leave the rest aside
as cash. Proportional gambling is no longer log-optimal. A parametric form for the optimal strategy can be found using KuhnâTucker
conditions (Problem 6.6.2); it has a simple âwater-ï¬llingâ interpretation.
6.2

GAMBLING AND SIDE INFORMATION

Suppose the gambler has some information that is relevant to the outcome
of the gamble. For example, the gambler may have some information
about the performance of the horses in previous races. What is the value
of this side information?
One deï¬nition of the ï¬nancial value of such information is the increase
in wealth that results from that information. In the setting described in
Section 6.1 the measure of the value of information is the increase in the
doubling rate due to that information. We will now derive a connection
between mutual information and the increase in the doubling rate.
To formalize the notion, let horse X â {1, 2, . . . , m} win the race with
probability p(x) and pay odds of o(x) for 1. Let
 (X, Y ) have joint
probability mass function p(x, y). Let b(x|y) â¥ 0, x b(x|y) = 1 be an
arbitrary conditional betting strategy depending on the side information

6.2

165

GAMBLING AND SIDE INFORMATION

Y , where b(x|y) is the proportion 
of wealth bet on horse x when y is
observed. As before, let b(x) â¥ 0,
b(x) = 1 denote the unconditional
betting scheme.
Let the unconditional and the conditional doubling rates be

p(x) log b(x)o(x),
(6.22)
W â (X) = max
b(x)

W â (X|Y ) = max

b(x|y)

and let

x



p(x, y) log b(x|y)o(x)

(6.23)

x,y

W = W â (X|Y ) â W â (X).

(6.24)

We observe that for (Xi , Yi ) i.i.d. horse races, wealth grows like 2nW
â
with side information and like 2nW (X) without side information.

â (X|Y )

Theorem 6.2.1 The increase W in doubling rate due to side information Y for a horse race X is
W = I (X; Y ).

(6.25)

Proof: With side information, the maximum value of W â (X|Y ) with
side information Y is achieved by conditionally proportional gambling
[i.e., bâ (x|y) = p(x|y)]. Thus,



p(x, y) log o(x)b(x|y) (6.26)
W â (X|Y ) = max E log S = max
b(x|y)

b(x|y)

=
=





p(x, y) log o(x)p(x|y)

(6.27)

p(x) log o(x) â H (X|Y ).

(6.28)

Without side information, the optimal doubling rate is

W â (X) =
p(x) log o(x) â H (X).

(6.29)

Thus, the increase in doubling rate due to the presence of side information
Y is
W = W â (X|Y ) â W â (X) = H (X) â H (X|Y ) = I (X; Y ).  (6.30)

166

GAMBLING AND DATA COMPRESSION

Hence, the increase in doubling rate is equal to the mutual information between the side information and the horse race. Not surprisingly,
independent side information does not increase the doubling rate.
This relationship can also be extended to the general stock market
(Chapter 16). In this case, however, one can only show the inequality
W â¤ I , with equality if and only if the market is a horse race.
6.3

DEPENDENT HORSE RACES AND ENTROPY RATE

The most common example of side information for a horse race is the
past performance of the horses. If the horse races are independent, this
information will be useless. If we assume that there is dependence among
the races, we can calculate the effective doubling rate if we are allowed
to use the results of previous races to determine the strategy for the next
race.
Suppose that the sequence {Xk } of horse race outcomes forms a stochastic process. Let the strategy for each race depend on the results of previous
races. In this case, the optimal doubling rate for uniform fair odds is
W â (Xk |Xkâ1 , Xkâ2 , . . . , X1 )

	
=E
max
E[log S(Xk )|Xkâ1 , Xkâ2 , . . . , X1 ]
b(Â·|Xkâ1 ,Xkâ2 ,...,X1 )

= log m â H (Xk |Xkâ1 , Xkâ2 , . . . , X1 ),

(6.31)

which is achieved by bâ (xk |xkâ1 , . . . , x1 ) = p(xk |xkâ1 , . . . , x1 ).
At the end of n races, the gamblerâs wealth is
Sn =

n


S(Xi ),

(6.32)

i=1

and the exponent in the growth rate (assuming m for 1 odds) is
1
1
E log Sn =
E log S(Xi )
n
n
1
=
(log m â H (Xi |Xiâ1 , Xiâ2 , . . . , X1 ))
n
H (X1 , X2 , . . . , Xn )
= log m â
.
n

(6.33)
(6.34)
(6.35)

6.3 DEPENDENT HORSE RACES AND ENTROPY RATE

167

The quantity n1 H (X1 , X2 , . . . , Xn ) is the average entropy per race. For
a stationary process with entropy rate H (X), the limit in (6.35) yields
1
E log Sn + H (X) = log m.
nââ n
lim

(6.36)

Again, we have the result that the entropy rate plus the doubling rate is a
constant.
The expectation in (6.36) can be removed if the process is ergodic. It
will be shown in Chapter 16 that for an ergodic sequence of horse races,
.

Sn = 2nW

with probability 1,

(6.37)

where W = log m â H (X) and
1
H (X) = lim H (X1 , X2 , . . . , Xn ).
n

(6.38)

Example 6.3.1 (Red and black ) In this example, cards replace horses
and the outcomes become more predictable as time goes on. Consider the
case of betting on the color of the next card in a deck of 26 red and 26
black cards. Bets are placed on whether the next card will be red or black,
as we go through the deck. We also assume that the game pays 2-for-1;
that is, the gambler gets back twice what he bets on the right color. These
are fair odds if red and black are equally probable.
We consider two alternative betting schemes:
1. If we bet sequentially, we can calculate the conditional probability
of the next card and bet proportionally. Thus, we should bet ( 12 , 12 )
25
on (red, black) for the ï¬rst card, ( 26
51 , 51 ) for the second card if the
ï¬rst card is black, and so on.
2. Alternatively,

52 we can bet on the entire sequence of 52 cards at once.
There are 26 possible sequences of 26 red and 26 black cards, all
of them
likely. Thus, proportional betting implies that we

 equally

put 1/ 52
of
our
money on each of these sequences and let each
26
bet âride.â
We will argue that these procedures are equivalent. For example, half
the sequences of 52 cards start with red, and so the proportion of money
bet on sequences that start with red in scheme 2 is also one-half, agreeing
with the proportion
used in the ï¬rst scheme. In general, we can verify that

 
betting 1/ 52
of
the
money on each of the possible outcomes will at each
26

168

GAMBLING AND DATA COMPRESSION

stage give bets that are proportional
to the probability of red and black

52
at that stage. Since we bet 1/ 26 of the wealth on each possible output
sequence, and a bet on a sequence increases wealth by a factor of 252 on
the sequence observed and 0 on all the others, the resulting wealth is
252
â
S52
= 
52 = 9.08.

(6.39)

26

Rather interestingly, the return does not depend on the actual sequence.
This is like the AEP in that the return is the same for all sequences. All
sequences are typical in this sense.
6.4

THE ENTROPY OF ENGLISH

An important example of an information source is English text. It is
not immediately obvious whether English is a stationary ergodic process.
Probably not! Nonetheless, we will be interested in the entropy rate of
English. We discuss various stochastic approximations to English. As we
increase the complexity of the model, we can generate text that looks like
English. The stochastic models can be used to compress English text. The
better the stochastic approximation, the better the compression.
For the purposes of discussion, we assume that the alphabet of English
consists of 26 letters and the space symbol. We therefore ignore punctuation and the difference between upper- and lowercase letters. We construct
models for English using empirical distributions collected from samples
of text. The frequency of letters in English is far from uniform. The most
common letter, E, has a frequency of about 13%, and the least common
letters, Q and Z, occur with a frequency of about 0.1%. The letter E is
so common that it is rare to ï¬nd a sentence of any length that does not
contain the letter. [A surprising exception to this is the 267-page novel,
Gadsby, by Ernest Vincent Wright (Lightyear Press, Boston, 1997; original publication in 1939), in which the author deliberately makes no use
of the letter E.]
The frequency of pairs of letters is also far from uniform. For example,
the letter Q is always followed by a U. The most frequent pair is TH,
which occurs normally with a frequency of about 3.7%. We can use
the frequency of the pairs to estimate the probability that a letter follows any other letter. Proceeding this way, we can also estimate higherorder conditional probabilities and build more complex models for the
language. However, we soon run out of data. For example, to build
a third-order Markov approximation, we must estimate the values of

6.4

THE ENTROPY OF ENGLISH

169

p(xi |xiâ1 , xiâ2 , xiâ3 ). There are 274 = 531, 441 entries in this table, and
we would need to process millions of letters to make accurate estimates
of these probabilities.
The conditional probability estimates can be used to generate random
samples of letters drawn according to these distributions (using a random
number generator). But there is a simpler method to simulate randomness
using a sample of text (a book, say). For example, to construct the secondorder model, open the book at random and choose a letter at random on
the page. This will be the ï¬rst letter. For the next letter, again open the
book at random and starting at a random point, read until the ï¬rst letter is
encountered again. Then take the letter after that as the second letter. We
repeat this process by opening to another page, searching for the second
letter, and taking the letter after that as the third letter. Proceeding this
way, we can generate text that simulates the second-order statistics of the
English text.
Here are some examples of Markov approximations to English from
Shannonâs original paper [472]:
1. Zero-order approximation. (The symbols are independent and equiprobable.)
XFOML RXKHRJFFJUJ ZLPWCFWKCYJ
FFJEYVKCQSGXYD QPAAMKBZAACIBZLHJQD

2. First-order approximation. (The symbols are independent. The frequency of letters matches English text.)
OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI
ALHENHTTPA OOBTTVA NAH BRL

3. Second-order approximation. (The frequency of pairs of letters
matches English text.)
ON IE ANTSOUTINYS ARE T INCTORE ST BE S DEAMY
ACHIN D ILONASIVE TUCOOWE AT TEASONARE FUSO
TIZIN ANDY TOBE SEACE CTISBE

4. Third-order approximation. (The frequency of triplets of letters
matches English text.)
IN NO IST LAT WHEY CRATICT FROURE BERS GROCID
PONDENOME OF DEMONSTURES OF THE REPTAGIN IS
REGOACTIONA OF CRE

170

GAMBLING AND DATA COMPRESSION

5. Fourth-order approximation. (The frequency of quadruplets of letters matches English text. Each letter depends on the previous three
letters. This sentence is from Luckyâs book, Silicon Dreams [366].)
THE GENERATED JOB PROVIDUAL BETTER TRAND THE DISPLAYED
CODE, ABOVERY UPONDULTS WELL THE CODERST IN THESTICAL
IT DO HOCK BOTHE MERG. (INSTATES CONS ERATION. NEVER
ANY OF PUBLE AND TO THEORY. EVENTIAL CALLEGAND TO ELAST
BENERATED IN WITH PIES AS IS WITH THE )

Instead of continuing with the letter models, we jump to word
models.
6. First-order word model . (The words are chosen independently but
with frequencies as in English.)
REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME CAN
DIFFERENT NATURAL HERE HE THE A IN CAME THE TO OF TO
EXPERT GRAY COME TO FURNISHES THE LINE MESSAGE HAD BE
THESE.

7. Second-order word model . (The word transition probabilities match
English text.)
THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER
THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER
METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD
THE PROBLEM FOR AN UNEXPECTED

The approximations get closer and closer to resembling English. For
example, long phrases of the last approximation could easily have occurred
in a real English sentence. It appears that we could get a very good approximation by using a more complex model. These approximations could be
used to estimate the entropy of English. For example, the entropy of the
zeroth-order model is log 27 = 4.76 bits per letter. As we increase the
complexity of the model, we capture more of the structure of English,
and the conditional uncertainty of the next letter is reduced. The ï¬rstorder model gives an estimate of the entropy of 4.03 bits per letter, while
the fourth-order model gives an estimate of 2.8 bits per letter. But even
the fourth-order model does not capture all the structure of English. In
Section 6.6 we describe alternative methods for estimating the entropy of
English.
The distribution of English is useful in decoding encrypted English text.
For example, a simple substitution cipher (where each letter is replaced

6.5 DATA COMPRESSION AND GAMBLING

171

by some other letter) can be solved by looking for the most frequent letter
and guessing that it is the substitute for E, and so on. The redundancy in
English can be used to ï¬ll in some of the missing letters after the other
letters are decrypted: for example,
TH R S NLY N W Y T F LL N TH V W LS N TH S S NT NC .

Some of the inspiration for Shannonâs original work on information
theory came out of his work in cryptography during World War II. The
mathematical theory of cryptography and its relationship to the entropy
of language is developed in Shannon [481].
Stochastic models of language also play a key role in some speech
recognition systems. A commonly used model is the trigram (second-order
Markov) word model, which estimates the probability of the next word
given the preceding two words. The information from the speech signal
is combined with the model to produce an estimate of the most likely
word that could have produced the observed speech. Random models do
surprisingly well in speech recognition, even when they do not explicitly
incorporate the complex rules of grammar that govern natural languages
such as English.
We can apply the techniques of this section to estimate the entropy rate
of other information sources, such as speech and images. A fascinating
nontechnical introduction to these issues may be found in the book by
Lucky [366].
6.5

DATA COMPRESSION AND GAMBLING

We now show a direct connection between gambling and data compression, by showing that a good gambler is also a good data compressor. Any
sequence on which a gambler makes a large amount of money is also a
sequence that can be compressed by a large factor. The idea of using
the gambler as a data compressor is based on the fact that the gamblerâs
bets can be considered to be his estimate of the probability distribution
of the data. A good gambler will make a good estimate of the probability
distribution. We can use this estimate of the distribution to do arithmetic
coding (Section 13.3). This is the essential idea of the scheme described
below.
We assume that the gambler has a mechanically identical twin, who
will be used for the data decompression. The identical twin will place the
same bets on possible sequences of outcomes as the original gambler (and
will therefore make the same amount of money). The cumulative amount

172

GAMBLING AND DATA COMPRESSION

of money that the gambler would have made on all sequences that are
lexicographically less than the given sequence will be used as a code
for the sequence. The decoder will use the identical twin to gamble on
all sequences, and look for the sequence for which the same cumulative
amount of money is made. This sequence will be chosen as the decoded
sequence.
Let X1 , X2 , . . . , Xn be a sequence of random variables that we wish
to compress. Without loss of generality, we will assume that the random
variables are binary. Gambling on this sequence will be deï¬ned by a
sequence of bets


b(xk+1 | x1 , x2 , . . . , xk ) â¥ 0,

b(xk+1 | x1 , x2 , . . . , xk ) = 1,

xk+1

(6.40)
where b(xk+1 | x1 , x2 , . . . , xk ) is the proportion of money bet at time k on
the event that Xk+1 = xk+1 given the observed past x1 , x2 , . . . , xk . Bets
are paid at uniform odds (2-for-1). Thus, the wealth Sn at the end of the
sequence is given by
Sn = 2n

n


b(xk | x1 , . . . , xkâ1 )

(6.41)

k=1

= 2 b(x1 , x2 , . . . , xn ),
n

(6.42)

where
b(x1 , x2 , . . . , xn ) =

n


b(xk |xkâ1 , . . . , x1 ).

(6.43)

k=1

So sequential gambling can also be considered
as an assignment of proba
bilities (or bets) b(x1 , x2 , . . . , xn ) â¥ 0, x1 ,...,xn b(x1 , . . . , xn ) = 1, on the
2n possible sequences.
This gambling elicits both an estimate of the true probability of the text
sequence
(pÌ(x1, . . . , xn ) = Sn /2n ) as well as an estimate of the entropy

HÌ = â n1 log pÌ of the text from which the sequence was drawn. We now
wish to show that high values of wealth Sn lead to high data compression.
Speciï¬cally, we argue that if the text in question results in wealth Sn ,
then log Sn bits can be saved in a naturally associated deterministic data
compression scheme. We further assert that if the gambling is log optimal,
the data compression achieves the Shannon limit H .

6.6 GAMBLING ESTIMATE OF THE ENTROPY OF ENGLISH

173

Consider the following data compression algorithm that maps the
text x = x1 x2 Â· Â· Â· xn â {0, 1}n into a code sequences c1 c2 Â· Â· Â· ck , ci â
{0, 1}. Both the compressor and the decompressor know n. Let
the 2n text sequences be arranged in lexicographical order: for
example, 0100101 < 0101101. The encoder observes the sequence

x n = (x1 , x2 , . . . , xn ). He then calculates what his wealth Sn (x (n))

would have
x (n) â¤ x(n) and calculates
 been on all sequences

F (x(n)) = x  (n)â¤x(n) 2ân Sn (x (n)). Clearly, F (x(n)) â [0, 1]. Let k =
	n â log Sn (x(n))
. Now express F (x(n)) as a binary decimal to k-place
accuracy: F (x(n)) = .c1 c2 Â· Â· Â· ck . The sequence c(k) = (c1 , c2 , . . . , ck )
is transmitted to the decoder.

The decoder twin can calculate the precise value S(x (n)) associated

with each of the 2n sequences x (n). He thus knows the cumulative sum

of 2ân S(x (n)) up through any sequence x(n). He tediously calculates
this sum until it ï¬rst exceeds .c(k). The ï¬rst sequence x(n) such that
the cumulative sum falls in the interval [.c1 Â· Â· Â· ck , .c1 . . . ck + (1/2)k ] is
deï¬ned uniquely, and the size of S(x(n))/2n guarantees that this sequence
will be precisely the encoded x(n).
Thus, the twin uniquely recovers x(n). The number of bits required
is k = 	n â log S(x(n))
. The number of bits saved is n â k =
log S(x(n)). For proportional gambling, 
S(x(n)) = 2n p(x(n)). Thus,
the expected number of bits is Ek = p(x(n))	â log p(x(n))
 â¤
H (X1 , . . . , Xn ) + 1.
We see that if the betting operation is deterministic and is known
both to the encoder and the decoder, the number of bits necessary to
encode x1 , . . . , xn is less than n â log Sn + 1. Moreover, if p(x) is known,
and if proportional gambling is used, the description length expected is
E(n â log Sn ) â¤ H (X1 , . . . , Xn ) + 1. Thus, the gambling results correspond precisely to the data compression that would have been achieved
by the given human encoderâdecoder identical twin pair.
The data compression scheme using a gambler is similar to the idea
of arithmetic coding (Section 13.3) using a distribution b(x1 , x2 , . . . , xn )
rather than the true distribution. The procedure above brings out the duality
between gambling and data compression. Both involve estimation of the
true distribution. The better the estimate, the greater the growth rate of
the gamblerâs wealth and the better the data compression.
6.6

GAMBLING ESTIMATE OF THE ENTROPY OF ENGLISH

We now estimate the entropy rate for English using a human gambler to
estimate probabilities. We assume that English consists of 27 characters

174

GAMBLING AND DATA COMPRESSION

(26 letters and a space symbol). We therefore ignore punctuation and case
of letters. Two different approaches have been proposed to estimate the
entropy of English.
1. Shannon guessing game. In this approach the human subject is
given a sample of English text and asked to guess the next letter.
An optimal subject will estimate the probabilities of the next letter
and guess the most probable letter ï¬rst, then the second most probable letter next, and so on. The experimenter records the number of
guesses required to guess the next letter. The subject proceeds this
way through a fairly large sample of text. We can then calculate the
empirical frequency distribution of the number of guesses required
to guess the next letter. Many of the letters will require only one
guess; but a large number of guesses will usually be needed at the
beginning of words or sentences.
Now let us assume that the subject can be modeled as a computer
making a deterministic choice of guesses given the past text. Then
if we have the same machine and the sequence of guess numbers,
we can reconstruct the English text. Just let the machine run, and if
the number of guesses at any position is k, choose the kth guess of
the machine as the next letter. Hence the amount of information in
the sequence of guess numbers is the same as in the English text.
The entropy of the guess sequence is the entropy of English text. We
can bound the entropy of the guess sequence by assuming that the
samples are independent. Hence, the entropy of the guess sequence
is bounded above by the entropy of the histogram in the experiment.
The experiment was conducted in 1950 by Shannon [482], who
obtained a value of 1.3 bits per symbol for the entropy of English.
2. Gambling estimate. In this approach we let a human subject gamble
on the next letter in a sample of English text. This allows ï¬ner
gradations of judgment than does guessing. As in the case of a horse
race, the optimal bet is proportional to the conditional probability
of the next letter. The payoff is 27-for-1 on the correct letter.
Since sequential betting is equivalent to betting on the entire
sequence, we can write the payoff after n letters as
Sn = (27)n b(X1 , X2 , . . . , Xn ).

(6.44)

Thus, after n rounds of betting, the expected log wealth satisï¬es
1
1
E log Sn = log 27 + E log b(X1 , X2 , . . . , Xn )
n
n

(6.45)

SUMMARY

= log 27 +

1
p(x n ) log b(x n )
n n

175

(6.46)

x

= log 27 â
+

1
n

p(x n )
b(x n )

p(x n ) log

xn

1
p(x n ) log p(x n )
n xn

(6.47)

1
1
= log 27 â D(p(x n )||b(x n )) â H (X1 , X2 , . . . , Xn )
n
n
(6.48)
1
â¤ log 27 â H (X1 , X2 , . . . , Xn )
n
â¤ log 27 â H (X),

(6.49)
(6.50)

where H (X) is the entropy rate of English. Thus, log 27 â E n1 log Sn
is an upper bound on the entropy rate of English. The upper bound
estimate, HÌ (X) = log 27 â n1 log Sn , converges to H (X) with probability 1 if English is ergodic and the gambler uses b(x n ) = p(x n ).
An experiment [131] with 12 subjects and a sample of 75 letters
from the book Jefferson the Virginian by Dumas Malone (Little,
Brown, Boston, 1948; the source used by Shannon) resulted in an
estimate of 1.34 bits per letter for the entropy of English.

SUMMARY
Doubling rate. W (b, p) = E(log S(X)) =

m

k=1 pk

log bk ok .

Optimal doubling rate. W â (p) = maxb W (b, p).
Proportional gambling is log-optimal

W â (p) = max W (b, p) =
pi log oi â H (p)
b

is achieved by bâ = p.
.

Growth rate. Wealth grows as Sn =2nW

â (p)

.

(6.51)

176

GAMBLING AND DATA COMPRESSION

Conservation law. For uniform fair odds,
H (p) + W â (p) = log m.

(6.52)

Side information. In a horse race X, the increase W in doubling
rate due to side information Y is
W = I (X; Y ).

(6.53)

PROBLEMS
6.1

Horse race. Three horses run a race. A gambler offers 3-for1 odds on each horse. These are fair odds under the assumption
that all horses are equally likely to win the race. The true win
probabilities are known to be


1 1 1
p = (p1 , p2 , p3 ) =
, ,
.
(6.54)
2 4 4

bi = 1, be the amount invested on
Let b = (b1 , b2 , b3 ), bi â¥ 0,
each of the horses. The expected log wealth is thus
W (b) =

3


pi log 3bi .

(6.55)

i=1

(a) Maximize this over b to ï¬nd bâ and W â . Thus, the wealth
achieved in repeated horse races should grow to inï¬nity like
â
2nW with probability 1.
(b) Show that if instead we put all of our money on horse 1, the
most likely winner, we will eventually go broke with probability 1.
6.2

Horse race with subfair odds. If the odds are bad (due to a track
take), the gambler may wish to keep money in his pocket. Let b(0)
be the amount in his pocket and let b(1), b(2), . . . , b(m) be the
amount bet on horses 1, 2, . . . , m, with odds o(1), o(2), . . . , o(m),
and win probabilities p(1), p(2), . . . , p(m). Thus, the resulting
wealth is S(x) = b(0) + b(x)o(x), with probability p(x), x =
1, 2, . . . , m.

(a) Find bâ maximizing E log S if
1/o(i) < 1.

PROBLEMS

177


(b) Discuss bâ if
1/o(i) > 1. (There isnât an easy closed-form
solution in this case, but a âwater-ï¬llingâ solution results from
the application of the KuhnâTucker conditions.)
6.3

Cards. An ordinary deck of cards containing 26 red cards and
26 black cards is shufï¬ed and dealt out one card at time without
replacement. Let Xi be the color of the ith card.
(a) Determine H (X1 ).
(b) Determine H (X2 ).
(c) Does H (Xk | X1 , X2 , . . . , Xkâ1 ) increase or decrease?
(d) Determine H (X1 , X2 , . . . , X52 ).

6.4

Gambling. Suppose that one gambles sequentially on the card
outcomes in Problem 6.6.3. Even odds of 2-for-1 are paid. Thus,
the wealth Sn at time n is Sn = 2n b(x1 , x2 , . . . , xn ), where
b(x1 , x2 , . . . , xn ) is the proportion of wealth bet on x1 , x2 , . . . , xn .
Find maxb(Â·) E log S52 .

6.5

Beating the public odds.
probabilities

Consider a three-horse race with win

(p1 , p2 , p3 ) =



1 1 1
, ,
2 4 4



and fair odds with respect to the (false) distribution


1 1 1
(r1 , r2 , r3 ) =
, ,
.
4 4 2
Thus, the odds are
(o1 , o2 , o3 ) = (4, 4, 2).
(a) What is the entropy of the race?
(b) Find the set of bets (b1 , b2 , b3 ) such that the compounded
wealth in repeated plays will grow to inï¬nity.
6.6

Horse race. A three-horse race has win probabilities p =
(p1 , p2 , p3 ), and odds
 o = (1, 1, 1). The gambler places bets b =
(b1 , b2 , b3 ), bi â¥ 0, bi = 1, where bi denotes the proportion on
wealth bet on horse i. These odds are very bad. The gambler gets
his money back on the winning horse and loses the other bets.
Thus, the wealth Sn at time n resulting from independent gambles
goes exponentially to zero.
(a) Find the exponent.

178

GAMBLING AND DATA COMPRESSION

(b) Find the optimal gambling scheme b (i.e., the bet bâ that maximizes the exponent).
(c) Assuming that b is chosen as in part (b), what distribution p
causes Sn to go to zero at the fastest rate?
6.7

Horse race. Consider a horse race with four horses. Assume that
each horse pays 4-for-1 if it wins. Let the probabilities of winning of the horses be { 12 , 14 , 18 , 18 }. If you started with $100 and
bet optimally to maximize your long-term growth rate, what are
your optimal bets on each horse? Approximately how much money
would you have after 20 races with this strategy?

6.8

Lotto. The following analysis is a crude approximation to the
games of Lotto conducted by various states. Assume that the player
of the game is required to pay $1 to play and is asked to choose
one number from a range 1 to 8. At the end of every day, the state
lottery commission picks a number uniformly over the same range.
The jackpot (i.e., all the money collected that day) is split among
all the people who chose the same number as the one chosen by the
state. For example, if 100 people played today, 10 of them chose
the number 2, and the drawing at the end of the day picked 2, the
$100 collected is split among the 10 people (i.e., each person who
picked 2 will receive $10, and the others will receive nothing).
The general population does not choose numbers uniformlyânumbers such as 3 and 7 are supposedly lucky and are
more popular than 4 or 8. Assume that the fraction of people choosing the various numbers 1, 2, . . . , 8 is (f1 , f2 , . . . , f8 ), and assume
that n people play every day. Also assume that n is very large, so
that any single personâs choice does not change the proportion of
people betting on any number.
(a) What is the optimal strategy to divide your money among
the various possible tickets so as to maximize your long-term
growth rate? (Ignore the fact that you cannot buy fractional
tickets.)
(b) What is the optimal growth rate that you can achieve in this
game?
1 1 1 1 1
(c) If (f1 , f2 , . . . , f8 ) = ( 18 , 18 , 14 , 16
, 16 , 16 , 4 , 16 ), and you start
with $1, how long will it be before you become a millionaire?

6.9

Horse race. Suppose that one is interested in maximizing the
doubling rate for a horse race. Let p1 , p2 , . . . , pm denote the win
probabilities of the m horses. When do the odds (o1 , o2 , . . . , om )

yield a higher doubling rate than the odds (o1 , o2 , . . . , om
)?

PROBLEMS

179

6.10

Horse race with probability estimates.
(a) Three horses race. Their probabilities of winning are ( 12 , 14 , 14 ).
The odds are 4-for-1, 3-for-1, and 3-for-1. Let W â be the optimal doubling rate. Suppose you believe that the probabilities
are ( 14 , 12 , 14 ). If you try to maximize the doubling rate, what
doubling rate W will you achieve? By how much has your doubling rate decrease due to your poor estimate of the probabilities
(i.e., what is W = W â â W )?
(b) Now let the horse race be among m horses, with probabilities p = (p1 , p2 , . . . , pm ) and odds o = (o1 , o2 , . . . , om ). If
you believe the true probabilities to be q = (q1 , q2 , . . . , qm ),
and try to maximize the doubling rate W , what is W â â W ?

6.11

Two-envelope problem. One envelope contains b dollars, the other
2b dollars. The amount b is unknown. An envelope is selected at
random. Let X be the amount observed in this envelope, and let Y
be the amount in the other envelope. Adopt the strategy of switching to the other envelope with probability p(x), where p(x) =
eâx
. Let Z be the amount that the player receives. Thus,
(eâx +ex )

(b, 2b) with probability 12
(6.56)
(X, Y ) =
(2b, b) with probability 12

X with probability 1 â p(x)
(6.57)
Z=
Y with probability p(x).
(a) Show that E(X) = E(Y ) = 3b
2 .
5
(b) Show that E(Y /X) = 4 . Since the expected ratio of the
amount in the other envelope is 54 , it seems that one should
always switch. (This is the origin of the switching paradox.)
However, observe that E(Y ) 
= E(X)E(Y /X). Thus, although
E(Y /X) > 1, it does not follow that E(Y ) > E(X).
(c) Let J be the index of the envelope containing the maximum
amount of money, and let J  be the index of the envelope
chosen by the algorithm. Show that for any b, I (J ; J  ) > 0.
Thus, the amount in the ï¬rst envelope always contains some
information about which envelope to choose.
(d) Show that E(Z) > E(X). Thus, you can do better than always
staying or always switching. In fact, this is true for any monotonic decreasing switching function p(x). By randomly switching according to p(x), you are more likely to trade up than to
trade down.

180

GAMBLING AND DATA COMPRESSION

6.12

Gambling. Find the horse win probabilities p1 , p2 , . . . , pm :
(a) Maximizing the doubling rate W â for given ï¬xed known odds
o1 , o2 , . . . , om .
(b) Minimizing the doubling rate for given ï¬xed odds o1 , o2 , . . . ,
om .

6.13

Dutch book .

Consider a horse race with m = 2 horses,
X = 1, 2
p = 12 ,

1
2

odds (for one) = 10, 30
bets = b, 1 â b.
The odds are superfair.
(a) There is a bet b that guarantees the same payoff regardless of
which horse wins. Such a bet is called a Dutch book. Find this
b and the associated wealth factor S(X).
(b) What is the maximum growth rate of the wealth for the optimal
choice of b? Compare it to the growth rate for the Dutch book.
6.14

Horse race. Suppose that one is interested in maximizing the
doubling rate for a horse race. Let p1 , p2 , . . . , pm denote the win
probabilities of the m horses. When do the odds (o1 , o2 , . . . , om )

yield a higher doubling rate than the odds (o1 , o2 , . . . , om
)?

6.15

Entropy of a fair horse race. Let X â¼ p(x), x = 1, 2, . . . , m,
denote the winner of a horse race. Suppose that the odds o(x)
1
]. Let b(x) be the
are fair with respect to p(x) [i.e., o(x) = p(x)
m
amount bet on horse x, b(x) â¥ 0, 1 b(x) = 1. Then the resulting
wealth factor is S(x) = b(x)o(x), with probability p(x).
(a) Find the expected wealth ES(X).
(b) Find W â , the optimal growth rate of wealth.
(c) Suppose that

Y =

1, X = 1 or 2
0, otherwise.

If this side information is available before the bet, how much
does it increase the growth rate W â ?
(d) Find I (X; Y ).

PROBLEMS

181

6.16

Negative horse race. Consider a horse race with m horses with
win probabilities p1 , p2 , . . . , pm . Here the gambler 
hopes that a
given horse will lose. He places bets (b1 , b2 , . . . , bm ), m
i=1 bi = 1,
on the horses, loses his bet bi if horse
i
wins,
and
retains
the rest of

his bets. (No odds.) Thus,S = j 
=i bj , with probability pi , and
one wishes to maximize
pi ln(1 â bi ) subject to the constraint

bi = 1.
(a) Find the growth rate optimal investment strategy bâ . Do not
constrain the bets to be positive, but do constrain the bets to
sum to 1. (This effectively allows short selling and margin.)
(b) What is the optimal growth rate?

6.17

St. Petersburg paradox . Many years ago in ancient St. Petersburg
the following gambling proposition caused great consternation. For
an entry fee of c units, a gambler receives a payoff of 2k units with
probability 2âk , k = 1, 2, . . . .
(a) Show that the expected payoff for this game is inï¬nite. For this
reason, it was argued that c = â was a âfairâ price to pay to
play this game. Most people ï¬nd this answer absurd.
(b) Suppose that the gambler can buy a share of the game. For
example, if he invests c/2 units in the game, he receives 12 a
share and a return X/2, where Pr(X = 2k ) = 2âk , k = 1, 2, . . . .
Suppose that X1 , X2 , . . . are i.i.d. according to this distribution
and that the gambler reinvests all his wealth each time. Thus,
his wealth Sn at time n is given by
Sn =

n

Xi
i=1

c

.

(6.58)

Show that this limit is â or 0, with probability 1, accordingly
as c < câ or c > câ . Identify the âfairâ entry fee câ .
More realistically, the gambler should be allowed to keep a proportion b = 1 â b of his money in his pocket and invest the rest
in the St. Petersburg game. His wealth at time n is then

n 

bXi
Sn =
.
(6.59)
b+
c
i=1

Let
W (b, c) =

â

k=1

âk

2



b2k
.
log 1 â b +
c

(6.60)

182

GAMBLING AND DATA COMPRESSION

We have
.

Sn = 2nW (b,c) .

(6.61)

W â (c) = max W (b, c).

(6.62)

Let
0â¤bâ¤1

Here are some questions about W â (c).
(a) For what value of the entry fee c does the optimizing value bâ
drop below 1?
(b) How does bâ vary with c?
(c) How does W â (c) fall off with c?
Note that since W â (c) > 0, for all c, we can conclude that any
entry fee c is fair.
6.18

Super St. Petersburg. Finally, we have the super St. Petersk
burg paradox, where Pr(X = 22 ) = 2âk , k = 1, 2, . . . . Here the
expected log wealth is inï¬nite for all b > 0, for all c, and the
gamblerâs wealth grows to inï¬nity faster than exponentially for
any b > 0. But that doesnât mean that all investment ratios b are
equally good. To see this, we wish to maximize the relative growth
rate with respect to some other portfolio, say, b = ( 12 , 12 ). Show
that there exists a unique b maximizing
E ln

b + bX/c
1
2

+ 12 X/c

and interpret the answer.
HISTORICAL NOTES
The original treatment of gambling on a horse race is due to Kelly [308],
who found that W = I . Log-optimal portfolios go back to the work
of Bernoulli, Kelly [308], LataneÌ [346], and LataneÌ and Tuttle [347].
Proportional gambling is sometimes referred to as the Kelly gambling
scheme. The improvement in the probability of winning by switching
envelopes in Problem 6.11 is based on Cover [130].
Shannon studied stochastic models for English in his original paper
[472]. His guessing game for estimating the entropy rate of English is
described in [482]. Cover and King [131] provide a gambling estimate
for the entropy of English. The analysis of the St. Petersburg paradox
is from Bell and Cover [39]. An alternative analysis can be found in
Feller [208].

CHAPTER 7

CHANNEL CAPACITY
What do we mean when we say that A communicates with B? We mean
that the physical acts of A have induced a desired physical state in B. This
transfer of information is a physical process and therefore is subject to the
uncontrollable ambient noise and imperfections of the physical signaling
process itself. The communication is successful if the receiver B and the
transmitter A agree on what was sent.
In this chapter we ï¬nd the maximum number of distinguishable signals
for n uses of a communication channel. This number grows exponentially with n, and the exponent is known as the channel capacity. The
characterization of the channel capacity (the logarithm of the number of
distinguishable signals) as the maximum mutual information is the central
and most famous success of information theory.
The mathematical analog of a physical signaling system is shown
in Figure 7.1. Source symbols from some ï¬nite alphabet are mapped
into some sequence of channel symbols, which then produces the output sequence of the channel. The output sequence is random but has a
distribution that depends on the input sequence. From the output sequence,
we attempt to recover the transmitted message.
Each of the possible input sequences induces a probability distribution
on the output sequences. Since two different input sequences may give rise
to the same output sequence, the inputs are confusable. In the next few
sections, we show that we can choose a ânonconfusableâ subset of input
sequences so that with high probability there is only one highly likely input
that could have caused the particular output. We can then reconstruct the
input sequences at the output with a negligible probability of error. By
mapping the source into the appropriate âwidely spacedâ input sequences
to the channel, we can transmit a message with very low probability of
error and reconstruct the source message at the output. The maximum rate
at which this can be done is called the capacity of the channel.
Deï¬nition We deï¬ne a discrete channel to be a system consisting of an
input alphabet X and output alphabet Y and a probability transition matrix
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

183

184

CHANNEL CAPACITY

W
Message

Encoder

Xn

Channel
p(y |x)

Yn

^

Decoder

W
Estimate
of
Message

FIGURE 7.1. Communication system.

p(y|x) that expresses the probability of observing the output symbol y
given that we send the symbol x. The channel is said to be memoryless
if the probability distribution of the output depends only on the input at
that time and is conditionally independent of previous channel inputs or
outputs.
Deï¬nition We deï¬ne the âinformationâ channel capacity of a discrete
memoryless channel as
C = max I (X; Y ),
p(x)

(7.1)

where the maximum is taken over all possible input distributions p(x).
We shall soon give an operational deï¬nition of channel capacity as the
highest rate in bits per channel use at which information can be sent with
arbitrarily low probability of error. Shannonâs second theorem establishes
that the information channel capacity is equal to the operational channel
capacity. Thus, we drop the word information in most discussions of
channel capacity.
There is a duality between the problems of data compression and data
transmission. During compression, we remove all the redundancy in the
data to form the most compressed version possible, whereas during data
transmission, we add redundancy in a controlled fashion to combat errors
in the channel. In Section 7.13 we show that a general communication
system can be broken into two parts and that the problems of data compression and data transmission can be considered separately.
7.1
7.1.1

EXAMPLES OF CHANNEL CAPACITY
Noiseless Binary Channel

Suppose that we have a channel whose the binary input is reproduced
exactly at the output (Figure 7.2).
In this case, any transmitted bit is received without error. Hence, one
error-free bit can be transmitted per use of the channel, and the capacity is

7.1 EXAMPLES OF CHANNEL CAPACITY

0

185

0

X

Y

1

1

FIGURE 7.2. Noiseless binary channel. C = 1 bit.

1 bit. We can also calculate the information capacity C = max I (X; Y ) =
1 bit, which is achieved by using p(x) = ( 12 , 12 ).
7.1.2

Noisy Channel with Nonoverlapping Outputs

This channel has two possible outputs corresponding to each of the two
inputs (Figure 7.3). The channel appears to be noisy, but really is not.
Even though the output of the channel is a random consequence of the
input, the input can be determined from the output, and hence every transmitted bit can be recovered without error. The capacity of this channel is
also 1 bit per transmission. We can also calculate the information capacity
C = max I (X; Y ) = 1 bit, which is achieved by using p(x) = ( 12 , 12 ).

1
1/2
0
1/2

2

X

Y
3
1/3
1
2/3

4

FIGURE 7.3. Noisy channel with nonoverlapping outputs. C = 1 bit.

186

7.1.3

CHANNEL CAPACITY

Noisy Typewriter

In this case the channel input is either received unchanged at the
output with probability 12 or is transformed into the next letter with
probability 12 (Figure 7.4). If the input has 26 symbols and we
use every alternate input symbol, we can transmit one of 13 symbols without error with each transmission. Hence, the capacity of
this channel is log 13 bits per transmission. We can also calculate
the information capacity C = max I (X; Y ) = max (H (Y ) â H (Y |X)) =
max H (Y ) â 1 = log 26 â 1 = log 13, achieved by using p(x) distributed
uniformly over all the inputs.

A

A

B

B

C

C

D

D

E

E

A

C

Z
Noisy channel

C
D

Y
Z

A
B

Y

Z
Noiseless subset of inputs

FIGURE 7.4. Noisy Typewriter. C = log 13 bits.

7.1 EXAMPLES OF CHANNEL CAPACITY

7.1.4

187

Binary Symmetric Channel

Consider the binary symmetric channel (BSC), which is shown in Fig. 7.5.
This is a binary channel in which the input symbols are complemented
with probability p. This is the simplest model of a channel with errors,
yet it captures most of the complexity of the general problem.
When an error occurs, a 0 is received as a 1, and vice versa. The bits
received do not reveal where the errors have occurred. In a sense, all
the bits received are unreliable. Later we show that we can still use such
a communication channel to send information at a nonzero rate with an
arbitrarily small probability of error.
We bound the mutual information by
I (X; Y ) = H (Y ) â H (Y |X)

= H (Y ) â
p(x)H (Y |X = x)

= H (Y ) â
p(x)H (p)

(7.2)
(7.3)
(7.4)

= H (Y ) â H (p)

(7.5)

â¤ 1 â H (p),

(7.6)

where the last inequality follows because Y is a binary random variable.
Equality is achieved when the input distribution is uniform. Hence, the
information capacity of a binary symmetric channel with parameter p is
C = 1 â H (p)

0

bits.

1âp

(7.7)

0

p
p

1

1âp

1

FIGURE 7.5. Binary symmetric channel. C = 1 â H (p) bits.

188

7.1.5

CHANNEL CAPACITY

Binary Erasure Channel

The analog of the binary symmetric channel in which some bits are lost
(rather than corrupted) is the binary erasure channel. In this channel, a
fraction Î± of the bits are erased. The receiver knows which bits have
been erased. The binary erasure channel has two inputs and three outputs
(Figure 7.6).
We calculate the capacity of the binary erasure channel as follows:
C = max I (X; Y )

(7.8)

p(x)

= max(H (Y ) â H (Y |X))

(7.9)

p(x)

= max H (Y ) â H (Î±).

(7.10)

p(x)

The ï¬rst guess for the maximum of H (Y ) would be log 3, but we cannot
achieve this by any choice of input distribution p(x). Letting E be the
event {Y = e}, using the expansion
H (Y ) = H (Y, E) = H (E) + H (Y |E),

(7.11)

and letting Pr(X = 1) = Ï , we have
H (Y ) = H ((1 â Ï )(1 â Î±), Î±, Ï(1 â Î±)) = H (Î±) + (1 â Î±)H (Ï ).
(7.12)
0

1âa

0

a
a

1

1âa

e

1

FIGURE 7.6. Binary erasure channel.

7.2

SYMMETRIC CHANNELS

189

Hence
C = max H (Y ) â H (Î±)
p(x)

(7.13)

= max(1 â Î±)H (Ï ) + H (Î±) â H (Î±)

(7.14)

= max(1 â Î±)H (Ï )

(7.15)

= 1 â Î±,

(7.16)

Ï
Ï

where capacity is achieved by Ï = 12 .
The expression for the capacity has some intuitive meaning: Since a
proportion Î± of the bits are lost in the channel, we can recover (at most)
a proportion 1 â Î± of the bits. Hence the capacity is at most 1 â Î±. It is
not immediately obvious that it is possible to achieve this rate. This will
follow from Shannonâs second theorem.
In many practical channels, the sender receives some feedback from
the receiver. If feedback is available for the binary erasure channel, it is
very clear what to do: If a bit is lost, retransmit it until it gets through.
Since the bits get through with probability 1 â Î±, the effective rate of
transmission is 1 â Î±. In this way we are easily able to achieve a capacity
of 1 â Î± with feedback.
Later in the chapter we prove that the rate 1 â Î± is the best that can be
achieved both with and without feedback. This is one of the consequences
of the surprising fact that feedback does not increase the capacity of
discrete memoryless channels.
7.2

SYMMETRIC CHANNELS

The capacity of the binary symmetric channel is C = 1 â H (p) bits per
transmission, and the capacity of the binary erasure channel is C = 1 â
Î± bits per transmission. Now consider the channel with transition matrix:
ï£®
ï£¹
0.3 0.2 0.5
p(y|x) = ï£° 0.5 0.3 0.2 ï£».
(7.17)
0.2 0.5 0.3
Here the entry in the xth row and the yth column denotes the conditional
probability p(y|x) that y is received when x is sent. In this channel, all
the rows of the probability transition matrix are permutations of each other
and so are the columns. Such a channel is said to be symmetric. Another
example of a symmetric channel is one of the form
Y =X+Z

(mod c),

(7.18)

190

CHANNEL CAPACITY

where Z has some distribution on the integers {0, 1, 2, . . . , c â 1}, X has
the same alphabet as Z, and Z is independent of X.
In both these cases, we can easily ï¬nd an explicit expression for the
capacity of the channel. Letting r be a row of the transition matrix, we
have
I (X; Y ) = H (Y ) â H (Y |X)

(7.19)

= H (Y ) â H (r)

(7.20)

â¤ log |Y| â H (r)

(7.21)

with equality if the output distribution is uniform. But p(x) = 1/|X|
achieves a uniform distribution on Y , as seen from
p(y) =



p(y|x)p(x) =

xâX

1
1
1 
p(y|x) = c
=
,
|X|
|X|
|Y|

(7.22)

where c is the sum of the entries in one column of the probability transition
matrix.
Thus, the channel in (7.17) has the capacity
C = max I (X; Y ) = log 3 â H (0.5, 0.3, 0.2),
p(x)

(7.23)

and C is achieved by a uniform distribution on the input.
The transition matrix of the symmetric channel deï¬ned above is doubly
stochastic. In the computation of the capacity, we used the facts that the
rows were permutations of one another and that all the column sums were
equal.
Considering these properties, we can deï¬ne a generalization of the
concept of a symmetric channel as follows:
Deï¬nition A channel is said to be symmetric if the rows of the channel
transition matrix p(y|x) are permutations of each other and the columns
are permutations of each other. A channel is said to be weakly symmetric
if every row of the transition matrix
 p(Â·|x) is a permutation of every other
row and all the column sums x p(y|x) are equal.
For example, the channel with transition matrix


p(y|x) =

1
3
1
3

is weakly symmetric but not symmetric.

1
6
1
2

1
2
1
6

(7.24)

7.4

191

PREVIEW OF THE CHANNEL CODING THEOREM

The above derivation for symmetric channels carries over to weakly
symmetric channels as well. We have the following theorem for weakly
symmetric channels:
Theorem 7.2.1

For a weakly symmetric channel,

C = log |Y| â H (row of transition matrix),

(7.25)

and this is achieved by a uniform distribution on the input alphabet.
7.3
1.
2.
3.
4.
5.

PROPERTIES OF CHANNEL CAPACITY
C â¥ 0 since I (X; Y ) â¥ 0.
C â¤ log |X| since C = max I (X; Y ) â¤ max H (X) = log |X|.
C â¤ log |Y| for the same reason.
I (X; Y ) is a continuous function of p(x).
I (X; Y ) is a concave function of p(x) (Theorem 2.7.4). Since
I (X; Y ) is a concave function over a closed convex set, a local
maximum is a global maximum. From properties 2 and 3, the maximum is ï¬nite, and we are justiï¬ed in using the term maximum rather
than supremum in the deï¬nition of capacity. The maximum can then
be found by standard nonlinear optimization techniques such as gradient search. Some of the methods that can be used include the
following:
â¢
â¢
â¢

Constrained maximization using calculus and the KuhnâTucker
conditions.
The FrankâWolfe gradient search algorithm.
An iterative algorithm developed by Arimoto [25] and Blahut
[65]. We describe the algorithm in Section 10.8.

In general, there is no closed-form solution for the capacity. But for
many simple channels it is possible to calculate the capacity using properties such as symmetry. Some of the examples considered earlier are of
this form.
7.4

PREVIEW OF THE CHANNEL CODING THEOREM

So far, we have deï¬ned the information capacity of a discrete memoryless
channel. In the next section we prove Shannonâs second theorem, which

192

CHANNEL CAPACITY
Yn
Xn

FIGURE 7.7. Channels after n uses.

gives an operational meaning to the deï¬nition of capacity as the number
of bits we can transmit reliably over the channel. But ï¬rst we will try to
give an intuitive idea as to why we can transmit C bits of information over
a channel. The basic idea is that for large block lengths, every channel
looks like the noisy typewriter channel (Figure 7.4) and the channel has a
subset of inputs that produce essentially disjoint sequences at the output.
For each (typical) input n-sequence, there are approximately 2nH (Y |X)
possible Y sequences, all of them equally likely (Figure 7.7). We wish
to ensure that no two X sequences produce the same Y output sequence.
Otherwise, we will not be able to decide which X sequence was sent.
The total number of possible (typical) Y sequences is â 2nH (Y ) . This set
has to be divided into sets of size 2nH (Y |X) corresponding to the different
input X sequences. The total number of disjoint sets is less than or equal
to 2n(H (Y )âH (Y |X)) = 2nI (X;Y ) . Hence, we can send at most â 2nI (X;Y )
distinguishable sequences of length n.
Although the above derivation outlines an upper bound on the capacity,
a stronger version of the above argument will be used in the next section
to prove that this rate I is achievable with an arbitrarily low probability
of error.
Before we proceed to the proof of Shannonâs second theorem, we need
a few deï¬nitions.
7.5

DEFINITIONS

We analyze a communication system as shown in Figure 7.8.
A message W , drawn from the index set {1, 2, . . . , M}, results in the
signal X n (W ), which is received by the receiver as a random sequence

7.5

W
Message

Encoder

Xn

Channel
p(y |x)

Yn

193

DEFINITIONS

^

Decoder

W
Estimate
of
Message

FIGURE 7.8. Communication channel.

Y n â¼ p(y n |x n ). The receiver then guesses the index W by an appropriate
decoding rule WÌ = g(Y n ). The receiver makes an error if WÌ is not the
same as the index W that was transmitted. We now deï¬ne these ideas
formally.
Deï¬nition A discrete channel, denoted by (X, p(y|x), Y), consists of
two ï¬nite sets X and Y and a collection of probability mass functions
p(y|x), one for
 each x â X, such that for every x and y, p(y|x) â¥ 0, and
for every x, y p(y|x) = 1, with the interpretation that X is the input
and Y is the output of the channel.
Deï¬nition The nth extension of the discrete memoryless channel (DMC)
is the channel (X n , p(y n |x n ), Y n ), where
p(yk |x k , y kâ1 ) = p(yk |xk ),

k = 1, 2, . . . , n.

(7.26)

Remark If the channel is used without feedback [i.e., if the input symbols do not depend on the past output symbols, namely, p(xk |x kâ1 , y kâ1 )
= p(xk |x kâ1 )], the channel transition function for the nth extension of the
discrete memoryless channel reduces to
p(y |x ) =
n

n

n
	

p(yi |xi ).

(7.27)

i=1

When we refer to the discrete memoryless channel, we mean the discrete
memoryless channel without feedback unless we state explicitly otherwise.
Deï¬nition An (M, n) code for the channel (X, p(y|x), Y) consists of
the following:
1. An index set {1, 2, . . . , M}.
2. An encoding function X n : {1, 2, . . . , M} â X n , yielding codewords
x n (1), x n (2), . . ., x n (M). The set of codewords is called the codebook.

194

CHANNEL CAPACITY

3. A decoding function
g : Y n â {1, 2, . . . , M},

(7.28)

which is a deterministic rule that assigns a guess to each possible
received vector.
Deï¬nition (Conditional probability of error ) Let
Î»i = Pr(g(Y n ) = i|X n = x n (i)) =



p(y n |x n (i))I (g(y n ) = i) (7.29)

yn

be the conditional probability of error given that index i was sent, where
I (Â·) is the indicator function.
Deï¬nition The maximal probability of error Î»(n) for an (M, n) code is
deï¬ned as
Î»(n) =

max

iâ{1,2,...,M}

Î»i .

(7.30)

Deï¬nition The (arithmetic) average probability of error Pe(n) for an
(M, n) code is deï¬ned as
Pe(n) =

M
1 
Î»i .
M

(7.31)

i=1

Note that if the index W is chosen according to a uniform distribution
over the set {1, 2, . . . , M}, and X n = x n (W ), then


Pe(n) = Pr(W = g(Y n )),

(7.32)

(i.e., Pe(n) is the probability of error). Also, obviously,
Pe(n) â¤ Î»(n) .

(7.33)

One would expect the maximal probability of error to behave quite differently from the average probability. But in the next section we prove that
a small average probability of error implies a small maximal probability
of error at essentially the same rate.

7.6

JOINTLY TYPICAL SEQUENCES

195

It is worth noting that Pe(n) deï¬ned in (7.32) is only a mathematical
construct of the conditional probabilities of error Î»i and is itself a probability of error only if the message is chosen uniformly over the message
set {1, 2, . . . , 2M }. However, both in the proof of achievability and the
converse, we choose a uniform distribution on W to bound the probability
of error. This allows us to establish the behavior of Pe(n) and the maximal
probability of error Î»(n) and thus characterize the behavior of the channel
regardless of how it is used (i.e., no matter what the distribution of W ).
Deï¬nition The rate R of an (M, n) code is
R=

log M
n

bits per transmission.

(7.34)

Deï¬nition
A rate R is said to be achievable if there exists a sequence

 
of ( 2nR , n) codes such that the maximal probability of error Î»(n) tends
to 0 as n â â.

 
Later, we write (2nR , n) codes to mean ( 2nR , n) codes. This will
simplify the notation.
Deï¬nition The capacity of a channel is the supremum of all achievable
rates.
Thus, rates less than capacity yield arbitrarily small probability of error
for sufï¬ciently large block lengths.
7.6

JOINTLY TYPICAL SEQUENCES

Roughly speaking, we decode a channel output Y n as the ith index if
the codeword X n (i) is âjointly typicalâ with the received signal Y n . We
now deï¬ne the important idea of joint typicality and ï¬nd the probability of joint typicality when X n (i) is the true cause of Y n and when it
is not.
Deï¬nition The set A(n)
of jointly typical sequences {(x n , y n )} with

respect to the distribution p(x, y) is the set of n-sequences with empirical
entropies -close to the true entropies:
 n n
n
n
A(n)
 = (x , y ) â X Ã Y :




 1

â log p(x n ) â H (X)
 < ,


 n

(7.35)

196

CHANNEL CAPACITY





 1
n

â log p(y ) â H (Y )
 < ,

 n






 1

â log p(x n , y n ) â H (X, Y )
 <  ,


 n

(7.36)
(7.37)

where
p(x , y ) =
n

n

n
	

p(xi , yi ).

(7.38)

i=1

Theorem 7.6.1 (Joint AEP ) Let 
(X n , Y n ) be sequences of length n
drawn i.i.d. according to p(x n , y n ) = ni=1 p(xi , yi ). Then:
1. Pr((X n , Y n ) â A(n)
 ) â 1 as n â â.
(n)
n(H (X,Y )+)
2. |A | â¤ 2
.
n
n
n
3. If (XÌ , YÌ ) â¼ p(x )p(y n ) [i.e., XÌ n and YÌ n are independent with the
same marginals as p(x n , y n )], then


Pr (XÌ n , YÌ n ) â A(n)
(7.39)
â¤ 2ân(I (X;Y )â3) .

Also, for sufï¬ciently large n,


n
n
(n)
(7.40)
Pr (XÌ , YÌ ) â A â¥ (1 â )2ân(I (X;Y )+3) .
Proof
1. We begin by showing that with high probability, the sequence is in
the typical set. By the weak law of large numbers,
1
â log p(X n ) â âE[log p(X)] = H (X)
n

in probability.

(7.41)
Hence, given  > 0, there exists n1 , such that for all n > n1 ,




 1


n


Pr 
â log p(X ) â H (X)
 â¥  < .
(7.42)
n
3
Similarly, by the weak law,
1
â log p(Y n ) â âE[log p(Y )] = H (Y )
n

in probability (7.43)

7.6

JOINTLY TYPICAL SEQUENCES

197

and
1
â log p(X n , Y n ) â âE[log p(X, Y )] = H (X, Y ) in probability,
n
(7.44)
and there exist n2 and n3 , such that for all n â¥ n2 ,





 1

n
Pr 

â log p(Y ) â H (Y )

 â¥  <
(7.45)
n
3
and for all n â¥ n3 ,




 1


n
n


Pr 
â log p(X , Y ) â H (X, Y )
 â¥  < .
n
3

(7.46)

Choosing n > max{n1 , n2 , n3 }, the probability of the union of the
sets in (7.42), (7.45), and (7.46) must be less than . Hence for n
sufï¬ciently large, the probability of the set A(n)
 is greater than 1 â ,
establishing the ï¬rst part of the theorem.
2. To prove the second part of the theorem, we have
1=



p(x n , y n )

(7.47)

p(x n , y n )

(7.48)

ân(H (X,Y )+)
â¥ |A(n)
,
 |2

(7.49)

n(H (X,Y )+)
|A(n)
.
 |â¤2

(7.50)

â¥


(n)
A

and hence

3. Now if XÌ n and YÌ n are independent but have the same marginals as
X n and Y n , then
Pr((XÌ n , YÌ n ) â A(n)
 )=



p(x n )p(y n )

(7.51)

â¤ 2n(H (X,Y )+) 2ân(H (X)â) 2ân(H (Y )â)

(7.52)

= 2ân(I (X;Y )â3) .

(7.53)

(n)
(x n ,y n )âA

198

CHANNEL CAPACITY

For sufï¬ciently large n, Pr(A(n)
 ) â¥ 1 â , and therefore
1â â¤



p(x n , y n )

(7.54)

(n)
(x n ,y n )âA

ân(H (X,Y )â)
â¤ |A(n)
 |2

(7.55)

and
n(H (X,Y )â)
|A(n)
.
 | â¥ (1 â )2

(7.56)

By similar arguments to the upper bound above, we can also show
that for n sufï¬ciently large,

Pr((XÌ n , YÌ n ) â A(n)
)
=
p(x n )p(y n )
(7.57)

(n)

A

â¥ (1 â )2n(H (X,Y )â) 2ân(H (X)+) 2ân(H (Y )+)
(7.58)
= (1 â )2ân(I (X;Y )+3) .



(7.59)

The jointly typical set is illustrated in Figure 7.9. There are about
typical X sequences and about 2nH (Y ) typical Y sequences. However, since there are only 2nH (X,Y ) jointly typical sequences, not all pairs
of typical X n and typical Y n are also jointly typical. The probability that

2nH (X)

xn

yn

.

.. .
.
.. .
.
.
. .. .
.. .
.
. . .
.. . .
.
. .
.

.
. . . .. . . .. .
.
. .
.
. .. . .
.
.
.
.
.
. . . .. . . .
.
.
.
.
.
. . .. .
. .
.
.. ..
.
.
.
.
.
.
. . ..
. .
..
.

FIGURE 7.9. Jointly typical sequences.

7.7

CHANNEL CODING THEOREM

199

any randomly chosen pair is jointly typical is about 2ânI (X;Y ) . Hence,
we can consider about 2nI (X;Y ) such pairs before we are likely to come
across a jointly typical pair. This suggests that there are about 2nI (X;Y )
distinguishable signals X n .
Another way to look at this is in terms of the set of jointly typical
sequences for a ï¬xed output sequence Y n , presumably the output sequence
resulting from the true input signal X n . For this sequence Y n , there are
about 2nH (X|Y ) conditionally typical input signals. The probability that
some randomly chosen (other) input signal X n is jointly typical with Y n
is about 2nH (X|Y ) /2nH (X) = 2ânI (X;Y ) . This again suggests that we can
choose about 2nI (X;Y ) codewords X n (W ) before one of these codewords
will get confused with the codeword that caused the output Y n .
7.7

CHANNEL CODING THEOREM

We now prove what is perhaps the basic theorem of information theory,
the achievability of channel capacity, ï¬rst stated and essentially proved
by Shannon in his original 1948 paper. The result is rather counterintuitive; if the channel introduces errors, how can one correct them all? Any
correction process is also subject to error, ad inï¬nitum.
Shannon used a number of new ideas to prove that information can be
sent reliably over a channel at all rates up to the channel capacity. These
ideas include:
â¢
â¢
â¢

Allowing an arbitrarily small but nonzero probability of error
Using the channel many times in succession, so that the law of large
numbers comes into effect
Calculating the average of the probability of error over a random
choice of codebooks, which symmetrizes the probability, and which
can then be used to show the existence of at least one good code

Shannonâs outline of the proof was based on the idea of typical sequences, but the proof was not made rigorous until much later. The proof given
below makes use of the properties of typical sequences and is probably
the simplest of the proofs developed so far. As in all the proofs, we
use the same essential ideasârandom code selection, calculation of the
average probability of error for a random choice of codewords, and so
on. The main difference is in the decoding rule. In the proof, we decode
by joint typicality; we look for a codeword that is jointly typical with the
received sequence. If we ï¬nd a unique codeword satisfying this property,
we declare that word to be the transmitted codeword. By the properties

200

CHANNEL CAPACITY

of joint typicality stated previously, with high probability the transmitted
codeword and the received sequence are jointly typical, since they are
probabilistically related. Also, the probability that any other codeword
looks jointly typical with the received sequence is 2ânI . Hence, if we
have fewer then 2nI codewords, then with high probability there will be
no other codewords that can be confused with the transmitted codeword,
and the probability of error is small.
Although jointly typical decoding is suboptimal, it is simple to analyze
and still achieves all rates below capacity.
We now give the complete statement and proof of Shannonâs second
theorem:
Theorem 7.7.1 (Channel coding theorem) For a discrete memoryless channel, all rates below capacity C are achievable. Speciï¬cally, for
every rate R < C, there exists a sequence of (2nR , n) codes with maximum
probability of error Î»(n) â 0.
Conversely, any sequence of (2nR , n) codes with Î»(n) â 0 must have
R â¤ C.
Proof: We prove that rates R < C are achievable and postpone proof of
the converse to Section 7.9.
Achievability: Fix p(x). Generate a (2nR , n) code at random according
to the distribution p(x). Speciï¬cally, we generate 2nR codewords independently according to the distribution
p(x n ) =

n
	

p(xi ).

(7.60)

i=1

We exhibit the 2nR codewords as the rows of a matrix:
ï£¹
ï£®
x2 (1) Â· Â· Â· xn (1)
x1 (1)
ï£º
ï£¯
..
..
..
..
C=ï£°
ï£».
.
.
.
.
nR
nR
nR
x1 (2 ) x2 (2 ) Â· Â· Â· xn (2 )

(7.61)

Each entry in this matrix is generated i.i.d. according to p(x). Thus, the
probability that we generate a particular code C is
nR

Pr(C) =

n
2
	
	
w=1 i=1

p(xi (w)).

(7.62)

7.7

CHANNEL CODING THEOREM

201

Consider the following sequence of events:
1. A random code C is generated as described in (7.62) according to
p(x).
2. The code C is then revealed to both sender and receiver. Both sender
and receiver are also assumed to know the channel transition matrix
p(y|x) for the channel.
3. A message W is chosen according to a uniform distribution
Pr(W = w) = 2ânR ,

w = 1, 2, . . . , 2nR .

(7.63)

4. The wth codeword X n (w), corresponding to the wth row of C, is
sent over the channel.
5. The receiver receives a sequence Y n according to the distribution
P (y n |x n (w)) =

n
	

p(yi |xi (w)).

(7.64)

i=1

6. The receiver guesses which message was sent. (The optimum procedure to minimize probability of error is maximum likelihood decoding (i.e., the receiver should choose the a posteriori most likely
message). But this procedure is difï¬cult to analyze. Instead, we will
use jointly typical decoding, which is described below. Jointly typical decoding is easier to analyze and is asymptotically optimal.) In
jointly typical decoding, the receiver declares that the index WÌ was
sent if the following conditions are satisï¬ed:
â¢
â¢

(X n (WÌ ), Y n ) is jointly typical.
There is no other index W 
 = WÌ such that (X n (W 
 ), Y n ) â
A(n)
 .

If no such WÌ exists or if there is more than one such, an error is
declared. (We may assume that the receiver outputs a dummy index
such as 0 in this case.)
7. There is a decoding error if WÌ = W . Let E be the event {WÌ = W }.
Analysis of the probability of error
Outline: We ï¬rst outline the analysis. Instead of calculating the probability of error for a single code, we calculate the average over all codes
generated at random according to the distribution (7.62). By the symmetry
of the code construction, the average probability of error does not depend

202

CHANNEL CAPACITY

on the particular index that was sent. For a typical codeword, there are two
different sources of error when we use jointly typical decoding: Either the
output Y n is not jointly typical with the transmitted codeword or there is
some other codeword that is jointly typical with Y n . The probability that
the transmitted codeword and the received sequence are jointly typical
goes to 1, as shown by the joint AEP. For any rival codeword, the probability that it is jointly typical with the received sequence is approximately
2ânI , and hence we can use about 2nI codewords and still have a low
probability of error. We will later extend the argument to ï¬nd a code with
a low maximal probability of error.
Detailed calculation of the probability of error: We let W be drawn
according to a uniform distribution over {1, 2, . . . , 2nR } and use jointly
typical decoding WÌ (y n ) as described in step 6. Let E = {WÌ (Y n ) = W }
denote the error event. We will calculate the average probability of error,
averaged over all codewords in the codebook, and averaged over all codebooks; that is, we calculate

Pr(E) =
Pr(C)Pe(n) (C)
(7.65)
C

=


C

nR

2
1 
Pr(C) nR
Î»w (C)
2

(7.66)

w=1

2nR

=

1 
Pr(C)Î»w (C),
2nR

(7.67)

w=1 C

Pe(n) (C)

is deï¬ned for jointly typical decoding. By the symmetry
where
of the code construction, the average probability of error averaged over
all
 codes does not depend on the particular index that was sent [i.e.,
C Pr(C)Î»w (C) does not depend on w]. Thus, we can assume without
loss of generality that the message W = 1 was sent, since
nR

2
1 
Pr(C)Î»w (C)
Pr(E) = nR
2
w=1 C

=
Pr(C)Î»1 (C)

(7.68)
(7.69)

C

= Pr(E|W = 1).

(7.70)

Deï¬ne the following events:
Ei = { (X n (i), Y n ) is in A(n)
 },

i â {1, 2, . . . , 2nR },

(7.71)

7.7

CHANNEL CODING THEOREM

203

where Ei is the event that the ith codeword and Y n are jointly typical.
Recall that Y n is the result of sending the ï¬rst codeword X n (1) over the
channel.
Then an error occurs in the decoding scheme if either E1c occurs (when
the transmitted codeword and the received sequence are not jointly typical)
or E2 âª E3 âª Â· Â· Â· âª E2nR occurs (when a wrong codeword is jointly typical
with the received sequence). Hence, letting P (E) denote Pr(E|W = 1), we
have


(7.72)
Pr(E|W = 1) = P E1c âª E2 âª E3 âª Â· Â· Â· âª E2nR |W = 1
nR

â¤ P (E1c |W = 1) +

2


P (Ei |W = 1),

(7.73)

i=2

by the union of events bound for probabilities. Now, by the joint AEP,
P (E1c |W = 1) â 0, and hence
P (E1c |W = 1) â¤ 

for n sufï¬ciently large.

(7.74)

Since by the code generation process, X n (1) and X n (i) are independent
for i = 1, so are Y n and X n (i). Hence, the probability that X n (i) and Y n
are jointly typical is â¤ 2ân(I (X;Y )â3) by the joint AEP. Consequently,
nR

Pr(E) = Pr(E|W = 1) â¤

P (E1c |W

= 1) +

2


P (Ei |W = 1) (7.75)

i=2
nR

â¤+

2


2ân(I (X;Y )â3)

(7.76)

i=2



=  + 2nR â 1 2ân(I (X;Y )â3)

(7.77)

â¤  + 23n 2ân(I (X;Y )âR)

(7.78)

â¤ 2

(7.79)

if n is sufï¬ciently large and R < I (X; Y ) â 3. Hence, if R < I (X; Y ),
we can choose  and n so that the average probability of error, averaged
over codebooks and codewords, is less than 2.
To ï¬nish the proof, we will strengthen this conclusion by a series of
code selections.
1. Choose p(x) in the proof to be p â (x), the distribution on X that
achieves capacity. Then the condition R < I (X; Y ) can be replaced
by the achievability condition R < C.

204

CHANNEL CAPACITY

2. Get rid of the average over codebooks. Since the average probability of error over codebooks is small (â¤ 2), there exists at least
one codebook Câ with a small average probability of error. Thus,
Pr(E|Câ ) â¤ 2. Determination of Câ can be achieved by an exhaustive search over all (2nR , n) codes. Note that
nR

2
1 
Pr(E|C ) = nR
Î»i (Câ ),
2
â

(7.80)

i=1

since we have chosen WÌ according to a uniform distribution as
speciï¬ed in (7.63).
3. Throw away the worst half of the codewords in the best codebook
(n)
Câ . Since the arithmetic average probability of error Pe (Câ ) for
this code is less then 2, we have
1 
Pr(E|Câ ) â¤ nR
(7.81)
Î»i (Câ ) â¤ 2,
2
which implies that at least half the indices i and their associated
codewords X n (i) must have conditional probability of error Î»i less
than 4 (otherwise, these codewords themselves would contribute
more than 2 to the sum). Hence the best half of the codewords
have a maximal probability of error less than 4. If we reindex these
codewords, we have 2nRâ1 codewords. Throwing out half the codewords has changed the rate from R to R â n1 , which is negligible
for large n.
Combining all these improvements, we have constructed a code of rate
R = R â n1 , with maximal probability of error Î»(n) â¤ 4. This proves the

achievability of any rate below capacity.



Random coding is the method of proof for Theorem 7.7.1, not the
method of signaling. Codes are selected at random in the proof merely to
symmetrize the mathematics and to show the existence of a good deterministic code. We proved that the average over all codes of block length
n has a small probability of error. We can ï¬nd the best code within this
set by an exhaustive search. Incidentally, this shows that the Kolmogorov
complexity (Chapter 14) of the best code is a small constant. This means
that the revelation (in step 2) to the sender and receiver of the best code
Câ requires no channel. The sender and receiver merely agree to use the
best (2nR , n) code for the channel.
Although the theorem shows that there exist good codes with arbitrarily small probability of error for long block lengths, it does not provide
a way of constructing the best codes. If we used the scheme suggested

7.8

ZERO-ERROR CODES

205

by the proof and generate a code at random with the appropriate distribution, the code constructed is likely to be good for long block lengths.
However, without some structure in the code, it is very difï¬cult to decode
(the simple scheme of table lookup requires an exponentially large table).
Hence the theorem does not provide a practical coding scheme. Ever
since Shannonâs original paper on information theory, researchers have
tried to develop structured codes that are easy to encode and decode.
In Section 7.11, we discuss Hamming codes, the simplest of a class of
algebraic error correcting codes that can correct one error in a block of
bits. Since Shannonâs paper, a variety of techniques have been used to
construct error correcting codes, and with turbo codes have come close
to achieving capacity for Gaussian channels.
7.8

ZERO-ERROR CODES

The outline of the proof of the converse is most clearly motivated by
going through the argument when absolutely no errors are allowed. We
will now prove that Pe(n) = 0 implies that R â¤ C. Assume that we have a
(2nR , n) code with zero probability of error [i.e., the decoder output g(Y n )
is equal to the input index W with probability 1]. Then the input index W
is determined by the output sequence [i.e., H (W |Y n ) = 0]. Now, to obtain
a strong bound, we arbitrarily assume that W is uniformly distributed
over {1, 2, . . . , 2nR }. Thus, H (W ) = nR. We can now write the string of
inequalities:
nR = H (W ) = H (W |Y n ) +I (W ; Y n )
  

(7.82)

=0

= I (W ; Y n )
(a)
â¤ I (X n ; Y n )
(b)

â¤

n


I (Xi ; Yi )

(7.83)
(7.84)
(7.85)

i=1
(c)

â¤ nC,

(7.86)

where (a) follows from the data-processing inequality (since W â X n (W )
â Y n forms a Markov chain), (b) will be proved in Lemma 7.9.2 using
the discrete memoryless assumption, and (c) follows from the deï¬nition
of (information) capacity. Hence, for any zero-error (2nR , n) code, for
all n,
R â¤ C.

(7.87)

206

CHANNEL CAPACITY

7.9 FANOâS INEQUALITY AND THE CONVERSE
TO THE CODING THEOREM
We now extend the proof that was derived for zero-error codes to the case
of codes with very small probabilities of error. The new ingredient will be
Fanoâs inequality, which gives a lower bound on the probability of error
in terms of the conditional entropy. Recall the proof of Fanoâs inequality,
which is repeated here in a new context for reference.
Let us deï¬ne the setup under consideration. The index W is uniformly
distributed on the set W = {1, 2, . . . , 2nR }, and the sequence Y n is related
probabilistically to W . From Y n , we estimate the index W that was sent.
Let the estimate be WÌ = g(Y n ). Thus, W â X n (W ) â Y n â WÌ forms
a Markov chain. Note that the probability of error is


1 
Pr WÌ = W = nR
Î»i = Pe(n) .
(7.88)
2
i

We begin with the following lemma, which has been proved in
Section 2.10:
Lemma 7.9.1 (Fanoâs inequality) For a discrete memoryless channel
with a codebook C and the input message W uniformly distributed over
2nR , we have
H (W |WÌ ) â¤ 1 + Pe(n) nR.

(7.89)

Proof: Since W is uniformly distributed, we have Pe(n) = Pr(W = WÌ ).
We apply Fanoâs inequality (Theorem 2.10.1) for W in an alphabet of

size 2nR .
We will now prove a lemma which shows that the capacity per transmission is not increased if we use a discrete memoryless channel many
times.
Lemma 7.9.2 Let Y n be the result of passing X n through a discrete
memoryless channel of capacity C. Then
I (X n ; Y n ) â¤ nC

for all p(x n ).

(7.90)

Proof
I (X n ; Y n ) = H (Y n ) â H (Y n |X n )
= H (Y n ) â

n


(7.91)

H (Yi |Y1 , . . . , Yiâ1 , X n )

(7.92)

H (Yi |Xi ),

(7.93)

i=1

= H (Y n ) â

n

i=1

7.9

FANOâS INEQUALITY AND THE CONVERSE TO THE CODING THEOREM

207

since by the deï¬nition of a discrete memoryless channel, Yi depends only
on Xi and is conditionally independent of everything else. Continuing the
series of inequalities, we have
n

H (Yi |Xi )
(7.94)
I (X n ; Y n ) = H (Y n ) â
i=1

â¤

n


H (Yi ) â

n


i=1

=

n


H (Yi |Xi )

(7.95)

i=1

I (Xi ; Yi )

(7.96)

i=1

â¤ nC,

(7.97)

where (7.95) follows from the fact that the entropy of a collection of random variables is less than the sum of their individual entropies, and (7.97)
follows from the deï¬nition of capacity. Thus, we have proved that using the
channel many times does not increase the information capacity in bits per
transmission.

We are now in a position to prove the converse to the channel coding
theorem.
Proof: Converse to Theorem 7.7.1 (Channel coding theorem). We have
to show that any sequence of (2nR , n) codes with Î»(n) â 0 must have R â¤
C. If the maximal probability of error tends to zero, the average probability
of error for the sequence of codes also goes to zero [i.e., Î»(n) â 0 implies
Pe(n) â 0, where Pe(n) is deï¬ned in (7.32)]. For a ï¬xed encoding rule
X n (Â·) and a ï¬xed decoding rule WÌ = g(Y n ), we have W â X n (W ) â
Y n â WÌ . For each n, let W be drawn according to a uniform distribution
over {1, 2, .
. . , 2nR }. Since W has a uniform distribution, Pr(WÌ = W ) =
(n)
1
Pe = 2nR i Î»i . Hence,
( a)
nR = H (W )
(b)

= H (W |WÌ ) + I (W ; WÌ )
( c)
â¤ 1 + Pe(n) nR + I (W ; WÌ )

(d)

â¤ 1 + Pe(n) nR + I (X n ; Y n )

( e)

â¤ 1 + Pe(n) nR + nC,

(7.98)
(7.99)
(7.100)
(7.101)
(7.102)

208

CHANNEL CAPACITY

where (a) follows from the assumption that W is uniform over {1, 2, . . . ,
2nR }, (b) is an identity, (c) is Fanoâs inequality for W taking on at most 2nR
values, (d) is the data-processing inequality, and (e) is from Lemma 7.9.2.
Dividing by n, we obtain
1
+ C.
(7.103)
n
Now letting n â â, we see that the ï¬rst two terms on the right-hand
side tend to 0, and hence
R â¤ Pe(n) R +

R â¤ C.

(7.104)

We can rewrite (7.103) as
1
C
â
.
(7.105)
R nR
This equation shows that if R > C, the probability of error is bounded
away from 0 for sufï¬ciently large n (and hence for all n, since if Pe(n) = 0
for small n, we can construct codes for large n with Pe(n) = 0 by concatenating these codes). Hence, we cannot achieve an arbitrarily low
probability of error at rates above capacity.

Pe(n) â¥ 1 â

This converse is sometimes called the weak converse to the channel
coding theorem. It is also possible to prove a strong converse, which states
that for rates above capacity, the probability of error goes exponentially
to 1. Hence, the capacity is a very clear dividing pointâat rates below
capacity, Pe(n) â 0 exponentially, and at rates above capacity, Pe(n) â 1
exponentially.
7.10 EQUALITY IN THE CONVERSE TO THE CHANNEL
CODING THEOREM
We have proved the channel coding theorem and its converse. In essence,
these theorems state that when R < C, it is possible to send information with an arbitrarily low probability of error, and when R > C, the
probability of error is bounded away from zero.
It is interesting and rewarding to examine the consequences of equality
in the converse; hopefully, it will give some ideas as to the kinds of codes
that achieve capacity. Repeating the steps of the converse in the case when
Pe = 0, we have
nR = H (W )
= H (W |WÌ ) + I (W ; WÌ )

(7.106)
(7.107)

7.10

EQUALITY IN THE CONVERSE TO THE CHANNEL CODING THEOREM

209

= I (W ; WÌ ))
( a)
â¤ I (X n (W ); Y n )

(7.109)

= H (Y n ) â H (Y n |X n )

(7.110)

= H (Y n ) â

n


(7.108)

H (Yi |Xi )

(7.111)

i=1
(b)

â¤

n


H (Yi ) â

n


i=1

=

n


H (Yi |Xi )

(7.112)

i=1

I (Xi ; Yi )

(7.113)

i=1
( c)

â¤ nC.

(7.114)

We have equality in (a), the data-processing inequality, only if I (Y n ;
X n (W )|W ) = 0 and I (X n ; Y n |WÌ ) = 0, which is true if all the codewords
are distinct and if WÌ is a sufï¬cient statistic for decoding. We have equality
in (b) only if the Yi âs are independent, and equality in (c) only if the
distribution of Xi is p â (x), the distribution on X that achieves capacity.
We have equality in the converse only if these conditions are satisï¬ed. This
indicates that a capacity-achieving zero-error code has distinct codewords
and the distribution of the Yi âs must be i.i.d. with

p â (x)p(y|x),
(7.115)
p â (y) =
x

the distribution on Y induced by the optimum distribution on X. The
distribution referred to in the converse is the empirical distribution on X
and Y induced by a uniform distribution over codewords, that is,
nR

2
1 
p(xi , yi ) = nR
I (Xi (w) = xi )p(yi |xi ).
2

(7.116)

w=1

We can check this result in examples of codes that achieve capacity:
1. Noisy typewriter. In this case we have an input alphabet of 26 letters, and each letter is either printed out correctly or changed to the
next letter with probability 12 . A simple code that achieves capacity
(log 13) for this channel is to use every alternate input letter so that

210

CHANNEL CAPACITY

no two letters can be confused. In this case, there are 13 codewords
of block length 1. If we choose the codewords i.i.d. according to a
uniform distribution on {1, 3, 5, 7, . . . , 25}, the output of the channel
is also i.i.d. and uniformly distributed on {1, 2, . . . , 26}, as expected.
2. Binary symmetric channel . Since given any input sequence, every
possible output sequence has some positive probability, it will not
be possible to distinguish even two codewords with zero probability
of error. Hence the zero-error capacity of the BSC is zero. However, even in this case, we can draw some useful conclusions. The
efï¬cient codes will still induce a distribution on Y that looks i.i.d.
â¼ Bernoulli( 12 ). Also, from the arguments that lead up to the converse, we can see that at rates close to capacity, we have almost
entirely covered the set of possible output sequences with decoding
sets corresponding to the codewords. At rates above capacity, the
decoding sets begin to overlap, and the probability of error can no
longer be made arbitrarily small.
7.11

HAMMING CODES

The channel coding theorem promises the existence of block codes that
will allow us to transmit information at rates below capacity with an
arbitrarily small probability of error if the block length is large enough.
Ever since the appearance of Shannonâs original paper [471], people have
searched for such codes. In addition to achieving low probabilities of
error, useful codes should be âsimple,â so that they can be encoded and
decoded efï¬ciently.
The search for simple good codes has come a long way since the publication of Shannonâs original paper in 1948. The entire ï¬eld of coding
theory has been developed during this search. We will not be able to
describe the many elegant and intricate coding schemes that have been
developed since 1948. We will only describe the simplest such scheme
developed by Hamming [266]. It illustrates some of the basic ideas underlying most codes.
The object of coding is to introduce redundancy so that even if some
of the information is lost or corrupted, it will still be possible to recover
the message at the receiver. The most obvious coding scheme is to repeat
information. For example, to send a 1, we send 11111, and to send a 0, we
send 00000. This scheme uses ï¬ve symbols to send 1 bit, and therefore
has a rate of 15 bit per symbol. If this code is used on a binary symmetric
channel, the optimum decoding scheme is to take the majority vote of
each block of ï¬ve received bits. If three or more bits are 1, we decode

7.11

HAMMING CODES

211

the block as a 1; otherwise, we decode it as 0. An error occurs if and
only if more than three of the bits are changed. By using longer repetition
codes, we can achieve an arbitrarily low probability of error. But the rate
of the code also goes to zero with block length, so even though the code
is âsimple,â it is really not a very useful code.
Instead of simply repeating the bits, we can combine the bits in some
intelligent fashion so that each extra bit checks whether there is an error in
some subset of the information bits. A simple example of this is a parity
check code. Starting with a block of n â 1 information bits, we choose
the nth bit so that the parity of the entire block is 0 (the number of 1âs
in the block is even). Then if there is an odd number of errors during
the transmission, the receiver will notice that the parity has changed and
detect the error. This is the simplest example of an error-detecting code.
The code does not detect an even number of errors and does not give any
information about how to correct the errors that occur.
We can extend the idea of parity checks to allow for more than one
parity check bit and to allow the parity checks to depend on various subsets
of the information bits. The Hamming code that we describe below is an
example of a parity check code. We describe it using some simple ideas
from linear algebra.
To illustrate the principles of Hamming codes, we consider a binary
code of block length 7. All operations will be done modulo 2. Consider
the set of all nonzero binary vectors of length 3. Arrange them in columns
to form a matrix:
ï£®
ï£¹
0 0 0 1 1 1 1
H = ï£° 0 1 1 0 0 1 1 ï£».
(7.117)
1 0 1 0 1 0 1
Consider the set of vectors of length 7 in the null space of H (the vectors
which when multiplied by H give 000). From the theory of linear spaces,
since H has rank 3, we expect the null space of H to have dimension 4.
These 24 codewords are
0000000
0001111
0010110
0011001

0100101
0101010
0110011
0111100

1000011
1001100
1010101
1011010

1100110
1101001
1110000
1111111

Since the set of codewords is the null space of a matrix, it is linear in the
sense that the sum of any two codewords is also a codeword. The set of
codewords therefore forms a linear subspace of dimension 4 in the vector
space of dimension 7.

212

CHANNEL CAPACITY

Looking at the codewords, we notice that other than the all-0 codeword,
the minimum number of 1âs in any codeword is 3. This is called the
minimum weight of the code. We can see that the minimum weight of
a code has to be at least 3 since all the columns of H are different, so
no two columns can add to 000. The fact that the minimum distance is
exactly 3 can be seen from the fact that the sum of any two columns must
be one of the columns of the matrix.
Since the code is linear, the difference between any two codewords is
also a codeword, and hence any two codewords differ in at least three
places. The minimum number of places in which two codewords differ is
called the minimum distance of the code. The minimum distance of the
code is a measure of how far apart the codewords are and will determine
how distinguishable the codewords will be at the output of the channel.
The minimum distance is equal to the minimum weight for a linear code.
We aim to develop codes that have a large minimum distance.
For the code described above, the minimum distance is 3. Hence if a
codeword c is corrupted in only one place, it will differ from any other
codeword in at least two places and therefore be closer to c than to
any other codeword. But can we discover which is the closest codeword
without searching over all the codewords?
The answer is yes. We can use the structure of the matrix H for decoding. The matrix H , called the parity check matrix, has the property that
for every codeword c, H c = 0. Let ei be a vector with a 1 in the ith
position and 0âs elsewhere. If the codeword is corrupted at position i, the
received vector r = c + ei . If we multiply this vector by the matrix H ,
we obtain
H r = H (c + ei ) = H c + H ei = H ei ,

(7.118)

which is the vector corresponding to the ith column of H . Hence looking
at H r, we can ï¬nd which position of the vector was corrupted. Reversing this bit will give us a codeword. This yields a simple procedure for
correcting one error in the received sequence. We have constructed a codebook with 16 codewords of block length 7, which can correct up to one
error. This code is called a Hamming code.
We have not yet identiï¬ed a simple encoding procedure; we could use
any mapping from a set of 16 messages into the codewords. But if we
examine the ï¬rst 4 bits of the codewords in the table, we observe that
they cycle through all 24 combinations of 4 bits. Thus, we could use
these 4 bits to be the 4 bits of the message we want to send; the other
3 bits are then determined by the code. In general, it is possible to modify
a linear code so that the mapping is explicit, so that the ï¬rst k bits in each

7.11

HAMMING CODES

213

codeword represent the message, and the last n â k bits are parity check
bits. Such a code is called a systematic code. The code is often identiï¬ed
by its block length n, the number of information bits k and the minimum
distance d. For example, the above code is called a (7,4,3) Hamming code
(i.e., n = 7, k = 4, and d = 3).
An easy way to see how Hamming codes work is by means of a Venn
diagram. Consider the following Venn diagram with three circles and with
four intersection regions as shown in Figure 7.10. To send the information
sequence 1101, we place the 4 information bits in the four intersection
regions as shown in the ï¬gure. We then place a parity bit in each of the
three remaining regions so that the parity of each circle is even (i.e., there
are an even number of 1âs in each circle). Thus, the parity bits are as
shown in Figure 7.11.
Now assume that one of the bits is changed; for example one of the
information bits is changed from 1 to 0 as shown in Figure 7.12. Then
the parity constraints are violated for two of the circles (highlighted in the
ï¬gure), and it is not hard to see that given these violations, the only single
bit error that could have caused it is at the intersection of the two circles
(i.e., the bit that was changed). Similarly working through the other error
cases, it is not hard to see that this code can detect and correct any single
bit error in the received codeword.
We can easily generalize this procedure to construct larger matrices
H . In general, if we use l rows in H , the code that we obtain will have
block length n = 2l â 1, k = 2l â l â 1 and minimum distance 3. All
these codes are called Hamming codes and can correct one error.

1

1
0

1

FIGURE 7.10. Venn diagram with information bits.

214

CHANNEL CAPACITY

1

1

0
1
1

0

0

FIGURE 7.11. Venn diagram with information bits and parity bits with even parity for each
circle.

1

1

0
1
0

0

0

FIGURE 7.12. Venn diagram with one of the information bits changed.

Hamming codes are the simplest examples of linear parity check codes.
They demonstrate the principle that underlies the construction of other
linear codes. But with large block lengths it is likely that there will be
more than one error in the block. In the early 1950s, Reed and Solomon
found a class of multiple error-correcting codes for nonbinary channels.
In the late 1950s, Bose and Ray-Chaudhuri [72] and Hocquenghem [278]
generalized the ideas of Hamming codes using Galois ï¬eld theory to construct t-error correcting codes (called BCH codes) for any t. Since then,
various authors have developed other codes and also developed efï¬cient

7.11

HAMMING CODES

215

decoding algorithms for these codes. With the advent of integrated circuits,
it has become feasible to implement fairly complex codes in hardware and
realize some of the error-correcting performance promised by Shannonâs
channel capacity theorem. For example, all compact disc players include
error-correction circuitry based on two interleaved (32, 28, 5) and (28, 24,
5) ReedâSolomon codes that allow the decoder to correct bursts of up to
4000 errors.
All the codes described above are block codes âthey map a block of
information bits onto a channel codeword and there is no dependence on
past information bits. It is also possible to design codes where each output
block depends not only on the current input block, but also on some of
the past inputs as well. A highly structured form of such a code is called
a convolutional code. The theory of convolutional codes has developed
considerably over the last 40 years. We will not go into the details, but
refer the interested reader to textbooks on coding theory [69, 356].
For many years, none of the known coding algorithms came close
to achieving the promise of Shannonâs channel capacity theorem. For a
binary symmetric channel with crossover probability p, we would need a
code that could correct up to np errors in a block of length n and have
n(1 â H (p)) information bits. For example, the repetition code suggested
earlier corrects up to n/2 errors in a block of length n, but its rate goes
to 0 with n. Until 1972, all known codes that could correct nÎ± errors for
block length n had asymptotic rate 0. In 1972, Justesen [301] described
a class of codes with positive asymptotic rate and positive asymptotic
minimum distance as a fraction of the block length.
In 1993, a paper by Berrou et al. [57] introduced the notion that the
combination of two interleaved convolution codes with a parallel cooperative decoder achieved much better performance than any of the earlier
codes. Each decoder feeds its âopinionâ of the value of each bit to the
other decoder and uses the opinion of the other decoder to help it decide
the value of the bit. This iterative process is repeated until both decoders
agree on the value of the bit. The surprising fact is that this iterative
procedure allows for efï¬cient decoding at rates close to capacity for a
variety of channels. There has also been a renewed interest in the theory
of low-density parity check (LDPC) codes that were introduced by Robert
Gallager in his thesis [231, 232]. In 1997, MacKay and Neal [368] showed
that an iterative message-passing algorithm similar to the algorithm used
for decoding turbo codes could achieve rates close to capacity with high
probability for LDPC codes. Both Turbo codes and LDPC codes remain
active areas of research and have been applied to wireless and satellite
communication channels.

216

CHANNEL CAPACITY

W
Message

Encoder

Xi (W,Y i â1)

Channel
p(y |x)

Yi

^

Decoder

W
Estimate
of
Message

FIGURE 7.13. Discrete memoryless channel with feedback.

7.12

FEEDBACK CAPACITY

A channel with feedback is illustrated in Figure 7.13. We assume that
all the received symbols are sent back immediately and noiselessly to
the transmitter, which can then use them to decide which symbol to send
next. Can we do better with feedback? The surprising answer is no, which
we shall now prove. We deï¬ne a (2nR , n) feedback code as a sequence
of mappings xi (W, Y iâ1 ), where each xi is a function only of the message W â 2nR and the previous received values, Y1 , Y2 , . . . , Yiâ1 , and a
sequence of decoding functions g : Yn â {1, 2, . . . , 2nR }. Thus,


Pe(n) = Pr g(Y n ) = W ,

(7.119)

when W is uniformly distributed over {1, 2, . . . , 2nR }.
Deï¬nition The capacity with feedback, CFB , of a discrete memoryless
channel is the supremum of all rates achievable by feedback codes.
Theorem 7.12.1

(Feedback capacity)
CFB = C = max I (X; Y ).
p(x)

(7.120)

Proof: Since a nonfeedback code is a special case of a feedback code,
any rate that can be achieved without feedback can be achieved with
feedback, and hence
CFB â¥ C.

(7.121)

Proving the inequality the other way is slightly more tricky. We cannot
use the same proof that we used for the converse to the coding theorem
without feedback. Lemma 7.9.2 is no longer true, since Xi depends on
the past received symbols, and it is no longer true that Yi depends only
on Xi and is conditionally independent of the future Xâs in (7.93).

7.12

FEEDBACK CAPACITY

217

There is a simple change that will ï¬x the problem with the proof.
Instead of using X n , we will use the index W and prove a similar series
of inequalities. Let W be uniformly distributed over {1, 2, . . . , 2nR }. Then
Pr(W = WÌ ) = Pe(n) and
nR = H (W ) = H (W |WÌ ) + I (W ; WÌ )

(7.122)

â¤ 1 + Pe(n) nR + I (W ; WÌ )

(7.123)

â¤ 1 + Pe(n) nR + I (W ; Y n ),

(7.124)

by Fanoâs inequality and the data-processing inequality. Now we can
bound I (W ; Y n ) as follows:
I (W ; Y n ) = H (Y n ) â H (Y n |W )
= H (Y n ) â

n


(7.125)

H (Yi |Y1 , Y2 , . . . , Yiâ1 , W )

(7.126)

H (Yi |Y1 , Y2 , . . . , Yiâ1 , W, Xi )

(7.127)

H (Yi |Xi ),

(7.128)

i=1

= H (Y ) â
n

n

i=1

= H (Y ) â
n

n

i=1

since Xi is a function of Y1 , . . . , Yiâ1 and W ; and conditional on Xi , Yi
is independent of W and past samples of Y . Continuing, we have
I (W ; Y ) = H (Y ) â
n

n

n


H (Yi |Xi )

(7.129)

i=1

â¤

n


H (Yi ) â

n


i=1

=

n


H (Yi |Xi )

(7.130)

i=1

I (Xi ; Yi )

(7.131)

i=1

â¤ nC

(7.132)

from the deï¬nition of capacity for a discrete memoryless channel. Putting
these together, we obtain
nR â¤ Pe(n) nR + 1 + nC,

(7.133)

218

CHANNEL CAPACITY

and dividing by n and letting n â â, we conclude that
R â¤ C.

(7.134)

Thus, we cannot achieve any higher rates with feedback than we can
without feedback, and
CF B = C. 

(7.135)

As we have seen in the example of the binary erasure channel, feedback
can help enormously in simplifying encoding and decoding. However, it
cannot increase the capacity of the channel.
7.13

SOURCEâCHANNEL SEPARATION THEOREM

It is now time to combine the two main results that we have proved so far:
data compression (R > H : Theorem 5.4.2) and data transmission (R <
C: Theorem 7.7.1). Is the condition H < C necessary and sufï¬cient for
sending a source over a channel? For example, consider sending digitized
speech or music over a discrete memoryless channel. We could design
a code to map the sequence of speech samples directly into the input
of the channel, or we could compress the speech into its most efï¬cient
representation, then use the appropriate channel code to send it over the
channel. It is not immediately clear that we are not losing something
by using the two-stage method, since data compression does not depend
on the channel and the channel coding does not depend on the source
distribution.
We will prove in this section that the two-stage method is as good as
any other method of transmitting information over a noisy channel. This
result has some important practical implications. It implies that we can
consider the design of a communication system as a combination of two
parts, source coding and channel coding. We can design source codes
for the most efï¬cient representation of the data. We can, separately and
independently, design channel codes appropriate for the channel. The combination will be as efï¬cient as anything we could design by considering
both problems together.
The common representation for all kinds of data uses a binary alphabet.
Most modern communication systems are digital, and data are reduced
to a binary representation for transmission over the common channel.
This offers an enormous reduction in complexity. Networks like, ATM
networks and the Internet use the common binary representation to allow
speech, video, and digital data to use the same communication channel.

7.13 SOURCEâCHANNEL SEPARATION THEOREM

219

The resultâthat a two-stage process is as good as any one-stage processâseems so obvious that it may be appropriate to point out that it
is not always true. There are examples of multiuser channels where the
decomposition breaks down. We also consider two simple situations where
the theorem appears to be misleading. A simple example is that of sending
English text over an erasure channel. We can look for the most efï¬cient
binary representation of the text and send it over the channel. But the
errors will be very difï¬cult to decode. If, however, we send the English
text directly over the channel, we can lose up to about half the letters and
yet be able to make sense out of the message. Similarly, the human ear has
some unusual properties that enable it to distinguish speech under very
high noise levels if the noise is white. In such cases, it may be appropriate
to send the uncompressed speech over the noisy channel rather than the
compressed version. Apparently, the redundancy in the source is suited to
the channel.
Let us deï¬ne the setup under consideration. We have a source V that
generates symbols from an alphabet V. We will not make any assumptions
about the kind of stochastic process produced by V other than that it is
from a ï¬nite alphabet and satisï¬es the AEP. Examples of such processes
include a sequence of i.i.d. random variables and the sequence of states
of a stationary irreducible Markov chain. Any stationary ergodic source
satisï¬es the AEP, as we show in Section 16.8.
We want to send the sequence of symbols V n = V1 , V2 , . . . , Vn over
the channel so that the receiver can reconstruct the sequence. To do this,
we map the sequence onto a codeword X n (V n ) and send the codeword
over the channel. The receiver looks at his received sequence Y n and
makes an estimate VÌ n of the sequence V n that was sent. The receiver
makes an error if V n = VËn . We deï¬ne the probability of error as
Pr(V n = VËn ) =


yn

p(v n )p(y n |x n (v n ))I (g(y n ) = v n ),

(7.136)

vn

where I is the indicator function and g(y n ) is the decoding function. The
system is illustrated in Figure 7.14.
We can now state the joint sourceâchannel coding theorem:

Vn

Encoder

X n(V n)

Channel
p(y |x)

Yn

^

Decoder

FIGURE 7.14. Joint source and channel coding.

Vn

220

CHANNEL CAPACITY

Theorem 7.13.1 (Sourceâchannel coding theorem) If V1 , V2 , . . . V n
is a ï¬nite alphabet stochastic process that satisï¬es the AEP and H (V) <
C, there exists a sourceâchannel code with probability of error Pr(VÌ n =
V n ) â 0. Conversely, for any stationary stochastic process, if H (V) > C,
the probability of error is bounded away from zero, and it is not possible
to send the process over the channel with arbitrarily low probability of
error.
Proof: Achievability. The essence of the forward part of the proof is
the two-stage encoding described earlier. Since we have assumed that the
stochastic process satisï¬es the AEP, it implies that there exists a typical
n(H (V )+)
set A(n)
which contains most of the probability. We
 of size â¤ 2
will encode only the source sequences belonging to the typical set; all
other sequences will result in an error. This will contribute at most  to
the probability of error.
We index all the sequences belonging to A(n)
 . Since there are at most
n(H +)
such sequences, n(H + ) bits sufï¬ce to index them. We can
2
transmit the desired index to the receiver with probability of error less
than  if
H (V) +  = R < C.

(7.137)

The receiver can reconstruct V n by enumerating the typical set A(n)

and choosing the sequence corresponding to the estimated index. This
sequence will agree with the transmitted sequence with high probability.
To be precise,
n
n
n
(n)
P (V n = VÌ n ) â¤ P (V n â
/ A(n)
 ) + P (g(Y ) = V |V â A ) (7.138)

â¤  +  = 2

(7.139)

for n sufï¬ciently large. Hence, we can reconstruct the sequence with low
probability of error for n sufï¬ciently large if
H (V) < C.

(7.140)

Converse: We wish to show that Pr(VÌ n = V n ) â 0 implies that H (V)
â¤ C for any sequence of source-channel codes
X n (V n ) : V n â X n ,

(7.141)

gn (Y n ) : Y n â V n .

(7.142)

7.13 SOURCEâCHANNEL SEPARATION THEOREM

221

Thus X n (Â·) is an arbitrary (perhaps random) assignment of codewords
to data sequences V n , and gn (Â·) is any decoding function (assignment of
estimates VÌ n to output sequences Y n . By Fanoâs inequality, we must have
H (V n |VÌ n ) â¤ 1 + Pr(VÌ n = V n ) log |V n | = 1 + Pr(VÌ n = V n )n log |V|.
(7.143)
Hence for the code,
(a) H (V1 , V2 , . . . , Vn )
H (V) â¤
n
H (V n )
=
n
1
1
= H (V n |VÌ n ) + I (V n ; VÌ n )
n
n
(b) 1
â¤ (1 + Pr(VÌ n = V n )n log |V|) +
n
(c) 1
â¤ (1 + Pr(VÌ n = V n )n log |V|) +
n
(d) 1
â¤ + Pr(VÌ n = V n ) log |V| + C,
n

(7.144)
(7.145)
(7.146)
1
I (V n ; VÌ n ) (7.147)
n
1
I (X n ; Y n ) (7.148)
n
(7.149)

where (a) follows from the deï¬nition of entropy rate of a stationary
process, (b) follows from Fanoâs inequality, (c) follows from the dataprocessing inequality (since V n â X n â Y n â VÌ n forms a Markov
chain) and (d) follows from the memorylessness of the channel. Now
letting n â â, we have Pr(VÌ n = V n ) â 0 and hence
H (V) â¤ C.

(7.150)

Hence, we can transmit a stationary ergodic source over a channel if and
only if its entropy rate is less than the capacity of the channel. The joint
sourceâchannel separation theorem enables us to consider the problem of
source coding separately from the problem of channel coding. The source
coder tries to ï¬nd the most efï¬cient representation of the source, and
the channel coder encodes the message to combat the noise and errors
introduced by the channel. The separation theorem says that the separate
encoders (Figure 7.15) can achieve the same rates as the joint encoder
(Figure 7.14).
With this result, we have tied together the two basic theorems of
information theory: data compression and data transmission. We will try
to summarize the proofs of the two results in a few words. The data

222

CHANNEL CAPACITY

Vn

Source
Encoder

n n
Channel X (V ) Channel Y n
Encoder
p(y |x)

Channel
Decoder

Source
Decoder

^

Vn

FIGURE 7.15. Separate source and channel coding.

compression theorem is a consequence of the AEP, which shows that
there exists a âsmallâ subset (of size 2nH ) of all possible source sequences
that contain most of the probability and that we can therefore represent
the source with a small probability of error using H bits per symbol.
The data transmission theorem is based on the joint AEP; it uses the
fact that for long block lengths, the output sequence of the channel is
very likely to be jointly typical with the input codeword, while any other
codeword is jointly typical with probability â 2ânI . Hence, we can use
about 2nI codewords and still have negligible probability of error. The
sourceâchannel separation theorem shows that we can design the source
code and the channel code separately and combine the results to achieve
optimal performance.
SUMMARY
Channel capacity. The logarithm of the number of distinguishable
inputs is given by
C = max I (X; Y ).
p(x)

Examples
â¢
â¢
â¢

Binary symmetric channel: C = 1 â H (p).
Binary erasure channel: C = 1 â Î±.
Symmetric channel: C = log |Y| â H (row of transition matrix).

Properties of C
1. 0 â¤ C â¤ min{log |X|, log |Y|}.
2. I (X; Y ) is a continuous concave function of p(x).
Joint typicality. The set A(n)
of jointly typical sequences {(x n , y n )}

with respect to the distribution p(x, y) is given by
 n n
n
n
(7.151)
A(n)
 = (x , y ) â X Ã Y :



 1


â log p(x n ) â H (X)
 < ,
(7.152)

 n


PROBLEMS




 1


â log p(y n ) â H (Y )
 < ,

 n






 1

â log p(x n , y n ) â H (X, Y )
 <  ,


 n

where p(x n , y n ) =

223

(7.153)
(7.154)

n

i=1 p(xi , yi ).

n
, Y n ) be sequences of length n drawn i.i.d. accordJoint AEP. Let (X
n
n
ing to p(x , y ) = ni=1 p(xi , yi ). Then:

1. Pr((X n , Y n ) â A(n)
 ) â 1 as n â â.
n(H (X,Y )+)
|
â¤
2
.
2. |A(n)



3. If (XÌ n , YÌ n ) â¼ p(x n )p(y n ), then Pr (XÌ n , YÌ n ) â A(n)

â¤ 2ân(I (X;Y )â3) .
Channel coding theorem. All rates below capacity C are achievable,
and all rates above capacity are not; that is, for all rates R < C, there
exists a sequence of (2nR , n) codes with probability of error Î»(n) â 0.
Conversely, for rates R > C, Î»(n) is bounded away from 0.
Feedback capacity. Feedback does not increase capacity for discrete
memoryless channels (i.e., CFB = C).
Sourceâchannel theorem. A stochastic process with entropy rate H
cannot be sent reliably over a discrete memoryless channel if H >
C. Conversely, if the process satisï¬es the AEP, the source can be
transmitted reliably if H < C.

PROBLEMS
7.1

Preprocessing the output. One is given a communication channel with transition probabilities p(y|x) and channel capacity C =
maxp(x) I (X; Y ). A helpful statistician preprocesses the output by
forming YÌ = g(Y ). He claims that this will strictly improve the
capacity.
(a) Show that he is wrong.
(b) Under what conditions does he not strictly decrease the
capacity?

224

7.2

CHANNEL CAPACITY

Additive noise channel. Find the channel capacity of the following
discrete memoryless channel:
Z

X

Y

where Pr{Z = 0} = Pr{Z = a} = 12 . The alphabet for x is X =
{0, 1}. Assume that Z is independent of X. Observe that the channel
capacity depends on the value of a.
7.3

Channels with memory have higher capacity. Consider a binary
symmetric channel with Yi = Xi â Zi , where â is mod 2 addition, and Xi , Yi â {0, 1}. Suppose that {Zi } has constant marginal
probabilities Pr{Zi = 1} = p = 1 â Pr{Zi = 0}, but that Z1 , Z2 ,
. . ., Zn are not necessarily independent. Assume that Z n is independent of the input X n . Let C = 1 â H (p, 1 â p). Show that
maxp(x1 ,x2 ,...,xn ) I (X1 , X2 , . . . , Xn ; Y1 , Y2 , . . . ,
Yn ) â¥ nC.

7.4

Channel capacity. Consider the discrete memoryless channel Y =
X + Z (mod 11), where

Z=

1, 2, 3
1
1
1
3, 3, 3



and X â {0, 1, . . . , 10}. Assume that Z is independent of X.
(a) Find the capacity.
(b) What is the maximizing p â (x)?
7.5

Using two channels at once. Consider two discrete memoryless
channels (X1 , p(y1 | x1 ), Y1 ) and (X2 , p(y2 | x2 ), Y2 ) with capacities C1 and C2 , respectively. A new channel (X1 Ã X2 , p(y1 |
x1 ) Ã p(y2 | x2 ), Y1 Ã Y2 ) is formed in which x1 â X1 and x2 â X2
are sent simultaneously, resulting in y1 , y2 . Find the capacity of this
channel.

7.6

Noisy typewriter. Consider a 26-key typewriter.
(a) If pushing a key results in printing the associated letter, what
is the capacity C in bits?

PROBLEMS

225

(b) Now suppose that pushing a key results in printing that letter or
the next (with equal probability). Thus, A â A or B, . . . , Z â
Z or A. What is the capacity?
(c) What is the highest rate code with block length one that you
can ï¬nd that achieves zero probability of error for the channel
in part (b)?
7.7

Cascade of binary symmetric channels. Show that a cascade of n
identical independent binary symmetric channels,
X0 â BSC â X1 â Â· Â· Â· â Xnâ1 â BSC â Xn ,
each with raw errorprobability p, is
 equivalent to a single BSC with
error probability 12 1 â (1 â 2p)n and hence that lim I (X0 ; Xn )
nââ
= 0 if p = 0, 1. No encoding or decoding takes place at the intermediate terminals X1 , . . . , Xnâ1 . Thus, the capacity of the cascade
tends to zero.

7.8

Z-channel. The Z-channel has binary input and output alphabets
and transition probabilities p(y|x) given by the following matrix:

Q=

1
0
1/2 1/2


x, y â {0, 1}

Find the capacity of the Z-channel and the maximizing input probability distribution.
7.9

Suboptimal codes. For the Z-channel of Problem 7.8, assume that
we choose a (2nR , n) code at random, where each codeword is a
sequence of fair coin tosses. This will not achieve capacity. Find the
maximum rate R such that the probability of error Pe(n) , averaged
over the randomly generated codes, tends to zero as the block length
n tends to inï¬nity.

7.10

Zero-error capacity. A channel with alphabet {0, 1, 2, 3, 4} has transition probabilities of the form

p(y|x) =

1/2 if y = x Â± 1 mod 5
0 otherwise.

(a) Compute the capacity of this channel in bits.

226

CHANNEL CAPACITY

(b) The zero-error capacity of a channel is the number of bits per
channel use that can be transmitted with zero probability of
error. Clearly, the zero-error capacity of this pentagonal channel is at least 1 bit (transmit 0 or 1 with probability 1/2). Find
a block code that shows that the zero-error capacity is greater
than 1 bit. Can you estimate the exact value of the zero-error
capacity? (Hint: Consider codes of length 2 for this channel.)
The zero-error capacity of this channel was ï¬nally found by
Lovasz [365].
7.11

Time-varying channels. Consider a time-varying discrete memoryless channel.
Let Y1 , Y2 , . . . , Yn be conditionally independent given X1
, X2 ,
. . . , Xn , with conditional distribution given by p(y | x) = ni=1
pi (yi | xi ). Let X = (X1 , X2 , . . . , Xn ), Y = (Y1 , Y2 , . . . , Yn ). Find
maxp(x) I (X; Y).
0

1 â pi

0

pi
pi

1

7.12

1

1 â pi

Unused symbols. Show that the capacity of the channel with probability transition matrix
ï£®
ï£¯
Py|x = ï£°

2
3
1
3

0

1
3
1
3
1
3

0
1
3
2
3

ï£¹
ï£º
ï£»

(7.155)

is achieved by a distribution that places zero probability on one
of input symbols. What is the capacity of this channel? Give an
intuitive reason why that letter is not used.
7.13

Erasures and errors in a binary channel. Consider a channel with
binary inputs that has both erasures and errors. Let the probability

PROBLEMS

227

of error be  and the probability of erasure be Î±, so the channel is
follows:
â

1âaâ

0

0

â
a
e
a
â
1âaâ

â

1

1

(a) Find the capacity of this channel.
(b) Specialize to the case of the binary symmetric channel (Î± = 0).
(c) Specialize to the case of the binary erasure channel ( = 0).
7.14

Channels with dependence between the letters. Consider the following channel over a binary alphabet that takes in 2-bit symbols
and produces a 2-bit output, as determined by the following mapping: 00 â 01, 01 â 10, 10 â 11, and 11 â 00. Thus, if the
2-bit sequence 01 is the input to the channel, the output is 10 with
probability 1. Let X1 , X2 denote the two input symbols and Y1 , Y2
denote the corresponding output symbols.
(a) Calculate the mutual information I (X1 , X2 ; Y1 , Y2 ) as a function of the input distribution on the four possible pairs of inputs.
(b) Show that the capacity of a pair of transmissions on this channel is 2 bits.
(c) Show that under the maximizing input distribution, I (X1 ; Y1 )
= 0. Thus, the distribution on the input sequences that achieves
capacity does not necessarily maximize the mutual information
between individual symbols and their corresponding outputs.

7.15

Jointly typical sequences. As we did in Problem 3.13 for the typical
set for a single random variable, we will calculate the jointly typical
set for a pair of random variables connected by a binary symmetric

228

CHANNEL CAPACITY

channel, and the probability of error for jointly typical decoding
for such a channel.
0.9
0

0

0.1
0.1

1

1

0.9

We consider a binary symmetric channel with crossover probability
0.1. The input distribution that achieves capacity is the uniform
distribution [i.e., p(x) = ( 12 , 12 )], which yields the joint distribution
p(x, y) for this channel is given by
X Y
0
1

0

1

0.45
0.05

0.05
0.45

The marginal distribution of Y is also ( 12 , 12 ).
(a) Calculate H (X), H (Y ), H (X, Y ), and I (X; Y ) for the joint
distribution above.
(b) Let X1 , X2 , . . . , Xn be drawn i.i.d. according the Bernoulli( 12 )
distribution. Of the 2n possible input sequences of length n,
which of them are typical [i.e., member of A(n)
 (X) for  =
(Y
)?
0.2]? Which are the typical sequences in A(n)

(n)
(c) The jointly typical set A (X, Y ) is deï¬ned as the set of
sequences that satisfy equations (7.35-7.37). The ï¬rst two
equations correspond to the conditions that x n and y n are in
(n)
A(n)
 (X) and A (Y ), respectively. Consider the last condition, which can be rewritten to state that â n1 log p(x n , y n ) â
(H (X, Y ) â , H (X, Y ) + ). Let k be the number of places
in which the sequence x n differs from y n (k is a function of
the two sequences). Then we can write
p(x , y ) =
n

n

n
	
i=1

p(xi , yi )

(7.156)

229

PROBLEMS

= (0.45)nâk (0.05)k
(7.157)
 n
1
=
(1 â p)nâk p k .
(7.158)
2
An alternative way at looking at this probability is to look at the
binary symmetric channel as in additive channel Y = X â Z,
where Z is a binary random variable that is equal to 1 with
probability p, and is independent of X. In this case,
p(x n , y n ) = p(x n )p(y n |x n )

(7.159)

= p(x n )p(zn |x n )

(7.160)

= p(x )p(z )
 n
1
=
(1 â p)nâk p k .
2

(7.161)

n

n

(7.162)

Show that the condition that (x n , y n ) being jointly typical is
equivalent to the condition that x n is typical and zn = y n â x n
is typical.
(d) We now calculate the size of A(n)
 (Z) for n = 25 and  = 0.2.
As in Problem 3.13, here is a table of the probabilities and
numbers of sequences with k ones:
k
0
1
2
3
4
5
6
7
8
9
10
11
12

n

n

k

k

1
25
300
2300
12650
53130
177100
480700
1081575
2042975
3268760
4457400
5200300

pk (1 â p)nâk
0.071790
0.199416
0.265888
0.226497
0.138415
0.064594
0.023924
0.007215
0.001804
0.000379
0.000067
0.000010
0.000001

â n1 log p(x n )
0.152003
0.278800
0.405597
0.532394
0.659191
0.785988
0.912785
1.039582
1.166379
1.293176
1.419973
1.546770
1.673567

[Sequences with more than 12 ones are omitted since their total
probability is negligible (and they are not in the typical set).]
What is the size of the set A(n)
 (Z)?

230

CHANNEL CAPACITY

(e) Now consider random coding for the channel, as in the proof
of the channel coding theorem. Assume that 2nR codewords
X n (1), X n (2),. . ., X n (2nR ) are chosen uniformly over the 2n
possible binary sequences of length n. One of these codewords
is chosen and sent over the channel. The receiver looks at
the received sequence and tries to ï¬nd a codeword in the
code that is jointly typical with the received sequence. As
argued above, this corresponds to ï¬nding a codeword X n (i)
n
such that Y n â X n (i) â A(n)
 (Z). For a ï¬xed codeword x (i),
what is the probability that the received sequence Y n is such
that (x n (i), Y n ) is jointly typical?
(f) Now consider a particular received sequence y n =
000000 . . . 0, say. Assume that we choose a sequence
X n at random, uniformly distributed among all the 2n possible
binary n-sequences. What is the probability that the chosen
sequence is jointly typical with this y n ? [Hint: This is the
probability of all sequences x n such that y n â x n â A(n)
 (Z).]
9
(g) Now consider a code with 2 = 512 codewords of length 12
chosen at random, uniformly distributed among all the 2n
sequences of length n = 25. One of these codewords, say
the one corresponding to i = 1, is chosen and sent over the
channel. As calculated in part (e), the received sequence, with
high probability, is jointly typical with the codeword that was
sent. What is the probability that one or more of the other
codewords (which were chosen at random, independent of the
sent codeword) is jointly typical with the received sequence?
[Hint: You could use the union bound, but you could also
calculate this probability exactly, using the result of part (f)
and the independence of the codewords.]
(h) Given that a particular codeword was sent, the probability of
error (averaged over the probability distribution of the channel and over the random choice of other codewords) can be
written as
Pr(Error|x n (1) sent) =


y n :y n causes

n n
error p(y |x (1)). (7.163)

There are two kinds of error: the ï¬rst occurs if the received
sequence y n is not jointly typical with the transmitted codeword, and the second occurs if there is another codeword
jointly typical with the received sequence. Using the result
of the preceding parts, calculate this probability of error. By

PROBLEMS

231

the symmetry of the random coding argument, this does not
depend on which codeword was sent.
The calculations above show that average probability of error for
a random code with 512 codewords of length 25 over the binary
symmetric channel of crossover probability 0.1 is about 0.34. This
seems quite high, but the reason for this is that the value of  that
we have chosen is too large. By choosing a smaller  and a larger
n in the deï¬nitions of A(n)
 , we can get the probability of error to
be as small as we want as long as the rate of the code is less than
I (X; Y ) â 3.
Also note that the decoding procedure described in the problem
is not optimal. The optimal decoding procedure is maximum likelihood (i.e., to choose the codeword that is closest to the received
sequence). It is possible to calculate the average probability of
error for a random code for which the decoding is based on an
approximation to maximum likelihood decoding, where we decode
a received sequence to the unique codeword that differs from the
received sequence in â¤ 4 bits, and declare an error otherwise. The
only difference with the jointly typical decoding described above
is that in the case when the codeword is equal to the received
sequence! The average probability of error for this decoding scheme
can be shown to be about 0.285.
7.16

Encoder and decoder as part of the channel. Consider a binary
symmetric channel with crossover probability 0.1. A possible coding scheme for this channel with two codewords of length 3 is to
encode message a1 as 000 and a2 as 111. With this coding scheme,
we can consider the combination of encoder, channel, and decoder
as forming a new BSC, with two inputs a1 and a2 and two outputs
a1 and a2 .
(a) Calculate the crossover probability of this channel.
(b) What is the capacity of this channel in bits per transmission of
the original channel?
(c) What is the capacity of the original BSC with crossover probability 0.1?
(d) Prove a general result that for any channel, considering the
encoder, channel, and decoder together as a new channel from
messages to estimated messages will not increase the capacity
in bits per transmission of the original channel.

7.17

Codes of length 3 for a BSC and BEC. In Problem 7.16, the probability of error was calculated for a code with two codewords of

232

CHANNEL CAPACITY

length 3 (000 and 111) sent over a binary symmetric channel with
crossover probability . For this problem, take  = 0.1.
(a) Find the best code of length 3 with four codewords for this
channel. What is the probability of error for this code? (Note
that all possible received sequences should be mapped onto
possible codewords.)
(b) What is the probability of error if we used all eight possible
sequences of length 3 as codewords?
(c) Now consider a binary erasure channel with erasure probability
0.1. Again, if we used the two-codeword code 000 and 111,
received sequences 00E, 0E0, E00, 0EE, E0E, EE0 would all
be decoded as 0, and similarly, we would decode 11E, 1E1,
E11, 1EE, E1E, EE1 as 1. If we received the sequence EEE,
we would not know if it was a 000 or a 111 that was sentâso
we choose one of these two at random, and are wrong half the
time. What is the probability of error for this code over the
erasure channel?
(d) What is the probability of error for the codes of parts (a) and
(b) when used over the binary erasure channel?
7.18

Channel capacity. Calculate the capacity of the following channels
with probability transition matrices:
(a) X = Y = {0, 1, 2}
ï£® 1 1 1 ï£¹
ï£¯
p(y|x) = ï£°

3
1
3
1
3

3
1
3
1
3

3
1
3
1
3

1
2

1
2
1
2

0

ï£º
ï£»

(7.164)

(b) X = Y = {0, 1, 2}
ï£®

ï£¯
p(y|x) = ï£° 0
1
2

0

1
2
1
2

ï£¹
ï£º
ï£»

(7.165)

(c) X = Y = {0, 1, 2, 3}
ï£®

ï£¹
p
1âp
0
0
p
0
0 ï£º
ï£¯ 1âp
p(y|x) = ï£°
0
0
q
1âq ï£»
0
0
1âq
q

(7.166)

PROBLEMS

233

7.19

Capacity of the carrier pigeon channel. Consider a commander of
an army besieged in a fort for whom the only means of communication to his allies is a set of carrier pigeons. Assume that each
carrier pigeon can carry one letter (8 bits), that pigeons are released
once every 5 minutes, and that each pigeon takes exactly 3 minutes
to reach its destination.
(a) Assuming that all the pigeons reach safely, what is the capacity
of this link in bits/hour?
(b) Now assume that the enemies try to shoot down the pigeons
and that they manage to hit a fraction Î± of them. Since the
pigeons are sent at a constant rate, the receiver knows when
the pigeons are missing. What is the capacity of this link?
(c) Now assume that the enemy is more cunning and that every
time they shoot down a pigeon, they send out a dummy pigeon
carrying a random letter (chosen uniformly from all 8-bit letters). What is the capacity of this link in bits/hour?
Set up an appropriate model for the channel in each of the above
cases, and indicate how to go about ï¬nding the capacity.

7.20

Channel with two independent looks at Y. Let Y1 and Y2 be conditionally independent and conditionally identically distributed given
X.
(a) Show that I (X; Y1 , Y2 ) = 2I (X; Y1 ) â I (Y1 , Y2 ).
(b) Conclude that the capacity of the channel
X

(Y1, Y2)

is less than twice the capacity of the channel
X

7.21

Y1

Tall, fat people. Suppose that the average height of people in a
room is 5 feet. Suppose that the average weight is 100 lb.
(a) Argue that no more than one-third of the population is 15 feet
tall.
(b) Find an upper bound on the fraction of 300-lb 10-footers in
the room.

234

CHANNEL CAPACITY

7.22

Can signal alternatives lower capacity? Show that adding a row to
a channel transition matrix does not decrease capacity.

7.23

Binary multiplier channel
(a) Consider the channel Y = XZ, where X and Z are independent
binary random variables that take on values 0 and 1. Z is
Bernoulli(Î±) [i.e., P (Z = 1) = Î±]. Find the capacity of this
channel and the maximizing distribution on X.
(b) Now suppose that the receiver can observe Z as well as Y .
What is the capacity?

7.24

Noise alphabets. Consider the channel
Z

X

â

Y

X = {0, 1, 2, 3}, where Y = X + Z, and Z is uniformly distributed
over three distinct integer values Z = {z1 , z2 , z3 }.
(a) What is the maximum capacity over all choices of the Z alphabet? Give distinct integer values z1 , z2 , z3 and a distribution on
X achieving this.
(b) What is the minimum capacity over all choices for the Z alphabet? Give distinct integer values z1 , z2 , z3 and a distribution on
X achieving this.

7.25

Bottleneck channel. Suppose that a signal X â X = {1, 2, . . . , m}
goes through an intervening transition X ââ V ââ Y :

X

p (v | x )

V

p (y | v )

Y

where x = {1, 2, . . . , m}, y = {1, 2, . . . , m}, and v = {1, 2, . . . , k}.
Here p(v|x) and p(y|v)
 are arbitrary and the channel has transition
probability p(y|x) = v p(v|x)p(y|v). Show that C â¤ log k.

PROBLEMS

7.26

235

Noisy typewriter. Consider the channel with x, y â {0, 1, 2, 3} and
transition probabilities p(y|x) given by the following matrix:
ï£® 1 1
ï£¹
0
0
2
2
ï£¯ 0 1 1 0 ï£º
ï£¯
ï£º
2
2
ï£¯
ï£º
ï£° 0 0 12 21 ï£»
1
1
2 0 0 2
(a) Find the capacity of this channel.
(b) Deï¬ne the random variable z = g(y), where

A if y â {0, 1}
g(y) =
B if y â {2, 3}.
For the following two PMFs for x, compute I (X; Z):
(i)
 1
if x â {1, 3}
p(x) = 2
0 if x â {0, 2}.
(ii)


p(x) =

0 if x â {1, 3}
1
2 if x â {0, 2}.

(c) Find the capacity of the channel between x and z, speciï¬cally
where x â {0, 1, 2, 3}, z â {A, B}, and the transition probabilities P (z|x) are given by

p(Z = z|X = x) =
P (Y = y0 |X = x).
g(y0 )=z

(d) For the X distribution of part (i) of (b), does X â Z â Y
form a Markov chain?
7.27

Erasure channel. Let {X, p(y|x), Y} be a discrete memoryless channel with capacity C. Suppose that this channel is cascaded immediately with an erasure channel {Y, p(s|y), S} that erases Î± of its
symbols.

X

p (y | x )

Y

S

e

236

CHANNEL CAPACITY

Speciï¬cally, S = {y1 , y2 , . . . , ym , e}, and
Pr{S = y|X = x} = Î±p(y|x),
Pr{S = e|X = x} = Î±.

y â Y,

Determine the capacity of this channel.
7.28

Choice of channels. Find the capacity C of the union of two channels (X1 , p1 (y1 |x1 ), Y1 ) and (X2 , p2 (y2 |x2 ), Y2 ), where at each
time, one can send a symbol over channel 1 or channel 2 but
not both. Assume that the output alphabets are distinct and do not
intersect.
(a) Show that 2C = 2C1 + 2C2 . Thus, 2C is the effective alphabet
size of a channel with capacity C.
(b) Compare with Problem 2.10 where 2H = 2H1 + 2H2 , and interpret part (a) in terms of the effective number of noise-free
symbols.
(c) Use the above result to calculate the capacity of the following
channel.
0

1âp

0

p
p

1

1âp

2

1

2
1

7.29

Binary multiplier channel
(a) Consider the discrete memoryless channel Y = XZ, where X
and Z are independent binary random variables that take on
values 0 and 1. Let P (Z = 1) = Î±. Find the capacity of this
channel and the maximizing distribution on X.
(b) Now suppose that the receiver can observe Z as well as Y .
What is the capacity?

PROBLEMS

7.30

237

Noise alphabets. Consider the channel
Z

X

â

Y

X = {0, 1, 2, 3}, where Y = X + Z, and Z is uniformly distributed
over three distinct integer values Z = {z1 , z2 , z3 }.
(a) What is the maximum capacity over all choices of the Z alphabet? Give distinct integer values z1 , z2 , z3 and a distribution on
X achieving this.
(b) What is the minimum capacity over all choices for the Z alphabet? Give distinct integer values z1 , z2 , z3 and a distribution on
X achieving this.

7.31

Source and channel. We wish to encode a Bernoulli(Î±) process
V1 , V2 , . . . for transmission over a binary symmetric channel with
crossover probability p.
1âp

Vn

p
p

X n (V n)

Yn

^

Vn

1âp

Find conditions on Î± and p so that the probability of error P (VÌ n =
V n ) can be made to go to zero as n ââ â.
7.32

Random 20 questions. Let X be uniformly distributed over {1, 2,
. . . , m}. Assume that m = 2n . We ask random questions: Is X â S1 ?
Is X â S2 ? . . . until only one integer remains. All 2m subsets S of
{1, 2, . . . , m} are equally likely.
(a) How many deterministic questions are needed to determine X?
(b) Without loss of generality, suppose that X = 1 is the random
object. What is the probability that object 2 yields the same
answers as object 1 for k questions?
(c) What is the expected number of objects in {2, 3, . . . , m} that
have the same answers to the questions as those of the correct
object 1?

238

CHANNEL CAPACITY

â
(d) Suppose that we ask n + n random questions. What is the
expected number of wrong objects agreeing with the answers?
(e) Use Markovâs inequality Pr{X â¥ tÂµ} â¤ 1t , to show that the
probability of error (one or more wrong object remaining) goes
to zero as n ââ â.
7.33

BSC with feedback. Suppose that feedback is used on a binary
symmetric channel with parameter p. Each time a Y is received,
it becomes the next transmission. Thus, X1 is Bern( 12 ), X2 = Y1 ,
X3 = Y2 , . . ., Xn = Ynâ1 .
(a) Find limnââ n1 I (X n ; Y n ).
(b) Show that for some values of p, this can be higher than capacity.
(c) Using this feedback transmission scheme, X n (W, Y n ) = (X1
(W ), Y1 , Y2 , . . . , Ymâ1 ), what is the asymptotic communication
rate achieved; that is, what is limnââ n1 I (W ; Y n )?

7.34

Capacity. Find the capacity of
(a) Two parallel BSCs:
1

1

p
p
2

2

X

Y
3

3

p
p
4

4

(b) BSC and a single symbol:
1

1

p
p
2

2

3

3

X

Y

PROBLEMS

239

(c) BSC and a ternary channel:
1
2

1

1

p

1
2

2

X

3
4

p

1
2
1
2

2
1
2
1
2

1
2
1
2
1
2

Y

3
4

1
2

p

5

5

(d) Ternary channel:
p(y|x) =
7.35

2
3

0

1
3
1
3

0
2
3

!
.

(7.167)

Capacity. Suppose that channel P has capacity C, where P is an
m Ã n channel matrix.
(a) What is the capacity of

PÌ =

P 0
0 1


?

(b) What about the capacity of

PÌ =

P 0
0 Ik


?

where Ik if the k Ã k identity matrix.
7.36

Channel with memory. Consider the discrete memoryless channel
Yi = Zi Xi with input alphabet Xi â {â1, 1}.
(a) What is the capacity of this channel when {Zi } is i.i.d. with

1,
p = 0.5
Zi =
(7.168)
â1,
p = 0.5?

240

CHANNEL CAPACITY

Now consider the channel with memory. Before transmission
begins, Z is randomly chosen and ï¬xed for all time. Thus,
Yi = ZXi .
(b) What is the capacity if

1,
p = 0.5
Z=
(7.169)
â1,
p = 0.5?
7.37

Joint typicality. Let (Xi , Yi , Zi ) be i.i.d. according to p(x, y, z). We
will say that (x n , y n , zn ) is jointly typical [written (x n , y n , zn ) â
A(n)
 ] if
â¢ p(x n ) â 2ân(H (X)Â±) .
â¢ p(y n ) â 2ân(H (Y )Â±) .
â¢ p(zn ) â 2ân(H (Z)Â±) .
â¢ p(x n , y n ) â 2ân(H (X,Y )Â±) .
â¢ p(x n , zn ) â 2ân(H (X,Z)Â±) .
â¢ p(y n , zn ) â 2ân(H (Y,Z)Â±) .
â¢ p(x n , y n , zn ) â 2ân(H (X,Y,Z)Â±) .
Now suppose that (XÌ n , YÌ n , ZÌ n ) is drawn according to p(x n )p(y n )
p(zn ). Thus, XÌ n , YÌ n , ZÌ n have the same marginals as p(x n , y n , zn )
but are independent. Find (bounds on) Pr{(XÌ n ,YÌ n ,ZÌ n ) â A(n)
 } in
terms of the entropies H (X),H (Y ),H (Z),H (X, Y ),H (X, Z),
H (Y, Z), and H (X, Y, Z).

HISTORICAL NOTES
The idea of mutual information and its relationship to channel capacity
was developed by Shannon in his original paper [472]. In this paper, he
stated the channel capacity theorem and outlined the proof using typical
sequences in an argument similar to the one described here. The ï¬rst
rigorous proof was due to Feinstein [205], who used a painstaking âcookiecuttingâ argument to ï¬nd the number of codewords that can be sent with a
low probability of error. A simpler proof using a random coding exponent
was developed by Gallager [224]. Our proof is based on Cover [121] and
on Forneyâs unpublished course notes [216].
The converse was proved by Fano [201], who used the inequality bearing his name. The strong converse was ï¬rst proved by Wolfowitz [565],
using techniques that are closely related to typical sequences. An iterative
algorithm to calculate the channel capacity was developed independently
by Arimoto [25] and Blahut [65].

HISTORICAL NOTES

241

The idea of the zero-error capacity was developed by Shannon [474];
in the same paper, he also proved that feedback does not increase the
capacity of a discrete memoryless channel. The problem of ï¬nding the
zero-error capacity is essentially combinatorial; the ï¬rst important result
in this area is due to Lovasz [365]. The general problem of ï¬nding the
zero error capacity is still open; see a survey of related results in KoÌrner
and Orlitsky [327].
Quantum information theory, the quantum mechanical counterpart to
the classical theory in this chapter, is emerging as a large research area in
its own right and is well surveyed in an article by Bennett and Shor [49]
and in the text by Nielsen and Chuang [395].

CHAPTER 8

DIFFERENTIAL ENTROPY

We now introduce the concept of differential entropy, which is the entropy
of a continuous random variable. Differential entropy is also related to the
shortest description length and is similar in many ways to the entropy of
a discrete random variable. But there are some important differences, and
there is need for some care in using the concept.

8.1

DEFINITIONS

Deï¬nition Let X be a random variable with cumulative distribution
function F (x) = Pr(X â¤ x). If F (x) is continuous, the random variable
is said to be continuous. Let f (x) = F  (x) when the derivative is deï¬ned.
â
If ââ f (x) = 1, f (x) is called the probability density function for X. The
set where f (x) > 0 is called the support set of X.
Deï¬nition The differential entropy h(X) of a continuous random variable X with density f (x) is deï¬ned as

h(X) = â

f (x) log f (x) dx,

(8.1)

S

where S is the support set of the random variable.
As in the discrete case, the differential entropy depends only on the
probability density of the random variable, and therefore the differential
entropy is sometimes written as h(f ) rather than h(X).
Remark As in every example involving an integral, or even a density,
we should include the statement if it exists. It is easy to construct examples
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

243

244

DIFFERENTIAL ENTROPY

of random variables for which a density function does not exist or for
which the above integral does not exist.
Example 8.1.1 (Uniform distribution) Consider a random variable distributed uniformly from 0 to a so that its density is 1/a from 0 to a and 0
elsewhere. Then its differential entropy is


a

h(X) = â
0

1
1
log dx = log a.
a
a

(8.2)

Note: For a < 1, log a < 0, and the differential entropy is negative. Hence,
unlike discrete entropy, differential entropy can be negative. However,
2h(X) = 2log a = a is the volume of the support set, which is always nonnegative, as we expect.
Example 8.1.2 (Normal distribution) Let X â¼ Ï(x) =
Then calculating the differential entropy in nats, we obtain

âx 2

â 1 e 2Ï 2
2Ï Ï 2

.


h(Ï) = â

=â

Ï ln Ï

(8.3)




x2
2
Ï(x) â 2 â ln 2Ï Ï
2Ï

(8.4)

EX 2 1
+ ln 2Ï Ï 2
2Ï 2
2
1 1
= + ln 2Ï Ï 2
2 2
1
1
= ln e + ln 2Ï Ï 2
2
2
1
= ln 2Ï eÏ 2
nats.
2

=

(8.5)
(8.6)
(8.7)
(8.8)

Changing the base of the logarithm, we have
h(Ï) =

1
log 2Ï eÏ 2
2

bits.

(8.9)

8.2

8.2

245

AEP FOR CONTINUOUS RANDOM VARIABLES

AEP FOR CONTINUOUS RANDOM VARIABLES

One of the important roles of the entropy for discrete random variables
is in the AEP, which states that for a sequence of i.i.d. random variables,
p(X1 , X2 , . . . , Xn ) is close to 2ânH (X) with high probability. This enables
us to deï¬ne the typical set and characterize the behavior of typical sequences.
We can do the same for a continuous random variable.
Theorem 8.2.1
Let X1 , X2 , . . . , Xn be a sequence of random variables drawn i.i.d. according to the density f (x). Then
1
â log f (X1 , X2 , . . . , Xn ) â E[â log f (X)] = h(X) in probability.
n
(8.10)
Proof: The proof follows directly from the weak law of large numbers.

This leads to the following deï¬nition of the typical set.
Deï¬nition For  > 0 and any n, we deï¬ne the typical set A(n)
with

respect to f (x) as follows:






(n)
n  1
A = (x1 , x2 , . . . , xn ) â S : â log f (x1 , x2 , . . . , xn ) â h(X) â¤  ,
n
(8.11)
	n
where f (x1 , x2 , . . . , xn ) = i=1 f (xi ).
The properties of the typical set for continuous random variables parallel those for discrete random variables. The analog of the cardinality of
the typical set for the discrete case is the volume of the typical set for
continuous random variables.
Deï¬nition The volume Vol(A) of a set A â Rn is deï¬ned as

dx1 dx2 Â· Â· Â· dxn .
Vol(A) =

(8.12)

A

Theorem 8.2.2
The typical set A(n)
 has the following properties:

 (n) 
1. Pr A > 1 â  for n sufï¬ciently large.



â¤ 2n(h(X)+) for all n.
2. Vol A(n)


 (n) 
3. Vol A â¥ (1 â )2n(h(X)â) for n sufï¬ciently large.

246

DIFFERENTIAL ENTROPY


Proof: By Theorem 8.2.1, â n1 log f (X n ) = â n1
log f (Xi ) â h(X) in
probability, establishing property 1. Also,

1=
f (x1 , x2 , . . . , xn ) dx1 dx2 Â· Â· Â· dxn
(8.13)


Sn

â¥

(n)



f (x1 , x2 , . . . , xn ) dx1 dx2 Â· Â· Â· dxn

(8.14)

2ân(h(X)+) dx1 dx2 Â· Â· Â· dxn

(8.15)

A

â¥

(n)

A

ân(h(X)+)



=2

dx1 dx2 Â· Â· Â· dxn

(n)

(8.16)

A




= 2ân(h(X)+) Vol A(n)
.


(8.17)

Hence we have property 2. We argue further that the volume of the typical
set is at least this large. If n is sufï¬ciently large so that property 1 is
satisï¬ed, then

1â â¤
f (x1 , x2 , . . . , xn ) dx1 dx2 Â· Â· Â· dxn
(8.18)
(n)


â¤

A

(n)
A

2ân(h(X)â) dx1 dx2 Â· Â· Â· dxn

ân(h(X)â)

=2

(8.19)


(n)

dx1 dx2 Â· Â· Â· dxn

(8.20)

A




= 2ân(h(X)â) Vol A(n)
,


(8.21)

establishing property 3. Thus for n sufï¬ciently large, we have
n(h(X)+)
.
(1 â )2n(h(X)â) â¤ Vol(A(n)
 )â¤2



(8.22)

Theorem 8.2.3 The set A(n)
 is the smallest volume set with probability
â¥ 1 â , to ï¬rst order in the exponent.


Proof: Same as in the discrete case.

This theorem indicates that the volume of the smallest set that contains
most of the probability is approximately 2nh . This is an n-dimensional
1

volume, so the corresponding side length is (2nh ) n = 2h . This provides

247

8.3 RELATION OF DIFFERENTIAL ENTROPY TO DISCRETE ENTROPY

an interpretation of the differential entropy: It is the logarithm of the
equivalent side length of the smallest set that contains most of the probability. Hence low entropy implies that the random variable is conï¬ned
to a small effective volume and high entropy indicates that the random
variable is widely dispersed.
Note. Just as the entropy is related to the volume of the typical set, there
is a quantity called Fisher information which is related to the surface
area of the typical set. We discuss Fisher information in more detail in
Sections 11.10 and 17.8.
8.3 RELATION OF DIFFERENTIAL ENTROPY TO DISCRETE
ENTROPY
Consider a random variable X with density f (x) illustrated in Figure 8.1.
Suppose that we divide the range of X into bins of length . Let us
assume that the density is continuous within the bins. Then, by the mean
value theorem, there exists a value xi within each bin such that

f (xi ) =

(i+1)

f (x) dx.

(8.23)

i

Consider the quantized random variable X  , which is deï¬ned by
X  = xi

if i â¤ X < (i + 1).

(8.24)

f(x)

â

x

FIGURE 8.1. Quantization of a continuous random variable.

248

DIFFERENTIAL ENTROPY

Then the probability that X  = xi is
 (i+1)
pi =
f (x) dx = f (xi ).

(8.25)

i

The entropy of the quantized version is
H (X  ) = â

â


pi log pi

(8.26)

f (xi ) log(f (xi ))

(8.27)

ââ

=â

â

ââ

=â
=â




f (xi ) log f (xi ) â



f (xi ) log 

f (xi ) log f (xi ) â log ,

(8.28)
(8.29)



since f (xi ) = f (x) = 1. If f (x) log f (x) is Riemann integrable (a
condition to ensure that the limit is well deï¬ned [556]), the ï¬rst term in
(8.29) approaches the integral of âf (x) log f (x) as  â 0 by deï¬nition
of Riemann integrability. This proves the following.
Theorem 8.3.1 If the density f (x) of the random variable X is Riemann integrable, then
H (X  ) + log  â h(f ) = h(X),

as  â 0.

(8.30)

Thus, the entropy of an n-bit quantization of a continuous random variable X is approximately h(X) + n.
Example 8.3.1
1. If X has a uniform distribution on [0, 1] and we let  = 2ân ,
then h = 0, H (X  ) = n, and n bits sufï¬ce to describe X to n
bit accuracy.
2. If X is uniformly distributed on [0, 18 ], the ï¬rst 3 bits to the right
of the decimal point must be 0. To describe X to n-bit accuracy
requires only n â 3 bits, which agrees with h(X) = â3.
3. If X â¼ N(0, Ï 2 ) with Ï 2 = 100, describing X to n bit accuracy
would require on the average n + 12 log(2Ï eÏ 2 ) = n + 5.37 bits.

8.4 JOINT AND CONDITIONAL DIFFERENTIAL ENTROPY

249

In general, h(X) + n is the number of bits on the average required to
describe X to n-bit accuracy.
The differential entropy of a discrete random variable can be considered
to be ââ. Note that 2ââ = 0, agreeing with the idea that the volume of
the support set of a discrete random variable is zero.

8.4

JOINT AND CONDITIONAL DIFFERENTIAL ENTROPY

As in the discrete case, we can extend the deï¬nition of differential entropy
of a single random variable to several random variables.
Deï¬nition The differential entropy of a set X1 , X2 , . . . , Xn of random
variables with density f (x1 , x2 , . . . , xn ) is deï¬ned as

h(X1 , X2 , . . . , Xn ) = â

f (x n ) log f (x n ) dx n .

(8.31)

Deï¬nition If X, Y have a joint density function f (x, y), we can deï¬ne
the conditional differential entropy h(X|Y ) as

h(X|Y ) = â

f (x, y) log f (x|y) dx dy.

(8.32)

Since in general f (x|y) = f (x, y)/f (y), we can also write
h(X|Y ) = h(X, Y ) â h(Y ).

(8.33)

But we must be careful if any of the differential entropies are inï¬nite.
The next entropy evaluation is used frequently in the text.
Theorem 8.4.1 (Entropy of a multivariate normal distribution) Let
X1 , X2 , . . . , Xn have a multivariate normal distribution with mean Âµ and
covariance matrix K. Then
h(X1 , X2 , . . . , Xn ) = h(Nn (Âµ, K)) =

1
log(2Ï e)n |K|
2

where |K| denotes the determinant of K.

bits, (8.34)

250

DIFFERENTIAL ENTROPY

Proof: The probability density function of X1 , X2 , . . . , Xn is
1
1
T â1
f (x) = â n
eâ 2 (xâÂµ) K (xâÂµ) .
1
2Ï |K| 2

(8.35)

Then



â n
1
1
T â1
2Ï |K| 2 dx
f (x) â (x â Âµ) K (x â Âµ) â ln
2


h(f ) = â

ï£¹

ï£®

(8.36)





1
1
= E ï£° (Xi â Âµi ) K â1 ij (Xj â Âµj )ï£» + ln(2Ï )n |K| (8.37)
2
2
i,j

ï£¹




1
1
= E ï£° (Xi â Âµi )(Xj â Âµj ) K â1 ij ï£» + ln(2Ï )n |K| (8.38)
2
2
ï£®

i,j

=




1
1
E[(Xj â Âµj )(Xi â Âµi )] K â1 ij + ln(2Ï )n |K|
2
2

(8.39)




1
 
1
Kj i K â1 ij + ln(2Ï )n |K|
2
2

(8.40)

1
1
(KK â1 )jj + ln(2Ï )n |K|
2
2

(8.41)

1
1
Ijj + ln(2Ï )n |K|
2
2

(8.42)

i,j

=

j

=

i

j

=

j

n 1
+ ln(2Ï )n |K|
2 2
1
= ln(2Ï e)n |K|
nats
2
1
= log(2Ï e)n |K|
bits.
2

=

8.5

(8.43)
(8.44)


(8.45)

RELATIVE ENTROPY AND MUTUAL INFORMATION

We now extend the deï¬nition of two familiar quantities, D(f ||g) and
I (X; Y ), to probability densities.

8.5

RELATIVE ENTROPY AND MUTUAL INFORMATION

251

Deï¬nition The relative entropy (or KullbackâLeibler distance) D(f ||g)
between two densities f and g is deï¬ned by

f
(8.46)
D(f ||g) = f log .
g
Note that D(f ||g) is ï¬nite only if the support set of f is contained in
the support set of g. [Motivated by continuity, we set 0 log 00 = 0.]
Deï¬nition The mutual information I (X; Y ) between two random variables with joint density f (x, y) is deï¬ned as

f (x, y)
dx dy.
(8.47)
I (X; Y ) = f (x, y) log
f (x)f (y)
From the deï¬nition it is clear that
I (X; Y ) = h(X) â h(X|Y ) = h(Y ) â h(Y |X) = h(X) + h(Y ) â h(X, Y )
(8.48)
and
I (X; Y ) = D(f (x, y)||f (x)f (y)).
(8.49)
The properties of D(f ||g) and I (X; Y ) are the same as in the discrete case. In particular, the mutual information between two random
variables is the limit of the mutual information between their quantized
versions, since
I (X  ; Y  ) = H (X  ) â H (X  |Y  )

(8.50)

â h(X) â log  â (h(X|Y ) â log )

(8.51)

= I (X; Y ).

(8.52)

More generally, we can deï¬ne mutual information in terms of ï¬nite
partitions of the range of the random variable. Let X be the range of a
random variable X. A partition P of X is a ï¬nite collection of disjoint
sets Pi such that âªi Pi = X. The quantization of X by P (denoted [X]P )
is the discrete random variable deï¬ned by

dF (x).
(8.53)
Pr([X]P = i) = Pr(X â Pi ) =
Pi

For two random variables X and Y with partitions P and Q, we can
calculate the mutual information between the quantized versions of X
and Y using (2.28). Mutual information can now be deï¬ned for arbitrary
pairs of random variables as follows:

252

DIFFERENTIAL ENTROPY

Deï¬nition The mutual information between two random variables X
and Y is given by
(8.54)
I (X; Y ) = sup I ([X]P ; [Y ]Q ),
P ,Q

where the supremum is over all ï¬nite partitions P and Q.
This is the master deï¬nition of mutual information that always applies,
even to joint distributions with atoms, densities, and singular parts. Moreover, by continuing to reï¬ne the partitions P and Q, one ï¬nds a monotonically increasing sequence I ([X]P ; [Y ]Q )  I .
By arguments similar to (8.52), we can show that this deï¬nition of
mutual information is equivalent to (8.47) for random variables that have
a density. For discrete random variables, this deï¬nition is equivalent to
the deï¬nition of mutual information in (2.28).
Example 8.5.1 (Mutual information between correlated Gaussian random variables with correlation Ï) Let (X, Y ) â¼ N(0, K), where

 2
ÏÏ 2
Ï
.
(8.55)
K=
ÏÏ 2
Ï2
Then h(X) = h(Y ) = 12 log(2Ï e)Ï 2 and h(X, Y ) =
1
2 4
2
2 log(2Ï e) Ï (1 â Ï ), and therefore

1
2

log(2Ï e)2 |K| =

1
(8.56)
I (X; Y ) = h(X) + h(Y ) â h(X, Y ) = â log(1 â Ï 2 ).
2
If Ï = 0, X and Y are independent and the mutual information is 0.
If Ï = Â±1, X and Y are perfectly correlated and the mutual information
is inï¬nite.
8.6 PROPERTIES OF DIFFERENTIAL ENTROPY, RELATIVE
ENTROPY, AND MUTUAL INFORMATION
Theorem 8.6.1
D(f ||g) â¥ 0

(8.57)

with equality iff f = g almost everywhere (a.e.).
Proof: Let S be the support set of f . Then

g
âD(f ||g) = f log
f
S

g
(by Jensenâs inequality)
â¤ log f
S f

(8.58)
(8.59)

253

8.6 DIFFERENTIAL ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION


= log

g

(8.60)

S

â¤ log 1 = 0.

(8.61)

We have equality iff we have equality in Jensenâs inequality, which
occurs iff f = g a.e.

Corollary

I (X; Y ) â¥ 0 with equality iff X and Y are independent.

Corollary

h(X|Y ) â¤ h(X) with equality iff X and Y are independent.

Theorem 8.6.2

(Chain rule for differential entropy)

h(X1 , X2 , . . . , Xn ) =

n


h(Xi |X1 , X2 , . . . , Xiâ1 ).

(8.62)

i=1



Proof: Follows directly from the deï¬nitions.
Corollary
h(X1 , X2 , . . . , Xn ) â¤



h(Xi ),

(8.63)

with equality iff X1 , X2 , . . . , Xn are independent.
Proof: Follows directly from Theorem 8.6.2 and the corollary to Theorem 8.6.1.

Application (Hadamardâs inequality) If we let X â¼ N(0, K) be a multivariate normal random variable, calculating the entropy in the above
inequality gives us
n

Kii ,
(8.64)
|K| â¤
i=1

which is Hadamardâs inequality. A number of determinant inequalities
can be derived in this fashion from information-theoretic inequalities
(Chapter 17).
Theorem 8.6.3

h(X + c) = h(X).

(8.65)

Translation does not change the differential entropy.
Proof: Follows directly from the deï¬nition of differential entropy.



254

DIFFERENTIAL ENTROPY

Theorem 8.6.4

h(aX) = h(X) + log |a|.

Proof: Let Y = aX. Then fY (y) =

y
1
|a| fX ( a ),

(8.66)

and


h(aX) = â

=â

=â

fY (y) log fY (y) dy

(8.67)


y 
 y 
1
1
fX
log
fX
dy
|a|
a
|a|
a

(8.68)

fX (x) log fX (x) dx + log |a|

(8.69)

= h(X) + log |a|,
after a change of variables in the integral.

(8.70)


Similarly, we can prove the following corollary for vector-valued random variables.
Corollary

h(AX) = h(X) + log |det(A)|.

(8.71)

We now show that the multivariate normal distribution maximizes the
entropy over all distributions with the same covariance.
Theorem 8.6.5
Let the random vector X â Rn have zero mean and
covariance K = EXXt (i.e., Kij = EXi Xj , 1 â¤ i, j â¤ n). Then h(X) â¤
1
n
2 log(2Ï e) |K|, with equality iff X â¼ N(0, K).

Proof: Let g(x) be any density satisfying g(x)xi xj dx = Kij for all
i, j . Let ÏK be the density of a N(0, K) vector as given in (8.35), where we
set Âµ = 0. Note that log ÏK (x) is a quadratic form and xi xj ÏK (x) dx =
Kij . Then
0 â¤ D(g||ÏK )
(8.72)

(8.73)
= g log(g/ÏK )

(8.74)
= âh(g) â g log ÏK

8.6 DIFFERENTIAL ENTROPY, RELATIVE ENTROPY, AND MUTUAL INFORMATION

255


= âh(g) â

ÏK log ÏK

(8.75)

= âh(g) + h(ÏK ),
(8.76)


where the substitution g log ÏK = ÏK log ÏK follows from the fact
that g and ÏK yield the same moments of the quadratic form log ÏK (x).

In particular, the Gaussian distribution maximizes the entropy over
all distributions with the same variance. This leads to the estimation
counterpart to Fanoâs inequality. Let X be a random variable with differential entropy h(X). Let XÌ be an estimate of X, and let E(X â XÌ)2 be
the expected prediction error. Let h(X) be in nats.
Theorem 8.6.6 (Estimation error and differential entropy)
random variable X and estimator XÌ,
E(X â XÌ)2 â¥

For any

1 2h(X)
e
,
2Ï e

with equality if and only if X is Gaussian and XÌ is the mean of X.
Proof: Let XÌ be any estimator of X; then
E(X â XÌ)2 â¥ min E(X â XÌ)2

(8.77)

= E (X â E(X))2

(8.78)

= var(X)

(8.79)

XÌ

â¥

1 2h(X)
,
e
2Ï e

(8.80)

where (8.78) follows from the fact that the mean of X is the best estimator
for X and the last inequality follows from the fact that the Gaussian
distribution has the maximum entropy for a given variance. We have
equality only in (8.78) only if XÌ is the best estimator (i.e., XÌ is the mean

of X and equality in (8.80) only if X is Gaussian).
Corollary

Given side information Y and estimator XÌ(Y ), it follows that
E(X â XÌ(Y ))2 â¥

1 2h(X|Y )
e
.
2Ï e

256

DIFFERENTIAL ENTROPY

SUMMARY

h(X) = h(f ) = â

f (x) log f (x) dx

(8.81)

S

f (X n )=2ânh(X)
.

. nh(X)
.
Vol(A(n)
 )=2

H ([X]2ân ) â h(X) + n.
1
log 2Ï eÏ 2 .
2
1
h(Nn (Âµ, K)) = log(2Ï e)n |K|.
2

f
D(f ||g) = f log â¥ 0.
g
h(N(0, Ï 2 )) =

h(X1 , X2 , . . . , Xn ) =

n


h(Xi |X1 , X2 , . . . , Xiâ1 ).

(8.82)
(8.83)
(8.84)
(8.85)
(8.86)
(8.87)
(8.88)

i=1

h(X|Y ) â¤ h(X).

(8.89)

h(aX) = h(X) + log |a|.

f (x, y)
â¥ 0.
I (X; Y ) = f (x, y) log
f (x)f (y)

(8.90)

1
log(2Ï e)n |K|.
2
EXXt =K
1 2h(X|Y )
E(X â XÌ(Y ))2 â¥
e
.
2Ï e
max h(X) =

(8.91)
(8.92)

2nH (X) is the effective alphabet size for a discrete random variable.
2nh(X) is the effective support set size for a continuous random variable.
2C is the effective alphabet size of a channel of capacity C.

PROBLEMS
8.1

Differential
entropy. Evaluate the differential entropy h(X) =

â f ln f for the following:
(a) The exponential density, f (x) = Î»eâÎ»x , x â¥ 0.

PROBLEMS

257

(b) The Laplace density, f (x) = 12 Î»eâÎ»|x| .
(c) The sum of X1 and X2 , where X1 and X2 are independent
normal random variables with means Âµi and variances Ïi2 , i =
1, 2.
8.2

Concavity of determinants. Let K1 and K2 be two symmetric nonnegative deï¬nite n Ã n matrices. Prove the result of Ky Fan [199]:
| Î»K1 + Î»K2 |â¥| K1 |Î» | K2 |Î»

for 0 â¤ Î» â¤ 1, Î» = 1 â Î»,

where | K | denotes the determinant of K. [Hint: Let Z = XÎ¸ ,
where X1 â¼ N (0, K1 ), X2 â¼ N (0, K2 ) and Î¸ = Bernoulli(Î»). Then
use h(Z | Î¸ ) â¤ h(Z).]
8.3

Uniformly distributed noise. Let the input random variable X to
a channel be uniformly distributed over the interval â 12 â¤ x â¤ + 12 .
Let the output of the channel be Y = X + Z, where the noise random variable is uniformly distributed over the interval âa/2 â¤ z â¤
+a/2.
(a) Find I (X; Y ) as a function of a.
(b) For a = 1 ï¬nd the capacity of the channel when the input X
is peak-limited; that is, the range of X is limited to â 12 â¤ x â¤
+ 12 . What probability distribution on X maximizes the mutual
information I (X; Y )?
(c) (Optional ) Find the capacity of the channel for all values of a,
again assuming that the range of X is limited to â 12 â¤ x â¤ + 12 .

8.4

Quantized random variables. Roughly how many bits are required
on the average to describe to three-digit accuracy the decay time
(in years) of a radium atom if the half-life of radium is 80 years?
Note that half-life is the median of the distribution.

Scaling. Let h(X) = â f (x) log f (x) dx. Show
h(AX) = log | det(A) | + h(X).

8.5
8.6

Variational inequality.
that

Verify for positive random variables X



log EP (X) = sup EQ (log X) â D(Q||P ) ,
(8.93)
Q


where EP (X) = x xP (x) and D(Q||P ) = x Q(x) log Q(x)
P (x) ,

and the supremum is over all Q(x) â¥ 0,
Q(x)
 = 1. It is enough
to extremize J (Q) = EQ ln XâD(Q||P )+Î»( Q(x)â1).

258

8.7

DIFFERENTIAL ENTROPY

Differential entropy bound on discrete entropy. Let X be a discrete random variable on the set X = {a1 , a2 , . . .} with Pr(X =
ai ) = pi . Show that
ï£«
ï£¶
2
â
â


1
1
H (p1 , p2 , . . .) â¤ log(2Ï e) ï£­
pi i 2 â
ipi + ï£¸ .
2
12
i=1

i=1

(8.94)
Moreover, for every permutation Ï ,
ï£«
ï£¶
2
â
â


1
1
pÏ (i) i 2 â
ipÏ (i) + ï£¸.
H (p1 , p2 , . . .) â¤ log(2Ï e) ï£­
2
12
i=1

i=1

(8.95)
[Hint: Construct a random variable X such that Pr(X = i) = pi .
Let U be a uniform (0,1] random variable and let Y = X  + U ,
where X  and U are independent. Use the maximum entropy bound
on Y to obtain the bounds in the problem. This bound is due to
Massey (unpublished) and Willems (unpublished).]




8.8

Channel with uniformly distributed noise. Consider a additive
channel whose input alphabet X = {0,Â±1,Â±2} and whose output
Y = X+Z, where Z is distributed uniformly over the interval
[â1, 1]. Thus, the input of the channel is a discrete random variable, whereas the output is continuous. Calculate the capacity C =
maxp(x) I (X; Y ) of this channel.

8.9

Gaussian mutual information. Suppose that (X, Y, Z) are jointly
Gaussian and that X â Y â Z forms a Markov chain. Let X and
Y have correlation coefï¬cient Ï1 and let Y and Z have correlation
coefï¬cient Ï2 . Find I (X; Z).

8.10

Shape of the typical set. Let Xi be i.i.d. â¼ f (x), where
f (x) = ceâx .
4


Let h = â f ln f . Describe the shape (or form) or the typical set
n
n
n
ân(hÂ±)
A(n)
}.
 = {x â R : f (x ) â 2
8.11

Nonergodic Gaussian process. Consider a constant signal V in
the presence of iid observational noise {Zi }. Thus, Xi = V + Zi ,
where V â¼ N (0, S) and Zi are iid â¼ N (0, N). Assume that V and
{Zi } are independent.
(a) Is {Xi } stationary?

HISTORICAL NOTES

259


(b) Find limnâââ n1 ni=1 Xi . Is the limit random?
(c) What is the entropy rate h of {Xi }?
(d) Find the least-mean-squared error predictor XÌn+1 (X n ), and ï¬nd
2
Ïâ
= limnâââ E(XÌn â Xn )2 .
(e) Does {Xi } have an AEP? That is, does â n1 log f (X n ) ââ h?
HISTORICAL NOTES
Differential entropy and discrete entropy were introduced in Shannonâs
original paper [472]. The general rigorous deï¬nition of relative entropy
and mutual information for arbitrary random variables was developed by
Kolmogorov [319] and Pinsker [425], who deï¬ned mutual information as
supP ,Q I ([X]P ; [Y ]Q ), where the supremum is over all ï¬nite partitions P
and Q.

CHAPTER 9

GAUSSIAN CHANNEL

The most important continuous alphabet channel is the Gaussian channel
depicted in Figure 9.1. This is a time-discrete channel with output Yi at
time i, where Yi is the sum of the input Xi and the noise Zi . The noise
Zi is drawn i.i.d. from a Gaussian distribution with variance N . Thus,
Yi = Xi + Zi ,

Zi â¼ N(0, N).

(9.1)

The noise Zi is assumed to be independent of the signal Xi . This channel
is a model for some common communication channels, such as wired and
wireless telephone channels and satellite links. Without further conditions,
the capacity of this channel may be inï¬nite. If the noise variance is zero,
the receiver receives the transmitted symbol perfectly. Since X can take
on any real value, the channel can transmit an arbitrary real number with
no error.
If the noise variance is nonzero and there is no constraint on the input,
we can choose an inï¬nite subset of inputs arbitrarily far apart, so that
they are distinguishable at the output with arbitrarily small probability of
error. Such a scheme has an inï¬nite capacity as well. Thus if the noise
variance is zero or the input is unconstrained, the capacity of the channel
is inï¬nite.
The most common limitation on the input is an energy or power constraint.
We assume an average power constraint. For any codeword (x1 , x2 , . . . , xn )
transmitted over the channel, we require that
1 2
xi â¤ P .
n
n

(9.2)

i=1

This communication channel models many practical channels, including radio and satellite links. The additive noise in such channels may be
due to a variety of causes. However, by the central limit theorem, the
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

261

262

GAUSSIAN CHANNEL

Zi

Xi

Yi

FIGURE 9.1. Gaussian channel.

cumulative effect of a large number of small random effects will be
approximately normal, so the Gaussian assumption is valid in a large
number of situations.
We ï¬rst analyze a simple suboptimal way to use this channel. Assume
that we want to send 1 bit over the channel in one use of the channel.
Given the power
â constraint,
â the best that we can do is to send one of
two levels, + P or â P . The receiver looks at the corresponding Y
received and tries to decide which of the two levels was sent. Assuming
that both levels are equally likely (this would be the case if we wish to
send exactly
1 bit of information), the optimum
â
â decoding rule is to decide
that + P was sent if Y > 0 and decide â P was sent if Y < 0. The
probability of error with such a decoding scheme is
â
â
1
1
Pr(Y < 0|X = + P ) + Pr(Y > 0|X = â P )
2
2
â
â
â
â
1
1
= Pr(Z < â P |X = + P ) + Pr(Z > P |X = â P )
2
2
â
= Pr(Z > P )


=1â
P /N ,

Pe =

where (x) is the cumulative normal function
 x
1 ât 2
(x) =
â e 2 dt.
2Ï
ââ

(9.3)
(9.4)
(9.5)
(9.6)

(9.7)

Using such a scheme, we have converted the Gaussian channel into a discrete binary symmetric channel with crossover probability Pe . Similarly,
by using a four-level input signal, we can convert the Gaussian channel

9.1 GAUSSIAN CHANNEL: DEFINITIONS

263

into a discrete four-input channel. In some practical modulation schemes,
similar ideas are used to convert the continuous channel into a discrete
channel. The main advantage of a discrete channel is ease of processing
of the output signal for error correction, but some information is lost in
the quantization.
9.1

GAUSSIAN CHANNEL: DEFINITIONS

We now deï¬ne the (information) capacity of the channel as the maximum of the mutual information between the input and output over all
distributions on the input that satisfy the power constraint.
Deï¬nition The information capacity of the Gaussian channel with
power constraint P is
C=

max

f (x):E X2 â¤P

I (X; Y ).

(9.8)

We can calculate the information capacity as follows: Expanding
I (X; Y ), we have
I (X; Y ) = h(Y ) â h(Y |X)

(9.9)

= h(Y ) â h(X + Z|X)

(9.10)

= h(Y ) â h(Z|X)

(9.11)

= h(Y ) â h(Z),

(9.12)

since Z is independent of X. Now, h(Z) =

1
2

log 2Ï eN . Also,

EY 2 = E(X + Z)2 = EX 2 + 2EXEZ + EZ 2 = P + N,

(9.13)

since X and Z are independent and EZ = 0. Given EY 2 = P + N , the
entropy of Y is bounded by 12 log 2Ï e(P + N ) by Theorem 8.6.5 (the
normal maximizes the entropy for a given variance).
Applying this result to bound the mutual information, we obtain
I (X; Y ) = h(Y ) â h(Z)
1
1
log 2Ï e(P + N ) â log 2Ï eN
2
2


1
P
.
= log 1 +
2
N
â¤

(9.14)
(9.15)
(9.16)

264

GAUSSIAN CHANNEL

Hence, the information capacity of the Gaussian channel is


1
P
,
C = max I (X; Y ) = log 1 +
2
N
EX 2 â¤P

(9.17)

and the maximum is attained when X â¼ N(0, P ).
We will now show that this capacity is also the supremum of the rates
achievable for the channel. The arguments are similar to the arguments
for a discrete channel. We will begin with the corresponding deï¬nitions.
Deï¬nition An (M, n) code for the Gaussian channel with power constraint P consists of the following:
1. An index set {1, 2, . . . , M}.
2. An encoding function x : {1, 2, . . . , M} â X n , yielding codewords
x n (1), x n (2), . . . , x n (M), satisfying the power constraint P ; that is,
for every codeword
n


xi2 (w) â¤ nP ,

w = 1, 2, . . . , M.

(9.18)

i=1

3. A decoding function
g : Y n â {1, 2, . . . , M}.

(9.19)

The rate and probability of error of the code are deï¬ned as in Chapter 7
for the discrete case. The arithmetic average of the probability of error is
deï¬ned by
Pe(n) =

1 
Î»i .
2nR

Deï¬nition A rate R is said to be achievable for a
with a power constraint P if there exists a sequence
with codewords satisfying the power constraint such
probability of error Î»(n) tends to zero. The capacity of
supremum of the achievable rates.

(9.20)
Gaussian channel
of (2nR , n) codes
that the maximal
the channel is the

Theorem 9.1.1 The capacity of a Gaussian channel with power constraint P and noise variance N is


P
1
bits per transmission.
(9.21)
C = log 1 +
2
N

265

9.1 GAUSSIAN CHANNEL: DEFINITIONS

Remark We ï¬rst present a plausibility argument as to why we may be
able to construct (2nC , n) codes with a low probability of error. Consider
any codeword of length n. The received vector is normally distributed with
mean equal to the true codeword and variance equal to the noise variance.
With
â high probability, the received vector is contained in a sphere of radius
n(N + ) around the true codeword. If we assign everything within this
sphere to the given codeword, when this codeword is sent there will be
an error only if the received vector falls outside the sphere, which has
low probability.
Similarly, we can choose other codewords and their corresponding
decoding spheres. How many such codewords can we choose? The volis the
ume of an n-dimensional sphere is of the form Cn r n , where r â
radius of the sphere. In this case, each decoding sphere has radius nN.
These spheres are scattered throughout the space of received vectors. The
received vectors have
â energy no greater than n(P + N ), so they lie in a
sphere of radius n(P + N ). The maximum number of nonintersecting
decoding spheres in this volume is no more than
n

Cn (n(P + N )) 2
n

Cn (nN ) 2

and the rate of the code is 12 log(1 +



P
= 2 log 1 +
N
n
2

(9.22)

P
N ). This idea is illustrated in Figure

FIGURE 9.2. Sphere packing for the Gaussian channel.

9.2.

266

GAUSSIAN CHANNEL

This sphere-packing argument indicates that we cannot hope to send
at rates greater than C with low probability of error. However, we can
actually do almost as well as this, as is proved next.
Proof: (Achievability). We will use the same ideas as in the proof of
the channel coding theorem in the case of discrete channels: namely,
random codes and joint typicality decoding. However, we must make
some modiï¬cations to take into account the power constraint and the fact
that the variables are continuous and not discrete.
1. Generation of the codebook . We wish to generate a codebook in
which all the codewords satisfy the power constraint. To ensure
this, we generate the codewords with each element i.i.d. according to
distribution with variance P â . Since for large
a normal
Xi2 â P â , the probability that a codeword does not satn, n1
isfy the power constraint will be small. Let Xi (w), i = 1, 2, . . . , n,
w = 1, 2, . . . , 2nR be i.i.d. â¼ N(0, P â ), forming codewords
X n (1), X n (2), . . . , X n (2nR ) â Rn .
2. Encoding. After the generation of the codebook, the codebook is
revealed to both the sender and the receiver. To send the message
index w, the transmitter sends the wth codeword X n (w) in the codebook.
3. Decoding. The receiver looks down the list of codewords {X n (w)}
and searches for one that is jointly typical with the received vector.
If there is one and only one such codeword X n (w), the receiver
declares WÌ = w to be the transmitted codeword. Otherwise, the
receiver declares an error. The receiver also declares an error if the
chosen codeword does not satisfy the power constraint.
4. Probability of error . Without loss of generality, assume that codeword 1 was sent. Thus, Y n = X n (1) + Z n . Deï¬ne the following
events:
ï£±
ï£¼
n
ï£²1 
ï£½
2
Xj (1) > P
E0 =
(9.23)
ï£³n
ï£¾
j =1

and


.
Ei = (X n (i), Y n ) is in A(n)


(9.24)

Then an error occurs if E0 occurs (the power constraint is violated)
or E1c occurs (the transmitted codeword and the received sequence
are not jointly typical) or E2 âª E3 âª Â· Â· Â· âª E2nR occurs (some wrong

267

9.1 GAUSSIAN CHANNEL: DEFINITIONS

codeword is jointly typical with the received sequence). Let E denote
the event WÌ = W and let P denote the conditional probability given
that W = 1. Hence,


Pr(E|W = 1) = P (E) = P E0 âª E1c âª E2 âª E3 âª Â· Â· Â· âª E2nR
(9.25)

nR

â¤ P (E0 ) + P (E1c ) +

2


P (Ei ),

(9.26)

i=2

by the union of events bound for probabilities. By the law of large
numbers, P (E0 ) â 0 as n â â. Now, by the joint AEP (which
can be proved using the same argument as that used in the discrete
case), P (E1c ) â 0, and hence
P (E1c ) â¤ 

for n sufï¬ciently large.

(9.27)

Since by the code generation process, X n (1) and X n (i) are independent, so are Y n and X n (i). Hence, the probability that X n (i) and Y n
will be jointly typical is â¤ 2ân(I (X;Y )â3) by the joint AEP.
Now let W be uniformly distributed over {1, 2, . . . , 2nR }, and consequently,
Pr(E) =
Then

1 
Î»i = Pe(n) .
nR
2

Pe(n) = Pr(E) = Pr(E|W = 1)

(9.28)

(9.29)

nR

â¤ P (E0 ) +

P (E1c )

+

2


P (Ei )

(9.30)

i=2
nR

â¤++

2


2ân(I (X;Y )â3)

(9.31)

i=2



= 2 + 2nR â 1 2ân(I (X;Y )â3)

(9.32)

â¤ 2 + 23n 2ân(I (X;Y )âR)

(9.33)

â¤ 3

(9.34)

268

GAUSSIAN CHANNEL

for n sufï¬ciently large and R < I (X; Y ) â 3. This proves the existence of a good (2nR , n) code.
Now choosing a good codebook and deleting the worst half of the
codewords, we obtain a code with low maximal probability of error. In
particular, the power constraint is satisï¬ed by each of the remaining codewords (since the codewords that do not satisfy the power constraint have
probability of error 1 and must belong to the worst half of the codewords).
Hence we have constructed a code that achieves a rate arbitrarily close to
capacity. The forward part of the theorem is proved. In the next section

we show that the achievable rate cannot exceed the capacity.
9.2 CONVERSE TO THE CODING THEOREM FOR GAUSSIAN
CHANNELS
In this section we complete the proof that the capacity of a Gaussian
P
channel is C = 12 log(1 + N
) by proving that rates R > C are not achievable. The proof parallels the proof for the discrete channel. The main new
ingredient is the power constraint.
Proof: (Converse to Theorem 9.1.1). We must show that if Pe(n) â 0 for
a sequence of (2nR , n) codes for a Gaussian channel with power constraint
P , then


1
P
.
(9.35)
R â¤ C = log 1 +
2
N
Consider any (2nR , n) code that satisï¬es the power constraint, that is,
1 2
xi (w) â¤ P ,
n
n

(9.36)

i=1

for w = 1, 2, . . . , 2nR . Proceeding as in the converse for the discrete case,
let W be distributed uniformly over {1, 2, . . . , 2nR }. The uniform distribution over the index set W â {1, 2, . . . , 2nR } induces a distribution on
the input codewords, which in turn induces a distribution over the input
alphabet. This speciï¬es a joint distribution on W â X n (W ) â Y n â WÌ .
To relate probability of error and mutual information, we can apply Fanoâs
inequality to obtain
H (W |WÌ ) â¤ 1 + nRPe(n) = nn ,

(9.37)

9.2 CONVERSE TO THE CODING THEOREM FOR GAUSSIAN CHANNELS

269

where n â 0 as Pe(n) â 0. Hence,
nR = H (W ) = I (W ; WÌ ) + H (W |WÌ )

(9.38)

â¤ I (W ; WÌ ) + nn

(9.39)

â¤ I (X n ; Y n ) + nn

(9.40)

= h(Y n ) â h(Y n |X n ) + nn

(9.41)

= h(Y n ) â h(Z n ) + nn

(9.42)

â¤

n


h(Yi ) â h(Z n ) + nn

(9.43)

i=1

=

n

i=1

=

n


h(Yi ) â

n


h(Zi ) + nn

(9.44)

i=1

I (Xi ; Yi ) + nn .

(9.45)

i=1

Here Xi = xi (W ), where W is drawn according to the uniform distribution
on {1, 2, . . . , 2nR }. Now let Pi be the average power of the ith column of
the codebook, that is,
Pi =

1  2
x (w).
2nR w i

(9.46)

Then, since Yi = Xi + Zi and since Xi and Zi are independent, the average power EYi 2 of Yi is Pi + N . Hence, since entropy is maximized by
the normal distribution,
h(Yi ) â¤

1
log 2Ï e(Pi + N ).
2

Continuing with the inequalities of the converse, we obtain

nR â¤
(h(Yi ) â h(Zi )) + nn


 1
1
â¤
log(2Ï e(Pi + N )) â log 2Ï eN + nn
2
2


1
Pi
log 1 +
+ nn .
=
2
N

(9.47)

(9.48)
(9.49)
(9.50)

270

GAUSSIAN CHANNEL

Since each of the codewords satisï¬es the power constraint, so does their
average, and hence
1
Pi â¤ P .
n

(9.51)

i

Since f (x) = 12 log(1 + x) is a concave function of x, we can apply
Jensenâs inequality to obtain



n
11
Pi
1
log 1 +
â¤ log 1 +
n
2
N
2
i=1

1
â¤ log 1 +
2
Thus R â¤

1
2

log(1 +

P
N)

1  Pi
n
N
i=1

P
.
N
n


(9.52)
(9.53)

+ n , n â 0, and we have the required converse.

Note that the power constraint enters the standard proof in (9.46).

9.3

BANDLIMITED CHANNELS

A common model for communication over a radio network or a telephone
line is a bandlimited channel with white noise. This is a continuoustime channel. The output of such a channel can be described as the
convolution
Y (t) = (X(t) + Z(t)) â h(t),

(9.54)

where X(t) is the signal waveform, Z(t) is the waveform of the white
Gaussian noise, and h(t) is the impulse response of an ideal bandpass
ï¬lter, which cuts out all frequencies greater than W . In this section we
give simpliï¬ed arguments to calculate the capacity of such a channel.
We begin with a representation theorem due to Nyquist [396] and Shannon [480], which shows that sampling a bandlimited signal at a sampling
1
is sufï¬cient to reconstruct the signal from the samples. Intuitively,
rate 2W
this is due to the fact that if a signal is bandlimited to W , it cannot change
by a substantial amount in a time less than half a cycle of the maximum
frequency in the signal, that is, the signal cannot change very much in
1
time intervals less than 2W
seconds.

9.3

BANDLIMITED CHANNELS

271

Theorem 9.3.1 Suppose that a function f (t) is bandlimited to W ,
namely, the spectrum of the function is 0 for all frequencies greater than
W . Then the function is completely determined by samples of the function
1
seconds apart.
spaced 2W
Proof: Let F (Ï) be the Fourier transform of f (t). Then
 â
1
f (t) =
F (Ï)eiÏt dÏ
2Ï ââ
 2Ï W
1
=
F (Ï)eiÏt dÏ,
2Ï â2Ï W

(9.55)
(9.56)

since F (Ï) is zero outside the band â2Ï W â¤ Ï â¤ 2Ï W . If we consider
1
samples spaced 2W
seconds apart, the value of the signal at the sample
points can be written
f

 2Ï W
 n 
n
1
=
F (Ï)eiÏ 2W dÏ.
2W
2Ï â2Ï W

(9.57)

The right-hand side of this equation is also the deï¬nition of the coefï¬cients
of the Fourier series expansion of the periodic extension of the function
F (Ï), taking the interval â2Ï W to 2Ï W as the fundamental period. Thus,
n
) determine the Fourier coefï¬cients and, by extenthe sample values f ( 2W
sion, they determine the value of F (Ï) in the interval (â2Ï W, 2Ï W ).
Since a function is uniquely speciï¬ed by its Fourier transform, and since
F (Ï) is zero outside the band W , we can determine the function uniquely
from the samples.
Consider the function
sinc(t) =

sin(2Ï W t)
.
2Ï W t

(9.58)

This function is 1 at t = 0 and is 0 for t = n/2W , n = 0. The spectrum
of this function is constant in the band (âW, W ) and is zero outside this
band. Now deï¬ne
g(t) =

â

n=ââ

f


 n 
n 
sinc t â
.
2W
2W

(9.59)

From the properties of the sinc function, it follows that g(t) is bandlimited to W and is equal to f (n/2W ) at t = n/2W . Since there is only

272

GAUSSIAN CHANNEL

one function satisfying these constraints, we must have g(t) = f (t). This
provides an explicit representation of f (t) in terms of its samples.

A general function has an inï¬nite number of degrees of freedomâthe
value of the function at every point can be chosen independently. The
NyquistâShannon sampling theorem shows that a bandlimited function
has only 2W degrees of freedom per second. The values of the function
at the sample points can be chosen independently, and this speciï¬es the
entire function.
If a function is bandlimited, it cannot be limited in time. But we can
consider functions that have most of their energy in bandwidth W and
have most of their energy in a ï¬nite time interval, say (0, T ). We can
describe these functions using a basis of prolate spheroidal functions. We
do not go into the details of this theory here; it sufï¬ces to say that there
are about 2T W orthonormal basis functions for the set of almost timelimited, almost bandlimited functions, and we can describe any function
within the set by its coordinates in this basis. The details can be found
in a series of papers by Landau, Pollak, and Slepian [340, 341, 500].
Moreover, the projection of white noise on these basis vectors forms
an i.i.d. Gaussian process. The above arguments enable us to view the
bandlimited, time-limited functions as vectors in a vector space of 2T W
dimensions.
Now we return to the problem of communication over a bandlimited
channel. Assuming that the channel has bandwidth W , we can represent
both the input and the output by samples taken 1/2W seconds apart. Each
of the input samples is corrupted by noise to produce the corresponding
output sample. Since the noise is white and Gaussian, it can be shown
that each noise sample is an independent, identically distributed Gaussian
random variable.
If the noise has power spectral density N0 /2 watts/hertz and bandwidth
W hertz, the noise has power N20 2W = N0 W and each of the 2W T noise
samples in time T has variance N0 W T /2W T = N0 /2. Looking at the
input as a vector in the 2T W -dimensional space, we see that the received
signal is spherically normally distributed about this point with covariance
N0
2 I.
Now we can use the theory derived earlier for discrete-time Gaussian
channels, where it was shown that the capacity of such a channel is


P
1
bits per transmission.
(9.60)
C = log 1 +
2
N
Let the channel be used over the time interval [0, T ]. In this case, the
energy per sample is P T /2W T = P /2W , the noise variance per sample

9.3

is

BANDLIMITED CHANNELS

273

N0
T
2 2W 2W T

= N0 /2, and hence the capacity per sample is




P
1 + 2W
1
P
1
C = log
bits per sample.
= log 1 +
N0
2
2
N0 W
2
(9.61)

Since there are 2W samples each second, the capacity of the channel can
be rewritten as


P
bits per second.
(9.62)
C = W log 1 +
N0 W
This equation is one of the most famous formulas of information theory. It
gives the capacity of a bandlimited Gaussian channel with noise spectral
density N0 /2 watts/Hz and power P watts.
A more precise version of the capacity argument [576] involves consideration of signals with a small fraction of their energy outside the
bandwidth W of the channel and a small fraction of their energy outside
the time interval (0, T ). The capacity above is then obtained as a limit as
the fraction of energy outside the band goes to zero.
If we let W â â in (9.62), we obtain
C=

P
log2 e
N0

bits per second

(9.63)

as the capacity of a channel with an inï¬nite bandwidth, power P , and
noise spectral density N0 /2. Thus, for inï¬nite bandwidth channels, the
capacity grows linearly with the power.
Example 9.3.1 (Telephone line) To allow multiplexing of many channels, telephone signals are bandlimited to 3300 Hz. Using a bandwidth of
3300 Hz and a SNR (signal-to-noise ratio) of 33 dB (i.e., P /N0 W =
2000) in (9.62), we ï¬nd the capacity of the telephone channel to be
about 36,000 bits per second. Practical modems achieve transmission rates
up to 33,600 bits per second in both directions over a telephone channel.
In real telephone channels, there are other factors, such as crosstalk, interference, echoes, and nonï¬at channels which must be compensated for to
achieve this capacity.
The V.90 modems that achieve 56 kb/s over the telephone channel
achieve this rate in only one direction, taking advantage of a purely digital
channel from the server to ï¬nal telephone switch in the network. In this
case, the only impairments are due to the digital-to-analog conversion at
this switch and the noise in the copper link from the switch to the home;

274

GAUSSIAN CHANNEL

these impairments reduce the maximum bit rate from the 64 kb/s for the
digital signal in the network to the 56 kb/s in the best of telephone lines.
The actual bandwidth available on the copper wire that links a home to
a telephone switch is on the order of a few megahertz; it depends on the
length of the wire. The frequency response is far from ï¬at over this band.
If the entire bandwidth is used, it is possible to send a few megabits per
second through this channel; schemes such at DSL (Digital Subscriber
Line) achieve this using special equipment at both ends of the telephone
line (unlike modems, which do not require modiï¬cation at the telephone
switch).
9.4

PARALLEL GAUSSIAN CHANNELS

In this section we consider k independent Gaussian channels in parallel
with a common power constraint. The objective is to distribute the total
power among the channels so as to maximize the capacity. This channel
models a nonwhite additive Gaussian noise channel where each parallel
component represents a different frequency.
Assume that we have a set of Gaussian channels in parallel as illustrated
in Figure 9.3. The output of each channel is the sum of the input and
Gaussian noise. For channel j ,
Yj = Xj + Zj ,

j = 1, 2, . . . , k,

(9.64)

with
Zj â¼ N(0, Nj ),

(9.65)

and the noise is assumed to be independent from channel to channel. We
assume that there is a common power constraint on the total power used,
that is,
E

k


Xj2 â¤ P .

(9.66)

j =1

We wish to distribute the power among the various channels so as to
maximize the total capacity.
The information capacity of the channel C is
C=

max

f (x1 ,x2 ,...,xk ):

E Xi2 â¤P

I (X1 , X2 , . . . , Xk ; Y1 , Y2 , . . . , Yk ).

(9.67)

9.4

PARALLEL GAUSSIAN CHANNELS

275

Z1

X1

Y1

Zk

Xk

Yk

FIGURE 9.3. Parallel Gaussian channels.

We calculate the distribution that achieves the information capacity for
this channel. The fact that the information capacity is the supremum of
achievable rates can be proved by methods identical to those in the proof
of the capacity theorem for single Gaussian channels and will be omitted.
Since Z1 , Z2 , . . . , Zk are independent,
I (X1 , X2 , . . . , Xk ; Y1 , Y2 , . . . , Yk )
= h(Y1 , Y2 , . . . , Yk ) â h(Y1 , Y2 , . . . , Yk |X1 , X2 , . . . , Xk )
= h(Y1 , Y2 , . . . , Yk ) â h(Z1 , Z2 , . . . , Zk |X1 , X2 , . . . , Xk )
= h(Y1 , Y2 , . . . , Yk ) â h(Z1 , Z2 , . . . , Zk )

= h(Y1 , Y2 , . . . , Yk ) â
h(Zi )
â¤


i

â¤

(9.69)

i

h(Yi ) â h(Zi )

(9.70)



Pi
log 1 +
,
2
Ni

(9.71)

1
i

(9.68)

276

GAUSSIAN CHANNEL

where Pi = EXi2 , and



Pi = P . Equality is achieved by
ï£« ï£®

P1 0
ï£¬ ï£¯ 0 P2
ï£¯
(X1 , X2 , . . . , Xk ) â¼ N ï£¬
..
ï£­0, ï£° ...
.
0 0

Â·Â·Â·
Â·Â·Â·
..
.

ï£¹ï£¶
0
0 ï£ºï£·
ï£·.
.. ï£º
. ï£»ï£¸

(9.72)

Â· Â· Â· Pk

So the problem is reduced to ï¬nding the powerallotment that maximizes the capacity subject to the constraint that
Pi = P . This is a
standard optimization problem and can be solved using Lagrange multipliers. Writing the functional as


 
1
Pi
J (P1 , . . . , Pk ) =
log 1 +
(9.73)
+Î»
Pi
2
Ni
and differentiating with respect to Pi , we have
1
1
+Î»=0
2 Pi + Ni

(9.74)

Pi = Î½ â Ni .

(9.75)

or

However, since the Pi âs must be nonnegative, it may not always be possible to ï¬nd a solution of this form. In this case, we use the KuhnâTucker
conditions to verify that the solution
Pi = (Î½ â Ni )+

(9.76)

is the assignment that maximizes capacity, where Î½ is chosen so that

(9.77)
(Î½ â Ni )+ = P .
Here (x)+ denotes the positive part of x:
!
x if x â¥ 0,
+
(x) =
0 if x < 0.

(9.78)

This solution is illustrated graphically in Figure 9.4. The vertical levels
indicate the noise levels in the various channels. As the signal power is
increased from zero, we allot the power to the channels with the lowest

9.5 CHANNELS WITH COLORED GAUSSIAN NOISE

277

Power

n

P1
P2
N3
N1
N2
Channel 1

Channel 2

Channel 3

FIGURE 9.4. Water-ï¬lling for parallel channels.

noise. When the available power is increased still further, some of the
power is put into noisier channels. The process by which the power is
distributed among the various bins is identical to the way in which water
distributes itself in a vessel, hence this process is sometimes referred to
as water-ï¬lling.
9.5

CHANNELS WITH COLORED GAUSSIAN NOISE

In Section 9.4, we considered the case of a set of parallel independent
Gaussian channels in which the noise samples from different channels
were independent. Now we will consider the case when the noise is dependent. This represents not only the case of parallel channels, but also the
case when the channel has Gaussian noise with memory. For channels
with memory, we can consider a block of n consecutive uses of the channel as n channels in parallel with dependent noise. As in Section 9.4, we
will calculate only the information capacity for this channel.
Let KZ be the covariance matrix of the noise, and let KX be the input
covariance matrix. The power constraint on the input can then be written as
1
EXi2 â¤ P ,
(9.79)
n
i

or equivalently,
1
tr(KX ) â¤ P .
n

(9.80)

278

GAUSSIAN CHANNEL

Unlike Section 9.4, the power constraint here depends on n; the capacity
will have to be calculated for each n.
Just as in the case of independent channels, we can write
I (X1 , X2 , . . . , Xn ; Y1 , Y2 , . . . , Yn ) = h(Y1 , Y2 , . . . , Yn )
â h(Z1 , Z2 , . . . , Zn ). (9.81)
Here h(Z1 , Z2 , . . . , Zn ) is determined only by the distribution of the noise
and is not dependent on the choice of input distribution. So ï¬nding the
capacity amounts to maximizing h(Y1 , Y2 , . . . , Yn ). The entropy of the
output is maximized when Y is normal, which is achieved when the input
is normal. Since the input and the noise are independent, the covariance
of the output Y is KY = KX + KZ and the entropy is
h(Y1 , Y2 , . . . , Yn ) =



1
log (2Ï e)n |KX + KZ | .
2

(9.82)

Now the problem is reduced to choosing KX so as to maximize |KX +
KZ |, subject to a trace constraint on KX . To do this, we decompose KZ
into its diagonal form,
KZ = QQt ,

where QQt = I.

(9.83)

Then
|KX + KZ | = |KX + QQt |

(9.84)

= |Q||Qt KX Q + ||Qt |

(9.85)

= |Qt KX Q + |

(9.86)

= |A + |,

(9.87)

where A = Qt KX Q. Since for any matrices B and C,
tr(BC) = tr(CB),

(9.88)

tr(A) = tr(Qt KX Q)

(9.89)

= tr(QQt KX )

(9.90)

= tr(KX ).

(9.91)

we have

279

9.5 CHANNELS WITH COLORED GAUSSIAN NOISE

Now the problem is reduced to maximizing |A + | subject to a trace
constraint tr(A) â¤ nP .
Now we apply Hadamardâs inequality, mentioned in Chapter 8. Hadamardâs inequality states that the determinant of any positive deï¬nite matrix
K is less than the product of its diagonal elements, that is,
"
|K| â¤
Kii
(9.92)
i

with equality iff the matrix is diagonal. Thus,
"
|A + | â¤
(Aii + Î»i )

(9.93)

i

with equality iff A is diagonal. Since A is subject to a trace constraint,
1
Aii â¤ P ,
n

(9.94)

i

and Aii â¥ 0, the maximum value of

#

i (Aii

+ Î»i ) is attained when

Aii + Î»i = Î½.

(9.95)

However, given the constraints, it may not always be possible to satisfy
this equation with positive Aii . In such cases, we can show by the standard
KuhnâTucker conditions that the optimum solution corresponds to setting
Aii = (Î½ â Î»i )+ ,

(9.96)


where the water level Î½ is chosen so that
Aii = nP . This value of A
maximizes the entropy of Y and hence the mutual information. We can
use Figure 9.4 to see the connection between the methods described above
and water-ï¬lling.
Consider a channel in which the additive Gaussian noise is a stochastic process with ï¬nite-dimensional covariance matrix KZ(n) . If the process
is stationary, the covariance matrix is Toeplitz and the density of eigenvalues on the real line tends to the power spectrum of the stochastic
process [262]. In this case, the above water-ï¬lling argument translates to
water-ï¬lling in the spectral domain.
Hence, for channels in which the noise forms a stationary stochastic
process, the input signal should be chosen to be a Gaussian process with
a spectrum that is large at frequencies where the noise spectrum is small.

280

GAUSSIAN CHANNEL
F(w)

w

FIGURE 9.5. Water-ï¬lling in the spectral domain.

This is illustrated in Figure 9.5. The capacity of an additive Gaussian
noise channel with noise power spectrum N (f ) can be shown to be [233]


 Ï
1
(Î½ â N (f ))+
log 1 +
df,
(9.97)
C=
N (f )
âÏ 2
$
where Î½ is chosen so that (Î½ â N (f ))+ df = P .
9.6

GAUSSIAN CHANNELS WITH FEEDBACK

In Chapter 7 we proved that feedback does not increase the capacity for
discrete memoryless channels, although it can help greatly in reducing
the complexity of encoding or decoding. The same is true of an additive
noise channel with white noise. As in the discrete case, feedback does not
increase capacity for memoryless Gaussian channels.
However, for channels with memory, where the noise is correlated
from time instant to time instant, feedback does increase capacity. The
capacity without feedback can be calculated using water-ï¬lling, but we do
not have a simple explicit characterization of the capacity with feedback.
In this section we describe an expression for the capacity in terms of the
covariance matrix of the noise Z. We prove a converse for this expression
for capacity. We then derive a simple bound on the increase in capacity
due to feedback.
The Gaussian channel with feedback is illustrated in Figure 9.6. The
output of the channel Yi is
Yi = Xi + Zi ,

Zi â¼ N(0, KZ(n) ).

(9.98)

9.6 GAUSSIAN CHANNELS WITH FEEDBACK

281

Zi

Xi

Yi

FIGURE 9.6. Gaussian channel with feedback.

The feedback allows the input of the channel to depend on the past values
of the output.
A (2nR , n) code for the Gaussian channel with feedback consists of
a sequence of mappings xi (W, Y iâ1 ), where W â {1, 2, . . . , 2nR } is the
input message and Y iâ1 is the sequence of past values of the output. Thus,
x(W, Â·) is a code function rather than a codeword. In addition, we require
that the code satisfy a power constraint,
&
n
1 2
xi (w, Y iâ1 ) â¤ P ,
E
n
%

w â {1, 2, . . . , 2nR },

(9.99)

i=1

where the expectation is over all possible noise sequences.
We characterize the capacity of the Gaussian channel is terms of the
covariance matrices of the input X and the noise Z. Because of the feedback, X n and Z n are not independent; Xi depends causally on the past
values of Z. In the next section we prove a converse for the Gaussian
channel with feedback and show that we achieve capacity if we take X
to be Gaussian.
We now state an informal characterization of the capacity of the channel
with and without feedback.
1. With feedback . The capacity Cn,FB in bits per transmission of the
time-varying Gaussian channel with feedback is
(n)

Cn,FB =

max

(n)
1
n tr(KX )â¤P

|K
|
1
log X+Z
,
(n)
2n
|KZ |

(9.100)

282

GAUSSIAN CHANNEL

where the maximization is taken over all X n of the form
Xi =

iâ1


bij Zj + Vi ,

i = 1, 2, . . . , n,

(9.101)

j =1

and V n is independent of Z n . To verify that the maximization over
(9.101) involves no loss of generality, note that the distribution on
X n + Z n achieving the maximum entropy is Gaussian. Since Z n is
also Gaussian, it can be veriï¬ed that a jointly Gaussian distribution on (X n , Z n , X n + Z n ) achieves the maximization in (9.100).
But since Z n = Y n â X n , the most general jointly normal causal
dependence of X n on Y n is of the form (9.101), where V n plays the
role of the innovations process. Recasting (9.100) and (9.101) using
X = BZ + V and Y = X + Z, we can write
Cn,FB = max

|(B + I )KZ(n) (B + I )t + KV |
1
log
,
2n
|KZ(n) |

(9.102)

where the maximum is taken over all nonnegative deï¬nite KV and
strictly lower triangular B such that
tr(BKZ(n) B t + KV ) â¤ nP .

(9.103)

Note that B is 0 if feedback is not allowed.
2. Without feedback . The capacity Cn of the time-varying Gaussian
channel without feedback is given by
Cn =

max

(n)
1
n tr(KX )â¤P

|KX(n) + KZ(n) |
1
.
log
2n
|KZ(n) |

(9.104)

(n)
This reduces to water-ï¬lling on the eigenvalues {Î»(n)
i } of KZ . Thus,


n
+
(Î» â Î»(n)
)
1 
i
log 1 +
,
(9.105)
Cn =
2n
Î»(n)
i
i=1

where (y)+ = max{y, 0} and where Î» is chosen so that
n

+
(Î» â Î»(n)
i ) = nP .
i=1

(9.106)

283

9.6 GAUSSIAN CHANNELS WITH FEEDBACK

We now prove an upper bound for the capacity of the Gaussian channel
with feedback. This bound is actually achievable [136], and is therefore
the capacity, but we do not prove this here.
Theorem 9.6.1 For a Gaussian channel with feedback, the rate Rn for
any sequence of (2nRn , n) codes with Pe(n) â 0 satisï¬es
Rn â¤ Cn,F B + n ,

(9.107)

with n â 0 as n â â, where Cn,F B is deï¬ned in (9.100).
Proof: Let W be uniform over 2nR , and therefore the probability of error
Pe(n) is bounded by Fanoâs inequality,
H (W |WÌ ) â¤ 1 + nRn Pe(n) = nn ,

(9.108)

where n â 0 as Pe(n) â 0. We can then bound the rate as follows:
nRn = H (W )

(9.109)

= I (W ; WÌ ) + H (W |WÌ )

(9.110)

â¤ I (W ; WÌ ) + nn

(9.111)

â¤ I (W ; Y n ) + nn
(9.112)

=
I (W ; Yi |Y iâ1 ) + nn
(9.113)

(a)  
=
h(Yi |Y iâ1 ) â h(Yi |W, Y iâ1 , Xi , X iâ1 , Z iâ1 ) + nn
(9.114)

(b)  
=
h(Yi |Y iâ1 ) â h(Zi |W, Y iâ1 , Xi , X iâ1 , Z iâ1 ) + nn
(9.115)



(c)
=
h(Yi |Y iâ1 ) â h(Zi |Z iâ1 ) + nn
(9.116)
= h(Y n ) â h(Z n ) + nn ,

(9.117)

where (a) follows from the fact that Xi is a function of W and the past
Yi âs, and Z iâ1 is Y iâ1 â X iâ1 , (b) follows from Yi = Xi + Zi and the
fact that h(X + Z|X) = h(Z|X), and (c) follows from the fact Zi and
(W, Y iâ1 , X i ) are conditionally independent given Z iâ1 . Continuing the

284

GAUSSIAN CHANNEL

chain of inequalities after dividing by n, we have
Rn â¤
â¤


1
h(Y n ) â h(Z n ) + n
n

(9.118)

|K (n) |
1
log Y(n) + n
2n
|KZ |

(9.119)

â¤ Cn,F B + n ,

(9.120)


by the entropy maximizing property of the normal.

We have proved an upper bound on the capacity of the Gaussian chan(n)
nel with feedback in terms of the covariance matrix KX+Z
. We now derive
(n)
bounds on the capacity with feedback in terms of KX and KZ(n) , which
will then be used to derive bounds in terms of the capacity without feedback. For simplicity of notation, we will drop the superscript n in the
symbols for covariance matrices.
We ï¬rst prove a series of lemmas about matrices and determinants.
Lemma 9.6.1

Let X and Z be n-dimensional random vectors. Then
KX+Z + KXâZ = 2KX + 2KZ .

(9.121)

Proof
KX+Z = E(X + Z)(X + Z)t

(9.122)

= EXX + EXZ + EZX + EZZ
t

t

t

= KX + KXZ + KZX + KZ .

t

(9.123)
(9.124)

Similarly,
KXâZ = KX â KXZ â KZX + KZ .
Adding these two equations completes the proof.

(9.125)


Lemma 9.6.2 For two n Ã n nonnegative deï¬nite matrices A and B, if
A â B is nonnegative deï¬nite, then |A| â¥ |B|.
Proof: Let C = A â B. Since B and C are nonnegative deï¬nite, we
can consider them as covariance matrices. Consider two independent normal random vectors X1 â¼ N(0, B) and X2 â¼ N(0, C). Let Y = X1 + X2 .

9.6 GAUSSIAN CHANNELS WITH FEEDBACK

285

Then
h(Y) â¥ h(Y|X2 )

(9.126)

= h(X1 |X2 )

(9.127)

= h(X1 ),

(9.128)

where the inequality follows from the fact that conditioning reduces differential entropy, and the ï¬nal equality from the fact that X1 and X2 are
independent. Substituting the expressions for the differential entropies of
a normal random variable, we obtain
1
1
log(2Ï e)n |A| > log(2Ï e)n |B|,
2
2

(9.129)


which is equivalent to the desired lemma.
Lemma 9.6.3

For two n-dimensional random vectors X and Z,
|KX+Z | â¤ 2n |KX + KZ |.

(9.130)

Proof: From Lemma 9.6.1,
2(KX + KZ ) â KX+Z = KXâZ  0,

(9.131)

where A  0 means that A is nonnegative deï¬nite. Hence, applying
Lemma 9.6.2, we have
|KX+Z | â¤ |2(KX + KZ )| = 2n |KX + KZ |,

(9.132)


which is the desired result.
Lemma 9.6.4

For A,B nonnegative deï¬nite matrices and 0 â¤ Î» â¤ 1,
|Î»A + (1 â Î»)B| â¥ |A|Î» |B|1âÎ» .

(9.133)

Proof: Let X â¼ Nn (0, A) and Y â¼ Nn (0, B). Let Z be the mixture random vector
!
X if Î¸ = 1
Z=
(9.134)
Y if Î¸ = 2,

286

GAUSSIAN CHANNEL

where

!

with probability Î»
with probability 1 â Î».

1
2

Î¸=

(9.135)

Let X, Y, and Î¸ be independent. Then
KZ = Î»A + (1 â Î»)B.

(9.136)

We observe that
1
ln(2Ï e)n |Î»A + (1 â Î»)B| â¥ h(Z)
2
â¥ h(Z|Î¸ )
= Î»h(X) + (1 â Î»)h(Y)
=

1
ln(2Ï e)n |A|Î» |B|1âÎ» ,
2

(9.137)
(9.138)
(9.139)
(9.140)

which proves the result. The ï¬rst inequality follows from the entropy
maximizing property of the Gaussian under the covariance constraint. 
Deï¬nition We say that a random vector X n is causally related to Z n if
f (x , z ) = f (z )
n

n

n

n
"

f (xi |x iâ1 , ziâ1 ).

(9.141)

i=1

Note that the feedback codes necessarily yield causally related (X n , Z n ).
Lemma 9.6.5

If X n and Z n are causally related, then
h(X n â Z n ) â¥ h(Z n )

(9.142)

|KXâZ | â¥ |KZ |,

(9.143)

and

where KXâZ and KZ are the covariance matrices of X n â Z n and Z n ,
respectively.
Proof: We have
( a) 
h(X n â Z n ) =
h(Xi â Zi |X iâ1 â Z iâ1 )
n

i=1

(9.144)

9.6 GAUSSIAN CHANNELS WITH FEEDBACK

(b)

â¥

n


287

h(Xi â Zi |X iâ1 , Z iâ1 , Xi )

(9.145)

h(Zi |X iâ1 , Z iâ1 , Xi )

(9.146)

h(Zi |Z iâ1 )

(9.147)

i=1
( c)

=

n

i=1

(d)

=

n

i=1

( e)

= h(Z n ).

(9.148)

Here (a) follows from the chain rule, (b) follows from conditioning
h(A|B) â¥ h(A|B, C), (c) follows from the conditional determinism of
Xi and the invariance of differential entropy under translation, (d) follows from the causal relationship of X n and Z n , and (e) follows from the
chain rule.
Finally, suppose that X n and Z n are causally related and the associated covariance matrices for Z n and X n â Z n are KZ and KXâZ . There
obviously exists a multivariate normal (causally related) pair of random
vectors XÌ n , ZÌ n with the same covariance structure. Thus, from (9.148),
we have
1
ln(2Ï e)n |KXâZ | = h(XÌ n â ZÌ n )
2
â¥ h(ZÌ n )
=

1
ln(2Ï e)n |KZ |,
2

(9.149)
(9.150)
(9.151)


thus proving (9.143).

We are now in a position to prove that feedback increases the capacity
of a nonwhite Gaussian additive noise channel by at most half a bit.
Theorem 9.6.2
Cn,FB â¤ Cn +

1
2

bits per transmission.

(9.152)

Proof: Combining all the lemmas, we obtain
Cn,FB â¤

max

tr(KX )â¤nP

|K |
1
log Y
2n
|KZ |

(9.153)

288

GAUSSIAN CHANNEL

â¤
=

max

2n |KX + KZ |
1
log
2n
|KZ |

(9.154)

max

|K + KZ | 1
1
log X
+
2n
|KZ |
2

(9.155)

tr(KX )â¤nP
tr(KX )â¤nP

â¤ Cn +

1
2

bits per transmission,

(9.156)

where the inequalities follow from Theorem 9.6.1, Lemma 9.6.3, and the

deï¬nition of capacity without feedback, respectively.
We now prove Pinskerâs statement that feedback can at most double
the capacity of colored noise channels.
Theorem 9.6.3

Cn,FB â¤ 2Cn .

Proof: It is enough to show that
|KX+Z |
1
|KX + KZ |
1 1
log
â¤
log
,
2 2n
|KZ |
2n
|KZ |

(9.157)

for it will then follow that by maximizing the right side and then the left
side that
1
Cn,FB â¤ Cn .
2

(9.158)

We have
| 1 KX+Z + 12 KXâZ |
1
|KX + KZ | (a) 1
=
log
log 2
2n
|KZ |
2n
|KZ |
1

1

|KX+Z | 2 |KXâZ | 2
1
log
â¥
2n
|KZ |

(b)

1

(9.159)
(9.160)

1

(c)

|KX+Z | 2 |KZ | 2
1
log
2n
|KZ |

(9.161)

(d)

|KX+Z |
1 1
log
2 2n
|KZ |

(9.162)

â¥

=

and the result is proved. Here (a) follows from Lemma 9.6.1, (b) is the
inequality in Lemma 9.6.4, and (c) is Lemma 9.6.5 in which causality is
used.


SUMMARY

289

Thus, we have shown that Gaussian channel capacity is not increased
by more than half a bit or by more than a factor of 2 when we have
feedback; feedback helps, but not by much.
SUMMARY
Maximum entropy. maxEX2 =Î± h(X) =

1
2

log 2Ï eÎ±.

Gaussian
channel. Yi = Xi + Zi ; Zi â¼ N(0, N); power constraint
1 n
2
x
i=1 i â¤ P ; and
n


1
P
C = log 1 +
bits per transmission.
(9.163)
2
N
Bandlimited additive white Gaussian noise channel. Bandwidth W ;
two-sided power spectral density N0 /2; signal power P ; and


P
C = W log 1 +
bits per second.
(9.164)
N0 W
Water-ï¬lling (k parallel Gaussian channels). Yj = Xj + Zj , j = 1,

2, . . . , k; Zj â¼ N(0, Nj ); kj =1 Xj2 â¤ P ; and
C=



(Î½ â Ni )+
log 1 +
,
2
Ni

k

1
i=1

where Î½ is chosen so that

(9.165)


(Î½ â Ni )+ = nP .

Additive nonwhite Gaussian noise channel. Yi = Xi + Zi ; Z n â¼
N(0, KZ ); and


n
11
(Î½ â Î»i )+
C=
log 1 +
,
(9.166)
n
2
Î»i
i=1

where
Î»1 , Î»2 , . . . , Î»n are the eigenvalues of KZ and Î½ is chosen so that

(Î½
â
Î»i )+ = P .
i
Capacity without feedback
Cn =

max

tr(KX )â¤nP

|K + KZ |
1
log X
.
2n
|KZ |

(9.167)

290

GAUSSIAN CHANNEL

Capacity with feedback
Cn,FB =

max

tr(KX )â¤nP

|K
|
1
log X+Z .
2n
|KZ |

(9.168)

Feedback bounds
1
Cn,FB â¤ Cn + .
2

(9.169)

Cn,FB â¤ 2Cn .

(9.170)

PROBLEMS
9.1

Channel with two independent looks at Y . Let Y1 and Y2 be conditionally independent and conditionally identically distributed
given X.
(a) Show that I (X; Y1 , Y2 ) = 2I (X; Y1 ) â I (Y1 ; Y2 ).
(b) Conclude that the capacity of the channel
X

(Y1, Y2)

is less than twice the capacity of the channel
X

9.2

Y1

Two-look Gaussian channel
X

(Y1, Y2)

Consider the ordinary Gaussian channel with two correlated looks
at X, that is, Y = (Y1 , Y2 ), where
Y1 = X + Z1

(9.171)

Y2 = X + Z2

(9.172)

PROBLEMS

291

with a power constraint P on X, and (Z1 , Z2 ) â¼ N2 (0, K), where
(
'
N
NÏ
.
(9.173)
K=
NÏ N
Find the capacity C for
(a) Ï = 1
(b) Ï = 0
(c) Ï = â1
9.3

Output power constraint. Consider an additive white Gaussian
noise channel with an expected output power constraint P . Thus,
Y = X + Z, Z â¼ N (0, Ï 2 ), Z is independent of X, and EY 2 â¤ P .
Find the channel capacity.

9.4

Exponential noise channels. Yi = Xi + Zi , where Zi is i.i.d. exponentially distributed noise with mean Âµ. Assume that we have
a mean constraint on the signal (i.e., EXi â¤ Î»). Show that the
capacity of such a channel is C = log(1 + ÂµÎ» ).

9.5

Fading channel .

Consider an additive noise fading channel
V

Z

X

Y

Y = XV + Z,
where Z is additive noise, V is a random variable representing
fading, and Z and V are independent of each other and of X.
Argue that knowledge of the fading factor V improves capacity by
showing that
I (X; Y |V ) â¥ I (X; Y ).
9.6

Parallel channels and water-ï¬lling. Consider a pair of parallel
Gaussian channels:
 
 


X1
Z1
Y1
=
+
,
(9.174)
Y2
X2
Z2

292

GAUSSIAN CHANNEL

where



Z1
Z2



 ' 2
(
Ï1 0
â¼ N 0,
,
0 Ï22

(9.175)

and there is a power constraint E(X12 + X22 ) â¤ 2P . Assume that
Ï12 > Ï22 . At what power does the channel stop behaving like a
single channel with noise variance Ï22 , and begin behaving like a
pair of channels?
9.7

Multipath Gaussian channel . Consider a Gaussian noise channel
with power constraint P , where the signal takes two different paths
and the received noisy signals are added together at the antenna.
Z1
Y1
â

X

Y

Y2
Z2

(a) Find the capacity of this channel if Z1 and Z2 are jointly normal
with covariance matrix
' 2
(
ÏÏ 2
Ï
KZ =
.
ÏÏ 2 Ï 2
(b) What is the capacity for Ï = 0, Ï = 1, Ï = â1?
9.8

Parallel Gaussian channels.
Gaussian channel:
Z1 ~

Consider the following parallel
(0, N1)

X1

Y1
Z2 ~

X2

(0, N2)

Y2

PROBLEMS

293

where Z1 â¼ N(0,N1 ) and Z2 â¼ N(0,N2 ) are independent Gaussian
random variables and Yi = Xi + Zi . We wish to allocate power
to the two parallel channels. Let Î²1 and Î²2 be ï¬xed. Consider
a total cost constraint Î²1 P1 + Î²2 P2 â¤ Î², where Pi is the power
allocated to the ith channel and Î²i is the cost per unit power in
that channel. Thus, P1 â¥ 0 and P2 â¥ 0 can be chosen subject to
the cost constraint Î².
(a) For what value of Î² does the channel stop acting like a single
channel and start acting like a pair of channels?
(b) Evaluate the capacity and ï¬nd P1 and P2 that achieve capacity
for Î²1 = 1, Î²2 = 2, N1 = 3, N2 = 2, and Î² = 10.
9.9

Vector Gaussian channel .
channel

Consider the vector Gaussian noise
Y = X + Z,

where X = (X1 , X2 , X3 ), Z = (Z1 , Z2 , Z3 ), Y = (Y1 , Y2 , Y3 ),
EX2 â¤ P , and
ï£« ï£®
ï£¹ï£¶
1 0 1
Z â¼ N ï£­0, ï£° 0 1 1 ï£»ï£¸ .
1 1 2
Find the capacity. The answer may be surprising.
9.10

Capacity of photographic ï¬lm. Here is a problem with a nice
answer that takes a little time. Weâre interested in the capacity
of photographic ï¬lm. The ï¬lm consists of silver iodide crystals,
Poisson distributed, with a density of Î» particles per square inch.
The ï¬lm is illuminated without knowledge of the position of the
silver iodide particles. It is then developed and the receiver sees
only the silver iodide particles that have been illuminated. It is
assumed that light incident on a cell exposes the grain if it is there
and otherwise results in a blank response. Silver iodide particles
that are not illuminated and vacant portions of the ï¬lm remain
blank. The question is: What is the capacity of this ï¬lm?
We make the following assumptions. We grid the ï¬lm very ï¬nely
into cells of area dA. It is assumed that there is at most one silver iodide particle per cell and that no silver iodide particle is
intersected by the cell boundaries. Thus, the ï¬lm can be considered to be a large number of parallel binary asymmetric channels
with crossover probability 1 â Î»dA. By calculating the capacity of
this binary asymmetric channel to ï¬rst order in dA (making the

294

GAUSSIAN CHANNEL

necessary approximations), one can calculate the capacity of the
ï¬lm in bits per square inch. It is, of course, proportional to Î». The
question is: What is the multiplicative constant?
The answer would be Î» bits per unit area if both illuminator and
receiver knew the positions of the crystals.
9.11

Gaussian mutual information. Suppose that (X, Y, Z) are jointly
Gaussian and that X â Y â Z forms a Markov chain. Let X and
Y have correlation coefï¬cient Ï1 and let Y and Z have correlation
coefï¬cient Ï2 . Find I (X; Z).

9.12

Time-varying channel . A train pulls out of the station at constant
velocity. The received signal energy thus falls off with time as
1/ i 2 . The total received signal at time i is
1
Yi = Xi + Zi ,
i
where Z1 , Z2 , . . . are i.i.d. â¼ N (0, N). The transmitter constraint
for block length n is
1 2
xi (w) â¤ P ,
n
n

w â {1, 2, . . . , 2nR }.

i=1

9.13

9.14

Using Fanoâs inequality, show that the capacity C is equal to zero
for this channel.
(
'
1 Ï
Feedback capacity. Let (Z1 , Z2 ) â¼ N (0, K), K =
.
Ï 1
|K
|
Find the maximum of 12 log |KX+Z
with and without feedback given
Z|
a trace (power) constraint tr(KX ) â¤ 2P .
Additive noise channel. Consider the channel Y = X + Z, where
X is the transmitted signal with power constraint P , Z is independent additive noise, and Y is the received signal. Let
)
Z=

0

with probability

Zâ

with probability

1
10
9
10 ,

where Z â â¼ N (0, N). Thus, Z has a mixture distribution that is
the mixture of a Gaussian distribution and a degenerate distribution
with mass 1 at 0.

PROBLEMS

295

(a) What is the capacity of this channel? This should be a pleasant
surprise.
(b) How would you signal to achieve capacity?
9.15

Discrete input, continuous output channel . Let Pr{X = 1} = p,
Pr{X = 0} = 1 â p, and let Y = X + Z, where Z is uniform over
the interval [0, a], a > 1, and Z is independent of X.
(a) Calculate
I (X; Y ) = H (X) â H (X|Y ).
(b) Now calculate I (X; Y ) the other way by
I (X; Y ) = h(Y ) â h(Y |X).
(c) Calculate the capacity of this channel by maximizing over p.

9.16

Gaussian mutual information. Suppose that (X, Y, Z) are jointly
Gaussian and that X â Y â Z forms a Markov chain. Let X and
Y have correlation coefï¬cient Ï1 and let Y and Z have correlation
coefï¬cient Ï2 . Find I (X; Z).

9.17

Impulse power.

Consider the additive white Gaussian channel
Zi

Xi

â

Yi

where Zi â¼ N (0, N), and the input signal has average power constraint P .
(a) Suppose that we use all our power at time 1 (i.e., EX12 = nP
and EXi2 = 0 for i = 2, 3, . . . , n). Find
I (X n ; Y n )
,
)
n

max
n

f (x

where the maximization is over all distributions f (x n ) subject
to the constraint EX12 = nP and EXi2 = 0 for i = 2, 3, . . . , n.

296

GAUSSIAN CHANNEL

(b) Find
max


n
2
i=1 Xi

f (x n ): E n1



â¤P

1
I (X n ; Y n )
n

and compare to part (a).
9.18

Gaussian channel with time-varying mean.
the following Gaussian channel:

Find the capacity of

Zi

Xi

Yi

Let Z1 , Z2 , . . . be independent and let there be a power constraint
P on x n (W ). Find the capacity when:
(a) Âµi = 0, for all i.
(b) Âµi = ei , i = 1, 2, . . .. Assume that Âµi is known to the transmitter and receiver.
(c) Âµi unknown, but Âµi i.i.d. â¼ N (0, N1 ) for all i.
9.19

Parametric form for channel capacity. Consider m parallel Gausand the noises
sian channels, Yi = Xi + Zi , where Zi â¼ N (0, Î»i )
1
Xi are independent random variables. Thus, C = m
i=1 2 log(1 +

+
(Î»âÎ»i )
+
), where Î» is chosen to satisfy m
i=1 (Î» â Î»i ) = P . Show
Î»i
that this can be rewritten in the form

P (Î») = i:Î»i â¤Î» (Î» â Î»i )

1
Î»
C(Î») = i:Î»i â¤Î» log .
2
Î»i
Here P (Î») is piecewise linear and C(Î») is piecewise logarithmic
in Î».

9.20

Robust decoding. Consider an additive noise channel whose output Y is given by
Y = X + Z,
where the channel input X is average power limited,
EX 2 â¤ P ,

PROBLEMS

297

and the noise process {Zk }â
k=ââ is i.i.d. with marginal distribution
pZ (z) (not necessarily Gaussian) of power N ,
EZ 2 = N.
(a) Show that the channel capacity, C = maxEX2 â¤P I (X; Y ), is
lower bounded by CG , where


P
1
CG = log 1 +
2
N
(i.e., the capacity CG corresponding to white Gaussian noise).
(b) Decoding the received vector to the codeword that is closest to
it in Euclidean distance is in general suboptimal if the noise is
non-Gaussian. Show, however, that the rate CG is achievable
even if one insists on performing nearest-neighbor decoding
(minimum Euclidean distance decoding) rather than the optimal
maximum-likelihood or joint typicality decoding (with respect
to the true noise distribution).
(c) Extend the result to the case where the noise is not i.i.d. but is
stationary and ergodic with power N .
(Hint for b and c: Consider a size 2nR random codebook whose
codewords are drawn independently of each other according
â to a
uniform distribution over the n-dimensional sphere of radius nP .)
(a) Using a symmetry argument, show that conditioned on the
noise vector, the ensemble average probability of error depends
on the noise vector only via its Euclidean norm z.
(b) Use a geometric argument to show that this dependence is
monotonic.
(c) Given a rate R < CG , choose some N 
 > N such that


1
P
R < log 1 + 
 .
2
N
Compare the case where the noise is i.i.d. N(0, N 
 ) to the case
at hand.
(d) Conclude the proof using the fact that the above ensemble of
codebooks can achieve the capacity of the Gaussian channel
(no need to prove that).

298

GAUSSIAN CHANNEL

9.21

Mutual information game.

Consider the following channel:
Z

X

Y

Throughout this problem we shall constrain the signal power
EX = 0,

EX 2 = P ,

(9.176)

EZ = 0,

EZ 2 = N,

(9.177)

and the noise power

and assume that X and Z are independent. The channel capacity is
given by I (X; X + Z).
Now for the game. The noise player chooses a distribution on Z to
minimize I (X; X + Z), while the signal player chooses a distribution on X to maximize I (X; X + Z). Letting X â â¼ N(0, P ), Z â â¼
N(0, N), show that Gaussian X â and Z â satisfy the saddlepoint
conditions
I (X; X + Z â ) â¤ I (X â ; X â + Z â ) â¤ I (X â ; X â + Z).

(9.178)

Thus,
min max I (X; X + Z) = max min I (X; X + Z)
Z
Z
X
X


1
P
,
= log 1 +
2
N

(9.179)
(9.180)

and the game has a value. In particular, a deviation from normal
for either player worsens the mutual information from that playerâs
standpoint. Can you discuss the implications of this?
Note: Part of the proof hinges on the entropy power inequality from
Section 17.8, which states that if X and Y are independent random
n-vectors with densities, then
2

2

2

2 n h(X+Y) â¥ 2 n h(X) + 2 n h(Y) .

(9.181)

HISTORICAL NOTES

9.22

299

Recovering the noise. Consider a standard Gaussian channel Y n =
n
X
+ Z n , where Zi is i.i.d. â¼ N(0, N), i = 1, 2, . . . , n, and
n
1
2
n
i=1 Xi â¤ P . Here we are interested in recovering the noise Z
n
n
n
and we donât care about the signal X . By sending X = (0, 0, . . . ,
0), the receiver gets Y n = Z n and can fully determine the value of
Z n . We wonder how much variability there can be in X n and still
recover the Gaussian noise Z n . Use of the channel looks like
Zn

Xn

Yn

^

Z n(Y n )

Argue that for some R > 0, the transmitter can arbitrarily send one
of 2nR different sequences of x n without affecting the recovery of
the noise in the sense that
Pr{ZÌ n = Z n } â 0

as n â â.

For what R is this possible?
HISTORICAL NOTES
The Gaussian channel was ï¬rst analyzed by Shannon in his original
paper [472]. The water-ï¬lling solution to the capacity of the colored
noise Gaussian channel was developed by Shannon [480] and treated in
detail by Pinsker [425]. The time-continuous Gaussian channel is treated
in Wyner [576], Gallager [233], and Landau, Pollak, and Slepian [340,
341, 500].
Pinsker [421] and Ebert [178] argued that feedback at most doubles
the capacity of a nonwhite Gaussian channel; the proof in the text is
from Cover and Pombra [136], who also show that feedback increases
the capacity of the nonwhite Gaussian channel by at most half a bit.
The most recent feedback capacity results for nonwhite Gaussian noise
channels are due to Kim [314].

CHAPTER 10

RATE DISTORTION THEORY

The description of an arbitrary real number requires an inï¬nite number
of bits, so a ï¬nite representation of a continuous random variable can
never be perfect. How well can we do? To frame the question appropriately, it is necessary to deï¬ne the âgoodnessâ of a representation of a
source. This is accomplished by deï¬ning a distortion measure which is a
measure of distance between the random variable and its representation.
The basic problem in rate distortion theory can then be stated as follows:
Given a source distribution and a distortion measure, what is the minimum
expected distortion achievable at a particular rate? Or, equivalently, what
is the minimum rate description required to achieve a particular distortion?
One of the most intriguing aspects of this theory is that joint descriptions
are more efï¬cient than individual descriptions. It is simpler to describe an
elephant and a chicken with one description than to describe each alone. This
is true even for independent random variables. It is simpler to describe X1
and X2 together (at a given distortion for each) than to describe each by itself.
Why donât independent problems have independent solutions? The answer
is found in the geometry. Apparently, rectangular grid points (arising from
independent descriptions) do not ï¬ll up the space efï¬ciently.
Rate distortion theory can be applied to both discrete and continuous
random variables. The zero-error data compression theory of Chapter 5
is an important special case of rate distortion theory applied to a discrete
source with zero distortion. We begin by considering the simple problem
of representing a single continuous random variable by a ï¬nite number
of bits.
10.1

QUANTIZATION

In this section we motivate the elegant theory of rate distortion by showing
how complicated it is to solve the quantization problem exactly for a single
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

301

302

RATE DISTORTION THEORY

random variable. Since a continuous random source requires inï¬nite precision to represent exactly, we cannot reproduce it exactly using a ï¬nite-rate
code. The question is then to ï¬nd the best possible representation for any
given data rate.
We ï¬rst consider the problem of representing a single sample from the
source. Let the random variable be represented be X and let the representation of X be denoted as XÌ(X). If we are given R bits to represent X,
the function XÌ can take on 2R values. The problem is to ï¬nd the optimum
set of values for XÌ (called the reproduction points or code points) and
the regions that are associated with each value XÌ.
For example, let X â¼ N(0, Ï 2 ), and assume a squared-error distortion
measure. In this case we wish to ï¬nd the function XÌ(X) such that XÌ takes
on at most 2R values and minimizes E(X â XÌ(X))2 . If we are given one
bit to represent X, it is clear that the bit should distinguish whether or
not X > 0. To minimize squared error, each reproduced symbol should
be the conditional mean of its region. This is illustrated in Figure 10.1.
Thus,
ï£± 
2
ï£´
ï£´
ï£²
Ï
if x â¥ 0,
Ï

(10.1)
XÌ(x) =
ï£´
2
ï£´
ï£³ â
Ï if x < 0.
Ï
If we are given 2 bits to represent the sample, the situation is not as
simple. Clearly, we want to divide the real line into four regions and use
0.4
0.35
0.3

f(x)

0.25
0.2
0.15
0.1
0.05
0
â2.5

â2 â1.5

â1

â0.5

â.79

0
x

0.5

1

1.5

2

2.5

+.79

FIGURE 10.1. One-bit quantization of Gaussian random variable.

10.2

DEFINITIONS

303

a point within each region to represent the sample. But it is no longer
immediately obvious what the representation regions and the reconstruction points should be. We can, however, state two simple properties of
optimal regions and reconstruction points for the quantization of a single
random variable:
â¢

â¢

Given a set {XÌ(w)} of reconstruction points, the distortion is minimized by mapping a source random variable X to the representation
XÌ(w) that is closest to it. The set of regions of X deï¬ned by this
mapping is called a Voronoi or Dirichlet partition deï¬ned by the
reconstruction points.
The reconstruction points should minimize the conditional expected
distortion over their respective assignment regions.

These two properties enable us to construct a simple algorithm to ï¬nd a
âgoodâ quantizer: We start with a set of reconstruction points, ï¬nd the optimal set of reconstruction regions (which are the nearest-neighbor regions
with respect to the distortion measure), then ï¬nd the optimal reconstruction points for these regions (the centroids of these regions if the distortion
is squared error), and then repeat the iteration for this new set of reconstruction points. The expected distortion is decreased at each stage in the
algorithm, so the algorithm will converge to a local minimum of the distortion. This algorithm is called the Lloyd algorithm [363] (for real-valued
random variables) or the generalized Lloyd algorithm [358] (for vectorvalued random variables) and is frequently used to design quantization
systems.
Instead of quantizing a single random variable, let us assume that we
are given a set of n i.i.d. random variables drawn according to a Gaussian
distribution. These random variables are to be represented using nR bits.
Since the source is i.i.d., the symbols are independent, and it may appear
that the representation of each element is an independent problem to be
treated separately. But this is not true, as the results on rate distortion
theory will show. We will represent the entire sequence by a single index
taking 2nR values. This treatment of entire sequences at once achieves a
lower distortion for the same rate than independent quantization of the
individual samples.
10.2

DEFINITIONS

Assume that we have a source that produces a sequence X1 , X2 , . . . , Xn
i.i.d. â¼ p(x), x â X. For the proofs in this chapter, we assume that the

RATE DISTORTION THEORY

fn(X n )
Xn

â

304

{1,2,...,2nR}

Encoder

Decoder

^

Xn

FIGURE 10.2. Rate distortion encoder and decoder.

alphabet is ï¬nite, but most of the proofs can be extended to continuous
random variables. The encoder describes the source sequence X n by an
index fn (X n ) â {1, 2, . . . , 2nR }. The decoder represents X n by an estimate
XÌ n â XÌ, as illustrated in Figure 10.2.
Deï¬nition A distortion function or distortion measure is a mapping
d : X Ã XÌ â R+

(10.2)

from the set of source alphabet-reproduction alphabet pairs into the set of
nonnegative real numbers. The distortion d(x, xÌ) is a measure of the cost
of representing the symbol x by the symbol xÌ.
Deï¬nition A distortion measure is said to be bounded if the maximum
value of the distortion is ï¬nite:
def

dmax =

max d(x, xÌ) < â.

xâX ,xÌâXÌ

(10.3)

In most cases, the reproduction alphabet XÌ is the same as the source
alphabet X.
Examples of common distortion functions are
â¢

Hamming (probability of error) distortion. The Hamming distortion
is given by

d(x, xÌ) =

0 if x = xÌ
1 if x = xÌ,

(10.4)

which results in a probability of error distortion, since Ed(X, XÌ) =
Pr(X = XÌ).

10.2
â¢

DEFINITIONS

305

Squared-error distortion. The squared-error distortion,
d(x, xÌ) = (x â xÌ)2 ,

(10.5)

is the most popular distortion measure used for continuous alphabets.
Its advantages are its simplicity and its relationship to least-squares
prediction. But in applications such as image and speech coding,
various authors have pointed out that the mean-squared error is not an
appropriate measure of distortion for human observers. For example,
there is a large squared-error distortion between a speech waveform
and another version of the same waveform slightly shifted in time,
even though both would sound the same to a human observer.
Many alternatives have been proposed; a popular measure of distortion
in speech coding is the ItakuraâSaito distance, which is the relative entropy
between multivariate normal processes. In image coding, however, there is
at present no real alternative to using the mean-squared error as the distortion
measure.
The distortion measure is deï¬ned on a symbol-by-symbol basis. We
extend the deï¬nition to sequences by using the following deï¬nition:
Deï¬nition The distortion between sequences x n and xÌ n is deï¬ned by
1
d(xi , xÌi ).
d(x , xÌ ) =
n
n

n

n

(10.6)

i=1

So the distortion for a sequence is the average of the per symbol distortion of the elements of the sequence. This is not the only reasonable
deï¬nition. For example, one may want to measure the distortion between
two sequences by the maximum of the per symbol distortions. The theory derived below does not apply directly to this more general distortion
measure.
Deï¬nition A (2nR , n)-rate distortion code consists of an encoding function,
fn : X n â {1, 2, . . . , 2nR },

(10.7)

and a decoding (reproduction) function,
gn : {1, 2, . . . , 2nR } â XË n .

(10.8)

306

RATE DISTORTION THEORY

The distortion associated with the (2nR , n) code is deï¬ned as
D = Ed(X n , gn (fn (X n ))),

(10.9)

where the expectation is with respect to the probability distribution on X:
D=



p(x n )d(x n , gn (fn (x n ))).

(10.10)

xn

The set of n-tuples gn (1), gn (2), . . . , gn (2nR ), denoted by XÌ n (1), . . . ,
XÌ n (2nR ), constitutes the codebook, and fnâ1 (1), . . . , fnâ1 (2nR ) are the
associated assignment regions.
Many terms are used to describe the replacement of X n by its quantized
version XÌ n (w). It is common to refer to XÌ n as the vector quantization, reproduction, reconstruction, representation, source code, or estimate
of X n .
Deï¬nition A rate distortion pair (R, D) is said to be achievable if
there exists a sequence of (2nR , n)-rate distortion codes (fn , gn ) with
limnââ Ed(X n , gn (fn (X n ))) â¤ D.
Deï¬nition The rate distortion region for a source is the closure of the
set of achievable rate distortion pairs (R, D).
Deï¬nition The rate distortion function R(D) is the inï¬mum of rates R
such that (R, D) is in the rate distortion region of the source for a given
distortion D.
Deï¬nition The distortion rate function D(R) is the inï¬mum of all distortions D such that (R, D) is in the rate distortion region of the source
for a given rate R.
The distortion rate function deï¬nes another way of looking at the
boundary of the rate distortion region. We will in general use the rate
distortion function rather than the distortion rate function to describe this
boundary, although the two approaches are equivalent.
We now deï¬ne a mathematical function of the source, which we call
the information rate distortion function. The main result of this chapter
is the proof that the information rate distortion function is equal to the
rate distortion function deï¬ned above (i.e., it is the inï¬mum of rates that
achieve a particular distortion).

10.3

307

CALCULATION OF THE RATE DISTORTION FUNCTION

Deï¬nition The information rate distortion function R (I ) (D) for a source
X with distortion measure d(x, xÌ) is deï¬ned as
R (I ) (D) =

p(xÌ|x):



min

(x,xÌ) p(x)p(xÌ|x)d(x,xÌ)â¤D

I (X; XÌ),

(10.11)

where the minimization is over all conditional distributions p(xÌ|x) for
which the joint distribution p(x, xÌ) = p(x)p(xÌ|x) satisï¬es the expected
distortion constraint.
Paralleling the discussion of channel capacity in Chapter 7, we initially
consider the properties of the information rate distortion function and
calculate it for some simple sources and distortion measures. Later we
prove that we can actually achieve this function (i.e., there exist codes with
rate R (I ) (D) with distortion D). We also prove a converse establishing
that R â¥ R (I ) (D) for any code that achieves distortion D.
The main theorem of rate distortion theory can now be stated as follows:
Theorem 10.2.1
The rate distortion function for an i.i.d. source X
with distribution p(x) and bounded distortion function d(x, xÌ) is equal to
the associated information rate distortion function. Thus,
R(D) = R (I ) (D) =

p(xÌ|x):



min

(x,xÌ) p(x)p(xÌ|x)d(x,xÌ)â¤D

I (X; XÌ)

(10.12)

is the minimum achievable rate at distortion D.
This theorem shows that the operational deï¬nition of the rate distortion
function is equal to the information deï¬nition. Hence we will use R(D)
from now on to denote both deï¬nitions of the rate distortion function.
Before coming to the proof of the theorem, we calculate the information
rate distortion function for some simple sources and distortions.
10.3
10.3.1

CALCULATION OF THE RATE DISTORTION FUNCTION
Binary Source

We now ï¬nd the description rate R(D) required to describe a Bernoulli(p)
source with an expected proportion of errors less than or equal to D.
Theorem 10.3.1 The rate distortion function for a Bernoulli(p) source
with Hamming distortion is given by

H (p) â H (D), 0 â¤ D â¤ min{p, 1 â p},
R(D) =
(10.13)
0,
D > min{p, 1 â p}.

308

RATE DISTORTION THEORY

Proof: Consider a binary source X â¼ Bernoulli(p) with a Hamming
distortion measure. Without loss of generality, we may assume that p < 12 .
We wish to calculate the rate distortion function,
R(D) =

p(xÌ|x):



min

(x,xÌ) p(x)p(xÌ|x)d(x,xÌ)â¤D

I (X; XÌ).

(10.14)

Let â denote modulo 2 addition. Thus, X â XÌ = 1 is equivalent to X =
XÌ. We do not minimize I (X; XÌ) directly; instead, we ï¬nd a lower bound
and then show that this lower bound is achievable. For any joint distribution satisfying the distortion constraint, we have
I (X; XÌ) = H (X) â H (X|XÌ)

(10.15)

= H (p) â H (X â XÌ|XÌ)

(10.16)

â¥ H (p) â H (X â XÌ)

(10.17)

â¥ H (p) â H (D),

(10.18)

since Pr(X = XÌ) â¤ D and H (D) increases with D for D â¤ 12 . Thus,
R(D) â¥ H (p) â H (D).

(10.19)

We now show that the lower bound is actually the rate distortion function
by ï¬nding a joint distribution that meets the distortion constraint and
has I (X; XÌ) = R(D). For 0 â¤ D â¤ p, we can achieve the value of the
rate distortion function in (10.19) by choosing (X, XÌ) to have the joint
distribution given by the binary symmetric channel shown in Figure 10.3.

1âpâD
1 â 2D

1âD

0

^

X

D

0

1âp

X

D

pâD
1 â 2D

1

1âD

1

FIGURE 10.3. Joint distribution for binary source.

p

10.3

CALCULATION OF THE RATE DISTORTION FUNCTION

309

We choose the distribution of XÌ at the input of the channel so that the
output distribution of X is the speciï¬ed distribution. Let r = Pr(XÌ = 1).
Then choose r so that
r(1 â D) + (1 â r)D = p,

(10.20)

or
r=

pâD
.
1 â 2D

(10.21)

If D â¤ p â¤ 12 , then Pr(XÌ = 1) â¥ 0 and Pr(XÌ = 0) â¥ 0. We then have
I (X; XÌ) = H (X) â H (X|XÌ) = H (p) â H (D),

(10.22)

and the expected distortion is Pr(X = XÌ) = D.
If D â¥ p, we can achieve R(D) = 0 by letting XÌ = 0 with probability
1. In this case, I (X; XÌ) = 0 and D = p. Similarly, if D â¥ 1 â p, we can
achieve R(D) = 0 by setting XÌ = 1 with probability 1. Hence, the rate
distortion function for a binary source is

H (p) â H (D), 0 â¤ D â¤ min{p, 1 â p},
(10.23)
R(D) =
0,
D > min{p, 1 â p}.


This function is illustrated in Figure 10.4.
1
0.9
0.8
0.7

R(D)

0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5
D

0.6

0.7

0.8

0.9

1

FIGURE 10.4. Rate distortion function for a Bernoulli ( 12 ) source.

310

RATE DISTORTION THEORY

The above calculations may seem entirely unmotivated. Why should
minimizing mutual information have anything to do with quantization?
The answer to this question must wait until we prove Theorem 10.2.1.
10.3.2

Gaussian Source

Although Theorem 10.2.1 is proved only for discrete sources with a
bounded distortion measure, it can also be proved for well-behaved continuous sources and unbounded distortion measures. Assuming this general
theorem, we calculate the rate distortion function for a Gaussian source
with squared-error distortion.
Theorem 10.3.2 The rate distortion function for a N(0, Ï 2 ) source with
squared-error distortion is
ï£±
Ï2
ï£² 1
log
, 0 â¤ D â¤ Ï 2,
R(D) =
(10.24)
2
D
ï£³
0,
D > Ï 2.
Proof: Let X be â¼ N(0, Ï 2 ). By the rate distortion theorem extended
to continuous alphabets, we have
R(D) =

min

f (xÌ|x):E(XÌâX)2 â¤D

I (X; XÌ).

(10.25)

As in the preceding example, we ï¬rst ï¬nd a lower bound for the rate
distortion function and then prove that this is achievable. Since E(X â
XÌ)2 â¤ D, we observe that
I (X; XÌ) = h(X) â h(X|XÌ)
=
â¥
â¥
=
â¥
=

1
log(2Ï e)Ï 2 â h(X â XÌ|XÌ)
2
1
log(2Ï e)Ï 2 â h(X â XÌ)
2
1
log(2Ï e)Ï 2 â h(N(0, E(X â XÌ)2 ))
2
1
1
log(2Ï e)Ï 2 â log(2Ï e)E(X â XÌ)2
2
2
1
1
log(2Ï e)Ï 2 â log(2Ï e)D
2
2
2
Ï
1
log ,
2
D

(10.26)
(10.27)
(10.28)
(10.29)
(10.30)
(10.31)
(10.32)

10.3

CALCULATION OF THE RATE DISTORTION FUNCTION

311

where (10.28) follows from the fact that conditioning reduces entropy and
(10.29) follows from the fact that the normal distribution maximizes the
entropy for a given second moment (Theorem 8.6.5). Hence,
R(D) â¥

Ï2
1
log .
2
D

(10.33)

To ï¬nd the conditional density f (xÌ|x) that achieves this lower bound,
it is usually more convenient to look at the conditional density f (x|xÌ),
which is sometimes called the test channel (thus emphasizing the duality of
rate distortion with channel capacity). As in the binary case, we construct
f (x|xÌ) to achieve equality in the bound. We choose the joint distribution
as shown in Figure 10.5. If D â¤ Ï 2 , we choose
X = XÌ + Z,

XÌ â¼ N(0, Ï 2 â D),

Z â¼ N(0, D),

(10.34)

where XÌ and Z are independent. For this joint distribution, we calculate
I (X; XÌ) =

Ï2
1
log ,
2
D

(10.35)

and E(X â XÌ)2 = D, thus achieving the bound in (10.33). If D > Ï 2 , we
choose XÌ = 0 with probability 1, achieving R(D) = 0. Hence, the rate
distortion function for the Gaussian source with squared-error distortion is
ï£±
Ï2
ï£² 1
log , 0 â¤ D â¤ Ï 2 ,
R(D) =
(10.36)
D
ï£³ 2
2
0,
D>Ï ,


as illustrated in Figure 10.6.

We can rewrite (10.36) to express the distortion in terms of the rate,
D(R) = Ï 2 2â2R .

Z~

^

X~

(0, s 2 â D)

(10.37)

(0,D)

X~

(0,s 2)

FIGURE 10.5. Joint distribution for Gaussian source.

312

RATE DISTORTION THEORY

5
4.5
4
3.5

R(D)

3
2.5
2
1.5
1
0.5
0

0

0.2

0.4

0.6

0.8

1
D

1.2

1.4

1.6

1.8

2

FIGURE 10.6. Rate distortion function for a Gaussian source.

Each bit of description reduces the expected distortion by a factor of 4.
With a 1-bit description, the best expected square error is Ï 2 /4. We can
compare this with the result of simple 1-bit quantization of a N(0, Ï 2 )
random variable as described in Section 10.1. In this case, using the two
regions corresponding to the positive and negative real lines and reproduction points as the centroids of the respective regions, the expected distortion is (ÏÏâ2) Ï 2 = 0.3633Ï 2 (see Problem 10.1). As we prove later, the
rate distortion limit R(D) is achieved by considering long block lengths.
This example shows that we can achieve a lower distortion by considering several distortion problems in succession (long block lengths) than can
be achieved by considering each problem separately. This is somewhat
surprising because we are quantizing independent random variables.

10.3.3 Simultaneous Description of Independent Gaussian
Random Variables
Consider the case of representing m independent (but not identically distributed) normal random sources X1 , . . . , Xm , where Xi are â¼ N(0, Ïi2 ),
with squared-error distortion. Assume that we are given R bits with which
to represent this random vector. The question naturally arises as to how
we should allot these bits to the various components to minimize the
total distortion. Extending the deï¬nition of the information rate distortion

10.3

CALCULATION OF THE RATE DISTORTION FUNCTION

313

function to the vector case, we have
R(D) =

min

f (xÌ m |x m ):Ed(Xm ,XÌ m )â¤D

I (X m ; XÌ m ),

(10.38)


2
where d(x m , xÌ m ) = m
i=1 (xi â xÌi ) . Now using the arguments in the preceding example, we have
I (X m ; XÌ m ) = h(X m ) â h(X m |XÌ m )
=
â¥

m


h(Xi ) â

m


i=1

i=1

m


m


h(Xi ) â

i=1

=

m


(10.39)

h(Xi |X iâ1 , XÌ m )

(10.40)

h(Xi |XÌi )

(10.41)

i=1

I (Xi ; XÌi )

(10.42)

R(Di )

(10.43)

i=1

â¥

m

i=1

	

+
m

Ïi2
1
log
,
=
2
Di

(10.44)

i=1

where Di = E(Xi â XÌi )2 and (10.41) follows from the fact that conditioning reduces
mentropy. We can achieve equality in (10.41) by choosing
m m
f (x |xÌ ) = i=1 f (xi |xÌi ) and in (10.43) by choosing the distribution of
each XÌi â¼ N(0, Ïi2 â Di ), as in the preceding example. Hence, the problem of ï¬nding the rate distortion function can be reduced to the following
optimization (using nats for convenience):
R(D) = min

Di =D

m

i=1




1 Ïi2
ln , 0 .
max
2 Di

(10.45)

Using Lagrange multipliers, we construct the functional
J (D) =

m

1
i=1

2


Ïi2
+Î»
Di ,
Di
m

ln

i=1

(10.46)

314

RATE DISTORTION THEORY

and differentiating with respect to Di and setting equal to 0, we have
âJ
1 1
=â
+Î»=0
âDi
2 Di

(10.47)

Di = Î»	 .

(10.48)

or

Hence, the optimum allotment of the bits to the various descriptions
results in an equal distortion for each random variable. This is possible if
the constant Î»	 in (10.48) is less than Ïi2 for all i. As the total allowable
distortion D is increased, the constant Î»	 increases until it exceeds Ïi2
for some i. At this point the solution (10.48) is on the boundary of the
allowable region of distortions. If we increase the total distortion, we must
use the KuhnâTucker conditions to ï¬nd the minimum in (10.46). In this
case the KuhnâTucker conditions yield
1 1
âJ
=â
+ Î»,
âDi
2 Di
where Î» is chosen so that
âJ
âDi



= 0 if Di < Ïi2
â¤ 0 if Di â¥ Ïi2 .

(10.49)

(10.50)

It is easy to check that the solution to the KuhnâTucker equations is given
by the following theorem:
Theorem 10.3.3 (Rate distortion for a parallel Gaussian source) Let
Xi â¼ N(0, Ïi2 ), i = 1, 2, . . . , m, be independent Gaussian
mrandom varim
m
ables, and let the distortion measure be d(x , xÌ ) = i=1 (xi â xÌi )2 .
Then the rate distortion function is given by
R(D) =

m

1
i=1

where


Di =

where Î» is chosen so that

m

Î»

2

log

Ïi2
,
Di

if Î» < Ïi2 ,

Ïi2 if Î» â¥ Ïi2 ,

i=1 Di

= D.

(10.51)

(10.52)

315

10.4 CONVERSE TO THE RATE DISTORTION THEOREM

s 2i

s 24
s 21

s 26

s^ 24

s^ 21

s^ 26

l
s 22
s 23

D1

D4

D2

s 25

D6

D3
D5

X1

X2

X3

X4

X5

X6

FIGURE 10.7. Reverse water-ï¬lling for independent Gaussian random variables.

This gives rise to a kind of reverse water-ï¬lling, as illustrated in
Figure 10.7. We choose a constant Î» and only describe those random variables with variances greater than Î». No bits are used to describe random
variables with variance less than Î». Summarizing, if
ï£¹
ï£¹
ï£® 2
ï£® 2
Ï1 Â· Â· Â· 0
ÏÌ1 Â· Â· Â· 0
ï£º
ï£º
ï£¯
ï£¯
X â¼ N(0, ï£° ... . . . ... ï£»), then XÌ â¼ N(0, ï£° ... . . . ... ï£»),
0 Â· Â· Â· Ïm2
0 Â· Â· Â· ÏÌm2
and E(Xi â XÌi )2 = Di , where Di = min{Î», Ïi2 }. More generally, the rate
distortion function for a multivariate normal vector can be obtained by
reverse water-ï¬lling on the eigenvalues. We can also apply the same arguments to a Gaussian stochastic process. By the spectral representation
theorem, a Gaussian stochastic process can be represented as an integral of independent Gaussian processes in the various frequency bands.
Reverse water-ï¬lling on the spectrum yields the rate distortion function.
10.4

CONVERSE TO THE RATE DISTORTION THEOREM

In this section we prove the converse to Theorem 10.2.1 by showing that
we cannot achieve a distortion of less than D if we describe X at a rate
less than R(D), where
R(D) =

p(xÌ|x):



min

(x,xÌ) p(x)p(xÌ|x)d(x,xÌ)â¤D

I (X; XÌ).

(10.53)

316

RATE DISTORTION THEORY

The minimization is over all conditional distributions p(xÌ|x) for which
the joint distribution p(x, xÌ) = p(x)p(xÌ|x) satisï¬es the expected distortion constraint. Before proving the converse, we establish some simple
properties of the information rate distortion function.
Lemma 10.4.1
(Convexity of R(D)) The rate distortion function R(D)
given in (10.53) is a nonincreasing convex function of D.
Proof: R(D) is the minimum of the mutual information over increasingly larger sets as D increases. Thus, R(D) is nonincreasing in D. To
prove that R(D) is convex, consider two rate distortion pairs, (R1 , D1 )
and (R2 , D2 ), which lie on the rate distortion curve. Let the joint distributions that achieve these pairs be p1 (x, xÌ) = p(x)p1 (xÌ|x) and p2 (x, xÌ) =
p(x)p2 (xÌ|x). Consider the distribution pÎ» = Î»p1 + (1 â Î»)p2 . Since the
distortion is a linear function of the distribution, we have D(pÎ» ) = Î»D1 +
(1 â Î»)D2 . Mutual information, on the other hand, is a convex function
of the conditional distribution (Theorem 2.7.4), and hence
IpÎ» (X; XÌ) â¤ Î»Ip1 (X; XÌ) + (1 â Î»)Ip2 (X; XÌ).

(10.54)

Hence, by the deï¬nition of the rate distortion function,
R(DÎ» ) â¤ IpÎ» (X; XÌ)

(10.55)

â¤ Î»Ip1 (X; XÌ) + (1 â Î»)Ip2 (X; XÌ)

(10.56)

= Î»R(D1 ) + (1 â Î»)R(D2 ),

(10.57)

which proves that R(D) is a convex function of D.



The converse can now be proved.
Proof: (Converse in Theorem 10.2.1). We must show for any source X
drawn i.i.d. â¼ p(x) with distortion measure d(x, xÌ) and any (2nR , n) rate
distortion code with distortion â¤ D, that the rate R of the code satisï¬es R â¥ R(D). In fact, we prove that R â¥ R(D) even for randomized
mappings fn and gn , as long as fn takes on at most 2nR values.
Consider any (2nR , n) rate distortion code deï¬ned by functions fn and
gn as given in (10.7) and (10.8). Let XÌ n = XÌ n (X n ) = gn (fn (X n )) be the
reproduced sequence corresponding to X n . Assume that Ed(X n , XÌ n ) â¥ D

10.4 CONVERSE TO THE RATE DISTORTION THEOREM

317

for this code. Then we have the following chain of inequalities:
(a)

nR â¥ H (fn (X n ))

(10.58)

(b)

â¥ H (fn (X n )) â H (fn (X n )|X n )

(10.59)

= I (X ; fn (X ))

(10.60)

n

n

(c)

â¥ I (X n ; XÌ n )

(10.61)

= H (X n ) â H (X n |XÌ n )

(10.62)

(d)

=

n


H (Xi ) â H (X n |XÌ n )

(10.63)

i=1
(e)

=

(f)

â¥

n


n


H (Xi ) â

i=1

i=1

n


n


H (Xi ) â

i=1

=

n


H (Xi |XÌ n , Xiâ1 , . . . , X1 )

(10.64)

H (Xi |XÌi )

(10.65)

i=1

I (Xi ; XÌi )

(10.66)

R(Ed(Xi , XÌi ))

(10.67)

i=1
(g)

â¥

n

i=1

	



n
1
=n
R(Ed(Xi , XÌi ))
n
i=1
	 n


(h)
1
â¥ nR
Ed(Xi , XÌi )
n

(10.68)

(10.69)

i=1

(i)

= nR(Ed(X n , XÌ n ))
(j)

= nR(D),
where
(a) follows from the fact that the range of fn is at most 2nR
(b) follows from the fact that H (fn (X n )|X n ) â¥ 0

(10.70)
(10.71)

318

RATE DISTORTION THEORY

(c) follows from the data-processing inequality
(d) follows from the fact that the Xi are independent
(e) follows from the chain rule for entropy
(f) follows from the fact that conditioning reduces entropy
(g) follows from the deï¬nition of the rate distortion function
(h) follows from the convexity of the rate distortion function (Lemma
10.4.1) and Jensenâs inequality
(i) follows from the deï¬nition of distortion for blocks of length n
(j) follows from the fact that R(D) is a nonincreasing function of D and
Ed(X n , XÌ n ) â¤ D
This shows that the rate R of any rate distortion code exceeds the rate
distortion function R(D) evaluated at the distortion level D = Ed(X n , XÌ n )

achieved by that code.
A similar argument can be applied when the encoded source is passed
through a noisy channel and hence we have the equivalent of the source
channel separation theorem with distortion:
Theorem 10.4.1 (Sourceâchannel separation theorem with distortion)
Let V1 , V2 , . . . , Vn be a ï¬nite alphabet i.i.d. source which is encoded as
a sequence of n input symbols X n of a discrete memoryless channel with
capacity C. The output of the channel Y n is mapped
onto the reconstruction
alphabet VÌ n = g(Y n ). Let D = Ed(V n , VÌ n ) = n1 ni=1 Ed(Vi , VÌi ) be the
average distortion achieved by this combined source and channel coding
scheme. Then distortion D is achievable if and only if C > R(D).

Vn

X n(V n)

Channel Capacity C

Yn

^

Vn

Proof: See Problem 10.17.

10.5



ACHIEVABILITY OF THE RATE DISTORTION FUNCTION

We now prove the achievability of the rate distortion function. We begin
with a modiï¬ed version of the joint AEP in which we add the condition
that the pair of sequences be typical with respect to the distortion measure.

10.5

ACHIEVABILITY OF THE RATE DISTORTION FUNCTION

319

Deï¬nition Let p(x, xÌ) be a joint probability distribution on X Ã XÌ and
let d(x, xÌ) be a distortion measure on X Ã XÌ. For any  > 0, a pair of
sequences (x n , xÌ n ) is said to be distortion -typical or simply distortion
typical if



 1
â log p(x n ) â H (X) < 
(10.72)

 n



 1
n
â log p(xÌ ) â H (XÌ) < 
(10.73)

 n



 1
â log p(x n , xÌ n ) â H (X, XÌ) < 
(10.74)

 n
|d(x n , xÌ n ) â Ed(X, XÌ)| < .

(10.75)

The set of distortion typical sequences is called the distortion typical set
and is denoted A(n)
d, .
Note that this is the deï¬nition of the jointly typical set (Section 7.6)
with the additional constraint that the distortion be close to the expected
value. Hence, the distortion typical set is a subset of the jointly typical
(n)
set (i.e., A(n)
d, â A ). If (Xi , XÌi ) are drawn i.i.d â¼ p(x, xÌ), the distortion
between two random sequences
1
d(Xi , XÌi )
n
n

d(X n , XÌ n ) =

(10.76)

i=1

is an average of i.i.d. random variables, and the law of large numbers
implies that it is close to its expected value with high probability. Hence
we have the following lemma.
Lemma 10.5.1
1 as n â â.

Let (Xi , XÌi ) be drawn i.i.d. â¼ p(x, xÌ). Then Pr(A(n)
d, ) â

Proof: The sums in the four conditions in the deï¬nition of A(n)
d, are
all normalized sums of i.i.d random variables and hence, by the law of
large numbers, tend to their respective expected values with probability 1.
Hence the set of sequences satisfying all four conditions has probability

tending to 1 as n â â.
The following lemma is a direct consequence of the deï¬nition of the
distortion typical set.

320

RATE DISTORTION THEORY

Lemma 10.5.2

For all (x n , xÌ n ) â A(n)
d, ,
p(xÌ n ) â¥ p(xÌ n |x n )2ân(I (X;XÌ)+3) .

(10.77)

Proof: Using the deï¬nition of A(n)
d, , we can bound the probabilities
n
n
n
n
p(x ), p(xÌ ) and p(x , xÌ ) for all (x n , xÌ n ) â A(n)
d, , and hence
p(xÌ n |x n ) =

p(x n , xÌ n )
p(x n )

= p(xÌ n )
â¤ p(xÌ n )

(10.78)

p(x n , xÌ n )
p(x n )p(xÌ n )

(10.79)

2ân(H (X,XÌ)â)
2ân(H (X)+) 2ân(H (XÌ)+)

= p(xÌ n )2n(I (X;XÌ)+3) ,

(10.80)
(10.81)


and the lemma follows immediately.
We also need the following interesting inequality.
Lemma 10.5.3

For 0 â¤ x, y â¤ 1, n > 0,
(1 â xy)n â¤ 1 â x + eâyn .

(10.82)

Proof: Let f (y) = eây â 1 + y. Then f (0) = 0 and f 	 (y) = âeây +
1 > 0 for y > 0, and hence f (y) > 0 for y > 0. Hence for 0 â¤ y â¤ 1,
we have 1 â y â¤ eây , and raising this to the nth power, we obtain
(1 â y)n â¤ eâyn .

(10.83)

Thus, the lemma is satisï¬ed for x = 1. By examination, it is clear that
the inequality is also satisï¬ed for x = 0. By differentiation, it is easy
to see that gy (x) = (1 â xy)n is a convex function of x, and hence for
0 â¤ x â¤ 1, we have
(1 â xy)n = gy (x)

(10.84)

â¤ (1 â x)gy (0) + xgy (1)

(10.85)

= (1 â x)1 + x(1 â y)

(10.86)

â¤ 1 â x + xe

âyn

â¤ 1 â x + eâyn . 

n

(10.87)
(10.88)

We use the preceding proof to prove the achievability of Theorem 10.2.1.

10.5

ACHIEVABILITY OF THE RATE DISTORTION FUNCTION

321

Proof: (Achievability in Theorem 10.2.1). Let X1 , X2 , . . . , Xn be drawn
i.i.d. â¼ p(x) and let d(x, xÌ) be a bounded distortion measure for this
source. Let the rate distortion function for this source be R(D). Then for
any D, and any R > R(D), we will show that the rate distortion pair
(R, D) is achievable by proving the existence of a sequence of rate distortion codes with rate R and asymptotic distortion D. Fix p(xÌ|x), where
p(xÌ|x) achieves
equality in (10.53). Thus, I (X; XÌ) = R(D). Calculate

p(xÌ) = x p(x)p(xÌ|x). Choose Î´ > 0. We will prove the existence of a
rate distortion code with rate R and distortion less than or equal to D + Î´.
Generation of codebook: Randomly generate a 
rate distortion codebook
C consisting of 2nR sequences XÌ n drawn i.i.d. â¼ ni=1 p(xÌi ). Index these
codewords by w â {1, 2, . . . , 2nR }. Reveal this codebook to the encoder
and decoder.
Encoding: Encode X n by w if there exists a w such that (X n , XÌ n (w)) â
(n)
Ad, , the distortion typical set. If there is more than one such w, send the
least. If there is no such w, let w = 1. Thus, nR bits sufï¬ce to describe
the index w of the jointly typical codeword.
Decoding: The reproduced sequence is XÌ n (w).
Calculation of distortion: As in the case of the channel coding theorem,
we calculate the expected distortion over the random choice of codebooks
C as
D = EXn ,C d(X n , XÌ n ),
(10.89)
where the expectation is over the random choice of codebooks and over
Xn .
For a ï¬xed codebook C and choice of  > 0, we divide the sequences
n
x â X n into two categories:
â¢

â¢

Sequences x n such that there exists a codeword XÌ n (w) that is distortion typical with x n [i.e., d(x n , xÌ n (w)) < D + ]. Since the total
probability of these sequences is at most 1, these sequences contribute
at most D +  to the expected distortion.
Sequences x n such that there does not exist a codeword XÌ n (w)
that is distortion typical with x n . Let Pe be the total probability of
these sequences. Since the distortion for any individual sequence is
bounded by dmax , these sequences contribute at most Pe dmax to the
expected distortion.

Hence, we can bound the total distortion by
Ed(X n , XÌ n (X n )) â¤ D +  + Pe dmax ,

(10.90)

322

RATE DISTORTION THEORY

which can be made less than D + Î´ for an appropriate choice of  if Pe is
small enough. Hence, if we show that Pe is small, the expected distortion
is close to D and the theorem is proved.
Calculation of Pe : We must bound the probability that for a random
choice of codebook C and a randomly chosen source sequence, there is
no codeword that is distortion typical with the source sequence. Let J (C)
denote the set of source sequences x n such that at least one codeword in
C is distortion typical with x n . Then
Pe =





P ( C)

p(x n ).

(10.91)

x n :x n âJ
/ (C )

C

This is the probability of all sequences not well represented by a code,
averaged over the randomly chosen code. By changing the order of summation, we can also interpret this as the probability of choosing a codebook that does not well represent sequence x n , averaged with respect to
p(x n ). Thus,
Pe =





p(x n )

p(C).

(10.92)

C :x n âJ
/ (C )

xn

Let us deï¬ne

K(x , xÌ ) =
n

n

1 if (x n , xÌ n ) â A(n)
d, ,
n
n
0 if (x , xÌ ) â
/ A(n)
d, .

(10.93)

The probability that a single randomly chosen codeword XÌ n does not well
represent a ï¬xed x n is
n
n
Pr((x n , XÌ n ) â
/ A(n)
d, ) = Pr(K(x , XÌ ) = 0) = 1 â



p(xÌ n )K(x n , xÌ n ),

xÌ n

(10.94)
and therefore the probability that 2nR independently chosen codewords do
not represent x n , averaged over p(x n ), is
Pe =


xn

=


xn



p(x n )

p(C)

(10.95)

C :x n âJ
/ (C )


p(x n ) 1 â


xÌ n

2nR
p(xÌ n )K(x n , xÌ n )

.

(10.96)

10.5

323

ACHIEVABILITY OF THE RATE DISTORTION FUNCTION

We now use Lemma 10.5.2 to bound the sum within the brackets. From
Lemma 10.5.2, it follows that


p(xÌ n )K(x n , xÌ n ) â¥
p(xÌ n |x n )2ân(I (X;XÌ)+3) K(x n , xÌ n ), (10.97)
xÌ n

xÌ n

and hence
Pe â¤



	
p(x n ) 1 â 2ân(I (X;XÌ)+3)

xn




2nR
p(xÌ n |x n )K(x n , xÌ n )

.

xÌ n

(10.98)
We now use Lemma 10.5.3 to bound the term on the right-hand side
of (10.98) and obtain
	
1 â 2ân(I (X;XÌ)+3)




2nR
p(xÌ n |x n )K(x n , xÌ n )

xÌ n

â¤1â


xÌ n

p(xÌ n |x n )K(x n , xÌ n ) + eâ(2

ân(I (X;XÌ)+3) 2nR )

. (10.99)

Substituting this inequality in (10.98), we obtain

ân(I (X;XÌ)+3) 2nR
Pe â¤ 1 â
p(x n )p(xÌ n |x n )K(x n , xÌ n ) + eâ2
.
xn

xÌ n

(10.100)
The last term in the bound is equal to
eâ2

n(RâI (X;XÌ)â3)

,

(10.101)

which goes to zero exponentially fast with n if R > I (X; XÌ) + 3. Hence
if we choose p(xÌ|x) to be the conditional distribution that achieves the
minimum in the rate distortion function, then R > R(D) implies that
R > I (X; XÌ) and we can choose  small enough so that the last term in
(10.100) goes to 0.
The ï¬rst two terms in (10.100) give the probability under the joint
distribution p(x n , xÌ n ) that the pair of sequences is not distortion typical.
Hence, using Lemma 10.5.1, we obtain

p(x n , xÌ n )K(x n , xÌ n ) = Pr((X n , XÌ n ) â
/ A(n)
1â
d, ) < 
xn

xÌ n

(10.102)

324

RATE DISTORTION THEORY

for n sufï¬ciently large. Therefore, by an appropriate choice of  and n,
we can make Pe as small as we like.
So, for any choice of Î´ > 0, there exists an  and n such that over all
randomly chosen rate R codes of block length n, the expected distortion
is less than D + Î´. Hence, there must exist at least one code Câ with this
rate and block length with average distortion less than D + Î´. Since Î´ was

arbitrary, we have shown that (R, D) is achievable if R > R(D).
We have proved the existence of a rate distortion code with an expected
distortion close to D and a rate close to R(D). The similarities between
the random coding proof of the rate distortion theorem and the random
coding proof of the channel coding theorem are now evident. We will
explore the parallels further by considering the Gaussian example, which
provides some geometric insight into the problem. It turns out that channel
coding is sphere packing and rate distortion coding is sphere covering.
Channel coding for the Gaussian channel. Consider a Gaussian channel,
Yi = Xi + Zi , where the Zi are i.i.d. â¼ N(0, N) and there is a power
constraint P on the power per symbol of the transmitted codeword. Consider a sequence of n transmissions. The power constraint
â implies that
the transmitted sequence lies within a sphere of radius nP in Rn . The
coding problem is equivalent to ï¬nding a set of 2nR sequences within
this sphere such that the probability of any
â of them being mistaken for
any other is smallâthe spheres of radius nN around each â
of them are
almost disjoint. This corresponds
to
ï¬lling
a
sphere
of
radius
n(P + N )
â
with spheres of radius nN . One would expect that the largest number
of spheres that could be ï¬t would be the ratio of their volumes, or, equivalently, the nth power of the ratio of their radii. Thus, if M is the number
of codewords that can be transmitted efï¬ciently, we have
â
n

P +N 2
( n(P + N ))n
=
.
Mâ¤
â
N
( nN )n

(10.103)

The results of the channel coding theorem show that it is possible to do
this efï¬ciently for large n; it is possible to ï¬nd approximately

nC

2

=

P +N
N

n
2

(10.104)

codewords such that the noise spheres around them are almost disjoint
(the total volume of their intersection is arbitrarily small).

10.6 STRONGLY TYPICAL SEQUENCES AND RATE DISTORTION

325

Rate distortion for the Gaussian source. Consider a Gaussian source of
variance Ï 2 . A (2nR , n) rate distortion code for this source with distortion
D is a set of 2nR sequences in Rn such that most source
sequences of
â
length n â
(all those that lie within a sphere of radius nÏ 2 ) are within a
distance nD of some codeword. Again, by the sphere-packing argument,
it is clear that the minimum number of codewords required is

2nR(D) =

Ï2
D

 n2
.

(10.105)

The rate distortion theorem shows that this minimum rate is asymptotically
â
achievable (i.e., that there exists a collection of spheres of radius nD
that cover the space except for a set of arbitrarily small probability).
The above geometric arguments also enable us to transform a good
code for channel transmission into a good code for rate distortion. In both
cases, the essential idea is to ï¬ll the space of source sequences: In channel
transmission, we want to ï¬nd the largest set of codewords that have a large
minimum distance between codewords, whereas in rate distortion, we wish
to ï¬nd the smallest set of codewords that covers the entire space. If we
have any set that meets the sphere packing bound for one, it will meet the
sphere packing bound for the other. In the Gaussian case, choosing the
codewords to be Gaussian with the appropriate variance is asymptotically
optimal for both rate distortion and channel coding.
10.6 STRONGLY TYPICAL SEQUENCES AND RATE
DISTORTION
In Section 10.5 we proved the existence of a rate distortion code of
rate R(D) with average distortion close to D. In fact, not only is the
average distortion close to D, but the total probability that the distortion is greater than D + Î´ is close to 0. The proof of this is similar
to the proof in Section 10.5; the main difference is that we will use
strongly typical sequences rather than weakly typical sequences. This
will enable us to give an upper bound to the probability that a typical
source sequence is not well represented by a randomly chosen codeword
in (10.94). We now outline an alternative proof based on strong typicality that will provide a stronger and more intuitive approach to the rate
distortion theorem.
We begin by deï¬ning strong typicality and quoting a basic theorem
bounding the probability that two sequences are jointly typical. The
properties of strong typicality were introduced by Berger [53] and were

326

RATE DISTORTION THEORY

explored in detail in the book by CsiszaÌr and KoÌrner [149]. We will
deï¬ne strong typicality (as in Chapter 11) and state a fundamental lemma
(Lemma 10.6.2).
Deï¬nition A sequence x n â X n is said to be -strongly typical with
respect to a distribution p(x) on X if:
1. For all a â X with p(a) > 0, we have


1

 N (a|x n ) â p(a) <  .
n
 |X|

(10.106)

2. For all a â X with p(a) = 0, N (a|x n ) = 0.
N (a|x n ) is the number of occurrences of the symbol a in the sequence
x .
The set of sequences x n â X n such that x n is strongly typical is called
â(n)
the strongly typical set and is denoted Aâ(n)
when the random
 (X) or A
variable is understood from the context.
n

Deï¬nition A pair of sequences (x n , y n ) â X n Ã Y n is said to be strongly typical with respect to a distribution p(x, y) on X Ã Y if:
1. For all (a, b) â X Ã Y with p(a, b) > 0, we have


1

n
n
 N (a, b|x , y ) â p(a, b) <  .
n
 |X||Y|

(10.107)

2. For all (a, b) â X Ã Y with p(a, b) = 0, N (a, b|x n , y n ) = 0.
N (a, b|x n , y n ) is the number of occurrences of the pair (a, b) in the pair
of sequences (x n , y n ).
The set of sequences (x n , y n ) â X n Ã Y n such that (x n , y n ) is strongly
typical is called the strongly typical set and is denoted Aâ(n)
 (X, Y ) or
â(n)
n
n
â(n)
A . From the deï¬nition, it follows that if (x , y ) â A (X, Y ), then
x n â Aâ(n)
 (X). From the strong law of large numbers, the following
lemma is immediate.
Lemma 10.6.1 Let (Xi , Yi ) be drawn i.i.d. â¼ p(x, y). Then Pr(Aâ(n)
 )
â 1 as n â â.
We will use one basic result, which bounds the probability that an
independently drawn sequence will be seen as jointly strongly typical

10.6 STRONGLY TYPICAL SEQUENCES AND RATE DISTORTION

327

with a given sequence. Theorem 7.6.1 shows that if we choose X n and
Y n independently, the probability that they will be weakly jointly typical
is â 2ânI (X;Y ) . The following lemma extends the result to strongly typical
sequences. This is stronger than the earlier result in that it gives a lower
bound on the probability that a randomly chosen sequence is jointly typical
with a ï¬xed typical x n .
Lemma 10.6.2 Let Y1 , Y2 , . . . , Yn be drawn i.i.d. â¼ p(y). For x n â
n
n
â(n)
Aâ(n)
is bounded by
 (X), the probability that (x , Y ) â A
ân(I (X;Y )â1 )
,
2ân(I (X;Y )+1 ) â¤ Pr((x n , Y n ) â Aâ(n)
 )â¤2

(10.108)

where 1 goes to 0 as  â 0 and n â â.
Proof: We will not prove this lemma, but instead, outline the proof in
Problem 10.16 at the end of the chapter. In essence, the proof involves

ï¬nding a lower bound on the size of the conditionally typical set.
We will proceed directly to the achievability of the rate distortion
function. We will only give an outline to illustrate the main ideas. The
construction of the codebook and the encoding and decoding are similar
to the proof in Section 10.5.

Proof: Fix p(xÌ|x). Calculate p(xÌ) = x p(x)p(xÌ|x). Fix  > 0. Later
we will choose  appropriately to achieve an expected distortion less than
D + Î´.
Generation of codebook: Generate a rate
distortion codebook C consisting of 2nR sequences XÌ n drawn i.i.d. â¼ i p(xÌi ). Denote the sequences
XÌ n (1), . . . , XÌ n (2nR ).
Encoding: Given a sequence X n , index it by w if there exists a w such
that (X n , XÌ n (w)) â Aâ(n)
 , the strongly jointly typical set. If there is more
than one such w, send the ï¬rst in lexicographic order. If there is no such
w, let w = 1.
Decoding: Let the reproduced sequence be XÌ n (w).
Calculation of distortion: As in the case of the proof in Section 10.5, we
calculate the expected distortion over the random choice of codebook as
D = EXn ,C d(X n , XÌ n )

= EC
p(x n )d(x n , XÌ n (x n ))
=


xn

(10.109)
(10.110)

xn

p(x n )EC d(x n , XÌ n ),

(10.111)

328

RATE DISTORTION THEORY

Typical
sequences
with
jointly
typical
codeword

Typical
sequences
without
jointly
typical
codeword

Nontypical sequences

FIGURE 10.8. Classes of source sequences in rate distortion theorem.

where the expectation is over the random choice of codebook. For a ï¬xed
codebook C, we divide the sequences x n â X n into three categories, as
shown in Figure 10.8.
â¢

â¢

â¢

Nontypical sequences x n â
/ Aâ(n)
 . The total probability of these
sequences can be made less than  by choosing n large enough. Since
the individual distortion between any two sequences is bounded by
dmax , the nontypical sequences can contribute at most dmax to the
expected distortion.
Typical sequences x n â Aâ(n)
such that there exists a codeword XÌ n (w)

that is jointly typical with x n . In this case, since the source sequence
and the codeword are strongly jointly typical, the continuity of the
distortion as a function of the joint distribution ensures that they
are also distortion typical. Hence, the distortion between these x n
and their codewords is bounded by D + dmax , and since the total
probability of these sequences is at most 1, these sequences contribute
at most D + dmax to the expected distortion.
Typical sequences x n â Aâ(n)
such that there does not exist a code
word XÌ n that is jointly typical with x n . Let Pe be the total probability
of these sequences. Since the distortion for any individual sequence
is bounded by dmax , these sequences contribute at most Pe dmax to the
expected distortion.

10.7

CHARACTERIZATION OF THE RATE DISTORTION FUNCTION

329

The sequences in the ï¬rst and third categories are the sequences that
may not be well represented by this rate distortion code. The probability
of the ï¬rst category of sequences is less than  for sufï¬ciently large n. The
probability of the last category is Pe , which we will show can be made
small. This will prove the theorem that the total probability of sequences
that are not well represented is small. In turn, we use this to show that
the average distortion is close to D.
Calculation of Pe : We must bound the probability that there is no codeword that is jointly typical with the given sequence X n . From the joint
AEP, we know that the probability that X n and any XÌ n are jointly typical
.
is = 2ânI (X;XÌ) . Hence the expected number of jointly typical XÌ n (w) is
2nR 2ânI (X;XÌ) , which is exponentially large if R > I (X; XÌ).
But this is not sufï¬cient to show that Pe â 0. We must show that the
probability that there is no codeword that is jointly typical with X n goes
to zero. The fact that the expected number of jointly typical codewords is
exponentially large does not ensure that there will at least one with high
probability. Just as in (10.94), we can expand the probability of error as


2nR
Pe =
p(x n ) 1 â Pr((x n , XÌ n ) â Aâ(n)
.
(10.112)
 )
â(n)

x n âA

From Lemma 10.6.2 we have
ân(I (X;XÌ)+1 )
Pr((x n , XÌ n ) â Aâ(n)
.
 )â¥2

(10.113)

Substituting this in (10.112) and using the inequality (1 â x)n â¤ eânx , we
have
Pe â¤ eâ(2

nR 2ân(I (X;XÌ)+1 ))

,

(10.114)

which goes to 0 as n â â if R > I (X; XÌ) + 1 . Hence for an appropriate
choice of  and n, we can get the total probability of all badly represented
sequences to be as small as we want. Not only is the expected distortion
close to D, but with probability going to 1, we will ï¬nd a codeword whose

distortion with respect to the given sequence is less than D + Î´.
10.7 CHARACTERIZATION OF THE RATE DISTORTION
FUNCTION
We have deï¬ned the information rate distortion function as
R(D) =

q(xÌ|x):



min

(x,xÌ) p(x)q(xÌ|x)d(x,xÌ)â¤D

I (X; XÌ),

(10.115)

330

RATE DISTORTION THEORY

where the minimization is over all conditional distributions q(xÌ|x) for
which the joint distribution p(x)q(xÌ|x) satisï¬es the expected distortion
constraint. This is a standard minimization problem
 of a convex function
over
the convex set of all q(xÌ|x) â¥ 0 satisfying xÌ q(xÌ|x) = 1 for all x
and
q(xÌ|x)p(x)d(x, xÌ) â¤ D.
We can use the method of Lagrange multipliers to ï¬nd the solution.
We set up the functional
J (q) =


x

xÌ

+Î»


x

+

q(xÌ|x)
x p(x)q(xÌ|x)

p(x)q(xÌ|x) log 



xÌ

Î½(x)

p(x)q(xÌ|x)d(x, xÌ)



x

q(xÌ|x),

(10.116)
(10.117)

xÌ

where the last term corresponds to the constraint that
 q(xÌ|x) is a conditional probability mass function. If we let q(xÌ) = x p(x)q(xÌ|x) be the
distribution on XÌ induced by q(xÌ|x), we can rewrite J (q) as
J (q) =


x

xÌ

+Î»

p(x)q(xÌ|x) log


x

+


x

xÌ

Î½(x)

q(xÌ|x)
q(xÌ)

p(x)q(xÌ|x)d(x, xÌ)


q(xÌ|x).

(10.118)
(10.119)

xÌ

Differentiating with respect to q(xÌ|x), we have

q(xÌ|x)
1
âJ
= p(x) log
+ p(x) â
p(x)
p(x 	 )q(xÌ|x 	 )
âq(xÌ|x)
q(xÌ)
q(xÌ)
	
x

+ Î»p(x)d(x, xÌ) + Î½(x) = 0.

(10.120)

Setting log Âµ(x) = Î½(x)/p(x), we obtain


q(xÌ|x)
p(x) log
+ Î»d(x, xÌ) + log Âµ(x) = 0
q(xÌ)

(10.121)

10.7

CHARACTERIZATION OF THE RATE DISTORTION FUNCTION

331

or
q(xÌ|x) =
Since


xÌ

q(xÌ)eâÎ»d(x,xÌ)
.
Âµ(x)

q(xÌ|x) = 1, we must have

q(xÌ)eâÎ»d(x,xÌ)
Âµ(x) =

(10.122)

(10.123)

xÌ

or
q(xÌ)eâÎ»d(x,xÌ)
.
q(xÌ|x) = 
âÎ»d(x,xÌ)
xÌ q(xÌ)e

(10.124)

Multiplying this by p(x) and summing over all x, we obtain
q(xÌ) = q(xÌ)


x

p(x)eâÎ»d(x,xÌ)
.
	 âÎ»d(x,xÌ 	 )
xÌ 	 q(xÌ )e



(10.125)

If q(xÌ) > 0, we can divide both sides by q(xÌ) and obtain

x



p(x)eâÎ»d(x,xÌ)
=1
	 âÎ»d(x,xÌ 	 )
xÌ 	 q(xÌ )e

(10.126)

for all xÌ â XÌ. We can combine these |XÌ| equations with the equation
deï¬ning the distortion and calculate Î» and the |XÌ| unknowns q(xÌ). We
can use this and (10.124) to ï¬nd the optimum conditional distribution.
The above analysis is valid if q(xÌ) is unconstrained (i.e., q(xÌ) > 0 for
all xÌ). The inequality condition q(xÌ) > 0 is covered by the KuhnâTucker
conditions, which reduce to
âJ
= 0 if q(xÌ|x) > 0,
âq(xÌ|x)
â¥ 0 if q(xÌ|x) = 0.

(10.127)

Substituting the value of the derivative, we obtain the conditions for the
minimum as

x

p(x)eâÎ»d(x,xÌ)
=1
	 âÎ»d(x,xÌ 	 )
xÌ 	 q(xÌ )e
â¤1



if q(xÌ) > 0,

(10.128)

if q(xÌ) = 0.

(10.129)

332

RATE DISTORTION THEORY

This characterization will enable us to check if a given q(xÌ) is a solution
to the minimization problem. However, it is not easy to solve for the
optimum output distribution from these equations. In the next section we
provide an iterative algorithm for computing the rate distortion function.
This algorithm is a special case of a general algorithm for ï¬nding the
minimum relative entropy distance between two convex sets of probability
densities.
10.8 COMPUTATION OF CHANNEL CAPACITY AND THE RATE
DISTORTION FUNCTION
Consider the following problem: Given two convex sets A and B in Rn
as shown in Figure 10.9, we would like to ï¬nd the minimum distance
between them:
dmin =

min d(a, b),

(10.130)

aâA,bâB

where d(a, b) is the Euclidean distance between a and b. An intuitively
obvious algorithm to do this would be to take any point x â A, and ï¬nd
the y â B that is closest to it. Then ï¬x this y and ï¬nd the closest point in
A. Repeating this process, it is clear that the distance decreases at each
stage. Does it converge to the minimum distance between the two sets?
CsiszaÌr and TusnaÌdy [155] have shown that if the sets are convex and
if the distance satisï¬es certain conditions, this alternating minimization
algorithm will indeed converge to the minimum. In particular, if the sets
are sets of probability distributions and the distance measure is the relative
entropy, the algorithm does converge to the minimum relative entropy
between the two sets of distributions.

A
B

FIGURE 10.9. Distance between convex sets.

10.8 COMPUTATION OF CHANNEL CAPACITY AND RATE DISTORTION FUNCTION

333

To apply this algorithm to rate distortion, we have to rewrite the rate
distortion function as a minimum of the relative entropy between two sets.
We begin with a simple lemma. A form of this lemma comes up again
in theorem 13.1.1, establishing the duality of channel capacity universal
data compression.
Lemma 10.8.1 Let p(x)p(y|x) be a given joint distribution. Then the
distribution r(y) that minimizes the relative entropy D(p(x)p(y|x)||p(x)
r(y)) is the marginal distribution r â (y) corresponding to p(y|x):
D(p(x)p(y|x)||p(x)r â (y)) = min D(p(x)p(y|x)||p(x)r(y)), (10.131)
r(y)

where r â (y) =
max

r(x|y)




x

p(x)p(y|x). Also,

p(x)p(y|x) log

x,y

r(x|y) 
r â (x|y)
p(x)p(y|x) log
=
,
p(x)
p(x)
x,y
(10.132)

where

p(x)p(y|x)
r â (x|y) = 
.
x p(x)p(y|x)

(10.133)

Proof
D(p(x)p(y|x)||p(x)r(y)) â D(p(x)p(y|x)||p(x)r â (y))

p(x)p(y|x)
(10.134)
p(x)p(y|x) log
=
p(x)r(y)
x,y
â



p(x)p(y|x) log

x,y

=



p(x)p(y|x) log

x,y

=



r â (y) log

y

= D(r â ||r)
â¥ 0.

r â (y)
r(y)

p(x)p(y|x)
p(x)r â (y)

r â (y)
r(y)

(10.135)

(10.136)

(10.137)
(10.138)
(10.139)

334

RATE DISTORTION THEORY

The proof of the second part of the lemma is left as an exercise.



We can use this lemma to rewrite the minimization in the deï¬nition of
the rate distortion function as a double minimization,

q(xÌ|x)
p(x)q(xÌ|x) log
.
R(D) = min
 min
r(xÌ) q(xÌ|x): p(x)q(xÌ|x)d(x,xÌ)â¤D
r(
xÌ)
x
xÌ

(10.140)
If A is the set of all joint distributions with marginal p(x) that satisfy the
distortion constraints and if B the set of product distributions p(x)r(xÌ)
with arbitrary r(xÌ), we can write
R(D) = min min D(p||q).

(10.141)

qâB pâA

We now apply the process of alternating minimization, which is called the
BlahutâArimoto algorithm in this case. We begin with a choice of Î» and
an initial output distribution r(xÌ) and calculate the q(xÌ|x) that minimizes
the mutual information subject to the distortion constraint. We can use the
method of Lagrange multipliers for this minimization to obtain
r(xÌ)eâÎ»d(x,xÌ)
.
q(xÌ|x) = 
âÎ»d(x,xÌ)
xÌ r(xÌ)e

(10.142)

For this conditional distribution q(xÌ|x), we calculate the output distribution r(xÌ) that minimizes the mutual information, which by Lemma 10.8.1
is

p(x)q(xÌ|x).
(10.143)
r(xÌ) =
x

We use this output distribution as the starting point of the next iteration.
Each step in the iteration, minimizing over q(Â·|Â·) and then minimizing over
r(Â·), reduces the right-hand side of (10.140). Thus, there is a limit, and
the limit has been shown to be R(D) by CsiszaÌr [139], where the value
of D and R(D) depends on Î». Thus, choosing Î» appropriately sweeps out
the R(D) curve.
A similar procedure can be applied to the calculation of channel capacity. Again we rewrite the deï¬nition of channel capacity,
C = max I (X; Y ) = max
r(x)

r(x)


x

y

r(x)p(y|x) log

r(x)

r(x)p(y|x)

	
	
x 	 r(x )p(y|x )
(10.144)

SUMMARY

335

as a double maximization using Lemma 10.8.1,
C = max max



q(x|y) r(x)

x

r(x)p(y|x) log

y

q(x|y)
.
r(x)

(10.145)

In this case, the CsiszaÌrâTusnady algorithm becomes one of alternating
maximizationâwe start with a guess of the maximizing distribution r(x)
and ï¬nd the best conditional distribution, which is, by Lemma 10.8.1,
r(x)p(y|x)
q(x|y) = 
.
x r(x)p(y|x)

(10.146)

For this conditional distribution, we ï¬nd the best input distribution
r(x) by solving the constrained maximization problem with Lagrange
multipliers. The optimum input distribution is


(q(x|y))p(y|x)
,
r(x) =  
p(y|x)
x
y (q(x|y))
y

(10.147)

which we can use as the basis for the next iteration.
These algorithms for the computation of the channel capacity and the
rate distortion function were established by Blahut [65] and Arimoto [25]
and the convergence for the rate distortion computation was proved by
CsiszaÌr [139]. The alternating minimization procedure of CsiszaÌr and Tusnady can be specialized to many other situations as well, including the EM
algorithm [166], and the algorithm for ï¬nding the log-optimal portfolio
for a stock market [123].

SUMMARY
Rate distortion. The rate distortion function for a source X â¼ p(x)
and distortion measure d(x, xÌ) is
R(D) =

p(xÌ|x):



min

(x,xÌ) p(x)p(xÌ|x)d(x,xÌ)â¤D

I (X; XÌ),

(10.148)

where the minimization is over all conditional distributions p(xÌ|x) for
which the joint distribution p(x, xÌ) = p(x)p(xÌ|x) satisï¬es the expected
distortion constraint.

336

RATE DISTORTION THEORY

Rate distortion theorem. If R > R(D), there exists a sequence of
codes XÌ n (X n ) with the number of codewords |XÌ n (Â·)| â¤ 2nR with
Ed(X n , XÌ n (X n )) â D. If R < R(D), no such codes exist.
Bernoulli source. For a Bernoulli source with Hamming distortion,
R(D) = H (p) â H (D).

(10.149)

Gaussian source. For a Gaussian source with squared-error distortion,
R(D) =

1
Ï2
log .
2
D

(10.150)

Sourceâchannel separation. A source with rate distortion R(D) can
be sent over a channel of capacity C and recovered with distortion D
if and only if R(D) < C.
Multivariate Gaussian source. The rate distortion function for a multivariate normal vector with Euclidean mean-squared-error distortion is
given by reverse water-ï¬lling on the eigenvalues.

PROBLEMS
10.1

One-bit quantization of a single Gaussian random variable. Let
X â¼ N(0, Ï 2 ) and let the distortion measure be squared error.
Here we do not allow block descriptions. Show
that the optimum
reproduction points for 1-bit quantization are Â±

expected distortion for 1-bit quantization is
with
the
distortion
rate
bound
R = 1.
10.2

Ï â2
Ï Ï

2
ÏÏ
2

and that the

. Compare this
for
D = Ï 2 2â2R

Rate distortion function with inï¬nite distortion. Find the rate distortion function R(D) = min I (X; XÌ) for X â¼ Bernoulli ( 12 ) and
distortion
ï£±
ï£² 0, x = xÌ
1, x = 1, xÌ = 0
d(x, xÌ) =
ï£³ â, x = 0, xÌ = 1.

PROBLEMS

10.3

Rate distortion for binary source with asymmetric distortion .
p(xÌ|x) and evaluate I (X; XÌ) and D for
 
1
,
X â¼ Bernoulli
2


0 a
.
d(x, xÌ) =
b 0

337

Fix

(The rate distortion function cannot be expressed in closed form.)
10.4

Properties of R(D). Consider a discrete source X â X =
{1, 2, . . . , m} with distribution p1 , p2 , . . . , pm and a distortion
measure d(i, j ). Let R(D) be the rate distortion function for
this source and distortion measure. Let d 	 (i, j ) = d(i, j ) â wi be
a new distortion measure, and let R 	 (D) be the corresponding
rate distortion
function. Show that R 	 (D) = R(D + w), where

w = pi wi , and use this to show that there is no essential loss of
generality in assuming that minxÌ d(i, xÌ) = 0 (i.e., for each x â X,
there is one symbol xÌ that reproduces the source with zero distortion). This result is due to Pinkston [420].

10.5

Rate distortion for uniform source with Hamming distortion.
Consider a source X uniformly distributed on the set {1, 2, . . . , m}.
Find the rate distortion function for this source with Hamming
distortion; that is,

0 if x = xÌ,
d(x, xÌ) =
1 if x = xÌ.

10.6

Shannon lower bound for the rate distortion function. Consider
a source X with a distortion measure d(x, xÌ) that satisï¬es the
following property: All columns of the distortion matrix are permutations of the set {d1 , d2 , . . . , dm }. Deï¬ne the function
Ï(D) =

p:

H (p).
mmax
i=1 pi di â¤D

(10.151)

The Shannon lower bound on the rate distortion function [485]
is proved by the following steps:
(a) Show that Ï(D) is a concave function of D.
(b) Justify the following series of inequalities for I (X; XÌ) if
Ed(X, XÌ) â¤ D,
I (X; XÌ) = H (X) â H (X|XÌ)

(10.152)

338

RATE DISTORTION THEORY

= H (X) â



p(xÌ)H (X|XÌ = xÌ)

(10.153)

p(xÌ)Ï(DxÌ )

(10.154)

xÌ

â¥ H (X) â


xÌ

	

â¥ H (X) â Ï





p(xÌ)DxÌ

(10.155)

xÌ

â¥ H (X) â Ï(D),
where DxÌ =
(c) Argue that


x

(10.156)

p(x|xÌ)d(x, xÌ).

R(D) â¥ H (X) â Ï(D),

(10.157)

which is the Shannon lower bound on the rate distortion function.
(d) If, in addition, we assume that the source has a uniform
distribution and that the rows of the distortion matrix are permutations of each other, then R(D) = H (X) â Ï(D) (i.e., the
lower bound is tight).
10.7

Erasure distortion. Consider X â¼ Bernoulli ( 12 ), and let the distortion measure be given by the matrix

d(x, xÌ) =


0 1 â
.
â 1 0

(10.158)

Calculate the rate distortion function for this source. Can you
suggest a simple scheme to achieve any value of the rate distortion
function for this source?
10.8

Bounds on the rate distortion function for squared-error distortion.
For the case of a continuous random variable X with mean zero
and variance Ï 2 and squared-error distortion, show that
h(X) â

1
1
Ï2
log(2Ï eD) â¤ R(D) â¤ log .
2
2
D

(10.159)

For the upper bound, consider the following joint distribution:

PROBLEMS

Z~

0,

Ds 2
s2 â D

s2 â D
s2

^

X
^

X =

339

X
s2 â D
s2

(X + Z )

Are Gaussian random variables harder or easier to describe than
other random variables with the same variance?
10.9

Properties of optimal rate distortion code. A good (R, D) rate
distortion code with R â R(D) puts severe constraints on the relationship of the source X n and the representations XÌ n . Examine the
chain of inequalities (10.58â10.71) considering the conditions for
equality and interpret as properties of a good code. For example,
equality in (10.59) implies that XÌ n is a deterministic function
of X n .

10.10

Rate distortion. Find and verify the rate distortion function R(D)
for X uniform on X = {1, 2, . . . , 2m} and

1 for x â xÌ odd,
d(x, xÌ) =
0 for x â xÌ even,
where XÌ is deï¬ned on XÌ = {1, 2, . . . , 2m}. (You may wish to use
the Shannon lower bound in your argument.)

10.11

Lower bound .

Let
X â¼ â

eâx

ââ e

4

âx 4 dx

and


x 4 eâx dx
= c.

4
eâx dx
4

Deï¬ne g(a) = max h(X) over all densities such that EX 4 â¤ a.
Let R(D) be the rate distortion function for X with the density
above and with distortion criterion d(x, xÌ) = (x â xÌ)4 . Show that
R(D) â¥ g(c) â g(D).

340

RATE DISTORTION THEORY

10.12

Adding a column to the distortion matrix . Let R(D) be the rate
distortion function for an i.i.d. process with probability mass function p(x) and distortion function d(x, xÌ), x â X, xÌ â XÌ. Now
suppose that we add a new reproduction symbol xÌ0 to XÌ with associated distortion d(x, xÌ0 ), x â X. Does this increase or decrease
R(D), and why?

10.13

Simpliï¬cation. Suppose that X = {1, 2, 3, 4}, XÌ = {1, 2, 3, 4},
p(i) = 14 , i = 1, 2, 3, 4, and X1 , X2 , . . . are i.i.d. â¼ p(x). The
distortion matrix d(x, xÌ) is given by

1
2
3
4

1
0
0
1
1

2
0
0
1
1

3
1
1
0
0

4
1
1
0
0

(a) Find R(0), the rate necessary to describe the process with
zero distortion.
(b) Find the rate distortion function R(D). There are some irrelevant distinctions in alphabets X and XÌ, which allow the
problem to be collapsed.
(c) Suppose that we have a nonuniform distribution p(i) = pi ,
i = 1, 2, 3, 4. What is R(D)?
10.14

Rate distortion for two independent sources. Can one compress
two independent sources simultaneously better than by compressing the sources individually? The following problem addresses this
question. Let {Xi } be i.i.d. â¼ p(x) with distortion d(x, xÌ) and rate
distortion function RX (D). Similarly, let {Yi } be i.i.d. â¼ p(y) with
distortion d(y, yÌ) and rate distortion function RY (D). Suppose we
now wish to describe the process {(Xi , Yi )} subject to distortions
Ed(X, XÌ) â¤ D1 and Ed(Y, YÌ ) â¤ D2 . Thus, a rate RX,Y (D1 , D2 )
is sufï¬cient, where
RX,Y (D1 , D2 ) =

min
p(xÌ,yÌ|x,y):Ed(X,XÌ)â¤D1 ,Ed(Y,YÌ )â¤D2

I (X, Y ; XÌ, YÌ ).

Now suppose that the {Xi } process and the {Yi } process are independent of each other.
(a) Show that
RX,Y (D1 , D2 ) â¥ RX (D1 ) + RY (D2 ).

PROBLEMS

341

(b) Does equality hold?
Now answer the question.
10.15

Distortion rate function.
D(R) =

Let
Ed(X, XÌ)

min

(10.160)

p(xÌ|x):I (X;XÌ)â¤R

be the distortion rate function.
(a) Is D(R) increasing or decreasing in R?
(b) Is D(R) convex or concave in R?
(c) Converse for distortion rate functions: We now wish to prove
the converse by focusing on D(R). Let X1 , X2 , . . . , Xn be
i.i.d. â¼ p(x). Suppose that one is given a (2nR , n) rate distortion code X n â i(X n ) â XÌ n (i(X n )), with i(X n ) â 2nR ,
and suppose that the resulting distortion is D = Ed(X n , XÌ n
(i(X n ))). We must show that D â¥ D(R). Give reasons for the
following steps in the proof:
D = Ed(X n , XÌ n (i(X n )))

(10.161)

1
=E
d(Xi , XÌi )
n

(10.162)

1
Ed(Xi , XÌi )
n

(10.163)

n

(a)

i=1

(b)

=

n

i=1

1 
D I (Xi ; XÌi )
n
i=1
	 n


(d)
1
â¥D
I (Xi ; XÌi )
n
i=1


(e)
1
n
n
I (X ; XÌ )
â¥D
n
(c)

n

â¥

(f)

â¥ D(R).

10.16

(10.164)

(10.165)
(10.166)
(10.167)

Probability of conditionally typical sequences. In Chapter 7 we
calculated the probability that two independently drawn sequences
X n and Y n are weakly jointly typical. To prove the rate distortion theorem, however, we need to calculate this probability when

342

RATE DISTORTION THEORY

one of the sequences is ï¬xed and the other is random. The techniques of weak typicality allow us only to calculate the average
set size of the conditionally typical set. Using the ideas of strong
typicality, on the other hand, provides us with stronger bounds
that work for all typical x n sequences. We outline the proof that
ânI (X;Y )
Pr{(x n , Y n ) â Aâ(n)
for all typical x n . This approach
 }â2
was introduced by Berger [53] and is fully developed in the book
by CsiszaÌr and KoÌrner [149].
Let (Xi , Yi ) be drawn i.i.d. â¼ p(x, y). Let the marginals of X and
Y be p(x) and p(y), respectively.
(a) Let Aâ(n)
be the strongly typical set for X. Show that

nH (X)
|Aâ(n)
.
 |=2
.

(10.168)

(Hint: Theorems 11.1.1 and 11.1.3.)
(b) The joint type of a pair of sequences (x n , y n ) is the proportion
of times (xi , yi ) = (a, b) in the pair of sequences:
1
1
px n ,y n (a, b) = N (a, b|x n , y n ) =
I (xi = a, yi = b).
n
n
n

i=1

(10.169)
The conditional type of a sequence y n given x n is a stochastic
matrix that gives the proportion of times a particular element
of Y occurred with each element of X in the pair of sequences.
Speciï¬cally, the conditional type Vy n |x n (b|a) is deï¬ned as
Vy n |x n (b|a) =

N (a, b|x n , y n )
.
N (a|x n )

(10.170)

Show that the number of conditional types is bounded by (n +
1)|X ||Y | .
(c) The set of sequences y n â Y n with conditional type V with
respect to a sequence x n is called the conditional type class
TV (x n ). Show that
1
2nH (Y |X) â¤ |TV (x n )| â¤ 2nH (Y |X) .
(n + 1)|X ||Y |

(10.171)

(d) The sequence y n â Y n is said to be -strongly conditionally
typical with the sequence x n with respect to the conditional
distribution V (Â·|Â·) if the conditional type is close to V . The
conditional type should satisfy the following two conditions:

PROBLEMS

343

(i) For all (a, b) â X Ã Y with V (b|a) > 0,

1 

.
N (a, b|x n , y n ) â V (b|a)N(a|x n ) â¤
n
|Y| + 1
(10.172)
(ii) N (a, b|x n , y n ) = 0 for all (a, b) such that V (b|a) = 0.
The set of such sequences is called the conditionally typin
cal set and is denoted Aâ(n)
 (Y |x ). Show that the number
n
of sequences y that are conditionally typical with a given
x n â X n is bounded by
1
n
2n(H (Y |X)â1 ) â¤ |Aâ(n)
 (Y |x )|
|
X
||
Y
|
(n + 1)
â¤ (n + 1)|X ||Y | 2n(H (Y |X)+1 ) , (10.173)
where 1 â 0 as  â 0.
(e) For a pair of random variables (X, Y ) with joint distribution
is the set of sequences
p(x, y), the -strongly typical set Aâ(n)

(x n , y n ) â X n Ã Y n satisfying
(i)


1

 N (a, b|x n , y n ) â p(a, b) < 
(10.174)
n
 |X||Y|
for every pair (a, b) â X Ã Y with p(a, b) > 0.
(ii) N (a, b|x n , y n ) = 0 for all (a, b) â X Ã Y with
p(a, b) = 0.
The set of -strongly jointly typical sequences is called the
-strongly jointly typical set and is denoted Aâ(n)
 (X, Y ). Let
(X, Y ) be drawn i.i.d. â¼ p(x, y). For any x n such that there
exists at least one pair (x n , y n ) â Aâ(n)
 (X, Y ), the set of sequences y n such that (x n , y n ) â Aâ(n)
satisï¬es

1
2n(H (Y |X)âÎ´()) â¤ |{y n : (x n , y n ) â Aâ(n)
 }|
(n + 1)|X ||Y |
â¤ (n + 1)|X ||Y | 2n(H (Y |X)+Î´()) ,

(10.175)

where Î´() â 0 as  â 0. In particular, we can write
n(H (Y |X)+2 )
2n(H (Y |X)â2 ) â¤ |{y n : (x n , y n ) â Aâ(n)
,
 }| â¤ 2
(10.176)

344

RATE DISTORTION THEORY

where we can make 2 arbitrarily small with an appropriate
choice of  and n.

(f) Let Y1 , Y2 , . . . , Yn be drawn i.i.d. â¼ p(yi ). For x n â Aâ(n)
 ,
n
n
â(n)
the probability that (x , Y ) â A is bounded by
ân(I (X;Y )â3 )
,
2ân(I (X;Y )+3 ) â¤ Pr((x n , Y n ) â Aâ(n)
 )â¤2
(10.177)

where 3 goes to 0 as  â 0 and n â â.
10.17

Sourceâchannel separation theorem with distortion. Let V1 ,
V2 , . . . , Vn be a ï¬nite alphabet i.i.d. source which is encoded
as a sequence of n input symbols X n of a discrete memoryless
channel. The output of the channel Y n is mapped onto the recon
struction alphabet VÌ n = g(Y n ). Let D = Ed(V n , VÌ n ) = n1 ni=1
Ed(Vi , VÌi ) be the average distortion achieved by this combined
source and channel coding scheme.

Vn

X n(V n)

Channel Capacity C

Yn

^

Vn

(a) Show that if C > R(D), where R(D) is the rate distortion
function for V , it is possible to ï¬nd encoders and decoders
that achieve a average distortion arbitrarily close to D.
(b) (Converse) Show that if the average distortion is equal to D,
the capacity of the channel C must be greater than R(D).
10.18

Rate distortion. Let d(x, xÌ) be a distortion function. We have
a source X â¼ p(x). Let R(D) be the associated rate distortion
function.
(a) Find RÌ(D) in terms of R(D), where RÌ(D) is the rate
distortion function associated with the distortion dÌ(x, xÌ) =
d(x, xÌ) + a for some constant a > 0. (They are not equal.)
(b) Now suppose that d(x, xÌ) â¥ 0 for all x, xÌ and deï¬ne a new
distortion function d â (x, xÌ) = bd(x, xÌ), where b is some number â¥ 0. Find the associated rate distortion function R â (D) in
terms of R(D).
(c) Let X â¼ N (0, Ï 2 ) and d(x, xÌ) = 5(x â xÌ)2 + 3. What is
R(D)?

HISTORICAL NOTES

10.19

345

Rate distortion with two constraints. Let Xi be iid â¼ p(x). We
are given two distortion functions, d1 (x, xÌ) and d2 (x, xÌ). We
wish to describe X n at rate R and reconstruct it with distortions
Ed1 (X n , XÌ1n ) â¤ D1 , and Ed2 (X n , XÌ2n ) â¤ D2 , as shown here:
X n ââ i(X n ) ââ (XÌ1n (i), XÌ2n (i))
D1 = Ed1 (X1n , XÌ1n )
D2 = Ed2 (X1n , XÌ2n ).
Here i(Â·) takes on 2nR values. What is the rate distortion function
R(D1 , D2 )?

10.20

Rate distortion. Consider the standard rate distortion problem,
Xi i.i.d. â¼ p(x), X n â i(X n ) â XÌ n , |i(Â·)| = 2nR . Consider two
distortion criteria d1 (x, xÌ) and d2 (x, xÌ). Suppose that d1 (x, xÌ) â¤
d2 (x, xÌ) for all x â X, xÌ â XÌ. Let R1 (D) and R2 (D) be the corresponding rate distortion functions.
(a) Find the inequality relationship between R1 (D) and R2 (D).
(b) Suppose that we must describe the source {Xi } at the minimum rate R achieving d1 (X n , XÌ1n ) â¤ D and d2 (X n , XÌ2n ) â¤ D.
Thus,
 n
XÌ1 (i(X n ))
n
n
X â i(X ) â
XÌ2n (i(X n ))
and |i(Â·)| = 2nR .
Find the minimum rate R.

HISTORICAL NOTES
The idea of rate distortion was introduced by Shannon in his original
paper [472]. He returned to it and dealt with it exhaustively in his 1959
paper [485], which proved the ï¬rst rate distortion theorem. Meanwhile,
Kolmogorov and his school in the Soviet Union began to develop rate
distortion theory in 1956. Stronger versions of the rate distortion theorem
have been proved for more general sources in the comprehensive book by
Berger [52].
The inverse water-ï¬lling solution for the rate distortion function for
parallel Gaussian sources was established by McDonald and Schultheiss

346

RATE DISTORTION THEORY

[381]. An iterative algorithm for the calculation of the rate distortion
function for a general i.i.d. source and arbitrary distortion measure was
described by Blahut [65], Arimoto [25], and CsiszaÌr [139]. This algorithm
is a special case of a general alternating minimization algorithm due to
CsiszaÌr and TusnaÌdy [155].

CHAPTER 11

INFORMATION THEORY
AND STATISTICS

We now explore the relationship between information theory and statistics.
We begin by describing the method of types, which is a powerful technique
in large deviation theory. We use the method of types to calculate the
probability of rare events and to show the existence of universal source
codes. We also consider the problem of testing hypotheses and derive the
best possible error exponents for such tests (the ChernoffâStein lemma).
Finally, we treat the estimation of the parameters of a distribution and
describe the role of Fisher information.
11.1

METHOD OF TYPES

The AEP for discrete random variables (Chapter 3) focuses our attention
on a small subset of typical sequences. The method of types is an even
more powerful procedure in which we consider sequences that have the
same empirical distribution. With this restriction, we can derive strong
bounds on the number of sequences with a particular empirical distribution
and the probability of each sequence in this set. It is then possible to derive
strong error bounds for the channel coding theorem and prove a variety
of rate distortion results. The method of types was fully developed by
CsiszaÌr and KoÌrner [149], who obtained most of their results from this
point of view.
Let X1 , X2 , . . . , Xn be a sequence of n symbols from an alphabet X =
{a1 , a2 , . . . , a|X | }. We use the notation x n and x interchangeably to denote
a sequence x1 , x2 , . . . , xn .
Deï¬nition The type Px (or empirical probability distribution) of a sequence x1 , x2 , . . . , xn is the relative proportion of occurrences of each
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

347

348

INFORMATION THEORY AND STATISTICS

symbol of X (i.e., Px (a) = N (a|x)/n for all a â X, where N (a|x) is the
number of times the symbol a occurs in the sequence x â X n ).
The type of a sequence x is denoted as Px . It is a probability mass
function on X. (Note that in this chapter, we will use capital letters to
denote types and distributions. We also loosely use the word distribution
to mean a probability mass function.)
Deï¬nition The probability simplex inRm is the set of points x =
(x1 , x2 , . . . , xm ) â Rm such that xi â¥ 0, m
i=1 xi = 1.
The probability simplex is an (m â 1)-dimensional manifold in
m-dimensional space. When m = 3, the probability simplex is the
set of points {(x1 , x2 , x3 ) : x1 â¥ 0, x2 â¥ 0, x3 â¥ 0, x1 + x2 + x3 = 1}
(Figure 11.1). Since this is a triangular two-dimensional ï¬at in R3 , we
use a triangle to represent the probability simplex in later sections of this
chapter.
Deï¬nition Let Pn denote the set of types with denominator n.
For example, if X = {0, 1}, the set of possible types with denominator
n is


 



0 n
1 nâ1
n 0
Pn = (P (0), P (1)) :
,
,
,
,...,
,
. (11.1)
n n
n
n
n n
Deï¬nition If P â Pn , the set of sequences of length n and type P is
called the type class of P , denoted T (P ):
T (P ) = {x â X n : Px = P }.

(11.2)

The type class is sometimes called the composition class of P .
x2

1

x1 + x2 + x3 = 1

x1
1
1

x3

FIGURE 11.1. Probability simplex in R3 .

11.1

METHOD OF TYPES

349

Example 11.1.1 Let X = {1, 2, 3}, a ternary alphabet. Let x = 11321.
Then the type Px is
3
Px (1) = ,
5

1
Px (2) = ,
5

1
Px (3) = .
5

(11.3)

The type class of Px is the set of all sequences of length 5 with three 1âs,
one 2, and one 3. There are 20 such sequences, and
T (Px ) = {11123, 11132, 11213, . . . , 32111}.
The number of elements in T (P ) is


5
5!
|T (P )| =
=
= 20.
3, 1, 1
3! 1! 1!

(11.4)

(11.5)

The essential power of the method of types arises from the following
theorem, which shows that the number of types is at most polynomial
in n.
Theorem 11.1.1
|Pn | â¤ (n + 1)|X | .

(11.6)

Proof: There are |X| components in the vector that speciï¬es Px . The
numerator in each component can take on only n + 1 values. So there are
at most (n + 1)|X | choices for the type vector. Of course, these choices
are not independent (e.g., the last choice is ï¬xed by the others). But this

is a sufï¬ciently good upper bound for our needs.
The crucial point here is that there are only a polynomial number of
types of length n. Since the number of sequences is exponential in n, it
follows that at least one type has exponentially many sequences in its
type class. In fact, the largest type class has essentially the same number
of elements as the entire set of sequences, to ï¬rst order in the exponent.
Now, we assume that the sequence X1 , X2 , . . . , Xn is drawn i.i.d.
according to a distribution Q(x). All sequences with the same type have
n n
the
n same probability, as shown in the following theorem. Let Q (x ) =
i=1 Q(xi ) denote the product distribution associated with Q.
Theorem 11.1.2 If X1 , X2 , . . . , Xn are drawn i.i.d. according to Q(x),
the probability of x depends only on its type and is given by
Qn (x) = 2ân(H (Px )+D(Px ||Q)) .

(11.7)

350

INFORMATION THEORY AND STATISTICS

Proof
Q (x) =
n

n


Q(xi )

(11.8)

Q(a)N (a|x)

(11.9)

Q(a)nPx (a)

(11.10)

2nPx (a) log Q(a)

(11.11)

2n(Px (a) log Q(a)âPx (a) log Px (a)+Px (a) log Px (a))

(11.12)

i=1

=



aâX

=



aâX

=



aâX

=



aâX

x (a) +P (a) log P (a))
n aâX (âPx (a) log PQ(a)
x
x

Corollary

=2

(11.13)

= 2n(âD(Px ||Q)âH (Px )) . 

(11.14)

If x is in the type class of Q, then
Qn (x) = 2ânH (Q) .

(11.15)

Proof: If x â T (Q), then Px = Q, which can be substituted into (11.14).

Example 11.1.2 The probability that a fair die produces a particular
sequence of length n with precisely n/6 occurrences of each face (n is a
1 1
1
multiple of 6) is 2ânH ( 6 , 6 ,..., 6 ) = 6ân . This is obvious. However, if the
1 1
, 12 , 0), the probability of
die has a probability mass function ( 13 , 13 , 16 , 12
observing a particular sequence with precisely these frequencies is pre1 1 1 1 1
cisely 2ânH ( 3 , 3 , 6 , 12 , 12 ,0) for n a multiple of 12. This is more interesting.
We now give an estimate of the size of a type class T (P ).
Theorem 11.1.3

(Size of a type class T (P )) For any type P â Pn ,
1
2nH (P ) â¤ |T (P )| â¤ 2nH (P ) .
(n + 1)|X |

(11.16)

Proof: The exact size of T (P ) is easy to calculate. It is a simple combinatorial problemâthe number of ways of arranging nP (a1 ), nP (a2 ), . . . ,

11.1

METHOD OF TYPES

nP (a|X | ) objects in a sequence, which is


n
.
|T (P )| =
nP (a1 ), nP (a2 ), . . . , nP (a|X | )

351

(11.17)

This value is hard to manipulate, so we derive simple exponential bounds
on its value.
We suggest two alternative proofs for the exponential bounds. The ï¬rst
proof uses Stirlingâs formula [208] to bound the factorial function, and
after some algebra, we can obtain the bounds of the theorem. We give an
alternative proof. We ï¬rst prove the upper bound. Since a type class must
have probability â¤ 1, we have
1 â¥ P n (T (P ))

=
P n (x)

(11.18)
(11.19)

xâT (P )

=



2ânH (P )

(11.20)

xâT (P )

= |T (P )|2ânH (P ) ,

(11.21)

using Theorem 11.1.2. Thus,
|T (P )| â¤ 2nH (P ) .

(11.22)

Now for the lower bound. We ï¬rst prove that the type class T (P )
has the highest probability among all type classes under the probability
distribution P :
P n (T (P )) â¥ P n (T (PÌ )) for all PÌ â Pn .
We lower bound the ratio of probabilities,

P n (T (P )) |T (P )| aâX P (a)nP (a)
=

P n (T (PÌ )) |T (PÌ )| aâX P (a)nPÌ (a)


	
n
=

=

aâX
nP (a1 ), nP (a2 ),...,nP (a|X | )
	


n
aâX
nPÌ (a1 ), nPÌ (a2 ),...,nPÌ (a|X | )

(11.24)
P (a)nP (a)
P (a)nPÌ (a)

 (nPÌ (a))!
P (a)n(P (a)âPÌ (a)) .
(nP (a))!

aâX

(11.23)

(11.25)

(11.26)

352

INFORMATION THEORY AND STATISTICS

Now using the simple bound (easy to prove by separately considering the
cases m â¥ n and m < n)
m!
â¥ nmân ,
n!

(11.27)

we obtain
P n (T (P ))
P n (T (PÌ ))

â¥



(nP (a))nPÌ (a)ânP (a) P (a)n(P (a)âPÌ (a))

(11.28)

nn(PÌ (a)âP (a))

(11.29)

aâX

=



aâX


n( aâX PË (a)â aâX P (a))

=n
=n

n(1â1)

(11.30)
(11.31)

= 1.

(11.32)

Hence, P n (T (P )) â¥ P n (T (PÌ )). The lower bound now follows easily
from this result, since

P n (T (Q))
(11.33)
1=
QâPn

â¤



max P n (T (Q))

(11.34)

P n (T (P ))

(11.35)

â¤ (n + 1)|X | P n (T (P ))

= (n + 1)|X |
P n (x)

(11.36)

QâPn

=



Q

QâPn

(11.37)

xâT (P )

= (n + 1)|X |



2ânH (P )

(11.38)

xâT (P )

= (n + 1)|X | |T (P )|2ânH (P ) ,

(11.39)

where (11.36) follows from Theorem 11.1.1 and (11.38) follows from

Theorem 11.1.2.
We give a slightly better approximation for the binary case.

11.1

METHOD OF TYPES

353

Example 11.1.3 (Binary alphabet) In this case, the type is deï¬ned
by the number
	 
 of 1âs in the sequence, and the size of the type class is
therefore nk . We show that
 
 
 
k
1 nH nk
n
nH n
2
â¤
â¤2
.
(11.40)
n+1
k
These bounds can be proved using Stirlingâs approximation for the factorial function (Lemma 17.5.1). But we provide a more intuitive proof
below.
We ï¬rst prove the upper bound. From the binomial formula, for any p,
n  

n k
p (1 â p)nâk = 1.
k

(11.41)

k=0

Since all the terms of the sum are positive for 0 â¤ p â¤ 1, each of the
terms is less than 1. Setting p = k/n and taking the kth term, we get

   k 
k
k nâk
n
1â¥
1â
n
n
k
 
n k log k +(nâk) log nâk
n
n
=
2
k
  

nâk
n n nk log nk + nâk
n log n
2
=
k
 
 
n ânH nk
.
=
2
k

(11.42)
(11.43)
(11.44)
(11.45)

Hence,
 
 
n
nH nk
.
â¤2
k

(11.46)

For the lower bound, let S be a random variable with a binomial
distribution with parameters n and p. The most likely value of S is
S = np. This can easily be veriï¬ed from the fact that
P (S = i + 1)
nâi p
=
P (S = i)
i+11âp

(11.47)

and considering the cases when i < np and when i > np. Then, since
there are n + 1 terms in the binomial sum,

354

INFORMATION THEORY AND STATISTICS

 
n  

n k
n k
nâk
p (1 â p)
p (1 â p)nâk (11.48)
1=
â¤ (n + 1) max
k
k
k
k=0


n
p np (1 â p)nânp .
= (n + 1)
(11.49)
np
Now let p = k/n. Then we have
   k 

n
k
k nâk
1 â¤ (n + 1)
,
1â
k
n
n
which by the arguments in (11.45) is equivalent to
 
 
1
n ânH nk
2
,
â¤
n+1
k
or
 
 
nH nk
n
2
.
â¥
n+1
k
Combining the two results, we see that
 
 
n . nH nk
=2
.
k

(11.50)

(11.51)

(11.52)

(11.53)

A more precise bound can be found in theorem 17.5.1 when k = 0 or n.
Theorem 11.1.4 (Probability of type class) for any P â Pn and any
distribution Q, the probability of the type class T (P ) under Qn is 2ânD(P ||Q)
to ï¬rst order in the exponent. More precisely,
1
2ânD(P ||Q) â¤ Qn (T (P )) â¤ 2ânD(P ||Q) .
(n + 1)|X |
Proof: We have
Qn (T (P )) =



(11.54)

Qn (x)

(11.55)

2ân(D(P ||Q)+H (P ))

(11.56)

xâT (P )

=



xâT (P )

= |T (P )|2ân(D(P ||Q)+H (P )) ,

(11.57)

11.2

LAW OF LARGE NUMBERS

355

by Theorem 11.1.2. Using the bounds on |T (P )| derived in Theorem
11.1.3, we have
1
2ânD(P ||Q) â¤ Qn (T (P )) â¤ 2ânD(P ||Q) .
(n + 1)|X |



(11.58)

We can summarize the basic theorems concerning types in four equations:
|Pn | â¤ (n + 1)|X | ,
Qn (x) = 2ân(D(Px ||Q)+H (Px )) ,
.

(11.59)
(11.60)

|T (P )| = 2nH (P ) ,

(11.61)

Qn (T (P )) = 2ânD(P ||Q) .

(11.62)

.

These equations state that there are only a polynomial number of types
and that there are an exponential number of sequences of each type. We
also have an exact formula for the probability of any sequence of type P
under distribution Q and an approximate formula for the probability of a
type class.
These equations allow us to calculate the behavior of long sequences
based on the properties of the type of the sequence. For example, for
long sequences drawn i.i.d. according to some distribution, the type of
the sequence is close to the distribution generating the sequence, and we
can use the properties of this distribution to estimate the properties of the
sequence. Some of the applications that will be dealt with in the next few
sections are as follows:
â¢
â¢
â¢
â¢
â¢

11.2

The law of large numbers
Universal source coding
Sanovâs theorem
The ChernoffâStein lemma and hypothesis testing
Conditional probability and limit theorems
LAW OF LARGE NUMBERS

The concept of type and type classes enables us to give an alternative
statement of the law of large numbers. In fact, it can be used as a proof
of a version of the weak law in the discrete case. The most important
property of types is that there are only a polynomial number of types, and

356

INFORMATION THEORY AND STATISTICS

an exponential number of sequences of each type. Since the probability
of each type class depends exponentially on the relative entropy distance
between the type P and the distribution Q, type classes that are far from
the true distribution have exponentially smaller probability.
Given an  > 0, we can deï¬ne a typical set TQ of sequences for the
distribution Qn as
TQ = {x n : D(Px n ||Q) â¤ }.

(11.63)

Then the probability that x n is not typical is


1 â Qn (TQ ) =

Qn (T (P ))

(11.64)

P :D(P ||Q)>



â¤

2ânD(P ||Q)

(Theorem 11.1.4)

(11.65)

P :D(P ||Q)>



â¤

2ân

(11.66)

P :D(P ||Q)>

â¤ (n + 1)|X | 2ân
ân



=2

log(n+1)
â|X |
n

(Theorem 11.1.1)



,

(11.67)
(11.68)

which goes to 0 as n â â. Hence, the probability of the typical set TQ
goes to 1 as n â â. This is similar to the AEP proved in Chapter 3,
which is a form of the weak law of large numbers. We now prove that
the empirical distribution PXn converges to P .
Theorem 11.2.1

Let X1 , X2 , . . . , Xn be i.i.d. â¼ P (x). Then

Pr {D(Px n ||P ) > } â¤ 2ân(â|X |

log(n+1)
)
n

,

(11.69)

and consequently, D(Px n ||P ) â 0 with probability 1.
Proof: The inequality (11.69) was proved in (11.68). Summing over n,
we ï¬nd that
â

n=1

Pr{D(Px n ||P ) > } < â.

(11.70)

11.3

UNIVERSAL SOURCE CODING

357

Thus, the expected number of occurrences of the event {D(Px n ||P ) > }
for all n is ï¬nite, which implies that the actual number of such occurrences is also ï¬nite with probability 1 (BorelâCantelli lemma). Hence

D(Px n ||P ) â 0 with probability 1.
We now deï¬ne a stronger version of typicality than in Chapter 3.
Deï¬nition We deï¬ne the strongly typical set Aâ(n)
to be the set of

sequences in X n for which the sample frequencies are close to the true
values:
ï£¼
ï£±


1


ï£´
ï£´
ï£²
 N (a|x) â P (a) <
, if P (a) > 0 ï£½


â(n)
n
n
|
X
|
.
A = x â X :
ï£´
ï£´
ï£¾
ï£³
N (a|x) = 0
if P (a) = 0
(11.71)
Hence, the typical set consists of sequences whose type does not differ
from the true probabilities by more than /|X| in any component. By the
strong law of large numbers, it follows that the probability of the strongly
typical set goes to 1 as n â â. The additional power afforded by strong
typicality is useful in proving stronger results, particularly in universal
coding, rate distortion theory, and large deviation theory.
11.3

UNIVERSAL SOURCE CODING

Huffman coding compresses an i.i.d. source with a known distribution
p(x) to its entropy limit H (X). However, if the code is designed for
some incorrect distribution q(x), a penalty of D(p||q) is incurred. Thus,
Huffman coding is sensitive to the assumed distribution.
What compression can be achieved if the true distribution p(x) is
unknown? Is there a universal code of rate R, say, that sufï¬ces to describe
every i.i.d. source with entropy H (X) < R? The surprising answer is yes.
The idea is based on the method of types. There are 2nH (P ) sequences of
type P . Since there are only a polynomial number of types with denominator n, an enumeration of all sequences x n with type Px n such that
H (Px n ) < R will require roughly nR bits. Thus, by describing all such
sequences, we are prepared to describe any sequence that is likely to arise
from any distribution Q having entropy H (Q) < R. We begin with a
deï¬nition.
Deï¬nition A ï¬xed-rate block code of rate R for a source X1 , X2 , . . . ,
Xn which has an unknown distribution Q consists of two mappings: the
encoder,
(11.72)
fn : X n â {1, 2, . . . , 2nR },

358

INFORMATION THEORY AND STATISTICS

and the decoder,
Ïn : {1, 2, . . . , 2nR } â X n .

(11.73)

Here R is called the rate of the code. The probability of error for the
code with respect to the distribution Q is
Pe(n) = Qn (X n : Ïn (fn (X n )) = X n )

(11.74)

Deï¬nition A rate R block code for a source will be called universal
if the functions fn and Ïn do not depend on the distribution Q and if
Pe(n) â 0 as n â â if R > H (Q).
We now describe one such universal encoding scheme, due to CsiszaÌr
and KoÌrner [149], that is based on the fact that the number of sequences
of type P increases exponentially with the entropy and the fact that there
are only a polynomial number of types.
Theorem 11.3.1 There exists a sequence of (2nR , n) universal source
codes such that Pe(n) â 0 for every source Q such that H (Q) < R.
Proof: Fix the rate R for the code. Let
Rn = R â |X|

log(n + 1)
.
n

(11.75)

Consider the set of sequences
A = {x â X n : H (Px ) â¤ Rn }.

(11.76)

Then


|A| =

|T (P )|

(11.77)

2nH (P )

(11.78)

2nRn

(11.79)

P âPn :H (P )â¤Rn



â¤

P âPn :H (P )â¤Rn



â¤

P âPn :H (P )â¤Rn

â¤ (n + 1)|X | 2nRn
log(n+1)
n(Rn +|X |
)
n

(11.80)

=2

(11.81)

= 2nR .

(11.82)

11.3

UNIVERSAL SOURCE CODING

359

By indexing the elements of A, we deï¬ne the encoding function fn as

index of x in A
if x â A,
(11.83)
fn (x) =
0
otherwise.
The decoding function maps each index onto the corresponding element
of A. Hence all the elements of A are recovered correctly, and all the
remaining sequences result in an error. The set of sequences that are
recovered correctly is illustrated in Figure 11.2.
We now show that this encoding scheme is universal. Assume that the
distribution of X1 , X2 , . . . , Xn is Q and H (Q) < R. Then the probability
of decoding error is given by
Pe(n) = 1 â Qn (A)

Qn (T (P ))
=

(11.84)
(11.85)

P :H (P )>Rn

â¤ (n + 1)|X |

max

P :H (P )>Rn

Qn (T (P ))

â¤ (n + 1)|X | 2ân minP :H (P )>Rn D(P ||Q) .

(11.86)
(11.87)

Since Rn â R and H (Q) < R, there exists n0 such that for all n â¥ n0 ,
Rn > H (Q). Then for n â¥ n0 , minP :H (P )>Rn D(P ||Q) must be greater
than 0, and the probability of error Pe(n) converges to 0 exponentially fast
as n â â.

A

H(P ) = R

FIGURE 11.2. Universal code and the probability simplex. Each sequence with type that
lies outside the circle is encoded by its index. There are fewer than 2nR such sequences.
Sequences with types within the circle are encoded by 0.

INFORMATION THEORY AND STATISTICS

Error exponent

360

H(Q)

Rate of code

FIGURE 11.3. Error exponent for the universal code.

On the other hand, if the distribution Q is such that the entropy H (Q)
is greater than the rate R, then with high probability the sequence will
have a type outside the set A. Hence, in such cases the probability of
error is close to 1.
The exponent in the probability of error is
â
DR,Q
=

min

P :H (P )>R

which is illustrated in Figure 11.3.

D(P ||Q),

(11.88)


The universal coding scheme described here is only one of many such
schemes. It is universal over the set of i.i.d. distributions. There are other
schemes, such as the LempelâZiv algorithm, which is a variable-rate universal code for all ergodic sources. The LempelâZiv algorithm, discussed
in Section 13.4, is often used in practice to compress data that cannot be
modeled simply, such as English text or computer source code.
One may wonder why it is ever necessary to use Huffman codes, which
are speciï¬c to a probability distribution. What do we lose in using a
universal code? Universal codes need a longer block length to obtain
the same performance as a code designed speciï¬cally for the probability
distribution. We pay the penalty for this increase in block length by the
increased complexity of the encoder and decoder. Hence, a distribution
speciï¬c code is best if one knows the distribution of the source.
11.4

LARGE DEVIATION THEORY

The subject of large deviationtheory can be illustrated by an example.
What is the probability that n1
Xi is near 13 if X1 , X2 , . . . , Xn are drawn
i.i.d. Bernoulli( 13 )? This is a small deviation (from the expected outcome)

11.4 LARGE DEVIATION THEORY

361


and the probability is near 1. Now what is the probability that n1
Xi
3
1
is greater than 4 given that X1 , X2 , . . . , Xn are Bernoulli( 3 )? This is
a large deviation, and the probability is exponentially small. We might
estimate the exponent using the central limit theorem, but this is a poor
approximation
for more than a few standard deviations. We note that
1 
3
Xi = 4 is equivalent to Px = ( 14 , 34 ). Thus, the probability that Xn is
n
near 34 is the probability that type PX is near ( 34 , 14 ). The probability of
3 1
1 2
this large deviation will turn out to be â 2ânD(( 4 , 4 )||( 3 , 3 )) . In this section
we estimate the probability of a set of nontypical types.
Let E be a subset of the set of probability mass functions. For example,
E may be the set of probability mass functions with mean Âµ. With a slight
abuse of notation, we write

Qn (x).
(11.89)
Qn (E) = Qn (E â© Pn ) =
x:Px âEâ©Pn

If E contains a relative entropy neighborhood of Q, then by the weak
law of large numbers (Theorem 11.2.1), Qn (E) â 1. On the other hand,
if E does not contain Q or a neighborhood of Q, then by the weak law
of large numbers, Qn (E) â 0 exponentially fast. We will use the method
of types to calculate the exponent.
Let us ï¬rst give some examples of the kinds of sets E that we are
considering. For example, assume that by observation we
ï¬nd that the
sample average of g(X) is greater than or equal to Î± [i.e., n1 i g(xi ) â¥ Î±].
This event is equivalent to the event PX â E â© Pn , where



E= P :
g(a)P (a) â¥ Î± ,
(11.90)
aâX

because

1
g(xi ) â¥ Î± â
PX (a)g(a) â¥ Î±
n
n

(11.91)

aâX

i=1

â PX â E â© Pn .

(11.92)

Thus,


1
g(Xi ) â¥ Î±
Pr
n
n

i=1


= Qn (E â© Pn ) = Qn (E).

(11.93)

362

INFORMATION THEORY AND STATISTICS

E

P*

Q

FIGURE 11.4. Probability simplex and Sanovâs theorem.

Here E is a half space in the space of probability vectors, as illustrated
in Figure 11.4.
Theorem 11.4.1 (Sanovâs theorem) Let X1 , X2 , . . . , Xn be i.i.d.
â¼ Q(x). Let E â P be a set of probability distributions. Then
Qn (E) = Qn (E â© Pn ) â¤ (n + 1)|X | 2ânD(P

â ||Q)

,

(11.94)

where
P â = arg min D(P ||Q)
P âE

(11.95)

is the distribution in E that is closest to Q in relative entropy.
If, in addition, the set E is the closure of its interior, then
1
log Qn (E) â âD(P â ||Q).
n

(11.96)

Proof: We ï¬rst prove the upper bound:
Qn (E) =



Qn (T (P ))

(11.97)

2ânD(P ||Q)

(11.98)

P âEâ©Pn

â¤



P âEâ©Pn

363

11.4 LARGE DEVIATION THEORY

â¤


P âEâ©Pn

=



max 2ânD(P ||Q)

(11.99)

2ân minP âEâ©Pn D(P ||Q)

(11.100)

2ân minP âE D(P ||Q)

(11.101)

P âEâ©Pn

P âEâ©Pn

â¤


P âEâ©Pn

=



2ânD(P

â ||Q)

(11.102)

P âEâ©Pn

â¤ (n + 1)|X | 2ânD(P

â ||Q)

,

(11.103)

where the last inequality follows from Theorem 11.1.1. Note that P â need
not be a member of Pn . We now come to the lower bound, for which we
need a âniceâ set E, so that for all large n, we can ï¬nd a distribution in
E â© Pn that is close to P â . If we now assume that E is the closure of its
interior (thus, the interior must be nonempty), then since âªn Pn is dense
in the set of all distributions, it follows that E â© Pn is nonempty for all
n â¥ n0 for some n0 . We can then ï¬nd a sequence of distributions Pn such
that Pn â E â© Pn and D(Pn ||Q) â D(P â ||Q). For each n â¥ n0 ,
Qn (E) =



Qn (T (P ))

(11.104)

P âEâ©Pn

â¥ Qn (T (Pn ))
1
â¥
2ânD(Pn ||Q) .
(n + 1)|X |

(11.105)
(11.106)

Consequently,
lim inf



1
|X| log(n + 1)
log Qn (E) â¥ lim inf â
â D(Pn ||Q)
n
n

= âD(P â ||Q).
Combining this with the upper bound establishes the theorem.

(11.107)


This argument can be extended to continuous distributions using quantization.

364

INFORMATION THEORY AND STATISTICS

11.5

EXAMPLES OF SANOVâS THEOREM

Suppose that we wish to ï¬nd Pr{ n1
Then the set E is deï¬ned as

E= P :



n

i=1 gj (Xi )

â¥ Î±j , j = 1, 2, . . . , k}.


P (a)gj (a) â¥ Î±j , j = 1, 2, . . . , k .

(11.108)

a

To ï¬nd the closest distribution in E to Q, we minimize D(P ||Q) subject
to the constraints in (11.108). Using Lagrange multipliers, we construct
the functional
J (P ) =



P (x) log

x


P (x)  
+
Î»i
P (x)gi (x) + Î½
P (x).
Q(x)
x
x
i

(11.109)
We then differentiate and calculate the closest distribution to Q to be of
the form


Q(x)e i Î»i gi (x)

,
P (x) = 
i Î»i gi (a)
aâX Q(a)e
â

(11.110)

where the constants Î»i are chosen to satisfy the constraints. Note that if
Q is uniform, P â is the maximum entropy distribution. Veriï¬cation that
P â is indeed the minimum follows from the same kinds of arguments as
given in Chapter 12.
Let us consider some speciï¬c examples:
Example 11.5.1 (Dice) Suppose that we toss a fair die n times; what
is the probability that the average of the throws is greater than or equal
to 4? From Sanovâs theorem, it follows that
Qn (E) = 2ânD(P
.

â ||Q)

,

(11.111)

where P â minimizes D(P ||Q) over all distributions P that satisfy
6

i=1

iP (i) â¥ 4.

(11.112)

11.5 EXAMPLES OF SANOVâS THEOREM

365

From (11.110), it follows that P â has the form
2Î»x
P â (x) = 6

Î»i
i=1 2

,

(11.113)

 â
with Î» chosen so that
iP (i) = 4. Solving numerically, we obtain
Î» = 0.2519, P â = (0.1031, 0.1227, 0.1461, 0.1740, 0.2072, 0.2468), and
therefore D(P â ||Q) = 0.0624 bit. Thus, the probability that the average
of 10000 throws is greater than or equal to 4 is â 2â624 .
Example 11.5.2 (C oins) Suppose that we have a fair coin and want
to estimate the probability of observing more than 700 heads in a series
of 1000 tosses. The problem is like Example 11.5.1. The probability is
P (Xn â¥ 0.7) = 2ânD(P
.

â ||Q)

,

(11.114)

where P â is the (0.7, 0.3) distribution and Q is the (0.5, 0.5) distribution.
In this case, D(P â ||Q) = 1 â H (P â ) = 1 â H (0.7) = 0.119. Thus, the
probability of 700 or more heads in 1000 trials is approximately 2â119 .
Example 11.5.3 (Mutual dependence) Let Q(x, y) be a given joint
distribution and let Q0 (x, y) = Q(x)Q(y) be the associated product distribution formed from the marginals of Q. We wish to know the likelihood
that a sample drawn according to Q0 will âappearâ to be jointly distributed according to Q. Accordingly, let (Xi , Yi ) be i.i.d. â¼ Q0 (x, y) =
Q(x)Q(y). We deï¬ne joint typicality as we did in Section 7.6; that is,
(x n , y n ) is jointly typical with respect to a joint distribution Q(x, y) iff
the sample entropies are close to their true values:


 1

n
â log Q(x ) â H (X) â¤ ,
(11.115)
 n


and




 1
â log Q(y n ) â H (Y ) â¤ ,

 n

(11.116)




 1
â log Q(x n , y n ) â H (X, Y ) â¤ .

 n

(11.117)

We wish to calculate the probability (under the product distribution) of
seeing a pair (x n , y n ) that looks jointly typical of Q [i.e., (x n , y n )

366

INFORMATION THEORY AND STATISTICS

satisï¬es (11.115)â(11.117)]. Thus, (x n , y n ) are jointly typical with respect
to Q(x, y) if Px n ,y n â E â© Pn (X, Y ), where


 



E = {P (x, y) : â
P (x, y) log Q(x) â H (X) â¤ ,
 x,y




 


P (x, y) log Q(y) â H (Y ) â¤ ,
â

 x,y



 


P (x, y) log Q(x, y) â H (X, Y ) â¤ }. (11.118)
â

 x,y
Using Sanovâs theorem, the probability is
Qn0 (E) = 2ânD(P
.

â ||Q

0)

,

(11.119)

where P â is the distribution satisfying the constraints that is closest to
Q0 in relative entropy. In this case, as  â 0, it can be veriï¬ed (Problem 11.10) that P â is the joint distribution Q, and Q0 is the product
distribution, so that the probability is 2ânD(Q(x,y)||Q(x)Q(y)) = 2ânI (X;Y ) ,
which is the same as the result derived in Chapter 7 for the joint AEP.
In the next section we consider the empirical distribution of the sequence
of outcomes given that the type is in a particular set of distributions E. We
will show that not only is the probability of the set E essentially determined
by D(P â ||Q), the distance of the closest element of E to Q, but also that
the conditional type is essentially P â , so that given that we are in set E, the
type is very likely to be close to P â .
11.6

CONDITIONAL LIMIT THEOREM

It has been shown that the probability of a set of types under a distribution
Q is determined essentially by the probability of the closest element of
â
the set to Q; the probability is 2ânD to ï¬rst order in the exponent, where
D â = min D(P ||Q).
P âE

(11.120)

This follows because the probability of the set of types is the sum of the
probabilities of each type, which is bounded by the largest term times the

11.6 CONDITIONAL LIMIT THEOREM

367

E
P

P*
Q

FIGURE 11.5. Pythagorean theorem for relative entropy.

number of terms. Since the number of terms is polynomial in the length
of the sequences, the sum is equal to the largest term to ï¬rst order in the
exponent.
We now strengthen the argument to show that not only is the probability of the set E essentially the same as the probability of the closest
type P â but also that the total probability of other types that are far
away from P â is negligible. This implies that with very high probability, the type observed is close to P â . We call this a conditional limit
theorem.
Before we prove this result, we prove a âPythagoreanâ theorem, which
gives some insight into the geometry of D(P ||Q). Since D(P ||Q) is not
a metric, many of the intuitive properties of distance are not valid for
D(P ||Q). The next theorem shows a sense in which D(P ||Q) behaves
like the square of the Euclidean metric (Figure 11.5).
/
Theorem 11.6.1 For a closed convex set E â P and distribution Q â
E, let P â â E be the distribution that achieves the minimum distance to
Q; that is,
D(P â ||Q) = min D(P ||Q).

(11.121)

D(P ||Q) â¥ D(P ||P â ) + D(P â ||Q)

(11.122)

P âE

Then

for all P â E.

368

INFORMATION THEORY AND STATISTICS

Note. The main use of this theorem is as follows: Suppose that we have
a sequence Pn â E that yields D(Pn ||Q) â D(P â ||Q). Then from the
Pythagorean theorem, D(Pn ||P â ) â 0 as well.
Proof: Consider any P â E. Let
PÎ» = Î»P + (1 â Î»)P â .

(11.123)

Then PÎ» â P â as Î» â 0. Also, since E is convex, PÎ» â E for 0 â¤ Î» â¤ 1.
Since D(P â ||Q) is the minimum of D(PÎ» ||Q) along the path P â â P ,
the derivative of D(PÎ» ||Q) as a function of Î» is nonnegative at Î» = 0.
Now
DÎ» = D(PÎ» ||Q) =



PÎ» (x) log

PÎ» (x)
Q(x)

(11.124)

and


dDÎ» 
PÎ» (x)
â
â
=
+ (P (x) â P (x)) . (11.125)
(P (x) â P (x)) log
dÎ»
Q(x)


Setting Î» = 0, so that PÎ» = P â and using the fact that
P (x) = P â
(x) = 1, we have


dDÎ»
(11.126)
0â¤
dÎ» Î»=0
=
=
=


P â (x)
(P (x) â P â (x)) log
Q(x)



(11.127)

P (x) log

P â (x)  â
P â (x)
â
P (x) log
Q(x)
Q(x)

P (x) log

P (x) P â (x)  â
P â (x)
â
P (x) log
(11.129)
Q(x) P (x)
Q(x)

= D(P ||Q) â D(P ||P â ) â D(P â ||Q),
which proves the theorem.

(11.128)

(11.130)


Note that the relative entropy D(P ||Q) behaves like the square of the
Euclidean distance. Suppose that we have a convex set E in Rn . Let A
be a point outside the set, B the point in the set closest to A, and C any

11.6 CONDITIONAL LIMIT THEOREM

369

A

B

C

FIGURE 11.6. Triangle inequality for distance squared.

other point in the set. Then the angle between the lines BA and BC must
2
2
2
â¥ lAB
+ lBC
, which is of the same form
be obtuse, which implies that lAC
as Theorem 11.6.1. This is illustrated in Figure 11.6.
We now prove a useful lemma which shows that convergence in relative
entropy implies convergence in the L1 norm.
Deï¬nition The L1 distance between any two distributions is deï¬ned as
||P1 â P2 ||1 =



|P1 (a) â P2 (a)|.

(11.131)

aâX

Let A be the set on which P1 (x) > P2 (x). Then
||P1 â P2 ||1 =



|P1 (x) â P2 (x)|

(11.132)

xâX



=
(P1 (x) â P2 (x)) +
(P2 (x) â P1 (x)) (11.133)
xâA

xâAc
c

= P1 (A) â P2 (A) + P2 (A ) â P1 (Ac )

(11.134)

= P1 (A) â P2 (A) + 1 â P2 (A) â 1 + P1 (A)

(11.135)

= 2(P1 (A) â P2 (A)).

(11.136)

370

INFORMATION THEORY AND STATISTICS

Also note that
max (P1 (B) â P2 (B)) = P1 (A) â P2 (A) =

BâX

||P1 â P2 ||1
.
2

(11.137)

The left-hand side of (11.137) is called the variational distance between
P1 and P2 .
Lemma 11.6.1
D(P1 ||P2 ) â¥

1
||P1 â P2 ||21 .
2 ln 2

(11.138)

Proof: We ï¬rst prove it for the binary case. Consider two binary distributions with parameters p and q with p â¥ q. We will show that
p log

p
1âp
4
+ (1 â p) log
â¥
(p â q)2 .
q
1âq
2 ln 2

(11.139)

The difference g(p, q) between the two sides is
g(p, q) = p log

p
1âp
4
+ (1 â p) log
â
(p â q)2 .
q
1âq
2 ln 2

(11.140)

Then
dg(p, q)
p
1âp
4
=â
+
â
2(q â p) (11.141)
dq
q ln 2 (1 â q) ln 2 2 ln 2
=

4
qâp
â
(q â p)
q(1 â q) ln 2 ln 2

â¤0

(11.142)
(11.143)

since q(1 â q) â¤ 14 and q â¤ p. For q = p, g(p, q) = 0, and hence
g(p, q) â¥ 0 for q â¤ p, which proves the lemma for the binary case.
For the general case, for any two distributions P1 and P2 , let
A = {x : P1 (x) > P2 (x)}.

(11.144)

Deï¬ne a new binary random variable Y = Ï(X), the indicator of the set A,
and let PË1 and PË2 be the distributions of Y . Thus, PÌ1 and PÌ2 correspond
to the quantized versions of P1 and P2 . Then by the data-processing

11.6 CONDITIONAL LIMIT THEOREM

371

inequality applied to relative entropies (which is proved in the same way
as the data-processing inequality for mutual information), we have
D(P1 ||P2 ) â¥ D(PË1 ||PË2 )
4
(P1 (A) â P2 (A))2
â¥
2 ln 2
1
||P1 â P2 ||21 ,
=
2 ln 2
by (11.137), and the lemma is proved.

(11.145)
(11.146)
(11.147)


We can now begin the proof of the conditional limit theorem. We ï¬rst
outline the method used. As stated at the beginning of the chapter, the
essential idea is that the probability of a type under Q depends exponentially on the distance of the type from Q, and hence types that are farther
away are exponentially less likely to occur. We divide the set of types in
E into two categories: those at about the same distance from Q as P â and
those a distance 2Î´ farther away. The second set has exponentially less
probability than the ï¬rst, and hence the ï¬rst set has a conditional probability tending to 1. We then use the Pythagorean theorem to establish that
all the elements in the ï¬rst set are close to P â , which will establish the
theorem.
The following theorem is an important strengthening of the maximum
entropy principle.
Theorem 11.6.2 (Conditional limit theorem) Let E be a closed convex subset of P and let Q be a distribution not in E. Let X1 , X2 , . . . , Xn
be discrete random variables drawn i.i.d. â¼ Q. Let P â achieve minP âE
D(P ||Q). Then
Pr(X1 = a|PXn â E) â P â (a)

(11.148)

in probability as n â â, i.e., the conditional distribution of X1 , given that
the type of the sequence is in E, is close to P â for large n.
Example 11.6.1

If Xi i.i.d. â¼ Q, then

 

1
2
Xi â¥ Î± â P â (a),
Pr X1 = a 
n


(11.149)

372

INFORMATION THEORY AND STATISTICS

where P â (a) minimizes D(P ||Q) over P satisfying
minimization results in
eÎ»a

â

P (a) = Q(a) 
a



P (a)a 2 â¥ Î±. This

2

Q(a)eÎ»a

2

,

(11.150)

 â
where Î» is chosen to satisfy
P (a)a 2 = Î±. Thus, the conditional distribution on X1 given a constraint on the sum of the squares is a (normalized) product of the original probability mass function and the maximum
entropy probability mass function (which in this case is Gaussian).
Proof of Theorem: Deï¬ne the sets
St = {P â P : D(P ||Q) â¤ t}.

(11.151)

The set St is convex since D(P ||Q) is a convex function of P . Let
D â = D(P â ||Q) = min D(P ||Q).
P âE

(11.152)

Then P â is unique, since D(P ||Q) is strictly convex in P . Now deï¬ne
the set
A = SDâ +2Î´ â© E

(11.153)

B = E â SDâ +2Î´ â© E.

(11.154)

and

Thus, A âª B = E. These sets are illustrated in Figure 11.7. Then
Qn (B) =



Qn (T (P ))

(11.155)

2ânD(P ||Q)

(11.156)

P âEâ©Pn :D(P ||Q)>D â +2Î´

â¤



P âEâ©Pn :D(P ||Q)>D â +2Î´

â¤



2ân(D

â +2Î´)

(11.157)

P âEâ©Pn :D(P ||Q)>D â +2Î´

â¤ (n + 1)|X | 2ân(D

â +2Î´)

(11.158)

11.6 CONDITIONAL LIMIT THEOREM

373

E
SD* + 2d

B

A

SD* + d

P*
Q

FIGURE 11.7. Conditional limit theorem.

since there are only a polynomial number of types. On the other hand,
Qn (A) â¥ Qn (SDâ +Î´ â© E)

=

(11.159)
Qn (T (P ))

(11.160)

1
2ânD(P ||Q)
(n + 1)|X |

(11.161)

P âEâ©Pn :D(P ||Q)â¤D â +Î´

â¥


P âEâ©Pn :D(P ||Q)â¤D â +Î´

â¥

1
â
2ân(D +Î´)
|
X
|
(n + 1)

for n sufï¬ciently large, (11.162)

since the sum is greater than one of the terms, and for sufï¬ciently large n,
there exists at least one type in SDâ +Î´ â© E â© Pn . Then, for n sufï¬ciently
large,
Qn (B â© E)
Qn (E)
Qn (B)
â¤ n
Q (A)

Pr(PXn â B|PXn â E) =

â¤

(n + 1)|X | 2ân(D

(11.163)
(11.164)
â +2Î´)

1
2ân(Dâ +Î´)
(n+1)|X |

= (n + 1)2|X | 2ânÎ´ ,

(11.165)
(11.166)

374

INFORMATION THEORY AND STATISTICS

which goes to 0 as n â â. Hence the conditional probability of B goes
to 0 as n â â, which implies that the conditional probability of A goes
to 1.
We now show that all the members of A are close to P â in relative
entropy. For all members of A,
D(P ||Q) â¤ D â + 2Î´.

(11.167)

Hence by the âPythagoreanâ theorem (Theorem 11.6.1),
D(P ||P â ) + D(P â ||Q) â¤ D(P ||Q) â¤ D â + 2Î´,

(11.168)

which in turn implies that
D(P ||P â ) â¤ 2Î´,

(11.169)

since D(P â ||Q) = D â . Thus, Px â A implies that D(Px ||Q) â¤ D â + 2Î´,
and therefore that D(Px ||P â ) â¤ 2Î´. Consequently, since Pr{PXn â A|PXn
â E} â 1, it follows that
Pr(D(PXn ||P â ) â¤ 2Î´|PXn â E) â 1

(11.170)

as n â â. By Lemma 11.6.1, the fact that the relative entropy is small
implies that the L1 distance is small, which in turn implies that maxaâX
|PXn (a) â P â (a)| is small. Thus, Pr(|PXn (a) â P â (a)| â¥ |PXn â E) â
0 as n â â. Alternatively, this can be written as
Pr(X1 = a|PXn â E) â P â (a)

in probability, a â X.

(11.171)

In this theorem we have only proved that the marginal distribution goes
to P â as n â â. Using a similar argument, we can prove a stronger
version of this theorem:
Pr(X1 = a1 , X2 = a2 , . . . , Xm
= am |PXn â E) â

m


P â (ai )

in probability.

(11.172)

i=1

This holds for ï¬xed m as n â â. The result is not true for m = n, since
there are end effects; given that the type of the sequence is in E, the
last elements of the sequence can be determined from the remaining elements, and the elements are no longer independent. The conditional limit

11.7 HYPOTHESIS TESTING

375

theorem states that the ï¬rst few elements are asymptotically independent
with common distribution P â .
Example 11.6.2 As an example of the conditional limit theorem, let us
consider the case when n fair dice are rolled. Suppose that the sum of the
outcomes exceeds 4n. Then by the conditional limit theorem, the probability that the ï¬rst die shows a number a â {1, 2, . . . , 6} is approximately
P â (a), where P â (a) is the distribution
 in E that is closest to the uniform distribution, where E = {P : P (a)a â¥ 4}. This is the maximum
entropy distribution given by
2Î»x
P â (x) = 6

Î»i
i=1 2

,

(11.173)

 â
with Î» chosen so that
iP (i) = 4 (see Chapter 12). Here P â is the
conditional distribution on the ï¬rst (or any other) die. Apparently, the
ï¬rst few dice inspected will behave as if they are drawn independently
according to an exponential distribution.
11.7

HYPOTHESIS TESTING

One of the standard problems in statistics is to decide between two alternative explanations for the data observed. For example, in medical testing,
one may wish to test whether or not a new drug is effective. Similarly, a
sequence of coin tosses may reveal whether or not the coin is biased.
These problems are examples of the general hypothesis-testing problem.
In the simplest case, we have to decide between two i.i.d. distributions.
The general problem can be stated as follows:
Problem 11.7.1
hypotheses:
â¢
â¢

Let X1 , X2 , . . . , Xn be i.i.d. â¼ Q(x). We consider two

H1 : Q = P1 .
H2 : Q = P2 .

Consider the general decision function g(x1 , x2 , . . . , xn ), where g(x1 ,
x2 , . . . , xn ) = 1 means that H1 is accepted and g(x1 , x2 , . . . , xn ) = 2
means that H2 is accepted. Since the function takes on only two values, the test can also be speciï¬ed by specifying the set A over which
g(x1 , x2 , . . . , xn ) is 1; the complement of this set is the set where
g(x1 , x2 , . . . , xn ) has the value 2. We deï¬ne the two probabilities of error:
Î± = Pr(g(X1 , X2 , . . . , Xn ) = 2|H1 true) = P1n (Ac )

(11.174)

376

INFORMATION THEORY AND STATISTICS

and
Î² = Pr(g(X1 , X2 , . . . , Xn ) = 1|H2 true) = P2n (A).

(11.175)

In general, we wish to minimize both probabilities, but there is a tradeoff. Thus, we minimize one of the probabilities of error subject to a
constraint on the other probability of error. The best achievable error
exponent in the probability of error for this problem is given by the
ChernoffâStein lemma.
We ï¬rst prove the NeymanâPearson lemma, which derives the form of
the optimum test between two hypotheses. We derive the result for discrete
distributions; the same results can be derived for continuous distributions
as well.
Theorem 11.7.1 (NeymanâPearson lemma) Let X1 , X2 , . . . , Xn be
drawn i.i.d. according to probability mass function Q. Consider the decision problem corresponding to hypotheses Q = P1 vs. Q = P2 . For T â¥ 0,
deï¬ne a region


n P1 (x1 , x2 , . . . , xn )
An (T ) = x :
>T .
(11.176)
P2 (x1 , x2 , . . . , xn )
Let
Î± â = P1n (Acn (T )),

Î² â = P2n (An (T ))

(11.177)

be the corresponding probabilities of error corresponding to decision region An . Let Bn be any other decision region with associated probabilities
of error Î± and Î². If Î± â¤ Î± â , then Î² â¥ Î² â .
Proof: Let A = An (T ) be the region deï¬ned in (11.176) and let B â X n
be any other acceptance region. Let ÏA and ÏB be the indicator functions of the decision regions A and B, respectively. Then for all x =
(x1 , x2 , . . . , xn ) â X n ,
(ÏA (x) â ÏB (x))(P1 (x) â T P2 (x)) â¥ 0.

(11.178)

This can be seen by considering separately the cases x â A and x â
/ A.
Multiplying out and summing this over the entire space, we obtain

(11.179)
0â¤
(ÏA P1 â T ÏA P2 â P1 ÏB + T P2 ÏB )

11.7 HYPOTHESIS TESTING

=



(P1 â T P2 ) â
(P1 â T P2 )
A

377

(11.180)

B
â

â

= (1 â Î± ) â T Î² â (1 â Î±) + T Î²

(11.181)

= T (Î² â Î² â ) â (Î± â â Î±).

(11.182)

Since T â¥ 0, we have proved the theorem.



The NeymanâPearson lemma indicates that the optimum test for two
hypotheses is of the form
P1 (X1 , X2 , . . . , Xn )
> T.
P2 (X1 , X2 , . . . , Xn )

(11.183)

1 ,X2 ,...,Xn )
This is the likelihood ratio test and the quantity PP12 (X
(X1 ,X2 ,...,Xn ) is called the
likelihood ratio. For example, in a test between two Gaussian distributions
[i.e., between f1 = N(1, Ï 2 ) and f2 = N(â1, Ï 2 )], the likelihood ratio
becomes

n

(Xi â1)2

â
2Ï 2
â 1
i=1 2Ï Ï 2 e

f1 (X1 , X2 , . . . , Xn )
=
f2 (X1 , X2 , . . . , Xn ) n

(Xi +1)2

(11.184)

â
2Ï 2
â 1
i=1 2Ï Ï 2 e

=e
=e

+

2

n
i=1 Xi
Ï2

+ 2nX2 n
Ï

.

(11.185)
(11.186)

Hence, the likelihood ratio test consists of comparing the sample mean
X n with a threshold. If we want the two probabilities of error to be equal,
we should set T = 1. This is illustrated in Figure 11.8.
In Theorem 11.7.1 we have shown that the optimum test is a likelihood
ratio test. We can rewrite the log-likelihood ratio as
L(X1 , X2 , . . . , Xn ) = log
=

n

i=1

=



P1 (X1 , X2 , . . . , Xn )
P2 (X1 , X2 , . . . , Xn )
log

P1 (Xi )
P2 (Xi )



aâX

(11.188)

nPXn (a) log

P1 (a)
P2 (a)

(11.189)

nPXn (a) log

P1 (a) PXn (a)
P2 (a) PXn (a)

(11.190)

aâX

=

(11.187)

378

INFORMATION THEORY AND STATISTICS

f(x)
0.4

f(x)

0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
â5

â4

â3

â2

â1

0

1

2

3

4

5

x

FIGURE 11.8. Testing between two Gaussian distributions.

=



nPXn (a) log

aâX

â



PXn (a)
P2 (a)

nPXn (a) log

aâX

PXn (a)
P1 (a)

= nD(PXn ||P2 ) â nD(PXn ||P1 ),

(11.191)
(11.192)

the difference between the relative entropy distances of the sample type
to each of the two distributions. Hence, the likelihood ratio test
P1 (X1 , X2 , . . . , Xn )
>T
P2 (X1 , X2 , . . . , Xn )

(11.193)

is equivalent to
D(PXn ||P2 ) â D(PXn ||P1 ) >

1
log T .
n

(11.194)

We can consider the test to be equivalent to specifying a region of the simplex of types that corresponds to choosing hypothesis H1 . The optimum
region is of the form (11.194), for which the boundary of the region is the
set of types for which the difference between the distances is a constant.
This boundary is the analog of the perpendicular bisector in Euclidean
geometry. The test is illustrated in Figure 11.9.
We now offer some informal arguments based on Sanovâs theorem to
show how to choose the threshold to obtain different probabilities of error.
Let B denote the set on which hypothesis 1 is accepted. The probability

11.7 HYPOTHESIS TESTING

379

P1

Pl

P2
D(P ||P1) = D(P ||P2) = 1
n log T

FIGURE 11.9. Likelihood ratio test on the probability simplex.

of error of the ï¬rst kind is
Î±n = P1n (PXn â B c ).

(11.195)

Since the set B c is convex, we can use Sanovâs theorem to show that the
probability of error is determined essentially by the relative entropy of
the closest member of B c to P1 . Therefore,
â

Î±n = 2ânD(P1 ||P1 ) ,
.

(11.196)

where P1â is the closest element of B c to distribution P1 . Similarly,
â

Î²n = 2ânD(P2 ||P2 ) ,
.

(11.197)

where P2â is the closest element in B to the distribution P2 .
Now minimizing D(P ||P2 ) subject to the constraint D(P ||P2 ) â
D(P ||P1 ) â¥ n1 log T will yield the type in B that is closest to P2 . Setting up the minimization of D(P ||P2 ) subject to D(P ||P2 ) â D(P ||P1 ) =
1
n log T using Lagrange multipliers, we have
J (P ) =



P (x) log



P1 (x)
P (x)
+Î»
P (x) log
+Î½
P (x).
P2 (x)
P2 (x)
(11.198)

Differentiating with respect to P (x) and setting to 0, we have
log

P (x)
P1 (x)
+ 1 + Î» log
+ Î½ = 0.
P2 (x)
P2 (x)

(11.199)

380

INFORMATION THEORY AND STATISTICS

Solving this set of equations, we obtain the minimizing P of the form
P2â = PÎ»â = 

P1Î» (x)P21âÎ» (x)
aâX

P1Î» (a)P21âÎ» (a)

,

(11.200)

where Î» is chosen so that D(PÎ»â ||P1 ) â D(PÎ»â ||P2 ) = n1 log T .
From the symmetry of expression (11.200), it is clear that P1â = P2â and
that the probabilities of error behave exponentially with exponents given
by the relative entropies D(P â ||P1 ) and D(P â ||P2 ). Also note from the
equation that as Î» â 1, PÎ» â P1 and as Î» â 0, PÎ» â P2 . The curve
that PÎ» traces out as Î» varies is a geodesic in the simplex. Here PÎ» is a
normalized convex combination, where the combination is in the exponent
(Figure 11.9).
In the next section we calculate the best error exponent when one of
the two types of error goes to zero arbitrarily slowly (the ChernoffâStein
lemma). We will also minimize the weighted sum of the two probabilities
of error and obtain the Chernoff information bound.
11.8

CHERNOFFâSTEIN LEMMA

We consider hypothesis testing in the case when one of the probabilities of error is held ï¬xed and the other is made as small as possible.
We will show that the other probability of error is exponentially small,
with an exponential rate equal to the relative entropy between the two
distributions. The method of proof uses a relative entropy version of the
AEP.
Theorem 11.8.1 (AEP for relative entropy) Let X1 , X2 , . . . , Xn be
a sequence of random variables drawn i.i.d. according to P1 (x), and let
P2 (x) be any other distribution on X. Then
P1 (X1 , X2 , . . . , Xn )
1
log
â D(P1 ||P2 )
n
P2 (X1 , X2 , . . . , Xn )

in probability.

(11.201)

Proof: This follows directly from the weak law of large numbers.
n
P1 (Xi )
1
P1 (X1 , X2 , . . . , Xn )
1
(11.202)
log
= log i=1
n
n
P2 (X1 , X2 , . . . , Xn )
n
i=1 P2 (Xi )

11.8

CHERNOFFâSTEIN LEMMA

1
P1 (Xi )
log
n
P2 (Xi )

381

n

=

(11.203)

i=1

â EP1 log

P1 (X)
in probability (11.204)
P2 (X)

= D(P1 ||P2 ).



(11.205)

Just as for the regular AEP, we can deï¬ne a relative entropy typical
sequence as one for which the empirical relative entropy is close to its
expected value.
Deï¬nition For a ï¬xed n and  > 0, a sequence (x1 , x2 , . . . , xn ) â X n
is said to be relative entropy typical if and only if
D(P1 ||P2 ) â  â¤

P1 (x1 , x2 , . . . , xn )
1
log
â¤ D(P1 ||P2 ) + . (11.206)
n
P2 (x1 , x2 , . . . , xn )

The set of relative entropy typical sequences is called the relative entropy
typical set A(n)
 (P1 ||P2 ).
As a consequence of the relative entropy AEP, we can show that the
relative entropy typical set satisï¬es the following properties:
Theorem 11.8.2
1. For (x1 , x2 , . . . , xn ) â A(n)
 (P1 ||P2 ),
P1 (x1 , x2 , . . . , xn )2ân(D(P1 ||P2 )+)
â¤ P2 (x1 , x2 , . . . , xn )
â¤ P1 (x1 , x2 , . . . , xn )2ân(D(P1 ||P2 )â) .

(11.207)

2. P1 (A(n)
 (P1 ||P2 )) > 1 â , for n sufï¬ciently large.
ân(D(P1 ||P2 )â)
.
3. P2 (A(n)
 (P1 ||P2 )) < 2
(n)
ân(D(P1 ||P2 )+)
4. P2 (A (P1 ||P2 )) > (1 â )2
, for n sufï¬ciently large.
Proof: The proof follows the same lines as the proof of Theorem 3.1.2,
with the counting measure replaced by probability measure P2 . The proof
of property 1 follows directly from the deï¬nition of the relative entropy

382

INFORMATION THEORY AND STATISTICS

typical set. The second property follows from the AEP for relative entropy
(Theorem 11.8.1). To prove the third property, we write

P2 (A(n)
P2 (x1 , x2 , . . . , xn ) (11.208)
 (P1 ||P2 )) =
(n)

â¤



x n âA (P1 ||P2 )

P1 (x1 , x2 , . . . , xn )2ân(D(P1 ||P2 )â) (11.209)

(n)

x n âA (P1 ||P2 )



= 2ân(D(P1 ||P2 )â)

P1 (x1 , x2 , . . . , xn ) (11.210)

(n)
x n âA (P1 ||P2 )

= 2ân(D(P1 ||P2 )â) P1 (A(n)
 (P1 ||P2 ))

(11.211)

â¤ 2ân(D(P1 ||P2 )â) ,

(11.212)

where the ï¬rst inequality follows from property 1, and the second inequality follows from the fact that the probability of any set under P1 is less
than 1.
To prove the lower bound on the probability of the relative entropy
typical set, we use a parallel argument with a lower bound on the probability:

P2 (A(n)
P2 (x1 , x2 , . . . , xn ) (11.213)
 (P1 ||P2 )) =
(n)

â¥



x n âA (P1 ||P2 )

P1 (x1 , x2 , . . . , xn )2ân(D(P1 ||P2 )+) (11.214)

(n)

x n âA (P1 ||P2 )

= 2ân(D(P1 ||P2 )+)



P1 (x1 , x2 , . . . , xn ) (11.215)

(n)
x n âA (P1 ||P2 )

= 2ân(D(P1 ||P2 )+) P1 (A(n)
 (P1 ||P2 ))

(11.216)

â¥ (1 â )2ân(D(P1 ||P2 )+) ,

(11.217)

where the second inequality follows from the second property of A(n)

(P1 ||P2 ).

With the standard AEP in Chapter 3, we also showed that any set that
has a high probability has a high intersection with the typical set, and
therefore has about 2nH elements. We now prove the corresponding result
for relative entropy.

11.8

CHERNOFFâSTEIN LEMMA

383

Lemma 11.8.1 Let Bn â X n be any set of sequences x1 , x2 , . . . , xn such
that P1 (Bn ) > 1 â . Let P2 be any other distribution such that D(P1 ||P2 )
< â. Then P2 (Bn ) > (1 â 2)2ân(D(P1 ||P2 )+) .
Proof: For simplicity, we will denote A(n)
 (P1 ||P2 ) by An . Since P1 (Bn )
> 1 â  and P (An ) > 1 â  (Theorem 11.8.2), we have, by the union of
events bound, P1 (Acn âª Bnc ) < 2, or equivalently, P1 (An â© Bn ) > 1 â 2.
Thus,
P2 (Bn ) â¥ P2 (An â© Bn )

=
P2 (x n )
x n âA

â¥

(11.219)

n â©Bn



x n âA

(11.218)

P1 (x n )2ân(D(P1 ||P2 )+)

n â©Bn

= 2ân(D(P1 ||P2 )+)



P1 (x n )

(11.220)
(11.221)

x n âAn â©Bn

= 2ân(D(P1 ||P2 )+) P1 (An â© Bn )

(11.222)

â¥ 2ân(D(P1 ||P2 )+) (1 â 2),

(11.223)

where the second inequality follows from the properties of the relative
entropy typical sequences (Theorem 11.8.2) and the last inequality follows

from the union bound above.
We now consider the problem of testing two hypotheses, P1 vs. P2 . We
hold one of the probabilities of error ï¬xed and attempt to minimize the
other probability of error. We show that the relative entropy is the best
exponent in probability of error.
Theorem 11.8.3 (ChernoffâStein Lemma)
Let X1 , X2 , . . . , Xn be
i.i.d. â¼ Q. Consider the hypothesis test between two alternatives, Q = P1
and Q = P2 , where D(P1 ||P2 ) < â. Let An â X n be an acceptance
region for hypothesis H1 . Let the probabilities of error be
Î±n = P1n (Acn ),

Î²n = P2n (An ).

(11.224)

and for 0 <  < 12 , deï¬ne
Î²n =

min
Î²n .
An â X n
Î±n < 

(11.225)

384

INFORMATION THEORY AND STATISTICS

Then
1
log Î²n = âD(P1 ||P2 ).
nââ n
lim

(11.226)

Proof: We prove this theorem in two parts. In the ï¬rst part we exhibit
a sequence of sets An for which the probability of error Î²n goes exponentially to zero as D(P1 ||P2 ). In the second part we show that no other
sequence of sets can have a lower exponent in the probability of error.
For the ï¬rst part, we choose as the sets An = A(n)
 (P1 ||P2 ). As proved in
Theorem 11.8.2, this sequence of sets has P1 (Acn ) <  for n large enough.
Also,
lim

nââ

1
log P2 (An ) â¤ â(D(P1 ||P2 ) â )
n

(11.227)

from property 3 of Theorem 11.8.2. Thus, the relative entropy typical set
satisï¬es the bounds of the lemma.
To show that no other sequence of sets can to better, consider any
sequence of sets Bn with P1 (Bn ) > 1 â . By Lemma 11.8.1, we have
P2 (Bn ) > (1 â 2)2ân(D(P1 ||P2 )+) , and therefore
1
1
log P2 (Bn ) > â(D(P1 ||P2 ) + ) + lim log(1 â 2)
nââ
n
n
(11.228)
= â(D(P1 ||P2 ) + ).

lim

nââ

Thus, no other sequence of sets has a probability of error exponent better
than D(P1 ||P2 ). Thus, the set sequence An = A(n)
 (P1 ||P2 ) is asymptotically optimal in terms of the exponent in the probability.

Not that the relative entropy typical set, although asymptotically optimal (i.e., achieving the best asymptotic rate), is not the optimal set for
any ï¬xed hypothesis-testing problem. The optimal set that minimizes the
probabilities of error is that given by the NeymanâPearson lemma.
11.9

CHERNOFF INFORMATION

We have considered the problem of hypothesis testing in the classical
setting, in which we treat the two probabilities of error separately. In the
derivation of the ChernoffâStein lemma, we set Î±n â¤  and achieved
.
Î²n = 2ânD . But this approach lacks symmetry. Instead, we can follow a Bayesian approach, in which we assign prior probabilities to both

11.9 CHERNOFF INFORMATION

385

hypotheses. In this case we wish to minimize the overall probability of
error given by the weighted sum of the individual probabilities of error.
The resulting error exponent is the Chernoff information.
The setup is as follows: X1 , X2 , . . . , Xn i.i.d. â¼ Q. We have two
hypotheses: Q = P1 with prior probability Ï1 and Q = P2 with prior
probability Ï2 . The overall probability of error is
Pe(n) = Ï1 Î±n + Ï2 Î²n .

(11.229)

1
D â = lim â log min n Pe(n) .
nââ n
An â X

(11.230)

Let

Theorem 11.9.1 (Chernoff )
The best achievable exponent in the
â
Bayesian probability of error is D , where
D â = D(PÎ»â ||P1 ) = D(PÎ»â ||P2 ),

(11.231)

with
PÎ» = 

P1Î» (x)P21âÎ» (x)

aâX

P1Î» (a)P21âÎ» (a)

,

(11.232)

and Î»â the value of Î» such that
D(PÎ»â ||P1 ) = D(PÎ»â ||P2 ).

(11.233)

Proof: The basic details of the proof were given in Section 11.8. We
have shown that the optimum test is a likelihood ratio test, which can be
considered to be of the form
D(PXn ||P2 ) â D(PXn ||P1 ) >

1
log T .
n

(11.234)

The test divides the probability simplex into regions corresponding to
hypothesis 1 and hypothesis 2, respectively. This is illustrated in Figure 11.10.
Let A be the set of types associated with hypothesis 1. From the discussion preceding (11.200), it follows that the closest point in the set Ac
to P1 is on the boundary of A and is of the form given by (11.232). Then
from the discussion in Section 11.8, it is clear that PÎ» is the distribution

386

INFORMATION THEORY AND STATISTICS

P1

Pl
P2

FIGURE 11.10. Probability simplex and Chernoff information.

in A that is closest to P2 ; it is also the distribution in Ac that is closest
to P1 . By Sanovâs theorem, we can calculate the associated probabilities
of error,
Î±n = P1n (Ac ) = 2ânD(PÎ»â ||P1 )

(11.235)

Î²n = P2n (A) = 2ânD(PÎ»â ||P2 ) .

(11.236)

.

and
.

In the Bayesian case, the overall probability of error is the weighted sum
of the two probabilities of error,
Pe = Ï1 2ânD(PÎ» ||P1 ) + Ï2 2ânD(PÎ» ||P2 ) = 2ân min{D(PÎ» ||P1 ),D(PÎ» ||P2 )} ,
(11.237)
.

.

since the exponential rate is determined by the worst exponent. Since
D(PÎ» ||P1 ) increases with Î» and D(PÎ» ||P2 ) decreases with Î», the maximum value of the minimum of {D(PÎ» ||P1 ), D(PÎ» ||P2 )} is attained when
they are equal. This is illustrated in Figure 11.11. Hence, we choose Î» so
that
D(PÎ» ||P1 ) = D(PÎ» ||P2 ).

(11.238)

Thus, C(P1 , P2 ) is the highest achievable exponent for the probability of

error and is called the Chernoff information.

11.9 CHERNOFF INFORMATION

387

2.5

Relative entropy

2

D(Pl||P1)

1.5

D(Pl||P2)

1

0.5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

l

FIGURE 11.11. Relative entropy D(PÎ» ||P1 ) and D(PÎ» ||P2 ) as a function of Î».

The deï¬nition D â = D(PÎ»â ||P1 ) = D(PÎ»â ||P2 ) is equivalent to the
standard deï¬nition of Chernoff information,




P1Î» (x)P21âÎ» (x) .
(11.239)
C(P1 , P2 ) = â min log
0â¤Î»â¤1

x

It is left as an exercise to the reader to show the equivalence of (11.231)
and (11.239).
We outline brieï¬y the usual derivation of the Chernoff information
bound. The maximum a posteriori probability decision rule minimizes the
Bayesian probability of error. The decision region A for hypothesis H1
for the maximum a posteriori rule is


Ï1 P1 (x)
A= x:
>1 ,
(11.240)
Ï2 P2 (x)
the set of outcomes where the a posteriori probability of hypothesis H1 is
greater than the a posteriori probability of hypothesis H2 . The probability
of error for this rule is
Pe = Ï1 Î±n + Ï2 Î²n


Ï1 P1 +
Ï2 P2
=
Ac

=



(11.241)
(11.242)

A

min{Ï1 P1 , Ï2 P2 }.

(11.243)

388

INFORMATION THEORY AND STATISTICS

Now for any two positive numbers a and b, we have
for all 0 â¤ Î» â¤ 1.

min{a, b} â¤ a Î» b1âÎ»

(11.244)

Using this to continue the chain, we have
Pe =



min{Ï1 P1 , Ï2 P2 }

â¤
(Ï1 P1 )Î» (Ï2 P2 )1âÎ»

â¤
P1Î» P21âÎ» .

For a sequence of i.i.d. observations, Pk (x) =
Pe(n) â¤



Ï1Î» Ï21âÎ»



n

i=1 Pk (xi ),

(11.245)
(11.246)
(11.247)
and

P1Î» (xi )P21âÎ» (xi )

(11.248)

P1Î» (xi )P21âÎ» (xi )

(11.249)

i

= Ï1Î» Ï21âÎ»
â¤




i

P1Î» P21âÎ»

xi


=



(11.250)

n
P1Î» P21âÎ»

,

(11.251)

x

where (11.250) follows since Ï1 â¤ 1, Ï2 â¤ 1. Hence, we have

1
log Pe(n) â¤ log
P1Î» (x)P21âÎ» (x).
n

(11.252)

Since this is true for all Î», we can take the minimum over 0 â¤ Î» â¤ 1,
resulting in the Chernoff information bound. This proves that the exponent
is no better than C(P1 , P2 ). Achievability follows from Theorem 11.9.1.
Note that the Bayesian error exponent does not depend on the actual
value of Ï1 and Ï2 , as long as they are nonzero. Essentially, the effect of
the prior is washed out for large sample sizes. The optimum decision rule
is to choose the hypothesis with the maximum a posteriori probability,
which corresponds to the test
Ï1 P1 (X1 , X2 , . . . , Xn ) >
< 1.
Ï2 P2 (X1 , X2 , . . . , Xn )

(11.253)

11.9 CHERNOFF INFORMATION

389

Taking the log and dividing by n, this test can be rewritten as
1
Ï1
1
P1 (Xi ) <
log
> 0,
+
log
n
Ï2 n
P2 (Xi )

(11.254)

i

where the second term tends to D(P1 ||P2 ) or âD(P2 ||P1 ) accordingly as
P1 or P2 is the true distribution. The ï¬rst term tends to 0, and the effect
of the prior distribution washes out.
Finally, to round off our discussion of large deviation theory and hypothesis testing, we consider an example of the conditional limit theorem.
Example 11.9.1 Suppose that major league baseball players have a batting average of 260 with a standard deviation of 15 and suppose that
minor league ballplayers have a batting average of 240 with a standard
deviation of 15. A group of 100 ballplayers from one of the leagues (the
league is chosen at random) are found to have a group batting average
greater than 250 and are therefore judged to be major leaguers. We are
now told that we are mistaken; these players are minor leaguers. What
can we say about the distribution of batting averages among these 100
players? The conditional limit theorem can be used to show that the distribution of batting averages among these players will have a mean of 250
and a standard deviation of 15. To see this, we abstract the problem as
follows.
Let us consider an example of testing between two Gaussian distributions, f1 = N(1, Ï 2 ) and f2 = N(â1, Ï 2 ), with different means and the
same variance. As discussed in Section 11.8, the likelihood ratio test in
this case is equivalent to comparing the sample mean
 with a threshold.
The Bayes test is âAccept the hypothesis f = f1 if n1 ni=1 Xi > 0.â Now
assume that we make an error of the ï¬rst kind (we say that f = f1 when
indeed f = f2 ) in this test. What is the conditional distribution of the
samples given that we have made an error?
We might guess at various possibilities:
â¢
â¢
â¢

The sample will look like a ( 12 , 12 ) mix of the two normal distributions.
Plausible as this is, it is incorrect.
Xi â 0 for all i. This is quite clearly very unlikely, although it is
conditionally likely that Xn is close to 0.
The correct answer is given by the conditional limit theorem. If the
true distribution is f2 and the sample type is in the set A, the conditional distribution is close to f â , the distribution in A that is closest to
f2 . By symmetry, this corresponds to Î» = 12 in (11.232). Calculating

390

INFORMATION THEORY AND STATISTICS

the distribution, we get

f â (x) =





(xâ1)2

â
â 1
e 2Ï 2
2ÏÏ 2

(xâ1)2

â
â 1
e 2Ï 2
2ÏÏ 2

 12 

 12 

(x+1)2

â
â 1
e 2Ï 2
2ÏÏ 2

(x+1)2

â
â 1
e 2Ï 2
2ÏÏ 2

 12

 12
dx
(11.255)

=



=â

â 1
e
2ÏÏ 2

2
â (x +1)
2Ï 2
(x 2 +1)

â
â 1
e 2Ï 2
2ÏÏ 2

1
2Ï Ï 2

e

(11.256)
dx

2
â x2
2Ï

(11.257)

= N (0, Ï 2 ).

(11.258)

It is interesting to note that the conditional distribution is normal with
mean 0 and with the same variance as the original distributions. This
is strange but true; if we mistake a normal population for another, the
âshapeâ of this population still looks normal with the same variance
and a different mean. Apparently, this rare event does not result from
bizarre-looking data.

Probability density

Probability density

Example 11.9.2 (Large deviation theory and football ) Consider a very
simple version of football in which the score is directly related to the
number of yards gained. Assume that the coach has a choice between two
strategies: running or passing. Associated with each strategy is a distribution on the number of yards gained. For example, in general, running

Yards gained in pass

Yards gained in run

FIGURE 11.12. Distribution of yards gained in a run or a pass play.

11.9 CHERNOFF INFORMATION

391

results in a gain of a few yards with very high probability, whereas passing
results in huge gains with low probability. Examples of the distributions
are illustrated in Figure 11.12.
At the beginning of the game, the coach uses the strategy that promises
the greatest expected gain. Now assume that we are in the closing minutes of the game and one of the teams is leading by a large margin.
(Let us ignore ï¬rst downs and adaptable defenses.) So the trailing team
will win only if it is very lucky. If luck is required to win, we might
as well assume that we will be lucky and play accordingly. What is the
appropriate strategy?
Assume that the team has only n plays left and it must gain l yards,
where l is much larger than n times the expected gain under each play. The
probability that the team succeeds in achieving l yards is exponentially
small; hence, we can use the large deviation results and Sanovâs theorem to
calculate the probability
nof this event. To be precise, we wish to calculate
the probability that
i=1 Zi â¥ nÎ±, where Zi are independent random
variables and Zi has a distribution corresponding to the strategy chosen.
The situation is illustrated in Figure 11.13. Let E be the set of types
corresponding to the constraint,



E= P :
P (a)a â¥ Î± .
(11.259)
aâX

If P1 is the distribution corresponding to passing all the time, the probability of winning is the probability that the sample type is in E, which by
â
Sanovâs theorem is 2ânD(P1 ||P1 ) , where P1â is the distribution in E that is
closest to P1 . Similarly, if the coach uses the running game all the time,

E

P2

P1

FIGURE 11.13. Probability simplex for a football game.

392

INFORMATION THEORY AND STATISTICS
â

the probability of winning is 2ânD(P2 ||P2 ) . What if he uses a mixture of
â
strategies? Is it possible that 2ânD(PÎ» ||PÎ» ) , the probability of winning with
a mixed strategy, PÎ» = Î»P1 + (1 â Î»)P2 , is better than the probability of
winning with either pure passing or pure running? The somewhat surprising answer is yes, as can be shown by example. This provides a reason
to use a mixed strategy other than the fact that it confuses the defense.
We end this section with another inequality due to Chernoff, which
is a special version of Markovâs inequality. This inequality is called the
Chernoff bound.
Lemma 11.9.1 Let Y be any random variable and let Ï(s) be the
moment generating function of Y ,
Ï(s) = EesY .

(11.260)

Pr(Y â¥ a) â¤ eâsa Ï(s),

(11.261)

Pr(Y â¥ a) â¤ min eâsa Ï(s).

(11.262)

Then for all s â¥ 0,

and thus
sâ¥0

Proof: Apply Markovâs inequality to the nonnegative random variable

esY .
11.10 FISHER INFORMATION AND THE CRAMEÌRâRAO
INEQUALITY
A standard problem in statistical estimation is to determine the parameters
of a distribution from a sample of data drawn from that distribution.
For example, let X1 , X2 , . . . , Xn be drawn i.i.d. â¼ N(Î¸, 1). Suppose that
we wish to estimate Î¸ from a sample of size n. There are a number of
functions of the data that we can use to estimate Î¸ . For example, we can
use the ï¬rst sample X1 . Although the expected value of X1 is Î¸ , it is clear
that we can do better by using more of the
data. We guess that the best
Xi . Indeed, it can be shown
estimate of Î¸ is the sample mean Xn = n1
that X n is the minimum mean-squared-error unbiased estimator.
We begin with a few deï¬nitions. Let {f (x; Î¸ )}, Î¸ â , denote an
indexed family of densities, f (x; Î¸ ) â¥ 0, f (x; Î¸ ) dx = 1 for all Î¸ â .
Here  is called the parameter set.
Deï¬nition An estimator for Î¸ for sample size n is a function T :
X n â .

11.10 FISHER INFORMATION AND THE CRAMEÌRâRAO INEQUALITY

393

An estimator is meant to approximate the value of the parameter. It
is therefore desirable to have some idea of the goodness of the approximation. We will call the difference T â Î¸ the error of the estimator. The
error is a random variable.
Deï¬nition The bias of an estimator T (X1 , X2 , . . . , Xn ) for the parameter Î¸ is the expected value of the error of the estimator [i.e., the bias is
EÎ¸ T (x1 , x2 , . . . , xn ) â Î¸ ]. The subscript Î¸ means that the expectation is
with respect to the density f (Â·; Î¸ ). The estimator is said to be unbiased
if the bias is zero for all Î¸ â  (i.e., the expected value of the estimator
is equal to the parameter).
Example 11.10.1 Let X1 , X2 , . . . , Xn drawn i.i.d. â¼ f (x) = (1/Î»)
eâx/Î» , x â¥ 0 be a sequence of exponentially distributed random variables.
Estimators of Î» include X1 and X n . Both estimators are unbiased.
The bias is the expected value of the error, and the fact that it is
zero does not guarantee that the error is low with high probability. We
need to look at some loss function of the error; the most commonly
chosen loss function is the expected square of the error. A good estimator should have a low expected squared error and should have an error
that approaches 0 as the sample size goes to inï¬nity. This motivates the
following deï¬nition:
Deï¬nition An estimator T (X1 , X2 , . . . , Xn ) for Î¸ is said to be consistent in probability if
T (X1 , X2 , . . . , Xn ) â Î¸ in probability as n â â.
Consistency is a desirable asymptotic property, but we are interested in
the behavior for small sample sizes as well. We can then rank estimators
on the basis of their mean-squared error.
Deï¬nition An estimator T1 (X1 , X2 , . . . , Xn ) is said to dominate
another estimator T2 (X1 , X2 , . . . , Xn ) if, for all Î¸ ,
E (T1 (X1 , X2 , . . . , Xn ) â Î¸ )2 â¤ E (T2 (X1 , X2 , . . . , Xn ) â Î¸ )2 .
(11.263)
This raises a natural question: Is there a best estimator of Î¸ that dominates every other estimator? To answer this question, we derive the
CrameÌrâRao lower bound on the mean-squared error of any estimator.
We ï¬rst deï¬ne the score function of the distribution f (x; Î¸ ). We then use
the CauchyâSchwarz inequality to prove the CrameÌrâRao lower bound
on the variance of all unbiased estimators.

394

INFORMATION THEORY AND STATISTICS

Deï¬nition The score V is a random variable deï¬ned by
â f (X; Î¸ )
â
â
ln f (X; Î¸ ) = Î¸
,
V =
âÎ¸
f (X; Î¸ )

(11.264)

where X â¼ f (x; Î¸ ).
The mean value of the score is


â
âÎ¸ f (x; Î¸ ) f (x; Î¸ ) dx
f (x; Î¸ )

â
=
f (x; Î¸ ) dx
âÎ¸

â
=
f (x; Î¸ ) dx
âÎ¸
â
1
=
âÎ¸
= 0,

EV =

(11.265)
(11.266)
(11.267)
(11.268)
(11.269)

and therefore EV 2 = var(V ). The variance of the score has a special
signiï¬cance.
Deï¬nition The Fisher information J (Î¸ ) is the variance of the score:

2
â
J (Î¸ ) = EÎ¸
(11.270)
ln f (X; Î¸ ) .
âÎ¸
If we consider a sample of n random variables X1 , X2 , . . . , Xn drawn
i.i.d. â¼ f (x; Î¸ ), we have
f (x1 , x2 , . . . , xn ; Î¸ ) =

n


f (xi ; Î¸ ),

(11.271)

i=1

and the score function is the sum of the individual score functions,
â
ln f (X1 , X2 , . . . , Xn ; Î¸ )
âÎ¸
n

â
=
ln f (Xi ; Î¸ )
âÎ¸

V (X1 , X2 , . . . , Xn ) =

(11.272)
(11.273)

i=1

=

n

i=1

V (Xi ),

(11.274)

11.10 FISHER INFORMATION AND THE CRAMEÌRâRAO INEQUALITY

395

where the V (Xi ) are independent, identically distributed with zero mean.
Hence, the n-sample Fisher information is

Jn (Î¸ ) = EÎ¸

â
ln f (X1 , X2 , . . . , Xn ; Î¸ )
âÎ¸

2

= EÎ¸ V 2 (X1 , X2 , . . . , Xn )
 n
2

V (Xi )
= EÎ¸

(11.275)
(11.276)
(11.277)

i=1

=

n


EÎ¸ V 2 (Xi )

(11.278)

i=1

= nJ (Î¸ ).

(11.279)

Consequently, the Fisher information for n i.i.d. samples is n times the
individual Fisher information. The signiï¬cance of the Fisher information
is shown in the following theorem.
Theorem 11.10.1 (CrameÌrâRao inequality) The mean-squared error
of any unbiased estimator T (X) of the parameter Î¸ is lower bounded by
the reciprocal of the Fisher information:
var(T ) â¥

1
.
J (Î¸ )

(11.280)

Proof: Let V be the score function and T be the estimator. By the
CauchyâSchwarz inequality, we have
(EÎ¸ [(V â EÎ¸ V )(T â EÎ¸ T )])2 â¤ EÎ¸ (V â EÎ¸ V )2 EÎ¸ (T â EÎ¸ T )2 .
(11.281)
Since T is unbiased, EÎ¸ T = Î¸ for all Î¸ . By (11.269), EÎ¸ V = 0 and hence
EÎ¸ (V â EÎ¸ V )(T â EÎ¸ T ) = EÎ¸ (V T ). Also, by deï¬nition, var(V ) = J (Î¸ ).
Substituting these conditions in (11.281), we have
[EÎ¸ (V T )]2 â¤ J (Î¸ )var(T ).

(11.282)

Now,

EÎ¸ (V T ) =

â
âÎ¸ f (x; Î¸ ) T (x)f (x; Î¸ ) dx
f (x; Î¸ )

(11.283)

396

INFORMATION THEORY AND STATISTICS



â
f (x; Î¸ )T (x) dx
âÎ¸

â
=
f (x; Î¸ )T (x) dx
âÎ¸
â
EÎ¸ T
=
âÎ¸
â
=
Î¸
âÎ¸
= 1,
=

(11.284)
(11.285)
(11.286)
(11.287)
(11.288)

where the interchange of differentiation and integration in (11.285) can be
justiï¬ed using the bounded convergence theorem for appropriately well
behaved f (x; Î¸ ), and (11.287) follows from the fact that the estimator T
is unbiased. Substituting this in (11.282), we obtain
var(T ) â¥

1
,
J (Î¸ )

which is the CrameÌrâRao inequality for unbiased estimators.

(11.289)


By essentially the same arguments, we can show that for any estimator
E(T â Î¸ )2 â¥

(1 + bT (Î¸ ))2
+ bT2 (Î¸ ),
J (Î¸ )

(11.290)

where bT (Î¸ ) = EÎ¸ T â Î¸ and bT (Î¸ ) is the derivative of bT (Î¸ ) with respect
to Î¸ . The proof of this is left as a problem at the end of the chapter.
Example 11.10.2 Let X1 , X2 , . . . , Xn be i.i.d. â¼ N(Î¸, Ï 2
), Ï 2 known.
1
Here J (Î¸ ) = n/Ï 2 . Let T (X1 , X2 , . . . , Xn ) = X n = n Xi . Then
EÎ¸ (Xn â Î¸ )2 = Ï 2 /n = 1/J (Î¸ ). Thus, Xn is the minimum variance unbiased estimator of Î¸ , since it achieves the CrameÌrâRao lower bound.
The CrameÌrâRao inequality gives us a lower bound on the variance
for all unbiased estimators. When this bound is achieved, we call the
estimator efï¬cient.
Deï¬nition An unbiased estimator T is said to be efï¬cient if it meets
1
the CrameÌrâRao bound with equality [i.e., if var(T ) = J (Î¸
) ].

SUMMARY

397

The Fisher information is therefore a measure of the amount of âinformationâ about Î¸ that is present in the data. It gives a lower bound on the
error in estimating Î¸ from the data. However, it is possible that there does
not exist an estimator meeting this lower bound.
We can generalize the concept of Fisher information to the multiparameter case, in which case we deï¬ne the Fisher information matrix J (Î¸ )
with elements

â
â
Jij (Î¸ ) = f (x; Î¸ )
ln f (x; Î¸ )
ln f (x; Î¸ ) dx.
(11.291)
âÎ¸i
âÎ¸j
The CrameÌrâRao inequality becomes the matrix inequality
 â¥ J â1 (Î¸ ),

(11.292)

where  is the covariance matrix of a set of unbiased estimators for the
parameters Î¸ and  â¥ J â1 (Î¸ ) in the sense that the difference  â J â1 is
a nonnegative deï¬nite matrix. We will not go into the details of the proof
for multiple parameters; the basic ideas are similar.
Is there a relationship between the Fisher information J (Î¸ ) and quantities such as entropy deï¬ned earlier? Note that Fisher information is deï¬ned
with respect to a family of parametric distributions, unlike entropy, which
is deï¬ned for all distributions. But we can parametrize any distribution
f (x) by a location parameter Î¸ and deï¬ne Fisher information with respect
to the family of densities f (x â Î¸ ) under translation. We explore the
relationship in greater detail in Section 17.8, where we show that while
entropy is related to the volume of the typical set, the Fisher information
is related to the surface area of the typical set. Further relationships of
Fisher information to relative entropy are developed in the problems.

SUMMARY
Basic identities
Qn (x) = 2ân(D(Px ||Q)+H (Px )) ,
|Pn | â¤ (n + 1)|X | ,
.

(11.293)
(11.294)

|T (P )| = 2nH (P ) ,

(11.295)

Qn (T (P )) = 2ânD(P ||Q) .

(11.296)

.

398

INFORMATION THEORY AND STATISTICS

Universal data compression
â

Pe(n) â¤ 2ânD(PR ||Q)

for all Q,

(11.297)

D(PRâ ||Q) =

D(P ||Q).

(11.298)

where
min

P :H (P )â¥R

Large deviations (Sanovâs theorem)
Qn (E) = Qn (E â© Pn ) â¤ (n + 1)|X | 2ânD(P

â ||Q)

,

D(P â ||Q) = min D(P ||Q).

(11.299)
(11.300)

P âE

If E is the closure of its interior, then
Qn (E) = 2ânD(P
.

â ||Q)

.

(11.301)

L1 bound on relative entropy

D(P1 ||P2 ) â¥

1
||P1 â P2 ||21 .
2 ln 2

(11.302)

Pythagorean theorem. If E is a convex set of types, distribution Q â
/
E, and P â achieves D(P â ||Q) = minP âE D(P ||Q), we have
D(P ||Q) â¥ D(P ||P â ) + D(P â ||Q)

(11.303)

for all P â E.
Conditional limit theorem. If X1 , X2 , . . . , Xn i.i.d. â¼ Q, then
Pr(X1 = a|PXn â E) â P â (a)

in probability,

where P â minimizes D(P ||Q) over P â E. In particular,
 n


1 
Q(a)eÎ»a

Xi â¥ Î± â 
.
Pr X1 = a 
Î»x
n
x Q(x)e

(11.304)

(11.305)

i=1

NeymanâPearson lemma. The optimum test between two densities
P1 and P2 has a decision region of the form âaccept P = P1 if
P1 (x1 ,x2 ,...,xn )
P2 (x1 ,x2 ,...,xn ) > T .â

PROBLEMS

399

ChernoffâStein lemma. The best achievable error exponent Î²n if
Î±n â¤ :
Î²n =

lim

nââ

min
Î²n ,
An â X n
Î±n < 

1
log Î²n = âD(P1 ||P2 ).
n

(11.306)

(11.307)

Chernoff information. The best achievable exponent for a Bayesian
probability of error is
D â = D(PÎ»â ||P1 ) = D(PÎ»â ||P2 ),

(11.308)

where
PÎ» = 

P1Î» (x)P21âÎ» (x)

aâX

P1Î» (a)P21âÎ» (a)

(11.309)

with Î» = Î»â chosen so that
D(PÎ» ||P1 ) = D(PÎ» ||P2 ).

(11.310)

Fisher information

J (Î¸ ) = EÎ¸

2
â
ln f (x; Î¸ ) .
âÎ¸

(11.311)

CrameÌrâRao inequality. For any unbiased estimator T of Î¸ ,
EÎ¸ (T (X) â Î¸ )2 = var(T ) â¥

1
.
J (Î¸ )

(11.312)

PROBLEMS
11.1

ChernoffâStein lemma. Consider the two-hypothesis test
H1 : f = f1
Find D(f1  f2 ) if

vs.

H2 : f = f2 .

400

INFORMATION THEORY AND STATISTICS

(a) fi (x) = N (0, Ïi2 ), i = 1, 2.
(b) fi (x) = Î»i eâÎ»i x , x â¥ 0, i = 1, 2.
(c) f1 (x) is the uniform density over the interval [0, 1] and f2 (x)
is the uniform density over [a, a + 1]. Assume that 0 < a < 1.
(d) f1 corresponds to a fair coin and f2 corresponds to a twoheaded coin.
11.2

Relation between D(P  Q) and chi-square.
statistic
Ï 2 = x

Show that the Ï 2

(P (x) â Q(x))2
Q(x)

is (twice) the ï¬rst term in the Taylor series expansion of D(P 
Q) about Q. Thus, D(P  Q) = 12 Ï 2 + Â· Â· Â· . [Suggestion: Write
P âQ
P
Q = 1 + Q and expand the log.]
11.3

Error exponent for universal codes. A universal source code of
â
.
rate R achieves a probability of error Pe(n) = eânD(P Q) , where
Q is the true distribution and P â achieves min D(P  Q) over all
P such that H (P ) â¥ R.
(a) Find P â in terms of Q and R.
(b) Now let X be binary. Find the region of source probabilities Q(x), x â {0, 1}, for which rate R is sufï¬cient for the
universal source code to achieve Pe(n) â 0.

11.4

Sequential projection. We wish to show that projecting
Q onto

QÌ
onto
P
is
the
same
P
P1 and then projecting the projection
1
2

as projecting Q directly onto P1 P2 . Let P1 be the set of probability mass functions on X satisfying

p(x) = 1,
(11.313)


x

p(x)hi (x) â¥ Î±i ,

i = 1, 2, . . . , r.

(11.314)

x

Let P2 be the set of probability mass functions on X satisfying

p(x) = 1,
(11.315)

x

x

p(x)gj (x) â¥ Î²j ,

j = 1, 2, . . . , s.

(11.316)

PROBLEMS

401


Suppose that Q â P1 P2 . Let P â minimize D(P Q) over all
P â P1 . Let R â minimize D(R  Q) over all R â
P1 P2 . Argue
that R â minimizes D(R  P â ) over all R â P1 P2 .
11.5

Counting. Let X = {1, 2, . . . ,
m}. Show that the number of se1
n
n
quences x â X satisfying n ni=1 g(xi ) â¥ Î± is approximately
â
equal to 2nH , to ï¬rst order in the exponent, for n sufï¬ciently large,
where
Hâ =

11.6

P:

m

max

H (P ).

(11.317)

i=1 P (i)g(i)â¥Î±

Biased estimates may be better . Consider the problem of estimating Âµ and Ï 2 from n samples of data drawn i.i.d. from a
N(Âµ, Ï 2 ) distribution.
(a) Show that Xn is an unbiased estimator of Âµ.
(b) Show that the estimator
1
=
(Xi â X n )2
n
n

Sn2

(11.318)

i=1

is a biased estimator of Ï 2 and the estimator
1 
(Xi â X n )2
nâ1
n

2
Snâ1
=

(11.319)

i=1

is unbiased.
(c) Show that Sn2 has a lower mean-squared error than that of
2
. This illustrates the idea that a biased estimator may be
Snâ1
âbetterâ than an unbiased estimator.
11.7

Fisher information and relative entropy.
family {pÎ¸ (x)} that
lim


Î¸ âÎ¸

11.8

Show for a parametric

1
1
D(pÎ¸ ||pÎ¸  ) =
J (Î¸ ).

2
(Î¸ â Î¸ )
ln 4

(11.320)

Examples of Fisher information. The Fisher information J ()
for the family fÎ¸ (x), Î¸ â R is deï¬ned by




(fÎ¸ )2
âfÎ¸ (X)/âÎ¸ 2
=
.
J (Î¸ ) = EÎ¸
fÎ¸ (X)
fÎ¸
Find the Fisher information for the following families:

402

INFORMATION THEORY AND STATISTICS

(a) fÎ¸ (x) = N (0, Î¸ ) =

x2

â 1 eâ 2Î¸
2Ï Î¸

(b) fÎ¸ (x) = Î¸ eâÎ¸ x , x â¥ 0
(c) What is the CrameÌrâRao lower bound on EÎ¸ (Î¸Ì (X) â Î¸ )2 ,
where Î¸Ì(X) is an unbiased estimator of Î¸ for parts (a) and
(b)?
11.9

Two conditionally independent looks double the Fisher information. Let gÎ¸ (x1 , x2 ) = fÎ¸ (x1 )fÎ¸ (x2 ). Show that Jg (Î¸ ) = 2Jf (Î¸ ).

11.10

Joint distributions and product distributions. Consider a joint
distribution Q(x, y) with marginals Q(x) and Q(y). Let E be
the set of types that look jointly typical with respect to Q:
E = {P (x, y) : â



P (x, y) log Q(x) â H (X) = 0,

x,y

â



P (x, y) log Q(y) â H (Y ) = 0,

x,y

â



P (x, y) log Q(x, y)

x,y

âH (X, Y ) = 0}.

(11.321)

(a) Let Q0 (x, y) be another distribution on X Ã Y. Argue
that the distribution P â in E that is closest to Q0 is of the
form
P â (x, y) = Q0 (x, y)eÎ»0 +Î»1 log Q(x)+Î»2 log Q(y)+Î»3 log Q(x,y) ,
(11.322)
where Î»0 , Î»1 , Î»2 , and Î»3 are chosen to satisfy the constraints.
Argue that this distribution is unique.
(b) Now let Q0 (x, y) = Q(x)Q(y). Verify that Q(x, y) is of the
form (11.322) and satisï¬es the constraints. Thus, P â (x, y) =
Q(x, y) (i.e., the distribution in E closest to the product distribution is the joint distribution).
11.11

CrameÌrâRao inequality with a bias term. Let X â¼ f (x; Î¸ ) and
let T (X) be an estimator for Î¸ . Let bT (Î¸ ) = EÎ¸ T â Î¸ be the bias
of the estimator. Show that
[1 + bT (Î¸ )]
+ bT2 (Î¸ ).
J (Î¸ )
2

E(T â Î¸ )2 â¥

(11.323)

PROBLEMS

11.12

403

Hypothesis testing. Let X1 , X2 , . . . , Xn be i.i.d. â¼ p(x). Consider the hypothesis test H1 : p = p1 vs. H2 : p = p2 . Let
ï£±
1
ï£´
ï£² 2 , x = â1
1
p1 (x) =
4, x = 0
ï£´
ï£³ 1, x = 1
4
and
p2 (x) =

ï£±
ï£´
ï£²
ï£´
ï£³

x = â1
x=0
x = 1.

1
4,
1
4,
1
2,

Find the error exponent for Pr{Decide H2 |H1 true} in the best
hypothesis test of H1 vs. H2 subject to Pr{Decide H1 |H2 true}
â¤ 12 .
11.13

Sanovâs theorem. Prove a simple version of Sanovâs theorem for
Bernoulli(q) random variables.
Let the proportion of 1âs in the sequence X1 , X2 , . . . , Xn be
1
Xn =
Xi .
n
n

(11.324)

i=1

By the law of large numbers, we would expect Xn to be close
to q for large n. Sanovâs theorem deals with the probability that
pXn is far away from q. In particular, for concreteness, if we take
p > q > 12 , Sanovâs theorem states that

1
log Pr (X1 , X2 , . . . , Xn ) : X n â¥ p
n
p
1âp
â p log + (1 â p) log
q
1âq

â

= D((p, 1 â p)||(q, 1 â q)).

(11.325)

Justify the following steps:
â¢

 
n

n i
q (1 â q)nâi .
Pr (X1 , X2 , . . . , Xn ) : X n â¥ p â¤
i


i=np

(11.326)

404

INFORMATION THEORY AND STATISTICS
â¢
â¢
â¢

Argue that the term corresponding to i = np is the largest
term in the sum on the right-hand side of the last equation.
Show that this term is approximately 2ânD .
Prove an upper bound on the probability in Sanovâs theorem
using the steps above. Use similar arguments to prove a lower
bound and complete the proof of Sanovâs theorem.

11.14

Sanov . Let Xi be i.i.d. â¼ N (0, Ï 2 ).

(a) Find the exponent in the behavior of Pr{ n1 ni=1 Xi2 â¥ Î± 2 }.
This can be done from ï¬rst principles (since the normal distribution is nice) or by using Sanovâs theorem.

(b) What do the data look like if n1 ni=1 Xi2 â¥ Î±? That is, what
is the P â that minimizes D(P  Q)?

11.15

Counting states. Suppose that an atom is equally likely to be in
each of six states, X â {s1 , s2 , s3 , . . . , s6 }. One observes n atoms
X1 , X2 , . . . , Xn independently drawn according to this uniform
distribution. It is observed that the frequency of occurrence of
state s1 is twice the frequency of occurrence of state s2 .
(a) To ï¬rst order in the exponent, what is the probability of
observing this event?
(b) Assuming n large, ï¬nd the conditional distribution of the state
of the ï¬rst atom X1 , given this observation.

11.16

Hypothesis testing. Let {Xi } be i.i.d. â¼ p(x), x â {1, 2, . . .}.
Consider two hypotheses,
	 
x H0 : p(x) = p0 (x) vs. H1 : p(x) =
p1 (x), where p0 (x) = 12 and p1 (x) = qp xâ1 , x = 1, 2, 3, . . . .
(a) Find D(p0  p1 ).
(b) Let Pr{H0 } = 12 . Find the minimal probability of error test for
H0 vs. H1 given data X1 , X2 , . . . , Xn â¼ p(x).

11.17

Maximum likelihood estimation. Let {fÎ¸ (x)} denote a parametric
family of densities with parameter Î¸  R. Let X1 , X2 , . . . , Xn be
i.i.d. â¼ fÎ¸ (x). The function
 n


lÎ¸ (x n ) = ln
fÎ¸ (xi )
i=1

is known as the log likelihood function. Let Î¸0 denote the true
parameter value.

PROBLEMS

405

(a) Let the expected log likelihood be
 n
  
n

n
fÎ¸ (xi )
fÎ¸0 (xi )dx n ,
EÎ¸0 lÎ¸ (X ) =
ln
i=1

i=1

and show that
EÎ¸0 (l(X n )) = (âh(fÎ¸0 ) â D(fÎ¸0 ||fÎ¸ ))n.
(b) Show that the maximum over Î¸ of the expected log likelihood
is achieved by Î¸ = Î¸0 .
11.18

Large deviations. Let X1 , X2 , . . . be i.i.d. random variables
drawn according to the geometric distribution
Pr{X = k} = p kâ1 (1 â p),

k = 1, 2, . . . .

Find good estimates (to ï¬rst order in the exponent) of:

(a) Pr{ n1 ni=1 Xi â¥ Î±}.

(b) Pr{X1 = k| n1 ni=1 Xi â¥ Î±}.
(c) Evaluate parts (a) and (b) for p = 12 , Î± = 4.
11.19

Another expression for Fisher information.
parts to show that
J (Î¸ ) = âE

11.20

Use integration by

â 2 ln fÎ¸ (x)
.
âÎ¸ 2

Stirlingâs approximation. Derive a weak form of Stirlingâs
approximation for factorials; that is, show that
 n n
 n n
â¤ n! â¤ n
(11.327)
e
e
using the approximation of integrals by sums. Justify the following
steps:
ln(n!) =

nâ1

i=2



nâ1

ln(i) + ln(n) â¤

ln x dx + ln n = Â· Â· Â·

2

(11.328)

406

INFORMATION THEORY AND STATISTICS

and
ln(n!) =

n

i=1

11.21



n

ln(i) â¥

ln x dx = Â· Â· Â· .

(11.329)

0

 
n
. Use the simple approximation of ProbAsymptotic value of
k
lem 11.20 to show that if 0 â¤ p â¤ 1, and k = np (i.e., k is the
largest integer less than or equal to np), then
 
n
1
lim log
= âp log p â (1 â p) log(1 â p) = H (p).
nââ n
k
(11.330)
, m be a probability distribution on m symNow let pi , i = 1, . . . 
bols (i.e., pi â¥ 0 and i pi = 1). What is the limiting value of


1
n

log
n
np1  np2  . . . npmâ1  n â mâ1
j =0 npj 
=

n!
1
log
?

n
np1 ! np2 ! . . . npmâ1 ! (n â mâ1
j =0 npj )!
(11.331)

11.22

Running difference. Let X1 , X2 , . . . , Xn be i.i.d. â¼ Q1 (x), and
n
n
Y1 , Y2 , . . . , Yn be i.i.d. â¼ Q
2n(y). Let X
n and Y be independent.
Find an expression for Pr{ i=1 Xi â i=1 Yi â¥ nt} good to ï¬rst
order in the exponent. Again, this answer can be left in parametric
form.

11.23

Large likelihoods. Let X1 , X2 , . . . be i.i.d. â¼ Q(x), x â {1, 2,
. . . , m}. Let P (x) be some other probability mass function. We
form the log likelihood ratio
P n (X1 , X2 , . . . , Xn )
1
P (Xi )
1
log n
=
log
n
Q (X1 , X2 , . . . , Xn )
n
Q(Xi )
n

i=1

of the sequence X n and ask for the probability that it exceeds a
certain threshold. Speciï¬cally, ï¬nd (to ï¬rst order in the exponent)


P (X1 , X2 , . . . , Xn )
n 1
log
>0 .
Q
n
Q(X1 , X2 , . . . , Xn )
There may be an undetermined parameter in the answer.

PROBLEMS

407

11.24

Fisher information for mixtures. Let f1 (x) and f0 (x) be two
given probability densities. Let Z be Bernoulli(Î¸ ), where Î¸ is
unknown. Let X â¼ f1 (x) if Z = 1 and X â¼ f0 (x) if Z = 0.
(a) Find the density fÎ¸ (x) of the observed X.
(b) Find the Fisher information J (Î¸ ).
(c) What is the CrameÌrâRao lower bound on the mean-squared
error of an unbiased estimate of Î¸ ?
(d) Can you exhibit an unbiased estimator of Î¸ ?

11.25

Bent coins.

Let {Xi } be iid â¼ Q, where

 
m k
q (1 â q)mâk for k = 0, 1, 2, . . . , m.
Q(k) = Pr(Xi = k) =
k
Thus, the Xi âs are iid â¼ Binomial(m, q). Show that as n â â,

 
1 n
Xi â¥ Î± â P â (k),
Pr X1 = k 
n


i=1

where

Pâ

is Binomial(m, Î») (i.e.,

P â (k)

some Î» â [0, 1]). Find Î».
11.26

 
m k
Î» (1 â Î»)mâk for
=
k

Conditional limiting distribution
(a) Find the exact value of




n
1 
1
Xi =
Pr X1 = 1 
n
4

(11.332)

i=1

if X1 , X2 , . . . , are Bernoulli( 23 ) and n is a multiple of 4.
(b) Now let Xi {â1, 0, 1} and let X1 , X2 . . . be i.i.d. uniform
over {â1, 0, +1}. Find the limit of



 
1 n 2 1
Pr X1 = +1 
Xi =
n
2
i=1

for n = 2k, k â â.

(11.333)

408

11.27

INFORMATION THEORY AND STATISTICS

Variational inequality.
that

Verify for positive random variables X

!
"
log EP (X) = sup EQ (log X) â D(Q||P ) ,

(11.334)

Q



where EP (X) = x xP (x) and D(Q||P ) = x Q(x) log Q(x)
P (x)

and the supremum is over all Q(x) â¥ 0,
Q(x) = 1.
It is enough to extremize J (Q) = EQ ln X â D(Q||P ) +
Î»( Q(x) â 1).
11.28

Type constraints
(a) Find constraints on the type PXn such that the sample variance

Xn2 â (Xn )2 â¤ Î±,
where
Xn2 = n1 ni=1 Xi2
and
1 n
Xn = n i=1 Xi .
(b) Find the exponent in the probability Qn (Xn2 â (Xn )2 â¤ Î±).
You can leave the answer in parametric form.

11.29

Uniform distribution on the simplex . Which of these methods
will generate a sample from
n the uniform distribution on the simplex {x â R n : xi â¥ 0,
i=1 xi = 1}?

(a) Let Yi be i.i.d. uniform [0, 1] with Xi = Yi / nj=1 Yj .
(b) Let Yi be 
i.i.d. exponentially distributed â¼ Î»eâÎ»y , y â¥ 0, with
Xi = Yi / nj=1 Yj .
(c) (Break stick into n parts) Let Y1 , Y2 , . . . , Ynâ1 be i.i.d. uniform [0, 1], and let Xi be the length of the ith interval.

HISTORICAL NOTES
The method of types evolved from notions of strong typicality; some
of the ideas were used by Wolfowitz [566] to prove channel capacity
theorems. The method was fully developed by CsiszaÌr and KoÌrner [149],
who derived the main theorems of information theory from this viewpoint.
The method of types described in Section 11.1 follows the development
in CsiszaÌr and KoÌrner. The L1 lower bound on relative entropy is due to
CsiszaÌr [138], Kullback [336], and Kemperman [309]. Sanovâs theorem
[455] was generalized by CsiszaÌr [141] using the method of types.

CHAPTER 12

MAXIMUM ENTROPY

The temperature of a gas corresponds to the average kinetic energy of the
molecules in the gas. What can we say about the distribution of velocities in the gas at a given temperature? We know from physics that this
distribution is the maximum entropy distribution under the temperature
constraint, otherwise known as the MaxwellâBoltzmann distribution. The
maximum entropy distribution corresponds to the macrostate (as indexed
by the empirical distribution) that has the most microstates (the individual
gas velocities). Implicit in the use of maximum entropy methods in physics
is a sort of AEP which says that all microstates are equally probable.
12.1

MAXIMUM ENTROPY DISTRIBUTIONS

Consider the following problem: Maximize the entropy h(f ) over all
probability densities f satisfying
1. f
 (x) â¥ 0, with equality outside the support set S
2. S f (x) dx = 1
3. S f (x)ri (x) dx = Î±i for 1 â¤ i â¤ m.

(12.1)

Thus, f is a density on support set S meeting certain moment constraints Î±1 , Î±2 , . . . , Î±m .
Approach 1 ( Calculus) The differential entropy h(f ) is a concave
function over a convex set. We form the functional



m

Î»i f ri
(12.2)
J (f ) = â f ln f + Î»0 f +
i=1

and âdifferentiateâ with respect to f (x), the xth component of f , to obtain

âJ
= â ln f (x) â 1 + Î»0 +
Î»i ri (x).
âf (x)
m

(12.3)

i=1

Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

409

410

MAXIMUM ENTROPY

Setting this equal to zero, we obtain the form of the maximizing density
f (x) = eÎ»0 â1+

m

i=1 Î»i ri (x)

x â S,

,

(12.4)

where Î»0 , Î»1 , . . . , Î»m are chosen so that f satisï¬es the constraints.
The approach using calculus only suggests the form of the density that
maximizes the entropy. To prove that this is indeed the maximum, we can
take the second variation. It is simpler to use the information inequality
D(g||f ) â¥ 0.
Approach 2 (Information inequality) If g satisï¬es (12.1) and if f â is
of the form (12.4), then 0 â¤ D(g||f â ) = âh(g) + h(f â ). Thus h(g) â¤
h(f â ) for all g satisfying the constraints. We prove this in the following
theorem.
Theorem
Let f â (x) = fÎ» (x)
 12.1.1 (Maximum entropy distribution)
Î»0 + m
Î»
r
(x)
i
i
i=1
, x â S, where Î»0 , . . . , Î»m are chosen so that f â satisï¬es
=e
â
(12.1). Then f uniquely maximizes h(f ) over all probability densities f
satisfying constraints (12.1).
Proof: Let g satisfy the constraints (12.1). Then

h(g) = â g ln g


g â
f
fâ
S

â
= âD(g||f ) â g ln f â

=â

g ln

(12.8)




g Î»0 +
Î»i ri

(12.9)




f â Î»0 +
Î»i ri

(12.10)

f â ln f â

(12.11)

S



= â

(c)

(12.7)

g ln f â

â¤â

(b)

(12.6)

S



(a)

(12.5)

S



S



S

=â

=â
S
â

= h(f ),

(12.12)

where (a) follows from the nonnegativity of relative entropy, (b) follows
from the deï¬nition of f â , and (c) follows from the fact that both f â
and g satisfy the constraints. Note that equality holds in (a) if and only

12.2

EXAMPLES

411

if g(x) = f â (x) for all x, except for a set of measure 0, thus proving
uniqueness.

The same approach holds for discrete entropies and for multivariate
distributions.
12.2

EXAMPLES

Example 12.2.1 (One-dimensional gas with a temperature constraint)
Let the constraints be EX = 0 and EX 2 = Ï 2 . Then the form of the
maximizing distribution is
f (x) = eÎ»0 +Î»1 x+Î»2 x .
2

(12.13)

To ï¬nd the appropriate constants, we ï¬rst recognize that this distribution
has the same form as a normal distribution. Hence, the density that satisï¬es
the constraints and also maximizes the entropy is the N(0, Ï 2 ) distribution:
2
1
âx
e 2Ï 2 .
(12.14)
f (x) = â
2Ï Ï 2
Example 12.2.2 (Dice, no constraints) Let S = {1, 2, 3, 4, 5, 6}. The
distribution that maximizes the entropy is the uniform distribution, p(x) =
1
6 for x â S.

Example 12.2.3 (Dice, with EX = ipi = Î±) This important example was used by Boltzmann. Suppose that n dice are thrown on the table
and we are told that the total number of spots showing is nÎ±. What
proportion of the dice are showing face i, i = 1, 2, . . . , 6?
One way of going about this is to count the number
 of ways that

n
n dice can fall so that ni dice show face i. There are
n 1 , n 2 , . . . , n6
such
a macrostate indexed by (n1 , n2 , . . . , n6 ) corresponding
 ways. This is 
n
microstates, each having probability 61n . To ï¬nd the
to
n1 , n2 , . . . , n6


n
under
most probable macrostate, we wish to maximize
n 1 , n 2 , . . . , n6
the constraint observed on the total number of spots,
6

ini = nÎ±.
(12.15)

i=1

Using a crude Stirlingâs approximation, n! â ( ne )n , we ï¬nd that


( n )n
n
(12.16)
â 	6 e n
i ni
n1 , n2 , . . . , n 6
(
)
i=1
e

412

MAXIMUM ENTROPY

=

6  ni


n
i=1

=e

nH

(12.17)

ni


n6
n1 n2
n , n ,..., n



.

(12.18)




n
Thus, maximizing
under the constraint (12.15) is almost
n 1 , n 2 , . . . , n6

equivalent to maximizing H (p1 , p2 , . . . , p6 ) under the constraint ipi =
Î±. Using Theorem 12.1.1 under this constraint, we ï¬nd the maximum
entropy probability mass function to be
eÎ»i
piâ = 6
,
(12.19)
Î»i
i=1 e

where Î» is chosen so that ipiâ = Î±. Thus, the most probable macrostate
is (np1â , np2â . . . . , np6â ), and we expect to ï¬nd nâi = npiâ dice showing
face i.
In Chapter 11 we show that the reasoning and the approximations are
essentially correct. In fact, we show that not only is the maximum entropy
macrostate the most likely, but it also contains almost all of the probability.
Speciï¬cally, for rational Î±,
 n





 Ni

Pr  â piâ  < , i = 1, 2, . . . , 6 
Xi = nÎ± â 1,
(12.20)

n
i=1

as n â â along the subsequence such that nÎ± is an integer.
Example 12.2.4 Let S = [a, b], with no other constraints. Then the
maximum entropy distribution is the uniform distribution over this range.
Example 12.2.5 S = [0, â) and EX = Âµ. Then the entropy-maximizing distribution is
1 x
f (x) = eâ Âµ , x â¥ 0.
(12.21)
Âµ
This problem has a physical interpretation. Consider the distribution of the
height X of molecules in the atmosphere. The average potential energy of
the molecules is ï¬xed, and the gas tends to the distribution that has the
maximum entropy subject to the constraint that E(mgX) is ï¬xed. This
is the exponential distribution with density f (x) = Î»eâÎ»x , x â¥ 0. The
density of the atmosphere does indeed have this distribution.

12.3 ANOMALOUS MAXIMUM ENTROPY PROBLEM

413

Example 12.2.6 S = (ââ, â), and EX = Âµ. Here the maximum entropy is inï¬nite, and there is no maximum entropy distribution. (Consider
normal distributions with larger and larger variances.)
Example 12.2.7 S = (ââ, â), EX = Î±1 , and EX 2 = Î±2 . The maximum entropy distribution is N(Î±1 , Î±2 â Î±12 ).
Example 12.2.8 S = Rn , EXi Xj = Kij , 1 â¤ i, j â¤ n. This is a multivariate example, but the same analysis holds and the maximum entropy
density is of the form
f (x) = eÎ»0 +


i,j

Î»ij xi xj

.

(12.22)

Since the exponent is a quadratic form, it is clear by inspection that the
density is a multivariate normal with zero mean. Since we have to satisfy
the second moment constraints, we must have a multivariate normal with
covariance Kij , and hence the density is
1 T â1
1
eâ 2 x K x ,
f (x) = â n
2Ï |K|1/2
which has an entropy

1
h(Nn (0, K)) = log(2Ï e)n |K|,
2
as derived in Chapter 8.

(12.23)

(12.24)

Example 12.2.9 Suppose that we have the same constraints as in Example 12.2.8, but EXi Xj = Kij only for some restricted set of (i, j ) â A.
For example, we might know only Kij for i = j Â± 2. Then by comparing
(12.22) and (12.23), we can conclude that (K â1 )ij = 0 for (i, j ) â Ac
(i.e., the entries in the inverse of the covariance matrix are 0 when (i, j )
is outside the constraint set).
12.3

ANOMALOUS MAXIMUM ENTROPY PROBLEM

We have proved that the maximum entropy distribution subject to the
constraints

hi (x)f (x) dx = Î±i
(12.25)
S

is of the form

f (x) = eÎ»0 +



Î»i hi (x)

if Î»0 , Î»1 , . . . , Î»p satisfying the constraints (12.25) exist.

(12.26)

414

MAXIMUM ENTROPY

We now consider a tricky problem in which the Î»i cannot be chosen
to satisfy the constraints. Nonetheless, the âmaximumâ entropy can be
found. We consider the following problem: Maximize the entropy subject
to the constraints
 â
f (x) dx = 1,
(12.27)



ââ
â

xf (x) dx = Î±1 ,

(12.28)

x 2 f (x) dx = Î±2 ,

(12.29)

x 3 f (x) dx = Î±3 .

(12.30)

ââ
â

ââ
 â
ââ

Here, the maximum entropy distribution, if it exists, must be of the form
f (x) = eÎ»0 +Î»1 x+Î»2 x

2 +Î» x 3
3

.

(12.31)

â
But if Î»3 is nonzero, ââ f = â and the density cannot be normalized.
So Î»3 must be 0. But then we have four equations and only three variables,
so that in general it is not possible to choose the appropriate constants.
The method seems to have failed in this case.
The reason for the apparent failure is simple: The entropy has a least
upper bound under these constraints, but it is not possible to attain it. Consider the corresponding problem with only ï¬rst and second moment constraints. In this case, the results of Example 12.2.1 show that the entropymaximizing distribution is the normal with the appropriate moments. With
the additional third moment constraint, the maximum entropy cannot be
higher. Is it possible to achieve this value?
We cannot achieve it, but we can come arbitrarily close. Consider a
normal distribution with a small âwiggleâ at a very high value of x. The
moments of the new distribution are almost the same as those of the old
one, the biggest change being in the third moment. We can bring the
ï¬rst and second moments back to their original values by adding new
wiggles to balance out the changes caused by the ï¬rst. By choosing the
position of the wiggles, we can get any value of the third moment without
reducing the entropy signiï¬cantly below that of the associated normal.
Using this method, we can come arbitrarily close to the upper bound for
the maximum entropy distribution. We conclude that
sup h(f ) = h(N(0, Î±2 â Î±12 )) =

1
ln 2Ï e(Î±2 â Î±12 ).
2

(12.32)

12.4

SPECTRUM ESTIMATION

415

This example shows that the maximum entropy may only be -achievable.
12.4

SPECTRUM ESTIMATION

Given a stationary zero-mean stochastic process {Xi }, we deï¬ne the autocorrelation function as
R(k) = EXi Xi+k .

(12.33)

The Fourier transform of the autocorrelation function for a zero-mean
process is the power spectral density S(Î»):
S(Î») =

â


R(m)eâimÎ» ,

âÏ < Î» â¤ Ï,

(12.34)

m=ââ

â
where i = â1. Since the power spectral density is indicative of the
structure of the process, it is useful to form an estimate from a sample of
the process.
There are many methods to estimate the power spectrum. The simplest
way is to estimate the autocorrelation function by taking sample averages
for a sample of length n,
RÌ(k) =

nâk
1 
Xi Xi+k .
nâk

(12.35)

i=1

If we use all the values of the sample correlation function RÌ(Â·) to calculate the spectrum, the estimate that we obtain from (12.34) does not
converge to the true power spectrum for large n. Hence, this method, the
periodogram method, is rarely used. One of the reasons for the problem
with the periodogram method is that the estimates of the autocorrelation
function from the data have different accuracies. The estimates for low
values of k (called the lags) are based on a large number of samples and
those for high k on very few samples. So the estimates are more accurate
at low k. The method can be modiï¬ed so that it depends only on the
autocorrelations at low k by setting the higher lag autocorrelations to 0.
However, this introduces some artifacts because of the sudden transition to
zero autocorrelation. Various windowing schemes have been suggested to
smooth out the transition. However, windowing reduces spectral resolution
and can give rise to negative power spectral estimates.
In the late 1960s, while working on the problem of spectral estimation for
geophysical applications, Burg suggested an alternative method. Instead of

416

MAXIMUM ENTROPY

setting the autocorrelations at high lags to zero, he set them to values that
make the fewest assumptions about the data (i.e., values that maximize the
entropy rate of the process). This is consistent with the maximum entropy
principle as articulated by Jaynes [294]. Burg assumed the process to be
stationary and Gaussian and found that the process which maximizes the
entropy subject to the correlation constraints is an autoregressive Gaussian
process of the appropriate order. In some applications where we can assume
an underlying autoregressive model for the data, this method has proved
useful in determining the parameters of the model (e.g., linear predictive
coding for speech). This method (known as the maximum entropy method
or Burgâs method ) is a popular method for estimation of spectral densities.
We prove Burgâs theorem in Section 12.6.
12.5

ENTROPY RATES OF A GAUSSIAN PROCESS

In Chapter 8 we deï¬ned the differential entropy of a continuous random
variable. We can now extend the deï¬nition of entropy rates to real-valued
stochastic processes.
Deï¬nition The differential entropy rate of a stochastic process {Xi }, Xi â
R, is deï¬ned to be
h(X1 , X2 , . . . , Xn )
nââ
n

h(X ) = lim

(12.36)

if the limit exists.
Just as in the discrete case, we can show that the limit exists for stationary processes and that the limit is given by the two expressions
h(X1 , X2 , . . . , Xn )
nââ
n
= lim h(Xn |Xnâ1 , . . . , X1 ).

h(X ) = lim

nââ

(12.37)
(12.38)

For a stationary Gaussian stochastic process, we have
h(X1 , X2 , . . . , Xn ) =

1
log(2Ï e)n |K (n) |,
2

(12.39)

where the covariance matrix K (n) is Toeplitz with entries R(0), R(1), . . . ,
R(n â 1) along the top row. Thus, Kij(n) = R(i â j ) = E(Xi â EXi )(Xj

12.6 BURGâS MAXIMUM ENTROPY THEOREM

417

â EXj ). As n â â, the density of the eigenvalues of the covariance
matrix tends to a limit, which is the spectrum of the stochastic process.
Indeed, Kolmogorov showed that the entropy rate of a stationary Gaussian
stochastic process can be expressed as
1
1
h(X ) = log 2Ï e +
2
4Ï



Ï

âÏ

log S(Î») dÎ».

(12.40)

The entropy rate is also lim nââ h(Xn |X nâ1 ). Since the stochastic process is Gaussian, the conditional distribution is also Gaussian, and hence
2
2
the conditional entropy is 12 log 2Ï eÏâ
, where Ïâ
is the variance of the
error in the best estimate of Xn given the inï¬nite past. Thus,
2
Ïâ
=

1 2h(X )
2
,
2Ï e

(12.41)

where h(X ) is given by (12.40). Hence, the entropy rate corresponds to
the minimum mean-squared error of the best estimator of a sample of the
process given the inï¬nite past.
12.6

BURGâS MAXIMUM ENTROPY THEOREM

Theorem 12.6.1 The maximum entropy rate stochastic process {Xi } satisfying the constraints
EXi Xi+k = Î±k ,

k = 0, 1, . . . , p

for all i,

(12.42)

is the pth-order GaussâMarkov process of the form
Xi = â

p


ak Xiâk + Zi ,

(12.43)

k=1

where the Zi are i.i.d. â¼ N(0, Ï 2 ) and a1 , a2 , . . . , ap , Ï 2 are chosen to
satisfy (12.42).
Remark We do not assume that {Xi } is (a) zero mean, (b) Gaussian, or
(c) wide-sense stationary.
Proof: Let X1 , X2 , . . . , Xn be any stochastic process that satisï¬es the
constraints (12.42). Let Z1 , Z2 , . . . , Zn be a Gaussian process with the
same covariance matrix as X1 , X2 , . . . , Xn . Then since the multivariate
normal distribution maximizes the entropy over all vector-valued random

418

MAXIMUM ENTROPY

variables under a covariance constraint, we have
h(X1 , X2 , . . . , Xn ) â¤ h(Z1 , Z2 , . . . , Zn )
= h(Z1 , . . . , Zp ) +

n


(12.44)
h(Zi |Ziâ1 , Ziâ2 , . . . , Z1 )

i=p+1

â¤ h(Z1 , . . . , Zp ) +

n


(12.45)
h(Zi |Ziâ1 , Ziâ2 , . . . , Ziâp )

i=p+1

(12.46)
by the chain rule and the fact that conditioning reduces entropy. Now
deï¬ne Z1
 , Z2
 , . . . , Zn
 as a pth-order GaussâMarkov process with the
same distribution as Z1 , Z2 , . . . , Zn for all orders up to p. (Existence of
such a process will be veriï¬ed using the YuleâWalker equations immediately after the proof.) Then since h(Zi |Ziâ1 , . . . , Ziâp ) depends only on




, . . . , Ziâp
),
the pth-order distribution, h(Zi |Ziâ1 , . . . , Ziâp ) = h(Zi
 |Ziâ1
and continuing the chain of inequalities, we obtain
h(X1 , X2 , . . . , Xn ) â¤ h(Z1 , . . . , Zp ) +

n


h(Zi |Ziâ1 , Ziâ2 , . . . , Ziâp )

i=p+1

(12.47)
=

h(Z1
 , . . . , Zp
 )

+

n








h(Zi
 |Ziâ1
, Ziâ2
, . . . , Ziâp
)

i=p+1

(12.48)
= h(Z1
 , Z2
 , . . . , Zn
 ),

(12.49)

where the last equality follows from the pth-order Markovity of the {Zi
 }.
Dividing by n and taking the limit, we obtain
1
1
lim h(X1 , X2 , . . . , Xn ) â¤ lim h(Z1
 , Z2
 , . . . , Zn
 ) = hâ ,
(12.50)
n
n
where
1
(12.51)
hâ = log 2Ï eÏ 2 ,
2
which is the entropy rate of the GaussâMarkov process. Hence, the maximum entropy rate stochastic process satisfying the constraints is the

pth-order GaussâMarkov process satisfying the constraints.
A bare-bones summary of the proof is that the entropy of a ï¬nite
segment of a stochastic process is bounded above by the entropy of a

12.6 BURGâS MAXIMUM ENTROPY THEOREM

419

segment of a Gaussian random process with the same covariance structure.
This entropy is in turn bounded above by the entropy of the minimal order
GaussâMarkov process satisfying the given covariance constraints. Such
a process exists and has a convenient characterization by means of the
YuleâWalker equations given below.
Note on the choice of a1 , . . . , ap and Ï 2 : Given a sequence of covariances
R(0), R(1), . . . , R(p), does there exist a pth-order GaussâMarkov process with these covariances? Given a process of the form (12.43), can we
choose the ak âs to satisfy the constraints? Multiplying (12.43) by Xiâl
and taking expectations, noting that R(k) = R(âk), we get
R(0) = â

p


ak R(âk) + Ï 2

(12.52)

k=1

and
R(l) = â

p


ak R(l â k),

l = 1, 2, . . . .

(12.53)

k=1

These equations are called the YuleâWalker equations. There are p + 1
equations in the p + 1 unknowns a1 , a2 , . . . , ap , Ï 2 . Therefore, we can
solve for the parameters of the process from the covariances.
Fast algorithms such as the Levinson algorithm and the Durbin algorithm [433] have been devised to use the special structure of these
equations to calculate the coefï¬cients a1 , a2 , . . . , ap efï¬ciently from the
covariances. (We set a0 = 1 for a consistent notation.) Not only do the
YuleâWalker equations provide a convenient set of linear equations for
calculating the ak âs and Ï 2 from the R(k)âs, they also indicate how the
autocorrelations behave for lags greater than p. The autocorrelations for
high lags are an extension of the values for lags less than p. These values are called the YuleâWalker extension of the autocorrelations. The
spectrum of the maximum entropy process is seen to be
S(Î») =

â


R(m)eâimÎ»

(12.54)

m=ââ

=
1 + p

Ï2

 ,
âikÎ» 2
a
e
k
k=1

âÏ â¤ Î» â¤ Ï.

(12.55)

This is the maximum entropy spectral density subject to the constraints
R(0), R(1), . . . , R(p).
However, for the pth-order GaussâMarkov process, it is possible to
calculate the entropy rate directly without calculating the ai âs. Let Kp be

420

MAXIMUM ENTROPY

the autocorrelation matrix corresponding to this processâthe matrix with
R0 , R1 , . . . , Rp along the top row. For this process, the entropy rate is
equal to
hâ = h(Xp |Xpâ1 , . . . , X0 ) = h(X0 , . . . , Xp ) â h(X0 , . . . , Xpâ1 )
(12.56)
1
1
log(2Ï e)p+1 |Kp | â log(2Ï e)p |Kpâ1 |
2
2
(12.57)

=

|Kp |
1
log(2Ï e)
.
2
|Kpâ1 |

=

(12.58)

In a practical problem, we are generally given a sample sequence
X1 , X2 , . . . , Xn , from which we calculate the autocorrelations. An important question is: How many autocorrelation lags should we consider (i.e.,
what is the optimum value of p)? A logically sound method is to choose
the value of p that minimizes the total description length in a two-stage
description of the data. This method has been proposed by Rissanen
[442, 447] and Barron [33] and is closely related to the idea of Kolmogorov complexity.

SUMMARY
Maximum entropy distribution. Let f be a probability density satisfying the constraints

f (x)ri (x) = Î±i for 1 â¤ i â¤ m.
(12.59)
S
m

Let f â (x) = fÎ» (x) = eÎ»0 + i=1 Î»i ri (x) , x â S, and let Î»0 , . . . , Î»m be chosen so that f â satisï¬es (12.59). Then f â uniquely maximizes h(f ) over
all f satisfying these constraints.
Maximum entropy spectral density estimation. The entropy rate of a
stochastic process subject to autocorrelation constraints R0 , R1 , . . . , Rp
is maximized by the pth order zero-mean Gauss-Markov process satisfying these constraints. The maximum entropy rate is
hâ =

|Kp |
1
log(2Ï e)
,
2
|Kpâ1 |

(12.60)

PROBLEMS

421

and the maximum entropy spectral density is
Ï2
S(Î») = 
 .

1 + p ak eâikÎ» 2
k=1

(12.61)

PROBLEMS
12.1

Maximum entropy. Find the maximum entropy density f ,
deï¬ned for
 x â¥ 0, satisfying EX
 = Î±1 , E ln X = Î±2 . That is, maximize â f ln f subject to xf (x) dx = Î±1 , (ln x)f (x) dx =
Î±2 , where the integral is over 0 â¤ x < â. What family of densities is this?

12.2

Min D(P  Q) under constraints on P . We wish to ï¬nd the
(parametric form) of the probability mass function P (x), x â {1, 2,
. . .} that
 minimizes the relative entropy D(P  Q) over all P such
that
P (x)gi (x) = Î±i , i = 1, 2, . . . .
(a) Use Lagrange multipliers to guess that
P â (x) = Q(x)e

â

i=1 Î»i gi (x)+Î»0

(12.62)

achieves this minimum if there exist Î»i âs satisfying the Î±i
constraints. This generalizes the theorem on maximum entropy distributions subject to constraints.
(b) Verify that P â minimizes D(P  Q).
12.3

Maximum entropy processes. Find the maximum entropy rate
stochastic process {Xi }â
ââ subject to the constraints:
2
(a) EXi = 1, i = 1, 2, . . . .
(b) EXi2 = 1, EXi Xi+1 = 12 , i = 1, 2, . . . .
(c) Find the maximum entropy spectrum for the processes in parts
(a) and (b).

12.4

Maximum entropy with marginals. What is the maximum entropy distribution p(x, y) that has the following marginals?

422

MAXIMUM ENTROPY

(Hint: You may wish to guess and verify a more general result.)
12.5

Processes with ï¬xed marginals. Consider the set of all densities
with ï¬xed pairwise marginals fX1 ,X2 (x1 , x2 ), fX2 ,X3 (x2 , x3 ), . . . ,
fXnâ1 ,Xn (xnâ1 , xn ). Show that the maximum entropy process with
these marginals is the ï¬rst-order (possibly time-varying) Markov
process with these marginals. Identify the maximizing f â (x1 , x2 ,
. . . , xn ).

12.6

Every density is a maximum entropy density. Let f0 (x) be a
given density. Given r(x), let
 gÎ± (x) be the density maximizing
h(X) over all f satisfying f (x)r(x) dx = Î±. Now let r(x) =
ln f0 (x). Show that gÎ± (x) = f0 (x) for an appropriate choice Î± =
Î±
 0 . Thus, f0 (x) is a maximum entropy density under the constraint
f ln f0 = Î±0 .

12.7

Mean-squared error. Let {Xi }ni=1 satisfy EXi Xi+k = Rk , k =
0, 1, . . . , p. Consider linear predictors for Xn ; that is,
XÌn =

nâ1


bi Xnâi .

i=1

Assume that n > p. Find
max min E(Xn â XÌn )2 ,

f (x n )

b

where the minimum is over all linear predictors b and the maximum is over all densities f satisfying R0 , . . . , Rp .
12.8

Maximum entropy characteristic functions. We ask for the maximum entropy density f (x), 0 â¤ x â¤ a, satisfying a constraint on
a
the characteristic function (u) = 0 eiux f (x) dx. The answers
need be given only in parametric form.
a
(a) Find the maximum entropy f satisfying 0 f (x) cos(u0 x) dx
= Î±, at a speciï¬ed point u0 .
a
(b) Find the maximum entropy f satisfying 0 f (x) sin(u0 x) dx
= Î².
(c) Find the maximum entropy density f (x), 0 â¤ x â¤ a, having a
given value of the characteristic function (u0 ) at a speciï¬ed
point u0 .
(d) What problem is encountered if a = â?

PROBLEMS

12.9

12.10

12.11

12.12

12.13

12.14

423

Maximum entropy processes
(a) Find the maximum entropy rate binary stochastic process
1
{Xi }â
i=ââ , Xi â {0, 1}, satisfying Pr{Xi = Xi+1 } = 3 for
all i.
(b) What is the resulting entropy rate?
Maximum entropy of sums. Let Y = X1 + X2 . Find the maximum entropy density for Y under the constraint EX12 = P1 , EX22
= P2 :
(a) If X1 and X2 are independent.
(b) If X1 and X2 are allowed to be dependent.
(c) Prove part (a).
Maximum entropy Markov chain. Let {Xi } be a stationary
Markov chain with Xi â {1, 2, 3}. Let I (Xn ; Xn+2 ) = 0 for all n.
(a) What is the maximum entropy rate process satisfying this
constraint?
(b) What if I (Xn ; Xn+2 ) = Î± for all n for some given value of
Î±, 0 â¤ Î± â¤ log 3?
Entropy bound on prediction error. Let {Xn } be an arbitrary real
valued stochastic process. Let XÌn+1 = E{Xn+1 |X n }. Thus the conditional mean XÌn+1 is a random variable depending on the n-past
X n . Here XÌn+1 is the minimum mean squared error prediction of
Xn+1 given the past.
(a) Find a lower bound on the conditional variance E{E{(Xn+1
â XÌn+1 )2 |X n }} in terms of the conditional differential entropy
h(Xn+1 |X n ).
(b) Is equality achieved when {Xn } is a Gaussian stochastic process?
Maximum entropy rate. What is the maximum entropy rate stochastic process {Xi } over the symbol set {0, 1} for which the
probability that 00 occurs in a sequence is zero?
Maximum entropy
(a) What is the parametric-form maximum entropy density f (x)
satisfying the two conditions
EX 16 = b?
EX 8 = a,
(b) What is the maximum entropy density satisfying the condition
E(X 8 + X 16 ) = a + b?
(c) Which entropy is higher?

424

12.15

MAXIMUM ENTROPY

Maximum entropy. Find the parametric form of the maximum
entropy density f satisfying the Laplace transform condition

f (x)eâx dx = Î±,
and give the constraints on the parameter.

12.16

Maximum entropy processes. Consider the set of all stochastic
processes with {Xi }, Xi â R, with
R0 = EXi2 = 1,

1
R1 = EXi Xi+1 = .
2

Find the maximum entropy rate.
12.17

Binary maximum entropy. Consider a binary process {Xi }, Xi â
{â1, +1}, with R0 = EXi2 = 1 and R1 = EXi Xi+1 = 12 .
(a) Find the maximum entropy process with these constraints.
(b) What is the entropy rate?
(c) Is there a Bernoulli process satisfying these constraints?

12.18

Maximum entropy. Maximize h(Z, Vx , Vy , Vz ) subject to the energy constraint E( 12 mV 2 + mgZ) = E0 . Show that the resulting
distribution yields
1
3
E mV 2 = E0 ,
2
5

2
EmgZ = E0 .
5

Thus, 25 of the energy is stored in the potential ï¬eld, regardless of
its strength g.
12.19

Maximum entropy discrete processes
(a) Find the maximum entropy rate binary stochastic process
1
{Xi }â
i=ââ , Xi â {0, 1}, satisfying Pr{Xi = Xi+1 } = 3 for all
i.
(b) What is the resulting entropy rate?

12.20

Maximum entropy of sums. Let Y = X1 + X2 . Find the maximum entropy of Y under the constraint EX12 = P1 , EX22 = P2 :
(a) If X1 and X2 are independent.
(b) If X1 and X2 are allowed to be dependent.

HISTORICAL NOTES

425

12.21

Entropy rate
(a) Find the maximum entropy rate stochastic process {Xi } with
EXi2 = 1, EXi Xi+2 = Î±, i = 1, 2, . . .. Be careful.
(b) What is the maximum entropy rate?
(c) What is EXi Xi+1 for this process?

12.22

Minimum expected value
(a) Find the minimum value of EX over all probability density
functions f (x) satisfying the following three constraints:
(i) f (x) = 0 for x â¤ 0.
â
(ii) ââ f (x) dx = 1.
(iii) h(f ) = h.
(b) Solve the same problem if (i) is replaced by
(i
 ) f (x) = 0 for x â¤ a.

HISTORICAL NOTES
The maximum entropy principle arose in statistical mechanics in the
nineteenth century and has been advocated for use in a broader context by Jaynes [294]. It was applied to spectral estimation by Burg [80].
The information-theoretic proof of Burgâs theorem is from Choi and
Cover [98].

CHAPTER 13

UNIVERSAL SOURCE CODING

Here we develop the basics of universal source coding. Minimax regret
data compression is deï¬ned, and the descriptive cost of universality is
shown to be the information radius of the relative entropy ball containing
all the source distributions. The minimax theorem shows this radius to
be the channel capacity for the associated channel given by the source
distribution. Arithmetic coding enables the use of a source distribution
that is learned on the ï¬y. Finally, individual sequence compression is
deï¬ned and achieved by a succession of LempelâZiv parsing algorithms.
In Chapter 5 we introduced the problem of ï¬nding the shortest representation of a source, and showed that the entropy is the fundamental
lower limit on the expected length of any uniquely decodable representation. We also showed that if we know the probability distribution for
the source, we can use the Huffman algorithm to construct the optimal
(minimal expected length) code for that distribution.
For many practical situations, however, the probability distribution
underlying the source may be unknown, and we cannot apply the methods
of Chapter 5 directly. Instead, all we know is a class of distributions. One
possible approach is to wait until we have seen all the data, estimate the
distribution from the data, use this distribution to construct the best code,
and then go back to the beginning and compress the data using this code.
This two-pass procedure is used in some applications where there is a
fairly small amount of data to be compressed. But there are many situations in which it is not feasible to make two passes over the data, and it
is desirable to have a one-pass (or online) algorithm to compress the data
that âlearnsâ the probability distribution of the data and uses it to compress the incoming symbols. We show the existence of such algorithms
that do well for any distribution within a class of distributions.
In yet other cases, there is no probability distribution underlying the
dataâall we are given is an individual sequence of outcomes. Examples
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

427

428

UNIVERSAL SOURCE CODING

of such data sources include text and music. We can then ask the question:
How well can we compress the sequence? If we do not put any restrictions on the class of algorithms, we get a meaningless answerâthere
always exists a function that compresses a particular sequence to one
bit while leaving every other sequence uncompressed. This function is
clearly âoverï¬ttedâ to the data. However, if we compare our performance
to that achievable by optimal word assignments with respect to Bernoulli
distributions or kth-order Markov processes, we obtain more interesting
answers that are in many ways analogous to the results for the probabilistic or average case analysis. The ultimate answer for compressibility for
an individual sequence is the Kolmogorov complexity of the sequence,
which we discuss in Chapter 14.
We begin the chapter by considering the problem of source coding as
a game in which the coder chooses a code that attempts to minimize
the average length of the representation and nature chooses a distribution
on the source sequence. We show that this game has a value that is
related to the capacity of a channel with rows of its transition matrix that
are the possible distributions on the source sequence. We then consider
algorithms for encoding the source sequence given a known or âestimatedâ
distribution on the sequence. In particular, we describe arithmetic coding,
which is an extension of the ShannonâFanoâElias code of Section 5.9
that permits incremental encoding and decoding of sequences of source
symbols.
We then describe two basic versions of the class of adaptive dictionary
compression algorithms called LempelâZiv, based on the papers by Ziv
and Lempel [603, 604]. We provide a proof of asymptotic optimality for
these algorithms, showing that in the limit they achieve the entropy rate
for any stationary ergodic source. In Chapter 16 we extend the notion of
universality to investment in the stock market and describe online portfolio
selection procedures that are analogous to the universal methods for data
compression.
13.1

UNIVERSAL CODES AND CHANNEL CAPACITY

Assume that we have a random variable X drawn according to a distribution from the family {pÎ¸ }, where the parameter Î¸ â {1, 2, . . . , m} is
unknown. We wish to ï¬nd an efï¬cient code for this source.
From the results of Chapter 5, if we know Î¸ , we can construct a code
with codeword lengths l(x) = log pÎ¸1(x) , achieving an average codeword

13.1

UNIVERSAL CODES AND CHANNEL CAPACITY

429


length equal to the entropy HÎ¸ (x) = â x pÎ¸ (x) log pÎ¸ (x), and this is the
best that we can do. For the purposes of this section, we will ignore the
integer constraints on l(x), knowing that applying the integer constraint
will cost at most one bit in expected length. Thus,


1
min EpÎ¸ [l(X)] = EpÎ¸ log
= H (pÎ¸ ).
(13.1)
l(x)
pÎ¸ (X)
What happens if we do not know the true distribution pÎ¸ , yet wish to
code as efï¬ciently as possible? In this case, using a code with codeword
lengths l(x) and implied probability q(x) = 2âl(x) , we deï¬ne the redundancy of the code as the difference between the expected length of the
code and the lower limit for the expected length:


1
R(pÎ¸ , q) = EpÎ¸ [l(X)] â EpÎ¸ log
(13.2)
pÎ¸ (X)



1
(13.3)
=
pÎ¸ (x) l(x) â log
p(x)
x



1
1
=
â log
(13.4)
pÎ¸ (x) log
q(x)
p(x)
x
=



pÎ¸ (x) log

x

pÎ¸ (x)
q(x)

(13.5)

= D(pÎ¸ q),

(13.6)

where q(x) = 2âl(x) is the distribution that corresponds to the codeword
lengths l(x).
We wish to ï¬nd a code that does well irrespective of the true distribution
pÎ¸ , and thus we deï¬ne the minimax redundancy as
R â = min max R(pÎ¸ , q) = min max D(pÎ¸ q).
q

pÎ¸

q

pÎ¸

(13.7)

This minimax redundancy is achieved by a distribution q that is at the
âcenterâ of the information ball containing the distributions pÎ¸ , that is,
the distribution q whose maximum distance from any of the distributions
pÎ¸ is minimized (Figure 13.1).
To ï¬nd the distribution q that is as close as possible to all the possible
pÎ¸ in relative entropy, consider the following channel:

430

UNIVERSAL SOURCE CODING

p1
q*
pm

FIGURE 13.1. Minimum radius information ball containing all the pÎ¸ âs

ï£¹
. . . p1 . . .
ï£¯ . . . p2 . . . ï£º
ï£º
ï£¯
..
ï£º
ï£¯
.
ï£º
ï£¯
Î¸ âï£¯
ï£º â X.
ï£¯ . . . pÎ¸ . . . ï£º
ï£º
ï£¯
..
ï£»
ï£°
.
. . . pm . . .
ï£®

(13.8)

This is a channel {Î¸, pÎ¸ (x), X} with the rows of the transition matrix equal
to the different pÎ¸ âs, the possible distributions of the source. We will show
that the minimax redundancy R â is equal to the capacity of this channel,
and the corresponding optimal coding distribution is the output distribution
of this channel induced by the capacity-achieving input distribution. The
capacity of this channel is given by
C = max I (Î¸ ; X) = max
Ï(Î¸ )

where

Ï(Î¸ )

qÏ (x) =



Ï(Î¸ )pÎ¸ (x) log

Î¸



Ï(Î¸ )pÎ¸ (x).

pÎ¸ (x)
,
qÏ (x)

(13.9)

(13.10)

Î¸

The equivalence of R â and C is expressed in the following theorem:
Theorem 13.1.1 (Gallager [229], Ryabko [450]) The capacity of a
channel p(x|Î¸ ) with rows p1 , p2 , . . . , pm is given by
C = R â = min max D(pÎ¸ q).
q

Î¸

(13.11)

13.1

UNIVERSAL CODES AND CHANNEL CAPACITY

431

The distribution q that achieves the minimum in (13.11) is the output
distribution q â (x) induced be the capacity-achieving input distribution
Ï â (Î¸ ):

Ï â (Î¸ )pÎ¸ (x).
(13.12)
q â (x) = qÏ â (x) =
Î¸

Proof: Let Ï(Î¸ ) be an input distribution on Î¸ â {1, 2, . . . , m}, and let
the induced output distribution be qÏ :
(qÏ )j =

m


Ïi pij ,

(13.13)

i=1

where pij = pÎ¸ (x) for Î¸ = i, x = j . Then for any distribution q on the
output, we have

pij
IÏ (Î¸ ; X) =
Ïi pij log
(13.14)
(qÏ )j
i,j

Ïi D(pi qÏ )
(13.15)
=
i

=



Ïi pij log

pij qj
qj (qÏ )j

(13.16)

Ïi pij log

pij 
qj
+
Ïi pij log
qj
(qÏ )j

(13.17)

pij 
qj
+
(qÏ )j log
qj
(qÏ )j

(13.18)

pij
â D(qÏ q)
qj

(13.19)

i,j

=


i,j

=



i,j

Ïi pij log

i,j

=


i,j

=



j

Ïi pij log

Ïi D(pi q) â D(qÏ q)

(13.20)

Ïi D(pi q)

(13.21)

i

â¤


i

for all q, with equality iff q = qÏ . Thus, for all q,


Ïi D(pi q) â¥
Ïi D(pi qÏ ),
i

i

(13.22)

432

UNIVERSAL SOURCE CODING

and therefore
IÏ (Î¸ ; X) = min



q

Ïi D(pi q)

(13.23)

i

is achieved when q = qÏ . Thus, the output distribution that minimizes the
average distance to all the rows of the transition matrix is the the output
distribution induced by the channel (Lemma 10.8.1).
The channel capacity can now be written as
C = max IÏ (Î¸ ; X)
Ï

= max min
Ïi D(pi q).
Ï

q

(13.24)
(13.25)

i

We can now apply a fundamental theorem of game theory, which states
that for a continuous function f (x, y), x â X, y â Y, if f (x, y) is convex
in x and concave in y, and X, Y are compact convex sets, then
min max f (x, y) = max min f (x, y).
xâX yâY

yâY xâX

(13.26)

The proof of this minimax theorem can be found in [305,
 392].
By convexity of relative entropy (Theorem 2.7.2),
i Ïi D(pi q) is
convex in q and concave in Ï , and therefore

Ïi D(pi q)
(13.27)
C = max min
Ï

q

= min max
q

Ï

i



Ïi D(pi q)

i

= min max D(pi q),
q

i

(13.28)
(13.29)

where the last equality follows from the fact that the maximum is achieved
by putting all the weight on the index i maximizing D(pi q) in (13.28).
It also follows that q â = qÏ â . This completes the proof.

Thus, the channel capacity of the channel from Î¸ to X is the minimax
expected redundancy in source coding.
Example 13.1.1 Consider the case when X = {1, 2, 3} and Î¸ takes only
two values, 1 and 2, and the corresponding distributions are p1 = (1 â Î±,
Î±, 0) and p2 = (0, Î±, 1 â Î±). We would like to encode a sequence of
symbols from X without knowing whether the distribution is p1 or p2 .
The arguments above indicate that the worst-case optimal code uses the

13.2 UNIVERSAL CODING FOR BINARY SEQUENCES

433

codeword lengths corresponding to the distribution that has a minimal
relative entropy distance from both distributions, in this 
case, the midpoint

1âÎ±
of the two distributions. Using this distribution, q = 1âÎ±
2 , Î±, 2 , we
achieve a redundancy of
Î±
1âÎ±
+ Î± log + 0 = 1 â Î±.
(1 â Î±)/2
Î±
(13.30)
The channel with transition matrix rows equal to p1 and p2 is equivalent
to the erasure channel (Section 7.1.5), and the capacity of this channel can
easily be calculated to be (1 â Î±), achieved with a uniform distribution on
the inputs. The output distribution
to the capacity-achieving


 corresponding
1âÎ±
,
Î±,
(i.e.,
the same as the distriinput distribution is equal to 1âÎ±
2
2
bution q above). Thus, if we donât know the distribution for this class of
sources, we code using the distribution q rather than p1 or p2 , and incur
an additional cost of 1 â Î± bits per source symbol above the ideal entropy
bound.
D(p1 q) = D(p2 q) = (1 â Î±) log

13.2

UNIVERSAL CODING FOR BINARY SEQUENCES

Now we consider an important special case of encoding a binary sequence
x n â {0, 1}n . We do not make any assumptions about the probability distribution for x1 , x2 , . . . , xn .

We begin with bounds on the size of nk , taken from Wozencraft and
Reiffen [567] proved in Lemma 17.5.1: For k = 0 or n,
 


n
n
n ânH (k/n)
2
â¤
â¤
.
(13.31)
k
8k(n â k)
Ï k(n â k)
We ï¬rst describe an ofï¬ine algorithm to describe the sequence; we
count the number of 1âs in the sequence, and after we have seen the entire
sequence, we send a two-stage description of the sequence. The
 ï¬rst stage
is a count of the number of 1âs in the sequence [i.e., k = i xi (using
log(n + 1)	 bits)], and the second stage is the index
of this sequence

among all sequences that have k 1âs (using log nk 	 bits). This two-stage
description requires total length
 
n
n
l(x ) â¤ log(n + 1) + log
+2
(13.32)
k


 
1
1
k (n â k)
k
â log n â log Ï
+ 3 (13.33)
â¤ log n + nH
n
2
2
n n

434

UNIVERSAL SOURCE CODING

= nH



 
1
1
knâk
k
+ log n â log Ï
+ 3.
n
2
2
n n

(13.34)

Thus, the cost of describing the sequence is approximately 12 log n bits
above the optimal cost with the Shannon code for a Bernoulli distribution
corresponding to k/n. The last term is unbounded at k = 0 or k = n, so
the bound is not useful for these cases (the actual description length is
log(n + 1) bits, whereas the entropy H (k/n) = 0 when k = 0 or k = n).
This counting approach requires the compressor to wait until he has
seen the entire sequence. We now describe a different approach using a
mixture distribution that achieves the same result on the ï¬y. We choose
the coding distribution q(x1 , x2 , . . . , xn ) = 2âl(x1 ,x2 ,...,xn ) to be a uniform
mixture of all Bernoulli(Î¸ ) distributions on x1 , x2 , . . . , xn . We will analyze
the performance of a code using this distribution and show that such codes
perform well for all input sequences.
We construct this distribution by assuming that Î¸ , the parameter of
the Bernoulli distribution is drawn according to a uniform distribution on
[0, 1]. The probability of a sequence x1 , x2 , . . . , xn with k ones is Î¸ k (1 â
Î¸ )nâk under the Bernoulli(Î¸ ) distribution. Thus, the mixture probability
of the sequence is
 1

p(x1 , x2 , . . . , xn ) =
Î¸ k (1 â Î¸ )nâk dÎ¸ = A(n, k).
(13.35)
0

Integrating by parts, setting u = (1 â Î¸ )nâk and dv = Î¸ k dÎ¸ , we have
1

 1
1
k
nâk
k+1
nâk
Î¸ (1 â Î¸ ) dÎ¸ =
Î¸ (1 â Î¸ )
k+1
0
0
 1
nâk
+
Î¸ k+1 (1 â Î¸ )nâkâ1 dÎ¸,
(13.36)
k+1 0
or
A(n, k) =
Now A(n, n) =
sion that

1
0

Î¸ n dÎ¸ =

1
n+1 ,

nâk
A(n, k + 1).
k+1

(13.37)

and we can easily verify from the recur-

p(x1 , x2 , . . . , xn ) = A(n, k) =

1
1
 .
n+1 n
k

(13.38)

13.2 UNIVERSAL CODING FOR BINARY SEQUENCES

435

The codeword length with respect to the mixture distribution is

log


 
1
n
â¤ log(n + 1) + log
+ 1,
k
q(x n )

(13.39)

which is within one bit of the length of the two-stage description above.
Thus, we have a similar bound on the codeword length
 


k
1
1
k (n â k)
+ log n â log Ï
+ 2 (13.40)
l(x1 , x2 , . . . , xn ) â¤ H
n
2
2
n n
for all sequences x1 , x2 , . . . , xn . This mixture distribution achieves a codeword length within 12 log n bits of the optimal code length nH (k/n) that
would be required if the source were really Bernoulli(k/n), without any
assumptions about the distribution of the source.
This mixture distribution yields a nice expression for the conditional probability of the next symbol given the previous symbols of x1 , x2 , . . . , xn . Let
ki be the number of 1âs in the ï¬rst i symbols of x1 , x2 , . . . , xn . Using (13.38),
we have
q(x i , 1)
q(x i )



1
1 1
1
=
 


i + 2 i+1
i + 1 ki

q(xi+1 = 1|x i ) =

ki +1

(13.42)

i

1 (ki + 1)!(n â ki )!
ki !(i â ki )!
(i + 1)
i+2
(i + 1)!
i!
ki + 1
=
.
i+2
=

(13.41)

(13.43)
(13.44)

This is the Bayesian posterior probability of 1 given the uniform prior
on Î¸ , and is called the Laplace estimate for the probability of the next
symbol. We can use this posterior probability as the probability of the next
symbol for arithmetic coding, and achieve the codeword length log q(x1 n )
in a sequential manner with ï¬nite-precision arithmetic. This is a horizonfree result, in that the procedure does not depend on the length of the
sequence.
One issue with the uniform mixture approach or the two-stage approach
is that the bound does not apply for k = 0 or k = n. The only uniform bound that we can give on the extra redundancy is log n, which
we can obtain by using the bounds of (11.40). The problem is that

436

UNIVERSAL SOURCE CODING

we are not assigning enough probability to sequences with k = 0 or
k = n. If instead
of using a uniform distribution

 on Î¸ , we used the
Dirichlet 12 , 12 distribution, also called the Beta 12 , 12 distribution, the
probability of a sequence x1 , x2 , . . . , xn becomes
 1
1
n
q 1 (x ) =
Î¸ k (1 â Î¸ )nâk â
dÎ¸
(13.45)
2
Ï Î¸ (1 â Î¸ )
0
and it can be shown that this achieves a description length
log

1
q 1 (x n )

â¤ H (k/n) +

2

Ï
1
log n + log
2
8

(13.46)

for all x n â {0, 1}n , achieving a uniform bound on the redundancy of the
universal mixture code. As in the case of the uniform prior, we can calculate the conditional distribution of the next symbol, given the previous
observations, as
ki + 12
q 1 (xi+1 = 1|x i ) =
,
(13.47)
2
i+1
which can be used with arithmetic coding to provide an online algorithm
to encode the sequence. We will analyze the performance of the mixture algorithm in greater detail when we analyze universal portfolios in
Section 16.7.
13.3

ARITHMETIC CODING

The Huffman coding procedure described in Chapter 5 is optimal for
encoding a random variable with a known distribution that has to be
encoded symbol by symbol. However, due to the fact that the codeword
lengths for a Huffman code were restricted to be integral, there could be a
loss of up to 1 bit per symbol in coding efï¬ciency. We could alleviate this
loss by using blocks of input symbolsâhowever, the complexity of this
approach increases exponentially with block length. We now describe a
method of encoding without this inefï¬ciency. In arithmetic coding, instead
of using a sequence of bits to represent a symbol, we represent it by a
subinterval of the unit interval.
The code for a sequence of symbols is an interval whose length decreases
as we add more symbols to the sequence. This property allows us to have a
coding scheme that is incremental (the code for an extension to a sequence
can be calculated simply from the code for the original sequence) and for
which the codeword lengths are not restricted to be integral. The motivation

13.3 ARITHMETIC CODING

437

for arithmetic coding is based on ShannonâFanoâElias coding (Section 5.9)
and the following lemma:
Lemma 13.3.1 Let Y be a random variable with continuous probability
distribution function F (y). Let U = F (Y ) (i.e., U is a function of Y deï¬ned
by its distribution function). Then U is uniformly distributed on [0, 1].
Proof: Since F (y) â [0, 1], the range of U is [0, 1]. Also, for u â [0, 1],
FU (u) = Pr(U â¤ u)

(13.48)

= Pr(F (Y ) â¤ u)

(13.49)

= Pr(Y â¤ F â1 (u))

(13.50)

= F (F â1 (u))

(13.51)

= u,

(13.52)

which proves that U has a uniform distribution in [0, 1].



Now consider an inï¬nite sequence of random variables X1 , X2 , . . . from
a ï¬nite alphabet X = 0, 1, 2, . . . , m. For any sequence x1 , x2 , . . . , from
this alphabet, we can place 0. in front of the sequence and consider it as
a real number (base m + 1) between 0 and 1. Let X be the real-valued
random variable X = 0.X1 X2 . . . . Then X has the following distribution
function:
FX (x) = Pr{X â¤ x = 0.x1 x2 Â· Â· Â·}

(13.53)

= Pr{0.X1 X2 Â· Â· Â· â¤ 0.x1 x2 Â· Â· Â·}

(13.54)

= Pr{X1 < x1 } + Pr{X1 = x1 , X2 < x2 } + Â· Â· Â· .

(13.55)

Now let U = FX (X) = FX (0.X1 X2 . . .) = 0.F1 F2 . . . . If the distribution
on inï¬nite sequences X â has no atoms, then, by the lemma above, U has a
uniform distribution on [0, 1], and therefore the bits F1 F2 . . . in the binary
expansion of U are Bernoulli( 12 ) (i.e., they are independent and uniformly
distributed on {0, 1}). These bits are therefore incompressible, and form a
compressed representation of the sequence 0.X1 X2 . . . . For Bernoulli or
Markov models, it is easy to calculate the cumulative distribution function,
as illustrated in the following example.
Example 13.3.1 Let X1 , X2 , . . . , Xn be Bernoulli(p). Then the sequence
x n = 110101 maps into

438

UNIVERSAL SOURCE CODING

F (x n ) = Pr(X1 < 1) + Pr(X1 = 1, X2 < 1)
+ Pr(X1 = 1, X2 = 1, X3 < 0)
+ Pr(X1 = 1, X2 = 1, X3 = 0, X4 < 1)
+ Pr(X1 = 1, X2 = 1, X3 = 0, X4 = 1, X5 < 0)
+ Pr(X1 = 1, X2 = 1, X3 = 0, X4 = 1, X5 = 0, X6 < 1)
(13.56)
= q + pq + p 2 Â·0 + p 2 qÂ·q + p 2 qpÂ·0 + p 2 qpqq

(13.57)

= q + pq + p 2 q 2 + p 3 q 3 .

(13.58)

Note that each term is easily computed from the previous terms. In general,
for an arbitrary binary process {Xi },
F (x n ) =

n


p(x kâ1 0)xk .

(13.59)

k=1

The probability transform thus forms an invertible mapping from inï¬nite source sequences to incompressible inï¬nite binary sequences. We
now consider the compression achieved by this transformation on ï¬nite
sequences. Let X1 , X2 , . . . , Xn be a sequence of binary random variables of length n, and let x1 , x2 , . . . , xn be a particular outcome. We
can treat this sequence as representing an interval [0.x1 x2 . . . xn 000 . . . ,
0.x1 x2 . . . xn 1111 . . .), or equivalently, [0.x1 x2 . . . xn , 0.x1 x2 . . . xn +
( 12 )n ). This is the set of inï¬nite sequences that start with 0.x1 x2 Â· Â· Â· xn .
Under the probability transform, this interval gets mapped into another
interval, [FY (0.x1 x2 Â· Â· Â· xn ), FY (0.x1 x2 Â· Â· Â· xn + ( 12 )n )), whose length is
equal to PX (x1 , x2 , . . . , xn ), the sum of the probabilities of all inï¬nite
sequences that start with 0.x1 x2 Â· Â· Â· xn . Under the probability inverse transform, any real number u within this interval maps into a sequence that
starts with x1 , x2 , . . . , xn , and therefore given u and n, we can reconstruct x1 , x2 , . . . , xn . The ShannonâFanoâElias coding scheme described
earlier allows one to construct a preï¬x-free code of length log
1
p(x1 ,x2 ,...,xn ) + 2 bits, and therefore it is possible to encode the sequence
1
x1 , x2 , . . . , xn with this length. Note that log p(x1 ,...,x
is the ideal coden)
word length for x n .
The process of encoding the sequence with the cumulative distribution
function described above assumes arbitrary accuracy for the computation. In practice, though, we have to implement all numbers with ï¬nite
precision, and we describe such an implementation. The key is to consider

13.3 ARITHMETIC CODING

439

not inï¬nite-precision points for the cumulative distribution function but
intervals in the unit interval. Any ï¬nite-length sequence of symbols can
be said to correspond to a subinterval of the unit interval. The objective
of the arithmetic coding algorithm is to represent a sequence of random
variables by a subinterval in [0, 1]. As the algorithm observes more input
symbols, the length of the subinterval corresponding to the input sequence
decreases. As the top end of the interval and the bottom end of the interval get closer, they begin to agree in the ï¬rst few bits. These will be ï¬rst
few bits of the output sequence. As soon as the two ends of the interval
agree, we can output the corresponding bits. We can therefore shift these
bits out of the calculation and effectively scale the remaining intervals so
that entire calculation can be done with ï¬nite precision. We will not go
into the details hereâthere is a very good description of the algorithm
and performance considerations in Bell et al. [41]
Example 13.3.2 (Arithmetic coding for a ternary input alphabet) Consider a random variable X with a ternary alphabet {A, B, C}, which
are assumed to have probabilities 0.4, 0.4, and 0.2, respectively. Let
the sequence to be encoded by ACAA. Thus, Fl (Â·) = (0, 0.4, 0.8) and
Fh (Â·) = (0.4, 0.8, 1.0). Initially, the input sequence is empty, and the corresponding interval is [0, 1). The cumulative distribution function after
the ï¬rst input symbol is shown in Figure 13.2. It is easy to calculate that
the interval in the algorithm without scaling after the ï¬rst symbol A is
F(x n )
1.0

0.8

0.4

A

B

C

xn

FIGURE 13.2. Cumulative distribution function after the ï¬rst symbol.

440

UNIVERSAL SOURCE CODING
F(x n )

0.4
0.32

AA

AB

AC

BA

BB

BC

CA

CB

CC

xn

FIGURE 13.3. Cumulative distribution function after the second symbol.

[0, 0.4); after the second symbol, C, it is [0.32, 0.4) (Figure 13.3); after
the third symbol A, it is [0.32,0.352); and after the fourth symbol A, it
is [0.32, 0.3328). Since the probability of this sequence is 0.0128, we
will use log(1/0.0128) + 2 (i.e., 9 bits) to encode the midpoint of the
interval sequence using ShannonâFanoâElias coding (0.3264, which is
0.010100111 binary).
In summary, the arithmetic coding procedure, given any length n and
probability mass function q(x1 x2 Â· Â· Â· xn ), enables one to encode the sequence
x1 x2 Â· Â· Â· xn in a code of length log q(x1 x12 Â·Â·Â·xn ) + 2 bits. If the source is i.i.d.
and the assumed distribution q is equal to the true distribution p of the data,
this procedure achieves an average length for the block that is within 2 bits
of the entropy. Although this is not necessarily optimal for any ï¬xed block
length (a Huffman code designed for the distribution could have a lower
average codeword length), the procedure is incremental and can be used for
any blocklength.
13.4

LEMPELâZIV CODING

In Section 13.3 we discussed the basic ideas of arithmetic coding and
mentioned some results on worst-case redundancy for coding a sequence
from an unknown distribution. We now discuss a popular class of techniques for source coding that are universally optimal (their asymptotic

13.4

LEMPELâZIV CODING

441

compression rate approaches the entropy rate of the source for any stationary ergodic source) and simple to implement. This class of algorithms
is termed LempelâZiv, named after the authors of two seminal papers
[603, 604] that describe the two basic algorithms that underlie this class.
The algorithms could also be described as adaptive dictionary compression
algorithms.
The notion of using dictionaries for compression dates back to the
invention of the telegraph. At the time, companies were charged by the
number of letters used, and many large companies produced codebooks for
the frequently used phrases and used the codewords for their telegraphic
communication. Another example is the notion of greetings telegrams
that are popular in Indiaâthere is a set of standard greetings such as
â25:Merry Christmasâ and â26:May Heavenâs choicest blessings be showered on the newly married couple.â A person wishing to send a greeting
only needs to specify the number, which is used to generate the actual
greeting at the destination.
The idea of adaptive dictionary-based schemes was not explored until
Ziv and Lempel wrote their papers in 1977 and 1978. The two papers
describe two distinct versions of the algorithm. We refer to these versions as LZ77 or sliding window LempelâZiv and LZ78 or tree-structured
LempelâZiv. (They are sometimes called LZ1 and LZ2, respectively.)
We ï¬rst describe the basic algorithms in the two cases and describe
some simple variations. We later prove their optimality, and end with
some practical issues. The key idea of the LempelâZiv algorithm is to
parse the string into phrases and to replace phrases by pointers to where
the same string has occurred in the past. The differences between the
algorithms is based on differences in the set of possible match locations
(and match lengths) the algorithm allows.
13.4.1

Sliding Window LempelâZiv Algorithm

The algorithm described in the 1977 paper encodes a string by ï¬nding the
longest match anywhere within a window of past symbols and represents
the string by a pointer to location of the match within the window and the
length of the match. There are many variations of this basic algorithm,
and we describe one due to Storer and Szymanski [507].
We assume that we have a string x1 , x2 , . . . to be compressed from a
ï¬nite alphabet. A parsing S of a string x1 x2 Â· Â· Â· xn is a division of the
string into phrases, separated by commas. Let W be the length of the
window. Then the algorithm can be described as follows: Assume that
we have compressed the string until time i â 1. Then to ï¬nd the next
phrase, ï¬nd the largest k such that for some j , i â 1 â W â¤ j â¤ i â 1,

442

UNIVERSAL SOURCE CODING

the string of length k starting at xj is equal to the string (of length k)
starting at xi (i.e., xj +l = xi+l for all 0 â¤ l < k). The next phrase is then
of length k (i.e., xi . . . xi+kâ1 ) and is represented by the pair (P , L), where
P is the location of the beginning of the match and L is the length of
the match. If a match is not found in the window, the next character is
sent uncompressed. To distinguish between these two cases, a ï¬ag bit
is needed, and hence the phrases are of two types: (F, P , L) or (F, C),
where C represents an uncompressed character.
Note that the target of a (pointer,length) pair could extend beyond the
window, so that it overlaps with the new phrase. In theory, this match
could be arbitrarily long; in practice, though, the maximum phrase length
is restricted to be less than some parameter.
For example, if W = 4 and the string is ABBABBABBBAABABA
and the initial window is empty, the string will be parsed as follows:
A,B,B,ABBABB,BA,A,BA,BA, which is represented by the sequence of
âpointersâ: (0,A),(0,B),(1,1,1),(1,3,6),(1,4,2),(1,1,1),(1,3,2),(1,2,2), where
the ï¬ag bit is 0 if there is no match and 1 if there is a match, and the
location of the match is measured backward from the end of the window.
[In the example, we have represented every match within the window
using the (P , L) pair; however, it might be more efï¬cient to represent
short matches as uncompressed characters. See Problem 13.8 for details.]
We can view this algorithm as using a dictionary that consists of all
substrings of the string in the window and of all single characters. The
algorithm ï¬nds the longest match within the dictionary and sends a pointer
to that match. We later show that a simple variation on this version of
LZ77 is asymptotically optimal. Most practical implementations of LZ77,
such as gzip and pkzip, are also based on this version of LZ77.
13.4.2

Tree-Structured LempelâZiv Algorithms

In the 1978 paper, Ziv and Lempel described an algorithm that parses a
string into phrases, where each phrase is the shortest phrase not seen earlier. This algorithm can be viewed as building a dictionary in the form of
a tree, where the nodes correspond to phrases seen so far. The algorithm is
particularly simple to implement and has become popular as one of the early
standard algorithms for ï¬le compression on computers because of its speed
and efï¬ciency. It is also used for data compression in high-speed modems.
The source sequence is sequentially parsed into strings that have not
appeared so far. For example, if the string is ABBABBABBBAABABAA
. . . , we parse it as A,B,BA,BB,AB,BBA,ABA,BAA . . . . After every comma, we look along the input sequence until we come to the shortest string
that has not been marked off before. Since this is the shortest such string,

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

443

all its preï¬xes must have occurred earlier. (Thus, we can build up a tree
of these phrases.) In particular, the string consisting of all but the last bit
of this string must have occurred earlier. We code this phrase by giving
the location of the preï¬x and the value of the last symbol. Thus, the string
above would be represented as (0,A),(0,B),(2,A),(2,B),(1,B),(4,A),(5,A),
(3,A), . . . .
Sending an uncompressed character in each phrase results in a loss of
efï¬ciency. It is possible to get around this by considering the extension
character (the last character of the current phrase) as part of the next
phrase. This variation, due to Welch [554], is the basis of most practical
implementations of LZ78, such as compress on Unix, in compression in
modems, and in the image ï¬les in the GIF format.
13.5
13.5.1

OPTIMALITY OF LEMPELâZIV ALGORITHMS
Sliding Window LempelâZiv Algorithms

In the original paper of Ziv and Lempel [603], the authors described the
basic LZ77 algorithm and proved that it compressed any string as well
as any ï¬nite-state compressor acting on that string. However, they did
not prove that this algorithm achieved asymptotic optimality (i.e., that the
compression ratio converged to the entropy for an ergodic source). This
result was proved by Wyner and Ziv [591].
The proof relies on a simple lemma due to Kac: the average length of
time that you need to wait to see a particular symbol is the reciprocal of
the probability of a symbol. Thus, we are likely to see the high-probability
strings within the window and encode these strings efï¬ciently. The strings
that we do not ï¬nd within the window have low probability, so that
asymptotically, they do not inï¬uence the compression achieved.
Instead of proving the optimality of the practical version of LZ77, we
will present a simpler proof for a different version of the algorithm, which,
though not practical, captures some of the basic ideas. This algorithm
assumes that both the sender and receiver have access to the inï¬nite past
of the string, and represents a string of length n by pointing to the last
time it occurred in the past.
We assume that we have a stationary and ergodic process deï¬ned
for time from ââ to â, and that both the encoder and decoder have
access to . . . , Xâ2 , Xâ1 , the inï¬nite past of the sequence. Then to encode
X0 , X1 , . . . , Xnâ1 (a block of length n), we ï¬nd the last time we have
seen these n symbols in the past. Let
Rn (X0 , X1 , . . . , Xnâ1 ) =
max{j < 0 : (Xâj , Xâj +1 . . . Xâj +nâ1 ) = (X0 , . . . , Xnâ1 )}. (13.60)

444

UNIVERSAL SOURCE CODING

Then to represent X0 , . . . , Xnâ1 , we need only to send Rn to the receiver,
who can then look back Rn bits into the past and recover X0 , . . . , Xnâ1 .
Thus, the cost of the encoding is the cost of representing Rn . We will show
that this cost is approximately log Rn and that asymptotically n1 E log Rn
â H (X), thus proving the asymptotic optimality of this algorithm.
We will need the following lemmas.
Lemma 13.5.1 There exists a preï¬x-free code for the integers such that
the length of the codeword for integer k is log k + 2 log log k + O(1).
Proof: If we knew that k â¤ m, we could encode k with log m bits. However, since we donât have an upper limit for k, we need to tell the receiver
the length of the encoding of k (i.e., we need to specify log k). Consider
the following encoding for the integer k: We ï¬rst represent log k	 in
unary, followed by the binary representation of k:
Â· Â· Â· x .
Â· Â· Â· 0 1 xx
C1 (k) = 00 
 
log k	 0âs k in binary

(13.61)

It is easy to see that the length of this representation is 2log k	 + 1 â¤
2 log k + 3. This is more than the length we are looking for since we are
using the very inefï¬cient unary code to send log k. However, if we use C1
to represent log k, it is now easy to see that this representation has a length
less than log k + 2 log log k + 4, which proves the lemma. A similar method

is presented in the discussion following Theorem 14.2.3.
The key result that underlies the proof of the optimality of LZ77 is
Kacâs lemma, which relates the average recurrence time to the probability of a symbol for any stationary ergodic process. For example, if
X1 , X2 , . . . , Xn is an i.i.d. process, we ask what is the expected waiting
time to see the symbol a again, conditioned on the fact that X1 = a. In
this case, the waiting time has a geometric distribution with parameter
p = p(X0 = a), and thus the expected waiting time is 1/p(X0 = a). The
somewhat surprising result is that the same is true even if the process is
not i.i.d., but stationary and ergodic. A simple intuitive reason for this
is that in a long sample of length n, we would expect to see a about
np(a) times, and the average distance between these occurrences of a is
n/(np(a)) (i.e., 1/p(a)).
Lemma 13.5.2 (Kac) Let . . . , U2 , U1 , U0 , U1 , . . . be a stationary
ergodic process on a countable alphabet. For any u such that p(u) > 0

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

445

and for i = 1, 2, . . . , let


Qu (i) = Pr Uâi = u; Uj = u for â i < j < 0|U0 = u

(13.62)

[i.e., Qu (i) is the conditional probability that the most recent previous
occurrence of the symbol u is i, given that U0 = u]. Then
E(R1 (U )|X0 = u) =


i

iQu (i) =

1
.
p(u)

(13.63)

Thus, the conditional expected waiting time to see the symbol u again,
looking backward from zero, is 1/p(u).
Note the amusing fact that the expected recurrence time
ER1 (U ) =



p(u)

1
= m,
p(u)

(13.64)

where m is the alphabet size.
Proof: Let U0 = u. Deï¬ne the events for j = 1, 2, . . . and k = 0, 1, 2, . . . :


Aj k = Uâj = u, Ul = u, âj < l < k, Uk = u .

(13.65)

Event Aj k corresponds to the event where the last time before zero at
which the process is equal to u is at âj , the ï¬rst time after zero at which
the process equals u is k. These events are disjoint, and by ergodicity, the
probability Pr{âªj,k Aj k } = 1. Thus,


1 = Pr âªj,k Aj k
(a)

=

â
â 


Pr{Aj k }

(13.66)
(13.67)

j =1 k=0

=

â
â 




Pr(Uk = u) Pr Uâj = u, Ul = u, âj < l < k|Uk = u

j =1 k=0

(13.68)
(b)

=

â
â 

j =1 k=0

Pr(Uk = u)Qu (j + k)

(13.69)

446

UNIVERSAL SOURCE CODING

( c)

=

â
â 


Pr(U0 = u)Qu (j + k)

(13.70)

j =1 k=0

= Pr(U0 = u)

â
â 


Qu (j + k)

(13.71)

j =1 k=0
(d)

= Pr(U0 = u)

â


iQu (i),

(13.72)

i=1

where (a) follows from the fact that the Aj k are disjoint, (b) follows from
the deï¬nition of Qu (Â·), (c) follows from stationarity, and (d) follows from
the fact that there are i pairs (j, k) such that j + k = i in the sum. Kacâs

lemma follows directly from this equation.
Corollary Let . . . , Xâ1 , X0 , X1 , . . . be a stationary ergodic process and
let Rn (X0 , . . . , Xnâ1 ) be the recurrence time looking backward as deï¬ned
in (13.60). Then


E Rn (X0 , . . . , Xnâ1 )|(X0 , . . . , Xnâ1 ) = x0nâ1 =

1
p(x0nâ1 )

.

(13.73)

Proof: Deï¬ne a new process with Ui = (Xi , Xi+1 , . . . , Xi+nâ1 ). The U
process is also stationary and ergodic, and thus by Kacâs lemma the average recurrence time for U conditioned on U0 = u is 1/p(u). Translating
this to the X process proves the corollary.

We are now in a position to prove the main result, which shows that
the compression ratio for the simple version of LempelâZiv using recurrence time approaches the entropy. The algorithm describes X0nâ1 by
describing Rn (X0nâ1 ), which by Lemma 13.5.1 can be done with log Rn +
2 log log Rn + 4 bits. We now prove the following theorem.
Theorem 13.5.1 Let Ln (X0nâ1 ) = log Rn + 2 log log Rn + O(1) be the
description length for X0nâ1 in the simple algorithm described above. Then
1
ELn (X0nâ1 ) â H (X)
n
as n â â, where H (X) is the entropy rate of the process {Xi }.

(13.74)

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

447

Proof: We will prove upper and lower bounds for ELn . The lower bound
follows directly from standard source coding results (i.e., ELn â¥ nH for
any preï¬x-free code). To prove the upper bound, we ï¬rst show that
1
lim E log Rn â¤ H
n

(13.75)

and later bound the other terms in the expression for Ln . To prove the
bound for E log Rn , we expand the expectation by conditioning on the
value of X0nâ1 and then applying Jensenâs inequality. Thus,
1
1
E log Rn =
p(x0nâ1 )E[log Rn (X0nâ1 )|X0nâ1 = x0nâ1 ]
n
n nâ1

(13.76)

1
p(x0nâ1 ) log E[Rn (X0nâ1 )|X0nâ1 = x0nâ1 ]
n nâ1

(13.77)

1
1
p(x0nâ1 ) log
n nâ1
p(x0nâ1 )

(13.78)

x0

â¤

x0

=

x0

1
H (X0nâ1 )
n

 H (X).

=

(13.79)
(13.80)

The second term in the expression for Ln is log log Rn , and we wish to
show that
1
E[log log Rn (X0nâ1 )] â 0.
(13.81)
n
Again, we use Jensenâs inequality,
1
1
E log log Rn â¤ log E[log Rn (X0nâ1 )]
n
n
1
â¤ log H (X0nâ1 ),
n

(13.82)
(13.83)

where the last inequality follows from (13.79). For any  > 0, for large
enough n, H (X0nâ1 ) < n(H + ), and therefore n1 log log Rn < n1
log n + n1 log(H + ) â 0. This completes the proof of the

theorem.
Thus, a compression scheme that represents a string by encoding the
last time it was seen in the past is asymptotically optimal. Of course, this
scheme is not practical, since it assumes that both sender and receiver

448

UNIVERSAL SOURCE CODING

have access to the inï¬nite past of a sequence. For longer strings, one
would have to look further and further back into the past to ï¬nd a match.
For example, if the entropy rate is 12 and the string has length 200 bits,
one would have to look an average of 2100 â 1030 bits into the past to
ï¬nd a match. Although this is not feasible, the algorithm illustrates the
basic idea that matching the past is asymptotically optimal. The proof of
the optimality of the practical version of LZ77 with a ï¬nite window is
based on similar ideas. We will not present the details here, but refer the
reader to the original proof in [591].
13.5.2

Optimality of Tree-Structured LempelâZiv Compression

We now consider the tree-structured version of LempelâZiv, where the
input sequence is parsed into phrases, each phrase being the shortest string
that has not been seen so far. The proof of the optimality of this algorithm
has a very different ï¬avor from the proof for LZ77; the essence of the
proof is a counting argument that shows that the number of phrases cannot
be too large if they are all distinct, and the probability of any sequence of
symbols can be bounded by a function of the number of distinct phrases
in the parsing of the sequence.
The algorithm described in Section 13.4.2 requires two passes over the
stringâin the ï¬rst pass, we parse the string and calculate c(n), the number
of phrases in the parsed string. We then use that to decide how many bits
[log c(n)] to allot to the pointers in the algorithm. In the second pass, we
calculate the pointers and produce the coded string as indicated above.
The algorithm can be modiï¬ed so that it requires only one pass over the
string and also uses fewer bits for the initial pointers. These modiï¬cations
do not affect the asymptotic efï¬ciency of the algorithm. Some of the
implementation details are discussed by Welch [554] and Bell et al. [41].
We will show that like the sliding window version of LempelâZiv,
this algorithm asymptotically achieves the entropy rate for the unknown
ergodic source. We ï¬rst deï¬ne a parsing of the string to be a decomposition
into phrases.
Deï¬nition A parsing S of a binary string x1 x2 Â· Â· Â· xn is a division of the
string into phrases, separated by commas. A distinct parsing is a parsing
such that no two phrases are identical. For example, 0,111,1 is a distinct
parsing of 01111, but 0,11,11 is a parsing that is not distinct.
The LZ78 algorithm described above gives a distinct parsing of the
source sequence. Let c(n) denote the number of phrases in the LZ78
parsing of a sequence of length n. Of course, c(n) depends on the sequence
X n . The compressed sequence (after applying the LempelâZiv algorithm)

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

449

consists of a list of c(n) pairs of numbers, each pair consisting of a pointer
to the previous occurrence of the preï¬x of the phrase and the last bit of
the phrase. Each pointer requires log c(n) bits, and hence the total length
of the compressed sequence is c(n)[log c(n) + 1] bits. We now show that
c(n)(log c(n)+1)
â H (X) for a stationary ergodic sequence X1 , X2 , . . . , Xn .
n
Our proof is based on the simple proof of asymptotic optimality of LZ78
coding due to Wyner and Ziv [575].
Before we proceed to the details of the proof, we provide an outline
of the main ideas. The ï¬rst lemma shows that the number of phrases in
a distinct parsing of a sequence is less than n/ log n; the main argument
in the proof is based on the fact that there are not enough distinct short
phrases. This bound holds for any distinct parsing of the sequence, not
just the LZ78 parsing.
The second key idea is a bound on the probability of a sequence based
on the number of distinct phrases. To illustrate this, consider an i.i.d.
sequence of random variables X1 , X2 , X3 , X4 that take on four possible
values, {A, B, C, D}, with probabilities pA , pB , pC , and pD , respectively. Now consider the probability of a sequence P (D, A, B, C) =
pD pA pB pC . Since pA + pB + pC + pD = 1, the product pD pA pB pC is
maximized when the probabilities are equal (i.e., the maximum value of
the probability of a sequence of four distinct symbols is 1/256). On the
other hand, if we consider a sequence A, B, A, B, the probability of this
sequence is maximized if pA = pB = 12 , pC = pD = 0, and the maximum
1
. A sequence of the form A, A, A, A could
probability for A, B, A, B is 16
have a probability of 1. All these examples illustrate a basic pointâsequences with a large number of distinct symbols (or phrases) cannot have
a large probability. Zivâs inequality (Lemma 13.5.5) is the extension of
this idea to the Markov case, where the distinct symbols are the phrases
of the distinct parsing of the source sequence.
Since the description length of a sequence after the parsing grows as
c log c, the sequences that have very few distinct phrases can be compressed efï¬ciently and correspond to strings that could have a high probability. On the other hand, strings that have a large number of distinct
phrases do not compress as well; but the probability of these sequences
could not be too large by Zivâs inequality. Thus, Zivâs inequality enables
us to connect the logarithm of the probability of the sequence with the
number of phrases in its parsing, and this is ï¬nally used to show that the
tree-structured LempelâZiv algorithm is asymptotically optimal.
We ï¬rst prove a few lemmas that we need for the proof of the theorem.
The ï¬rst is a bound on the number of phrases possible in a distinct parsing
of a binary sequence of length n.

450

UNIVERSAL SOURCE CODING

Lemma 13.5.3 (Lempel and Ziv [604]) The number of phrases c(n) in
a distinct parsing of a binary sequence X1 , X2 , . . . , Xn satisï¬es
c(n) â¤

n
,
(1 â n ) log n

(13.84)

n)+4
} â 0 as n â â.
where n = min{1, log(log
log n

Proof: Let
nk =

k


j 2j = (k â 1)2k+1 + 2

(13.85)

j =1

be the sum of the lengths of all distinct strings of length less than or equal
to k. The number of phrases c in a distinct parsing of a sequence of length
n is maximized when all the phrases are as short as possible. If n = nk ,
this occurs when all the phrases are of length â¤ k, and thus
c(nk ) â¤

k


2j = 2k+1 â 2 < 2k+1 â¤

j =1

nk
.
kâ1

(13.86)

If nk â¤ n < nk+1 , we write n = nk + , where  < (k + 1)2k+1 . Then
the parsing into shortest phrases has each of the phrases of length â¤ k
and /(k + 1) phrases of length k + 1. Thus,
c(n) â¤

nk

nk + 
n
+
â¤
=
.
kâ1 k+1
kâ1
kâ1

(13.87)

We now bound the size of k for a given n. Let nk â¤ n < nk+1 . Then
n â¥ nk = (k â 1)2k+1 + 2 â¥ 2k ,

(13.88)

k â¤ log n.

(13.89)

and therefore
Moreover,
n â¤ nk+1 = k2k+2 + 2 â¤ (k + 2)2k+2 â¤ (log n + 2)2k+2 ,

(13.90)

by (13.89), and therefore
k + 2 â¥ log

n
,
log n + 2

(13.91)

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

451

or for all n â¥ 4,
k â 1 â¥ log n â log(log n + 2) â 3


log(log n + 2) + 3
log n
= 1â
log n


log(2 log n) + 3
â¥ 1â
log n
log n


log(log n) + 4
log n
= 1â
log n

(13.92)
(13.93)
(13.94)
(13.95)

= (1 â n ) log n.

(13.96)

n)+4
Note that n = min{1, log(log
}. Combining (13.96) with (13.87), we
log n
obtain the lemma.


We will need a simple result on maximum entropy in the proof of the
main theorem.
Lemma 13.5.4 Let Z be a nonnegative integer-valued random variable
with mean Âµ. Then the entropy H (Z) is bounded by
H (Z) â¤ (Âµ + 1) log(Âµ + 1) â Âµ log Âµ.

(13.97)

Proof: The lemma follows directly from the results of Theorem 12.1.1,
which show that the geometric distribution maximizes the entropy of a
nonnegative integer-valued random variable subject to a mean constraint.

Let {Xi }â
i=ââ be a binary stationary ergodic process with probability mass function P (x1 , x2 , . . . , xn ). (Ergodic processes are discussed in
greater detail in Section 16.8.) For a ï¬xed integer k, deï¬ne the kth-order
Markov approximation to P as


Qk (xâ(kâ1) , . . . , x0 , x1 , . . . , xn ) =
j 

0
P (xâ(kâ1)
)

n

j =1

j â1

P (xj |xj âk ), (13.98)

0
will be
where xi = (xi , xi+1 , . . . , xj ), i â¤ j , and the initial state xâ(kâ1)
nâ1
part of the speciï¬cation of Qk . Since P (Xn |Xnâk ) is itself an ergodic

452

UNIVERSAL SOURCE CODING

process, we have
1
1
j â1
0
)=â
log P (Xj |Xj âk )
â log Qk (X1 , X2 , . . . , Xn |Xâ(kâ1)
n
n
n

j =1

(13.99)
j â1

â âE log P (Xj |Xj âk ) (13.100)
j â1

= H (Xj |Xj âk ).

(13.101)

We will bound the rate of the LZ78 code by the entropy rate of the kthorder Markov approximation for all k. The entropy rate of the Markov
j â1
approximation H (Xj |Xj âk ) converges to the entropy rate of the process
as k â â, and this will prove the result.
n
n
= xâ(kâ1)
, and suppose that x1n is parsed into c
Suppose that Xâ(kâ1)
distinct phrases, y1 , y2 , . . . , yc . Let Î½i be the index of the start of the
Î½ â1
Î½ â1
ith phrase (i.e., yi = xÎ½ii+1 ). For each i = 1, 2, . . . , c, deï¬ne si = xÎ½iiâk .
0
Thus, si is the k bits of x preceding yi . Of course, s1 = xâ(kâ1)
.
Let cls be the number of phrases yi with length l and preceding state
si = s for l = 1, 2, . . . and s â Xk . We then have

cls = c
(13.102)
l,s



and

lcls = n.

(13.103)

l,s

We now prove a surprising upper bound on the probability of a string
based on the parsing of the string.
Lemma 13.5.5 (Zivâs inequality) For any distinct parsing (in particular, the LZ78 parsing) of the string x1 x2 Â· Â· Â· xn , we have

log Qk (x1 , x2 , . . . , xn |s1 ) â¤ â
cls log cls .
(13.104)
l,s

Note that the right-hand side does not depend on Qk .
Proof: We write
Qk (x1 , x2 , . . . , xn |s1 ) = Qk (y1 , y2 , . . . , yc |s1 )

(13.105)

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

=

c


453

P (yi |si )

(13.106)

log P (yi |si )

(13.107)

i=1

or
log Qk (x1 , x2 , . . . , xn |s1 ) =

c

i=1

=





log P (yi |si )

(13.108)

l,s i:|yi |=l,si =s

=


l,s

â¤





cls

i:|yi |=l,si =s

ï£«

cls log ï£­

1
log P (yi |si )
cls

ï£¶



i:|yi |=l,si =s

l,s

(13.109)

1
P (yi |si )ï£¸ ,
cls
(13.110)

where the inequality follows from Jensenâs inequality and the concavity
of the logarithm.

Now since the yi are distinct, we have i:|yi |=l,si =s P (yi |si ) â¤ 1. Thus,
log Qk (x1 , x2 , . . . , xn |s1 ) â¤


l,s

cls log

1
,
cls

(13.111)


proving the lemma.
We can now prove the main theorem.

Theorem 13.5.2 Let {Xn } be a binary stationary ergodic process with
entropy rate H (X), and let c(n) be the number of phrases in a distinct
parsing of a sample of length n from this process. Then
lim sup
nââ

c(n) log c(n)
â¤ H (X)
n

(13.112)

with probability 1.
Proof: We begin with Zivâs inequality, which we rewrite as

cls c
cls log
log Qk (x1 , x2 , . . . , xn |s1 ) â¤ â
c
l,s

(13.113)

454

UNIVERSAL SOURCE CODING

= âc log c â c

 cls
c

ls

Writing Ïls =

cls
c ,

log

cls
.
c

(13.114)

we have




Ïls = 1,

l,s

lÏls =

l,s

n
,
c

(13.115)

from (13.102) and (13.103). We now deï¬ne random variables U , V such
that
Pr(U = l, V = s) = Ïls .
Thus, EU =

n
c

(13.116)

and

log Qk (x1 , x2 , . . . , xn |s1 ) â¤ cH (U, V ) â c log c

(13.117)

1
c
c
â log Qk (x1 , x2 , . . . , xn |s1 ) â¥ log c â H (U, V ).
n
n
n

(13.118)

or

Now
H (U, V ) â¤ H (U ) + H (V )

(13.119)

and H (V ) â¤ log |X|k = k. By Lemma 13.5.4, we have
H (U ) â¤ (EU + 1) log(EU + 1) â (EU ) log(EU )
%
$n
% n
$n
n
+ 1 log
+ 1 â log
=
c
c
c
c
%
$c
%
n $n
= log +
+ 1 log
+1 .
c
c
n

(13.120)
(13.121)
(13.122)

Thus,
c
c
n
c
H (U, V ) â¤ k + log + o(1).
n
n
n
c

(13.123)

For a given n, the maximum of nc log nc is attained for the maximum value
of c (for nc â¤ 1e ). But from Lemma 13.5.3, c â¤ logn n (1 + o(1)). Thus,
c
n
log â¤ O
n
c




log log n
,
log n

(13.124)

13.5 OPTIMALITY OF LEMPELâZIV ALGORITHMS

455

and therefore nc H (U, V ) â 0 as n â â. Therefore,
c(n) log c(n)
1
â¤ â log Qk (x1 , x2 , . . . , xn |s1 ) + k (n),
n
n

(13.125)

where k (n) â 0 as n â â. Hence, with probability 1,
lim sup
nââ

1
c(n) log c(n)
0
â¤ lim â log Qk (X1 , X2 , . . . , Xn |Xâ(kâ1)
)
nââ n
n
(13.126)
= H (X0 |Xâ1 , . . . , Xâk )

(13.127)

â H (X)

(13.128)

as k â â. 

We now prove that LZ78 coding is asymptotically optimal.
Theorem 13.5.3 Let {Xi }â
ââ be a binary stationary ergodic stochastic
process. Let l(X1 , X2 , . . . , Xn ) be the LZ78 codeword length associated
with X1 , X2 , . . . , Xn . Then
1
lim sup l(X1 , X2 , . . . , Xn ) â¤ H (X)
nââ n

with probability 1,

(13.129)

where H (X) is the entropy rate of the process.
Proof: We have shown that l(X1 , X2 , . . . , Xn ) = c(n)(log c(n) + 1),
where c(n) is the number of phrases in the LZ78 parsing of the
string X1 , X2 , . . . , Xn . By Lemma 13.5.3, lim sup c(n)/n = 0, and thus
Theorem 13.5.2 establishes that


c(n) log c(n) c(n)
l(X1 , X2 , . . . , Xn )
= lim sup
+
lim sup
n
n
n
â¤ H (X) with probability 1. 

(13.130)

Thus, the length per source symbol of the LZ78 encoding of an ergodic
source is asymptotically no greater than the entropy rate of the source.
There are some interesting features of the proof of the optimality of LZ78
that are worth noting. The bounds on the number of distinct phrases
and Zivâs inequality apply to any distinct parsing of the string, not just
the incremental parsing version used in the algorithm. The proof can be
extended in many ways with variations on the parsing algorithm; for
example, it is possible to use multiple trees that are context or state

456

UNIVERSAL SOURCE CODING

dependent [218, 426]. Zivâs inequality (Lemma 13.5.5) remains particularly intriguing since it relates a probability on one side with a purely
deterministic function of the parsing of a sequence on the other.
The LempelâZiv codes are simple examples of a universal code (i.e., a
code that does not depend on the distribution of the source). This code can
be used without knowledge of the source distribution and yet will achieve
an asymptotic compression equal to the entropy rate of the source.

SUMMARY
Ideal word length
1
.
p(x)

(13.131)

Ep l â (x) = H (p).

(13.132)

l â (x) = log
Average description length

1
Ë
Estimated probability distribution pÌ(x). If l(x)
= log pÌ(x)
, then

Ë
= H (p) + D(p||pÌ).
Ep l(x)

(13.133)

Average redundancy
Rp = Ep l(X) â H (p).

(13.134)

Minimax redundancy. For X â¼ pÎ¸ (x), Î¸ â Î¸ ,
D â = min max Rp = min max D(pÎ¸ ||q).
l

p

q

Î¸

(13.135)

Minimax theorem. D â = C, where C is the capacity of the channel
{Î¸, pÎ¸ (x), X}.
Bernoulli sequences. For X n â¼Bernoulli(Î¸ ), the redundancy is
Dnâ = min max D(pÎ¸ (x n )||q(x n )) â
q

Î¸

1
log n + o(log n).
2

(13.136)

Arithmetic coding. nH bits of F (x n ) reveal approximately n bits
of x n .

PROBLEMS

457

LempelâZiv coding (recurrence time coding). Let Rn (X n ) be the
last time in the past that we have seen a block of n symbols X n . Then
1
n log Rn â H (X), and encoding by describing the recurrence time is
asymptotically optimal.
LempelâZiv coding (sequence parsing). If a sequence is parsed into
the shortest phrases not seen before (e.g., 011011101 is parsed to
0,1,10,11,101,...) and l(x n ) is the description length of the parsed sequence, then
1
lim sup l(X n ) â¤ H (X)
n

with probability 1

(13.137)

for every stationary ergodic process {Xi }.

PROBLEMS
13.1

Minimax regret data compression and channel capacity. First
consider universal data compression with respect to two source
distributions. Let the alphabet V = {1, e, 0} and let p1 (v) put mass
1 â Î± on v = 1 and mass Î± on v = e. Let p2 (v) put mass 1 â Î± on
0 and mass Î± on v = e. We assign word lengths to V according to
1
, the ideal codeword length with respect to a clevl(v) = log p(v)
erly chosen probability mass function p(v). The worst-case excess
description length (above the entropy of the true distribution) is

max Epi log
i

1
1
â Epi log
p(V )
pi (V )


= max D(pi  p).
i

(13.138)
Thus, the minimax regret is D â = minp maxi D(pi  p).
(a) Find D â .
(b) Find the p(v) achieving D â .
(c) Compare D â to the capacity of the binary erasure channel


and comment.

1âÎ± Î±
0
0
Î± 1âÎ±



458

13.2

UNIVERSAL SOURCE CODING

Universal data compression.
tributions on X,
Pa = (0.7, 0.2, 0.1),

Consider three possible source dis-

Pb = (0.1, 0.7, 0.2),

Pc = (0.2, 0.1, 0.7).

(a) Find the minimum incremental cost of compression
D â = min max D(PÎ¸ P ),
P

Î¸

the associated mass function P = (p1 , p2 , p3 ), and ideal codeword lengths li = log(1/pi ).
(b) What is the channel capacity of a channel matrix with rows
Pa , Pb , Pc ?
13.3

Arithmetic coding. Let {Xi }â
i=0 be a stationary binary Markov
chain with transition matrix
'
&
pij =

3
4
1
4

1
4
3
4

.

(13.139)

Calculate the ï¬rst 3 bits of F (X â ) = 0.F1 F2 . . . when X â =
1010111 . . . . How many bits of X â does this specify?
13.4

Arithmetic coding.
&
transition matrix

1
3
2
3

Let X
i be binary stationary Markov with
2 '
3
1
3

.

(a) Find F (01110) = Pr{.X1 X2 X3 X4 X5 < .01110}.
(b) How many bits .F1 F2 . . . can be known for sure if it is not
known how X = 01110 continues?
13.5

LempelâZiv . Give the LZ78
00000011010100000110101.

parsing

and

encoding

of

13.6

Compression of constant sequence. We are given the constant
sequence x n = 11111 . . . .
(a) Give the LZ78 parsing for this sequence.
(b) Argue that the number of encoding bits per symbol for this
sequence goes to zero as n â â.

13.7

Another idealized version of LempelâZiv coding. An idealized
version of LZ was shown to be optimal: The encoder and decoder
both have available to them the âinï¬nite pastâ generated by the
process, . . . , Xâ1 , X0 , and the encoder describes the string (X1 ,
X2 , . . . , Xn ) by telling the decoder the position Rn in the past

PROBLEMS

459

of the ï¬rst recurrence of that string. This takes roughly log Rn +
2 log log Rn bits. Now consider the following variant: Instead of
describing Rn , the encoder describes Rnâ1 plus the last symbol, Xn . From these two the decoder can reconstruct the string
(X1 , X2 , . . . , Xn ).
(a) What is the number of bits per symbol used in this case to
encode (X1 , X2 , . . . , Xn )?
(b) Modify the proof given in the text to show that this version is
also asymptotically optimal: namely, that the expected number
of bits per symbol converges to the entropy rate.
13.8

Length of pointers in LZ77 . In the version of LZ77 due to Storer
and Szymanski [507] described in Section 13.4.1, a short match
can be represented by either (F, P , L) (ï¬ag, pointer, length) or
by (F, C) (ï¬ag, character). Assume that the window length is W ,
and assume that the maximum match length is M.
(a) How many bits are required to represent P ? To represent L?
(b) Assume that C, the representation of a character, is 8 bits
long. If the representation of P plus L is longer than 8 bits,
it would be better to represent a single character match as
an uncompressed character rather than as a match within the
dictionary. As a function of W and M, what is the shortest
match that one should represent as a match rather than as
uncompressed characters?
(c) Let W = 4096 and M = 256. What is the shortest match that
one would represent as a match rather than uncompressed
characters?

13.9

LempelâZiv 78 .
(a) Continue the LempelâZiv parsing of the sequence
0,00,001,00000011010111.
(b) Give a sequence for which the number of phrases in the LZ
parsing grows as fast as possible.
(c) Give a sequence for which the number of phrases in the LZ
parsing grows as slowly as possible.

13.10

Two versions of ï¬xed-database LempelâZiv . Consider a source
(A, P ). For simplicity assume that the alphabet is ï¬nite |A| =
A < â and the symbols are i.i.d. â¼ P . A ï¬xed database D is
given and is revealed to the decoder. The encoder parses the target sequence x1n into blocks of length l, and subsequently encodes
them by giving the binary description of their last appearance

460

UNIVERSAL SOURCE CODING

in the database. If a match is not found, the entire block is
sent uncompressed, requiring l log A bits. A ï¬ag is used to tell
the decoder whether a match location is being described or the
sequence itself. Parts (a) and (b) give some preliminaries you will
need in showing the optimality of ï¬xed-database LZ in part (c).
(a) Let xl be a Î´-typical sequence of length l starting at 0, and let
Rl (x l ) be the corresponding recurrence index in the inï¬nite
past . . . , Xâ2 , Xâ1 . Show that
(
)
E Rl (X l )|X l = x l â¤ 2l(H +Î´) ,
where H is the entropy rate of the source.


(b) Prove that for any  > 0, Pr Rl (X l ) > 2l(H +) â 0 as l â
â. (Hint: Expand the probability by conditioning on strings
x l , and break things up into typical and nontypical. Markovâs
inequality and the AEP should prove handy as well.)
(c) Consider the following two ï¬xed databases: (i) D1 is formed
by taking all Î´-typical l-vectors; and (ii) D2 formed by taking
the most recent LÌ = 2l(H +Î´) symbols in the inï¬nite past (i.e.,
XâLÌ , . . . , Xâ1 ). Argue that the algorithm described above is
asymptotically optimal: namely, that the expected number of
bits per symbol converges to the entropy rate when used in
conjunction with either database D1 or D2 .
13.11

Tunstall coding. The normal setting for source coding maps a
symbol (or a block of symbols) from a ï¬nite alphabet onto a variablelength string. An example of such a code is the Huffman code, which
is the optimal (minimal expected length) mapping from a set of
symbols to a preï¬x-free set of codewords. Now consider the dual
problem of variable-to-ï¬xed length codes, where we map a variablelength sequence of source symbols into a ï¬xed-length binary (or
D-ary) representation. A variable-to-ï¬xed length code for an i.i.d.
sequence of random variables X1 , X2 , . . . , Xn , Xi â¼ p(x), x â X
= {0, 1, . . . , m â 1}, is deï¬ned by a preï¬x-free set of phrases AD â
Xâ , where Xâ is the set of ï¬nite-length strings of symbols of X, and
|AD | = D. Given any sequence X1 , X2 , . . . , Xn , the string is parsed
into phrases from AD (unique because of the preï¬x-free property of
AD ) and represented by a sequence of symbols from a D-ary alphabet. Deï¬ne the efï¬ciency of this coding scheme by
R(AD ) =

log D
,
EL(AD )

(13.140)

HISTORICAL NOTES

461

where EL(AD ) is the expected length of a phrase from AD .
(a) Prove that R(AD ) â¥ H (X).
(b) The process of constructing AD can be considered as a process
of constructing an m-ary tree whose leaves are the phrases in
AD . Assume that D = 1 + k(m â 1) for some integer k â¥ 1.
Consider the following algorithm due to Tunstall:
(i) Start with A = {0, 1, . . . , m â 1} with probabilities p0 ,
p1 , . . . , pmâ1 . This corresponds to a complete m-ary tree
of depth 1.
(ii) Expand the node with the highest probability. For example, if p0 is the node with the highest probability, the
new set is A = {00, 01, . . . , 0(m â 1), 1, . . . , (m â 1)}.
(iii) Repeat step 2 until the number of leaves (number of
phrases) reaches the required value.
Show that the Tunstall algorithm is optimal in the sense that
it constructs a variable to a ï¬xed code with the best R(AD )
for a given D [i.e., the largest value of EL(AD ) for a given
D].
(c) Show that there exists a D such that R(AâD ) < H (X) + 1.
HISTORICAL NOTES
The problem of encoding a source with an unknown distribution was
analyzed by Fitingof [211] and Davisson [159], who showed that there
were classes of sources for which the universal coding procedure was
asymptotically optimal. The result relating the average redundancy of a
universal code and channel capacity is due to Gallager [229] and Ryabko
[450]. Our proof follows that of CsiszaÌr. This result was extended to
show that the channel capacity was the lower bound for the redundancy
for âmostâ sources in the class by Merhav and Feder [387], extending the
results obtained by Rissanen [444, 448] for the parametric case.
The arithmetic coding procedure has its roots in the ShannonâFano
code developed by Elias (unpublished), which was analyzed by Jelinek
[297]. The procedure for the construction of a preï¬x-free code described
in the text is due to Gilbert and Moore [249]. Arithmetic coding itself was
developed by Rissanen [441] and Pasco [414]; it was generalized by Langdon and Rissanen [343]. See also the enumerative methods in Cover [120].
Tutorial introductions to arithmetic coding can be found in Langdon [342]
and Witten et al. [564]. Arithmetic coding combined with the context-tree
weighting algorithm due to Willems et al. [560, 561] achieve the Rissanen

462

UNIVERSAL SOURCE CODING

lower bound [444] and therefore have the optimal rate of convergence to
the entropy for tree sources with unknown parameters.
The class of LempelâZiv algorithms was ï¬rst described in the seminal
papers of Lempel and Ziv [603, 604]. The original results were theoretically interesting, but people implementing compression algorithms did not
take notice until the publication of a simple efï¬cient version of the algorithm due to Welch [554]. Since then, multiple versions of the algorithms
have been described, many of them patented. Versions of this algorithm
are now used in many compression products, including GIF ï¬les for image
compression and the CCITT standard for compression in modems. The
optimality of the sliding window version of LempelâZiv (LZ77) is due to
Wyner and Ziv [575]. An extension of the proof of the optimality of LZ78
[426] shows that the redundancy of LZ78 is on the order of 1/ log(n),
as opposed to the lower bounds of log(n)/n. Thus even though LZ78
is asymptotically optimal for all stationary ergodic sources, it converges
to the entropy rate very slowly compared to the lower bounds for ï¬nitestate Markov sources. However, for the class of all ergodic sources, lower
bounds on the redundancy of a universal code do not exist, as shown by
examples due to Shields [492] and Shields and Weiss [494]. A lossless
block compression algorithm based on sorting the blocks and using simple
run-length encoding due to Burrows and Wheeler [81] has been analyzed
by Effros et al. [181]. Universal methods for prediction are discussed in
Feder, Merhav and Gutman [204, 386, 388].

CHAPTER 14

KOLMOGOROV COMPLEXITY

The great mathematician Kolmogorov culminated a lifetime of research
in mathematics, complexity, and information theory with his deï¬nition in
1965 of the intrinsic descriptive complexity of an object. In our treatment
so far, the object X has been a random variable drawn according to
a probability mass function p(x). If X is random, there is a sense in
1
, because
which the descriptive complexity of the event X = x is log p(x)
1
log p(x)  is the number of bits required to describe x by a Shannon code.
One notes immediately that the descriptive complexity of such an object
depends on the probability distribution.
Kolmogorov went further. He deï¬ned the algorithmic (descriptive)
complexity of an object to be the length of the shortest binary computer program that describes the object. (Apparently, a computer, the
most general form of data decompressor, will after a ï¬nite amount of
computation, use this description to exhibit the object described.) Thus,
the Kolmogorov complexity of an object dispenses with the probability
distribution. Kolmogorov made the crucial observation that the deï¬nition
of complexity is essentially computer independent. It is an amazing fact
that the expected length of the shortest binary computer description of a
random variable is approximately equal to its entropy. Thus, the shortest
computer description acts as a universal code which is uniformly good
for all probability distributions. In this sense, algorithmic complexity is a
conceptual precursor to entropy.
Perhaps a good point of view of the role of this chapter is to consider
Kolmogorov complexity as a way to think. One does not use the shortest
computer program in practice because it may take inï¬nitely long to ï¬nd
such a minimal program. But one can use very short, not necessarily minimal programs in practice; and the idea of ï¬nding such short programs leads
to universal codes, a good basis for inductive inference, a formalization
of Occamâs razor (âThe simplest explanation is bestâ) and to fundamental
understanding in physics, computer science, and communication theory.
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

463

464

KOLMOGOROV COMPLEXITY

Before formalizing the notion of Kolmogorov complexity, let us give
three strings as examples:
1. 0101010101010101010101010101010101010101010101010101010101010101
2. 0110101000001001111001100110011111110011101111001100100100001000
3. 1101111001110101111101101111101110101101111000101110010100111011

What are the shortest binary computer programs for each of these
sequences? The ï¬rst sequence is deï¬nitely simple. It consists of thirtytwo 01âs. The second sequence looks random and passes most tests for
randomness,
but it is in fact the initial segment of the binary expansion
â
of 2 â 1. Again, this is a simple sequence. The third again looks random, except that the proportion of 1âs is not near 12 . We shall assume
that it is otherwise random. It turns out that by describing the number
k of 1âs in the sequence, then giving the index of the sequence in a
lexicographic ordering of those with this number of 1âs, one can give a
description of the sequence in roughly log n + nH ( nk ) bits. This again is
substantially fewer than the n bits in the sequence. Again, we conclude
that the sequence, random though it is, is simple. In this case, however, it
is not as simple as the other two sequences, which have constant-length
programs. In fact, its complexity is proportional to n. Finally, we can
imagine a truly random sequence generated by pure coin ï¬ips. There are
2n such sequences and they are all equally probable. It is highly likely
that such a random sequence cannot be compressed (i.e., there is no better program for such a sequence than simply saying âPrint the following:
0101100111010. . . 0â). The reason for this is that there are not enough
short programs to go around. Thus, the descriptive complexity of a truly
random binary sequence is as long as the sequence itself.
These are the basic ideas. It will remain to be shown that this notion of
intrinsic complexity is computer independent (i.e., that the length of the
shortest program does not depend on the computer). At ï¬rst, this seems
like nonsense. But it turns out to be true, up to an additive constant. And
for long sequences of high complexity, this additive constant (which is
the length of the preprogram that allows one computer to mimic the other)
is negligible.

14.1

MODELS OF COMPUTATION

To formalize the notions of algorithmic complexity, we ï¬rst discuss acceptable models for computers. All but the most trivial computers are universal, in the sense that they can mimic the actions of other computers.

14.1

MODELS OF COMPUTATION

465

We touch brieï¬y on a certain canonical universal computer, the universal
Turing machine, the conceptually simplest universal computer.
In 1936, Turing was obsessed with the question of whether the thoughts
in a living brain could be held equally well by a collection of inanimate parts. In short, could a machine think? By analyzing the human
computational process, he posited some constraints on such a computer.
Apparently, a human thinks, writes, thinks some more, writes, and so on.
Consider a computer as a ï¬nite-state machine operating on a ï¬nite symbol
set. (The symbols in an inï¬nite symbol set cannot be distinguished in ï¬nite
space.) A program tape, on which a binary program is written, is fed left
to right into this ï¬nite-state machine. At each unit of time, the machine
inspects the program tape, writes some symbols on a work tape, changes
its state according to its transition table, and calls for more program. The
operations of such a machine can be described by a ï¬nite list of transitions. Turing argued that this machine could mimic the computational
ability of a human being.
After Turingâs work, it turned out that every new computational system could be reduced to a Turing machine, and conversely. In particular,
the familiar digital computer with its CPU, memory, and input output
devices could be simulated by and could simulate a Turing machine. This
led Church to state what is now known as Churchâs thesis, which states
that all (sufï¬ciently complex) computational models are equivalent in the
sense that they can compute the same family of functions. The class of
functions they can compute agrees with our intuitive notion of effectively
computable functions, that is, functions for which there is a ï¬nite prescription or program that will lead in a ï¬nite number of mechanically
speciï¬ed computational steps to the desired computational result.
We shall have in mind throughout this chapter the computer illustrated
in Figure 14.1. At each step of the computation, the computer reads a
symbol from the input tape, changes state according to its state transition
table, possibly writes something on the work tape or output tape, and

Input tape
...

p2p1

Finite
State
Machine

Output tape

x1x2

...

Work tape

FIGURE 14.1. A Turing machine.

466

KOLMOGOROV COMPLEXITY

moves the program read head to the next cell of the program read tape.
This machine reads the program from right to left only, never going back,
and therefore the programs form a preï¬x-free set. No program leading to a
halting computation can be the preï¬x of another such program. The restriction to preï¬x-free programs leads immediately to a theory of Kolmogorov
complexity which is formally analogous to information theory.
We can view the Turing machine as a map from a set of ï¬nite-length
binary strings to the set of ï¬nite- or inï¬nite-length binary strings. In
some cases, the computation does not halt, and in such cases the value of
the function is said to be undeï¬ned. The set of functions f : {0, 1}â â
{0, 1}â âª {0, 1}â computable by Turing machines is called the set of partial recursive functions.

14.2 KOLMOGOROV COMPLEXITY: DEFINITIONS
AND EXAMPLES
Let x be a ï¬nite-length binary string and let U be a universal computer.
Let l(x) denote the length of the string x. Let U(p) denote the output of
the computer U when presented with a program p.
We deï¬ne the Kolmogorov (or algorithmic) complexity of a string x
as the minimal description length of x.
Deï¬nition The Kolmogorov complexity KU (x) of a string x with respect
to a universal computer U is deï¬ned as
KU (x) =

min

p: U (p)=x

l(p),

(14.1)

the minimum length over all programs that print x and halt. Thus, KU (x)
is the shortest description length of x over all descriptions interpreted by
computer U.
A useful technique for thinking about Kolmogorov complexity is the
followingâif one person can describe a sequence to another person in
such a manner as to lead unambiguously to a computation of that sequence
in a ï¬nite amount of time, the number of bits in that communication is an
upper bound on the Kolmogorov complexity. For example, one can say
âPrint out the ï¬rst 1,239,875,981,825,931 bits of the square root of e.â
Allowing 8 bits per character (ASCII), we see that the unambiguous 73symbol program above demonstrates that the Kolmogorov complexity of
this huge number is no greater than (8)(73) = 584 bits. Most numbers of
this length (more than a quadrillion bits) have a Kolmogorov complexity

467

14.2 KOLMOGOROV COMPLEXITY: DEFINITIONS AND EXAMPLES

of nearly 1,239,875,981,825,931 bits. The fact that there is a simple algorithm to calculate the square root of e provides the saving in descriptive
complexity.
In the deï¬nition above, we have not mentioned anything about the
length of x. If we assume that the computer already knows the length of
x, we can deï¬ne the conditional Kolmogorov complexity knowing l(x) as
KU (x|l(x)) =

min

p: U (p,l(x))=x

l(p).

(14.2)

This is the shortest description length if the computer U has the length of
x made available to it.
It should be noted that KU (x|y) is usually deï¬ned as KU (x|y, y â ),
where y â is the shortest program for y. This is to avoid certain slight
asymmetries, but we will not use this deï¬nition here.
We ï¬rst prove some of the basic properties of Kolmogorov complexity
and then consider various examples.
Theorem 14.2.1 (Universality of Kolmogorov complexity) If U is a
universal computer, for any other computer A there exists a constant cA
such that
KU (x) â¤ KA (x) + cA

(14.3)

for all strings x â {0, 1}â , and the constant cA does not depend on x.
Proof: Assume that we have a program pA for computer A to print x.
Thus, A(pA ) = x. We can precede this program by a simulation program
sA which tells computer U how to simulate computer A. Computer U
will then interpret the instructions in the program for A, perform the
corresponding calculations and print out x. The program for U is p =
sA pA and its length is
l(p) = l(sA ) + l(pA ) = cA + l(pA ),

(14.4)

where cA is the length of the simulation program. Hence,
KU (x) =

min l(p) â¤

p:U (p)=x

for all strings x.

min

p:A(p)=x

(l(p) + cA ) = KA (x) + cA

(14.5)


The constant cA in the theorem may be very large. For example, A may
be a large computer with a large number of functions built into the system.

468

KOLMOGOROV COMPLEXITY

The computer U can be a simple microprocessor. The simulation program
will contain the details of the implementation of all these functions, in
fact, all the software available on the large computer. The crucial point is
that the length of this simulation program is independent of the length of
x, the string to be compressed. For sufï¬ciently long x, the length of this
simulation program can be neglected, and we can discuss Kolmogorov
complexity without talking about the constants.
If A and U are both universal, we have
|KU (x) â KA (x)| < c

(14.6)

for all x. Hence, we will drop all mention of U in all further deï¬nitions. We
will assume that the unspeciï¬ed computer U is a ï¬xed universal computer.
Theorem 14.2.2
sequence)

(Conditional complexity is less than the length of the
K(x|l(x)) â¤ l(x) + c.

(14.7)

Proof: A program for printing x is
Print the following l-bit sequence: x1 x2 . . . xl(x) .

Note that no bits are required to describe l since l is given. The program
is self-delimiting because l(x) is provided and the end of the program is
thus clearly deï¬ned. The length of this program is l(x) + c.

Without knowledge of the length of the string, we will need an additional stop symbol or we can use a self-punctuating scheme like the one
described in the proof of the next theorem.
Theorem 14.2.3

(Upper bound on Kolmogorov complexity)
K(x) â¤ K(x|l(x)) + 2 log l(x) + c.

(14.8)

Proof: If the computer does not know l(x), the method of Theorem
14.2.2 does not apply. We must have some way of informing the computer when it has come to the end of the string of bits that describes
the sequence. We describe a simple but inefï¬cient method that uses a
sequence 01 as a âcomma.â
Suppose that l(x) = n. To describe l(x), repeat every bit of the binary
expansion of n twice; then end the description with a 01 so that the
computer knows that it has come to the end of the description of n.

14.2 KOLMOGOROV COMPLEXITY: DEFINITIONS AND EXAMPLES

469

For example, the number 5 (binary 101) will be described as 11001101.
This description requires 2log n + 2 bits. Thus, inclusion of the binary
representation of l(x) does not add more than 2 log l(x) + c bits to the

length of the program, and we have the bound in the theorem.
A more efï¬cient method for describing n is to do so recursively. We
ï¬rst specify the number (log n) of bits in the binary representation of n
and then specify the actual bits of n. To specify log n, the length of the
binary representation of n, we can use the inefï¬cient method (2 log log n)
or the efï¬cient method (log log n + Â· Â· Â·). If we use the efï¬cient method at
each level, until we have a small number to specify, we can describe n
in log n + log log n + log log log n + Â· Â· Â· bits, where we continue the sum
until the last positive term. This sum of iterated logarithms is sometimes
written logâ n. Thus, Theorem 14.2.3 can be improved to
K(x) â¤ K(x|l(x)) + logâ l(x) + c.

(14.9)

We now prove that there are very few sequences with low complexity.
Theorem 14.2.4 (Lower bound on Kolmogorov complexity).
number of strings x with complexity K(x) < k satisï¬es
|{x â {0, 1}â : K(x) < k}| < 2k .

The
(14.10)

Proof: There are not very many short programs. If we list all the programs of length < k, we have
kâ1

  
 , 0, 1 , 00, 01, 10, 11, . . . , . . . , 11 . . . 1


 




1

2

4

(14.11)

2kâ1

and the total number of such programs is
1 + 2 + 4 + Â· Â· Â· + 2kâ1 = 2k â 1 < 2k .

(14.12)

Since each program can produce only one possible output sequence, the

number of sequences with complexity < k is less than 2k .
To avoid confusion and to facilitate exposition in the rest of this chapter,
we shall need to introduce a special notation for the binary entropy function
H0 (p) = âp log p â (1 â p) log(1 â p).

(14.13)

470

KOLMOGOROV COMPLEXITY


Thus, when we write H0 ( n1 ni=1 Xi ), we will mean âXn log X n â (1 â
Xn ) log(1 â X n ) and not the entropy of random variable Xn . When there
is no confusion, we shall simply write H (p) for H0 (p).
Now let us consider various examples of Kolmogorov complexity. The
complexity will depend on the computer, but only up to an additive constant. To be speciï¬c, we consider a computer that can accept unambiguous
commands in English (with numbers given in binary notation). We will
use the inequality
  

n
n
n
nH (k/n)
â¤
â¤
2
2nH (k/n) , k 
= 0, n,
k
8k(n â k)
Ï k(n â k)
(14.14)
which is proved in Lemma 17.5.1.
Example 14.2.1 (A sequence of n zeros) If we assume that the computer knows n, a short program to print this string is
Print the specified number of zeros.

The length of this program is a constant number of bits. This program
length does not depend on n. Hence, the Kolmogorov complexity of this
sequence is c, and
K(000 . . . 0|n) = c

for all n.

(14.15)

Example 14.2.2 (Kolmogorov complexity of Ï ) The ï¬rst n bits of Ï
can be calculated using a simple series expression. This program has a
small constant length if the computer already knows n. Hence,
K(Ï1 Ï2 Â· Â· Â· Ïn |n) = c.

(14.16)

Example 14.2.3 (Gotham weather) Suppose that we want the computer to print out the weather in Gotham for n days. We can write a
program that contains the entire sequence x = x1 x2 Â· Â· Â· xn , where xi = 1
indicates rain on day i. But this is inefï¬cient, since the weather is quite
dependent. We can devise various coding schemes for the sequence to
take the dependence into account. A simple one is to ï¬nd a Markov
model to approximate the sequence (using the empirical transition probabilities) and then code the sequence using the Shannon code for this
probability distribution. We can describe the empirical Markov transitions
1
in O(log n) bits and then use log p(x)
bits to describe x, where p is the

14.2 KOLMOGOROV COMPLEXITY: DEFINITIONS AND EXAMPLES

471

speciï¬ed Markov probability. Assuming that the entropy of the weather
is 15 bit per day, we can describe the weather for n days using about n/5
bits, and hence
K(Gotham weather|n) â

n
+ O(log n) + c.
5

(14.17)

Example 14.2.4 (Repeating sequence of the form 01010101. . .01 ) A
short program sufï¬ces. Simply print the speciï¬ed number of 01 pairs.
Hence,
K(010101010 . . . 01|n) = c.

(14.18)

Example 14.2.5 (Fractal) A fractal is part of the Mandelbrot set and
is generated by a simple computer program. For different points c in the
complex plane, one calculates the number of iterations of the map zn+1 =
zn2 + c (starting with z0 = 0) needed for |z| to cross a particular threshold.
The point c is then colored according to the number of iterations needed.
Thus, the fractal is an example of an object that looks very complex but
is essentially very simple. Its Kolmogorov complexity is essentially zero.
Example 14.2.6 (Mona Lisa) We can make use of the many structures
and dependencies in the painting. We can probably compress the image
by a factor of 3 or so by using some existing easily described image
compression algorithm. Hence, if n is the number of pixels in the image
of the Mona Lisa,
K(Mona Lisa|n) â¤

n
+ c.
3

(14.19)

Example 14.2.7 (Integer n) If the computer knows the number of bits
in the binary representation of the integer, we need only provide the values
of these bits. This program will have length c + log n.
In general, the computer will not know the length of the binary representation of the integer. So we must inform the computer in some way
when the description ends. Using the method to describe integers used
to derive (14.9), we see that the Kolmogorov complexity of an integer is
bounded by
K(n) â¤ logâ n + c.

(14.20)

Example 14.2.8 (Sequence of n bits with k ones) Can we compress a
sequence of n bits with k ones?
Our ï¬rst guess is no, since we have a series of bits that must be reproduced exactly. But consider the following program:

472

KOLMOGOROV COMPLEXITY

Generate, in lexicographic order, all sequences with k ones;
Of these sequences, print the ith sequence.

This program will print out the required sequence. The only variables in
the program are k	 (with
known range {0, 1, . . . , n}) and i (with conditional


range {1, 2, . . . , nk }). The total length of this program is
 
n
l(p) = c +
log n
+ log

k
  
to express k
to express i
	


â¤ c + log n + nH nk â 12 log n,

(14.21)

(14.22)

	 

since nk â¤ âÏ1npq 2nH0 (p) by (14.14) for p = k/n and q = 1 â p and k 
=

0 and k 
= n. We have used log n bits to represent k. Thus, if ni=1 xi = k,
then
 
1
k
K(x1 , x2 , . . . , xn |n) â¤ nH0
+ log n + c.
(14.23)
n
2
We can summarize Example 14.2.8 in the following theorem.
Theorem 14.2.5
bounded by

The Kolmogorov complexity of a binary string x is


K(x1 x2 Â· Â· Â· xn |n) â¤ nH0

1
xi
n
n



i=1

+

1
log n + c.
2

Proof: Use the program described in Example 14.2.8.

(14.24)


Remark Let x â {0, 1}â be the data that we wish to compress, and
consider the program p to be the compressed data. We will have succeeded
in compressing the data only if l(p) < l(x), or
K(x) < l(x).

(14.25)

In general, when the length l(x) of the sequence x is small, the constants
that appear in the expressions for the Kolmogorov complexity will overwhelm the contributions due to l(x). Hence, the theory is useful primarily
when l(x) is very large. In such cases we can safely neglect the terms
that do not depend on l(x).

14.3

14.3

KOLMOGOROV COMPLEXITY AND ENTROPY

473

KOLMOGOROV COMPLEXITY AND ENTROPY

We now consider the relationship between the Kolmogorov complexity of
a sequence of random variables and its entropy. In general, we show that
the expected value of the Kolmogorov complexity of a random sequence
is close to the Shannon entropy. First, we prove that the program lengths
satisfy the Kraft inequality.
Lemma 14.3.1

For any computer U,

2âl(p) â¤ 1.

(14.26)

p: U (p)halts

Proof: If the computer halts on any program, it does not look any further
for input. Hence, there cannot be any other halting program with this
program as a preï¬x. Thus, the halting programs form a preï¬x-free set,
and their lengths satisfy the Kraft inequality (Theorem 5.2.1).
We now show that
ï¬nite alphabet.

1
n
n EK(X |n)

â H (X) for i.i.d. processes with a

Theorem 14.3.1 (Relationship of Kolmogorov complexity and entropy)
Let the stochastic process {Xi } be drawn i.i.d. according to the probability
n
mass
n function f (x), x â X, where X is a ï¬nite alphabet. Let f (x ) =
i=1 f (xi ). Then there exists a constant c such that
H (X) â¤

1
(|X| â 1) log n c
f (x n )K(x n |n) â¤ H (X) +
+
(14.27)
n n
n
n
x

for all n. Consequently,
1
E K(X n |n) â H (X).
n

(14.28)

Proof: Consider the lower bound. The allowed programs satisfy the preï¬x property, and thus their lengths satisfy the Kraft inequality. We assign
to each x n the length of the shortest program p such that U(p, n) = x n .
These shortest programs also satisfy the Kraft inequality. We know from
the theory of source coding that the expected codeword length must be
greater than the entropy. Hence,

f (x n )K(x n |n) â¥ H (X1 , X2 , . . . , Xn ) = nH (X).
(14.29)
xn

474

KOLMOGOROV COMPLEXITY

We ï¬rst prove the upper bound when X is binary (i.e., X1 , X2 , . . . , Xn
are i.i.d. â¼ Bernoulli(Î¸ )). Using the method of Theorem 14.2.5, we can
bound the complexity of a binary string by

K(x1 x2 . . . xn |n) â¤ nH0

1
xi
n
n


+

i=1

1
log n + c.
2

(14.30)

Hence,

EK(X1 X2 . . . Xn |n) â¤ nEH0
( a)

â¤ nH0



1
Xi
n
n

i=1

1
n

n

i=1

= nH0 (Î¸ ) +

EXi


+

1
log n + c (14.31)
2

+

1
log n + c (14.32)
2



1
log n + c,
2

(14.33)

where (a) follows from Jensenâs inequality and the concavity of the
entropy. Thus, we have proved the upper bound in the theorem for binary
processes.
We can use the same technique for the case of a nonbinary ï¬nite alphabet. We ï¬rst describe the type of the sequence (the empirical frequency
of occurrence of each of the alphabet symbols as deï¬ned in Section 11.1)
using (|X| â 1) log n bits (the frequency of the last symbol can be calculated from the frequencies of the rest). Then we describe the index
of the sequence within the set of all sequences having the same type.
The type class has less than 2nH (Px n ) elements (where Px n is the type
of the sequence x n ) as shown in Chapter 11, and therefore the two-stage
description of a string x n has length
K(x n |n) â¤ nH (Px n ) + (|X| â 1) log n + c.

(14.34)

Again, taking the expectation and applying Jensenâs inequality as in the
binary case, we obtain
EK(X n |n) â¤ nH (X) + (|X| â 1) log n + c.

(14.35)

Dividing this by n yields the upper bound of the theorem.



14.4 KOLMOGOROV COMPLEXITY OF INTEGERS

475

Removing the conditioning on the length of the sequence is straightforward. By similar arguments, we can show that
H (X) â¤

1
(|X| + 1) log n c
+
f (x n )K(x n ) â¤ H (X) +
n xn
n
n

(14.36)

for all n. The lower bound follows from the fact that K(x n ) is also a
preï¬x-free code for the source, and the upper bound can be derived from
the fact that K(x n ) â¤ K(x n |n) + 2 log n + c. Thus,
1
E K(X n ) â H (X),
n

(14.37)

and the compressibility achieved by the computer goes to the entropy
limit.
14.4

KOLMOGOROV COMPLEXITY OF INTEGERS

In Section 14.3 we deï¬ned the Kolmogorov complexity of a binary string
as the length of the shortest program for a universal computer that prints
out that string. We can extend that deï¬nition to deï¬ne the Kolmogorov
complexity of an integer to be the Kolmogorov complexity of the corresponding binary string.
Deï¬nition The Kolmogorov complexity of an integer n is deï¬ned as
K(n) =

min

p: U (p)=n

l(p).

(14.38)

The properties of the Kolmogorov complexity of integers are very similar to those of the Kolmogorov complexity of bit strings. The following
properties are immediate consequences of the corresponding properties
for strings.
Theorem 14.4.1

For universal computers A and U,
KU (n) â¤ KA (n) + cA .

(14.39)

Also, since any number can be speciï¬ed by its binary expansion, we
have the following theorem.
Theorem 14.4.2
K(n) â¤ logâ n + c.

(14.40)

476

KOLMOGOROV COMPLEXITY

Theorem 14.4.3
K(n) > log n.

There are an inï¬nite number of integers n such that

Proof: We know from Lemma 14.3.1 that

2âK(n) â¤ 1

(14.41)

n

and


2â log n =

1

n

n

n

= â.

(14.42)

But if K(n) < log n for all n > n0 , then
â

n=n0

2âK(n) >

â


2â log n = â,

(14.43)

n=n0



which is a contradiction.
14.5 ALGORITHMICALLY RANDOM AND INCOMPRESSIBLE
SEQUENCES

From the examples in Section 14.2, it is clear that there are some long
sequences that are simple to describe, like the ï¬rst million bits of Ï . By
the same token, there are also large integers that are simple to describe,
such as
2
22

22
22

or (100!)!.
We now show that although there are some simple sequences, most
sequences do not have simple descriptions. Similarly, most integers are
not simple. Hence, if we draw a sequence at random, we are likely to
draw a complex sequence. The next theorem shows that the probability
that a sequence can be compressed by more than k bits is no greater than
2âk .
Theorem 14.5.1
( 12 ) process. Then

Let X1 , X2 , . . . , Xn be drawn according to a Bernoulli

P (K(X1 X2 . . . Xn |n) < n â k) < 2âk .

(14.44)

14.5 ALGORITHMICALLY RANDOM AND INCOMPRESSIBLE SEQUENCES

477

Proof:
P (K(X1 X2 . . . Xn |n) < n â k)

p(x1 , x2 , . . . , xn )
=
x1 x2 ...xn : K(x1 x2 ...xn |n)<nâk

(14.45)



=

2ân

(14.46)

x1 x2 ...xn : K(x1 x2 ...xn |n)<nâk

= |{x1 x2 . . . xn : K(x1 x2 . . . xn |n) < n â k}| 2ân
< 2nâk 2ân

(by Theorem 14.2.4)

(14.47)

= 2âk .

(14.48)

Thus, most sequences have a complexity close to their length. For
example, the fraction of sequences of length n that have complexity less
than n â 5 is less than 1/32. This motivates the following deï¬nition:
Deï¬nition A sequence x1 , x2 , . . . , xn is said to be algorithmically random if
K(x1 x2 . . . xn |n) â¥ n.

(14.49)

Note that by the counting argument, there exists, for each n, at least
one sequence x n such that
K(x n |n) â¥ n.

(14.50)

Deï¬nition We call an inï¬nite string x incompressible if
K(x1 x2 x3 Â· Â· Â· xn |n)
= 1.
nââ
n
lim

(14.51)

Theorem 14.5.2 (Strong law of large numbers for incompressible sequences) If a string x1 x2 . . . is incompressible, it satisï¬es the law of large
numbers in the sense that
1
1
xi â .
n
2
n

(14.52)

i=1

Hence the proportions of 0âs and 1âs in any incompressible string are
almost equal.

478

KOLMOGOROV COMPLEXITY


Proof: Let Î¸n = n1 ni=1 xi denote the proportion of 1âs in x1 x2 . . .
xn . Then using the method of Example 14.2, one can write a program
of length nH0 (Î¸n ) + 2 log(nÎ¸n ) + c to print x n . Thus,
log n c
K(x n |n)
< H0 (Î¸n ) + 2
+ .
n
n
n

(14.53)

By the incompressibility assumption, we also have the lower bound for
large enough n,
1â â¤

K(x n |n)
log n c
â¤ H0 (Î¸n ) + 2
+ .
n
n
n

(14.54)

Thus,
H0 (Î¸n ) > 1 â

2 log n + c
â .
n

(14.55)

Inspection of the graph of H0 (p) (Figure 14.2) shows that Î¸n is close to
1
2 for large n. Speciï¬cally, the inequality above implies that

Î¸n â


1
1
â Î´n , + Î´n ,
2
2

(14.56)

1
H0(p^n)
0.9
0.8
0.7

H(p)

0.6
0.5
0.4
0.3

dn

0.2
0.1
0
0

0.1

0.2

0.3

0.4

0.5
p

0.6

0.7

FIGURE 14.2. H0 (p) vs. p.

0.8

0.9

1

14.5 ALGORITHMICALLY RANDOM AND INCOMPRESSIBLE SEQUENCES

where Î´n is chosen so that


1
2 log n + cn + c
â Î´n = 1 â
,
H0
2
n
which implies that Î´n â 0 as n â â. Thus,

1
n



xi â

479

(14.57)
1
2

as n â â. 

We have now proved that incompressible sequences look random in the
sense that the proportion of 0âs and 1âs are almost equal. In general, we
can show that if a sequence is incompressible, it will satisfy all computable
statistical tests for randomness. (Otherwise, identiï¬cation of the test that x
fails will reduce the descriptive complexity of x, yielding a contradiction.)
In this sense, the algorithmic test for randomness is the ultimate test,
including within it all other computable tests for randomness.
We now prove a related law of large numbers for the Kolmogorov
complexity of Bernoulli(Î¸ ) sequences. The Kolmogorov complexity of
a sequence of binary random variables drawn i.i.d. according to a
Bernoulli(Î¸ ) process is close to the entropy H0 (Î¸ ). In Theorem 14.3.1
we proved that the expected value of the Kolmogorov complexity of a random Bernoulli sequence converges to the entropy [i.e.,
E n1 K(X1 X2 . . . Xn |n) â H0 (Î¸ )]. Now we remove the expectation.
Theorem 14.5.3
Then

Let X1 , X2 , . . . , Xn be drawn i.i.d. â¼ Bernoulli(Î¸ ).

1
in probability.
(14.58)
K(X1 X2 . . . Xn |n) â H0 (Î¸ )
n

Proof: Let X n = n1
Xi be the proportion of 1âs in X1 , X2 , . . . , Xn .
Then using the method described in (14.23), we have
	 

K(X1 X2 . . . Xn |n) â¤ nH0 X n + 2 log n + c,

(14.59)

and since by the weak law of large numbers, X n â Î¸ in probability, we
have


1
K(X1 X2 . . . Xn |n) â H0 (Î¸ ) â¥  â 0.
(14.60)
Pr
n
Conversely, we can bound the number of sequences with complexity signiï¬cantly lower than the entropy. From the AEP, we can divide the set
of sequences into the typical set and the nontypical set. There are at

480

KOLMOGOROV COMPLEXITY

least (1 â )2n(H0 (Î¸ )â) sequences in the typical set. At most 2n(H0 (Î¸ )âc)
of these typical sequences can have a complexity less than n(H0 (Î¸ ) â c).
The probability that the complexity of the random sequence is less than
n(H0 (Î¸ ) â c) is
Pr(K(X n |n) < n(H0 (Î¸ ) â c))
n
(n)
n
â¤ Pr(X n â
/ A(n)
 ) + Pr(X â A , K(X |n) < n(H0 (Î¸ ) â c))

â¤+
p(x n )
(14.61)
(n)

x n âA ,K(x n |n)<n(H0 (Î¸ )âc)



â¤+

2ân(H0 (Î¸ )â)

(14.62)

(n)
x n âA ,K(x n |n)<n(H0 (Î¸ )âc)

â¤  + 2n(H0 (Î¸ )âc) 2ân(H0 (Î¸ )â)

(14.63)

=  + 2ân(câ) ,

(14.64)

which is arbitrarily small for appropriate choice of , n, and c. Hence with
high probability, the Kolmogorov complexity of the random sequence is
close to the entropy, and we have
K(X1 , X2 , . . . , Xn |n)
â H0 (Î¸ )
n
14.6

in probability.

(14.65)


UNIVERSAL PROBABILITY

Suppose that a computer is fed a random program. Imagine a monkey
sitting at a keyboard and typing the keys at random. Equivalently, feed a
series of fair coin ï¬ips into a universal Turing machine. In either case, most
strings will not make sense to the computer. If a person sits at a terminal
and types keys at random, he will probably get an error message (i.e., the
computer will print the null string and halts). But with a certain probability
she will hit on something that makes sense. The computer will then print
out something meaningful. Will this output sequence look random?
From our earlier discussions, it is clear that most sequences of length n
have complexity close to n. Since the probability of an input program p
is 2âl(p) , shorter programs are much more probable than longer ones; and
when they produce long strings, shorter programs do not produce random
strings; they produce strings with simply described structure.
The probability distribution on the output strings is far from uniform.
Under the computer-induced distribution, simple strings are more likely

14.6 UNIVERSAL PROBABILITY

481

than complicated strings of the same length. This motivates us to deï¬ne
a universal probability distribution on strings as follows:
Deï¬nition The universal probability of a string x is

PU (x) =
2âl(p) = Pr(U(p) = x),

(14.66)

p: U (p)=x

which is the probability that a program randomly drawn as a sequence of
fair coin ï¬ips p1 , p2 , . . . will print out the string x.
This probability is universal in many senses. We can consider it as the
probability of observing such a string in nature; the implicit belief is that
simpler strings are more likely than complicated strings. For example, if
we wish to describe the laws of physics, we might consider the simplest
string describing the laws as the most likely. This principle, known as
Occamâs Razor, has been a general principle guiding scientiï¬c research
for centuries: If there are many explanations consistent with the observed
data, choose the simplest. In our framework, Occamâs Razor is equivalent
to choosing the shortest program that produces a given string.
This probability mass function is called universal because of the following theorem.
Theorem 14.6.1

For every computer A,

PA (x)
PU (x) â¥ cA

(14.67)


depends only on U and
for every string x â {0, 1}â , where the constant cA
A.

Proof: From the discussion of Section 14.2, we recall that for every
program p  for A that prints x, there exists a program p for U of length
not more than l(p  ) + cA produced by preï¬xing a simulation program for
A. Hence,




2âl(p) â¥
2âl(p )âcA = cA
PA (x). (14.68)
PU (x) =
p: U (p)=x

p :A(p )=x



Any sequence drawn according to a computable probability mass function on binary strings can be considered to be produced by some computer
A acting on a random input (via the probability inverse transformation
acting on a random input). Hence, the universal probability distribution
includes a mixture of all computable probability distributions.

482

KOLMOGOROV COMPLEXITY

Remark (Bounded likelihood ratio). In particular, Theorem 14.6.1 guarantees that a likelihood ratio test of the hypothesis that X is drawn
according to PU versus the hypothesis that it is drawn according to
PA will have bounded likelihood ratio. If U and A are universal, the
PU (x)/PA (x) is bounded away from 0 and inï¬nity for all x. This is in
contrast to other simple hypothesis-testing problems (like Bernoulli(Î¸1 )
versus Bernoulli(Î¸2 )), where the likelihood ratio goes to 0 or â as the
sample size goes to inï¬nity. Apparently, PU , which is a mixture of all
computable distributions, can never be rejected completely as the true
distribution of any data drawn according to some computable probability
distribution. In that sense we cannot reject the possibility that the universe
is the output of monkeys typing at a computer. However, we can reject
the hypothesis that the universe is random (monkeys with no computer).
In Section 14.11 we prove that
PU (x) â 2âK(x) ,

(14.69)

thus showing that K(x) and log P 1(x) have equal status as universal algoU
rithmic complexity measures. This is especially interesting since log P 1(x)
U
is the ideal codeword length (the Shannon codeword length) with respect
to the universal probability distribution PU (x).
We conclude this section with an example of a monkey at a typewriter
vs. a monkey at a computer keyboard. If the monkey types at random
on a typewriter, the probability that it types out all the works of Shakespeare (assuming that the text is 1 million bits long) is 2â1,000,000 . If the
monkey sits at a computer terminal, however, the probability that it types
out Shakespeare is now 2âK(Shakespeare) â 2â250,000 , which although
extremely small is still exponentially more likely than when the monkey
sits at a dumb typewriter.
The example indicates that a random input to a computer is much more
likely to produce âinterestingâ outputs than a random input to a typewriter.
We all know that a computer is an intelligence ampliï¬er. Apparently, it
creates sense from nonsense as well.
14.7 THE HALTING PROBLEM AND THE NONCOMPUTABILITY
OF KOLMOGOROV COMPLEXITY
Consider the following paradoxical statement:
This statement is false.
This paradox is sometimes stated in a two-statement form:

14.7 KOLMOGOROV COMPLEXITY

483

The next statement is false.
The preceding statement is true.
These paradoxes are versions of what is called the Epimenides liar paradox, and it illustrates the pitfalls involved in self-reference. In 1931, GoÌdel
used this idea of self-reference to show that any interesting system of
mathematics is not complete; there are statements in the system that are
true but that cannot be proved within the system. To accomplish this, he
translated theorems and proofs into integers and constructed a statement
of the above form, which can therefore not be proved true or false.
The halting problem in computer science is very closely connected
with GoÌdelâs incompleteness theorem. In essence, it states that for any
computational model, there is no general algorithm to decide whether a
program will halt or not (go on forever). Note that it is not a statement
about any speciï¬c program. Quite clearly, there are many programs that
can easily be shown to halt or go on forever. The halting problem says
that we cannot answer this question for all programs. The reason for this
is again the idea of self-reference.
To a practical person, the halting problem may not be of any immediate
signiï¬cance, but it has great theoretical importance as the dividing line
between things that can be done on a computer (given unbounded memory
and time) and things that cannot be done at all (such as proving all true
statements in number theory). GoÌdelâs incompleteness theorem is one of
the most important mathematical results of the twentieth century, and its
consequences are still being explored. The halting problem is an essential
example of GoÌdelâs incompleteness theorem.
One of the consequences of the nonexistence of an algorithm for the
halting problem is the noncomputability of Kolmogorov complexity. The
only way to ï¬nd the shortest program in general is to try all short programs
and see which of them can do the job. However, at any time some of
the short programs may not have halted and there is no effective (ï¬nite
mechanical) way to tell whether or not they will halt and what they will
print out. Hence, there is no effective way to ï¬nd the shortest program to
print a given string.
The noncomputability of Kolmogorov complexity is an example of the
Berry paradox . The Berry paradox asks for the shortest number not nameable in under 10 words. A number like 1,101,121 cannot be a solution
since the deï¬ning expression itself is less than 10 words long. This illustrates the problems with the terms nameable and describable; they are
too powerful to be used without a strict meaning. If we restrict ourselves
to the meaning âcan be described for printing out on a computer,â we can
resolve Berryâs paradox by saying that the smallest number not describable

484

KOLMOGOROV COMPLEXITY

in less than 10 words exists but is not computable. This âdescriptionâ is
not a program for computing the number. E. F. Beckenbach pointed out
a similar problem in the classiï¬cation of numbers as dull or interesting;
the smallest dull number must be interesting.
As stated at the beginning of the chapter, one does not really anticipate
that practitioners will ï¬nd the shortest computer program for a given
string. The shortest program is not computable, although as more and more
programs are shown to produce the string, the estimates from above of
the Kolmogorov complexity converge to the true Kolmogorov complexity.
(The problem, of course, is that one may have found the shortest program
and never know that no shorter program exists.) Even though Kolmogorov
complexity is not computable, it provides a framework within which to
consider questions of randomness and inference.
14.8



In this section we introduce Chaitinâs mystical, magical number, , which
has some extremely interesting properties.
Deï¬nition
=


p: U (p)

2âl(p) .

(14.70)

halts

Note that  = Pr(U(p) halts), the probability that the given universal
computer halts when the input to the computer is a binary string drawn
according to a Bernoulli( 12 ) process.
Since the programs that halt are preï¬x-free, their lengths satisfy the
Kraft inequality, and hence the sum above is always between 0 and 1. Let
n = .Ï1 Ï2 Â· Â· Â· Ïn denote the ï¬rst n bits of . The properties of  are
as follows:
1.  is noncomputable. There is no effective (ï¬nite, mechanical) way
to check whether arbitrary programs halt (the halting problem), so
there is no effective way to compute .
2.  is a âphilosopherâs stoneâ. Knowing  to an accuracy of n
bits will enable us to decide the truth of any provable or ï¬nitely
refutable mathematical theorem that can be written in less than n
bits. Actually, all that this means is that given n bits of , there
is an effective procedure to decide the truth of n-bit theorems; the
procedure may take an arbitrarily long (but ï¬nite) time. Of course,
without knowing , it is not possible to check the truth or falsity of

14.8



485

every theorem by an effective procedure (GoÌdelâs incompleteness
theorem).
The basic idea of the procedure using n bits of  is simple: We
run all programs until the sum of the masses 2âl(p) contributed
by programs that halt equals or exceeds n = 0.Ï1 Ï2 Â· Â· Â· Ïn , the
truncated version of  that we are given. Then, since
 â n < 2ân ,

(14.71)

we know that the sum of all further contributions of the form 2âl(p)
to  from programs that halt must also be less than 2ân . This implies
that no program of length â¤ n that has not yet halted will ever halt,
which enables us to decide the halting or nonhalting of all programs
of length â¤ n.
To complete the proof, we must show that it is possible for a computer to run all possible programs in âparallelâ in such a way that
any program that halts will eventually be found to halt. First, list all
possible programs, starting with the null program, :
, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, . . . .

(14.72)

Then let the computer execute one clock cycle of  for the ï¬rst
cycle. In the next cycle, let the computer execute two clock cycles
of  and two clock cycles of the program 0. In the third cycle, let
it execute three clock cycles of each of the ï¬rst three programs, and
so on. In this way, the computer will eventually run all possible
programs and run them for longer and longer times, so that if any
program halts, it will eventually be discovered to halt. The computer keeps track of which program is being executed and the cycle
number so that it can produce a list of all the programs that halt.
Thus, we will ultimately know whether or not any program of less
than n bits will halt. This enables the computer to ï¬nd any proof
of the theorem or a counterexample to the theorem if the theorem
can be stated in less than n bits. Knowledge of  turns previously
unprovable theorems into provable theorems. Here  acts as an
oracle.
Although  seems magical in this respect, there are other numbers
that carry the same information. For example, if we take the list of
programs and construct a real number in which the ith bit indicates
whether program i halts, this number can also be used to decide
any ï¬nitely refutable question in mathematics. This number is very
dilute (in information content) because one needs approximately 2n

486

KOLMOGOROV COMPLEXITY

bits of this indicator function to decide whether or not an n-bit
program halts. Given 2n bits, one can tell immediately without any
computation whether or not any program of length less than n halts.
However,  is the most compact representation of this information
since it is algorithmically random and incompressible.
What are some of the questions that we can resolve using ?
Many of the interesting problems in number theory can be stated
as a search for a counterexample. For example, it is straightforward
to write a program that searches over the integers x, y, z, and n
and halts only if it ï¬nds a counterexample to Fermatâs last theorem,
which states that
x n + y n = zn

(14.73)

has no solution in integers for n â¥ 3. Another example is Goldbachâs
conjecture, which states that any even number is a sum of two
primes. Our program would search through all the even numbers
starting with 2, check all prime numbers less than it and ï¬nd a
decomposition as a sum of two primes. It will halt if it comes across
an even number that does not have such a decomposition. Knowing
whether this program halts is equivalent to knowing the truth of
Goldbachâs conjecture.
We can also design a program that searches through all proofs
and halts only when it ï¬nds a proof of the theorem required. This
program will eventually halt if the theorem has a ï¬nite proof. Hence
knowing n bits of , we can ï¬nd the truth or falsity of all theorems
that have a ï¬nite proof or are ï¬nitely refutable and which can be
stated in less than n bits.
3.  is algorithmically random.
Theorem 14.8.1  cannot be compressed by more than a constant;
that is, there exists a constant c such that
K(Ï1 Ï2 . . . Ïn ) â¥ n â c

for all n.

(14.74)

Proof: We know that if we are given n bits of , we can determine
whether or not any program of length â¤ n halts. Using K(Ï1 Ï2 Â· Â· Â·
Ïn ) bits, we can calculate n bits of , and then we can generate a list
of all programs of length â¤ n that halt, together with their corresponding
outputs. We ï¬nd the ï¬rst string x0 that is not on this list. The string x0
is then the shortest string with Kolmogorov complexity K(x0 ) > n. The

14.9 UNIVERSAL GAMBLING

487

complexity of this program to print x0 is K(n ) + c, which must be at
least as long as the shortest program for x0 . Consequently,
K(n ) + c â¥ K(x0 ) > n

(14.75)

for all n. Thus, K(Ï1 Ï2 Â· Â· Â· Ïn ) > n â c, and  cannot be compressed by

more than a constant.
14.9

UNIVERSAL GAMBLING

Suppose that a gambler is asked to gamble sequentially on sequences
x â {0, 1}â . He has no idea of the origin of the sequence. He is given
fair odds (2-for-1) on each bit. How should he gamble? If he knew the
distribution of the elements of the string, he might use proportional betting
because of its optimal growth-rate properties, as shown in Chapter 6. If he
believes that the string occurred naturally, it seems intuitive that simpler
strings are more likely than complex ones. Hence, if he were to extend
the idea of proportional betting, he might bet according to the universal
probability of the string. For reference, note that if the gambler knows the
string x in advance, he can increase his wealth by a factor of 2l(x) simply
by betting all his wealth each time on the next
 symbol of x. Let the wealth
S(x) associated with betting scheme b(x),
b(x) = 1, be given by
S(x) = 2l(x) b(x).

(14.76)

Suppose that the gambler bets b(x) = 2âK(x) on a string x. This betting
strategy can be called universal gambling. We note that the sum of the
bets



b(x) =
2âK(x) â¤
2âl(p) =  â¤ 1,
(14.77)
x

x

p:p halts

and he will not have used all his money. For simplicity, let us assume
that he throws the rest away. For example, the amount of wealth resulting
from a bet b(0110) on a sequence x = 0110 is 2l(x) b(x) = 24 b(0110) plus
the amount won on all bets b(0110 . . .) on sequences that extend x.
Then we have the following theorem:
Theorem 14.9.1 The logarithm of the wealth a gambler achieves on a
sequence using universal gambling plus the complexity of the sequence is
no smaller than the length of the sequence, or
log S(x) + K(x) â¥ l(x).

(14.78)

488

KOLMOGOROV COMPLEXITY

Remark This is the counterpart of the gambling conservation theorem
W â + H = log m from Chapter 6.
Proof: The proof follows directly from the universal gambling scheme,
b(x) = 2âK(x) , since

S(x) =
2l(x) b(x  ) â¥ 2l(x) 2âK(x) ,
(14.79)
x  x

where x   x means that x is a preï¬x of x  . Taking logarithms establishes

the theorem.
The result can be understood in many ways. For inï¬nite sequences x
with ï¬nite Kolmogorov complexity,
S(x1 x2 Â· Â· Â· xl ) â¥ 2lâK(x) = 2lâc

(14.80)

for all l. Since 2l is the most that can be won in l gambles at fair odds,
this scheme does asymptotically as well as the scheme based on knowing
the sequence in advance. For example, if x = Ï1 Ï2 Â· Â· Â· Ïn Â· Â· Â·, the digits
in the expansion of Ï , the wealth at time n will be Sn = S(x n ) â¥ 2nâc
for all n.
If the string is actually generated by a Bernoulli process with parameter
p, then
S(X1 . . . Xn ) â¥ 2nânH0 (Xn )â2 log nâc â 2n(1âH0 (p)â2

log n c
n ân)

,

(14.81)

which is the same to ï¬rst order as the rate achieved when the gambler
knows the distribution in advance, as in Chapter 6.
From the examples we see that the universal gambling scheme on a
random sequence does asymptotically as well as a scheme that uses prior
knowledge of the true distribution.
14.10

OCCAMâS RAZOR

In many areas of scientiï¬c research, it is important to choose among
various explanations of data observed. After choosing the explanation,
we wish to assign a conï¬dence level to the predictions that ensue from
the laws that have been deduced. For example, Laplace considered the
probability that the sun will rise again tomorrow given that it has risen
every day in recorded history. Laplaceâs solution was to assume that the
rising of the sun was a Bernoulli(Î¸ ) process with unknown parameter Î¸ .
He assumed that Î¸ was uniformly distributed on the unit interval. Using

14.10

OCCAMâS RAZOR

489

the observed data, he calculated the posterior probability that the sun will
rise again tomorrow and found that it was
P (Xn+1 = 1|Xn = 1, Xnâ1 = 1, . . . , X1 = 1)
P (Xn+1 = 1, Xn = 1, Xnâ1 = 1, . . . , X1 = 1)
P (Xn = 1, Xnâ1 = 1, . . . , X1 = 1)
 1 n+1
Î¸
dÎ¸
= 0 1
n
0 Î¸ dÎ¸
=

=

n+1
,
n+2

(14.82)
(14.83)

which he put forward as the probability that the sun will rise on day n + 1
given that it has risen on days 1 through n.
Using the ideas of Kolmogorov complexity and universal probability,
we can provide an alternative approach to the problem. Under the universal
probability, let us calculate the probability of seeing a 1 next after having
observed n 1âs in the sequence so far. The conditional probability that
the next symbol is a 1 is the ratio of the probability of all sequences
with initial segment 1n and next bit equal to 1 to the probability of all
sequences with initial segment 1n . The simplest programs carry most of
the probability; hence we can approximate the probability that the next
bit is a 1 with the probability of the program that says âPrint 1âs forever.â
Thus,

p(1n 1y) â p(1â ) = c > 0.
(14.84)
y

Estimating the probability that the next bit is 0 is more difï¬cult. Since any
program that prints 1n 0 . . . yields a description of n, its length should at
least be K(n), which for most n is about log n + O(log log n), and hence
ignoring second-order terms, we have


p(1n 0y) â p(1n 0) â 2â log n â

y

1
.
n

(14.85)

Hence, the conditional probability of observing a 0 next is
p(0|1n ) =

1
p(1n 0)
â
,
p(1n 0) + p(1â )
cn + 1

(14.86)

which is similar to the result p(0|1n ) = 1/(n + 1) derived by Laplace.

490

KOLMOGOROV COMPLEXITY

This type of argument is a special case of Occamâs Razor, a general
principle governing scientiï¬c research, weighting possible explanations by
their complexity. William of Occam said âNunquam ponenda est pluralitas
sine necesitateâ: Explanations should not be multiplied beyond necessity
[516]. In the end, we choose the simplest explanation that is consistent
with the data observed. For example, it is easier to accept the general
theory of relativity than it is to accept a correction factor of c/r 3 to the
gravitational law to explain the precession of the perihelion of Mercury,
since the general theory explains more with fewer assumptions than does
a âpatchedâ Newtonian theory.
14.11 KOLMOGOROV COMPLEXITY AND UNIVERSAL
PROBABILITY
We now prove an equivalence between Kolmogorov complexity and universal probability. We begin by repeating the basic deï¬nitions.
K(x) =
PU (x) =

min l(p)

p:U (p)=x



2âl(p) .

(14.87)
(14.88)

p:U (p)=x

Theorem 14.11.1 (Equivalence of K(x) and log P 1(x)) .)
U
a constant c, independent of x, such that
2âK(x) â¤ PU (x) â¤ c2âK(x)

There exists
(14.89)

for all strings x. Thus, the universal probability of a string x is determined
essentially by its Kolmogorov complexity.
Remark This implies that K(x) and log P 1(x) have equal status as uniU
versal complexity measures, since
K(x) â c â¤ log

1
â¤ K(x).
PU (x)

(14.90)

Recall that the complexity deï¬ned with respect to two different computers
KU and KU  are essentially equivalent complexity measures if |KU (x) â
KU  (x)| is bounded. Theorem 14.11.1 shows that KU (x) and log P 1(x) are
U
essentially equivalent complexity measures.
Notice the striking similarity between the relationship of K(x) and
log P 1(x) in Kolmogorov complexity and the relationship of H (X) and
U
1
log p(x)
in information theory. The ideal Shannon code length assignment

14.11 KOLMOGOROV COMPLEXITY AND UNIVERSAL PROBABILITY

491

1
l(x) = log p(x)
achieves an average description length H (X), while in
Kolmogorov complexity theory, the ideal description length log P 1(x) is
U
1
is the natural notion of descriptive
almost equal to K(X). Thus, log p(x)
complexity of x in algorithmic as well as probabilistic settings.
The upper bound in (14.90) is obvious from the deï¬nitions, but the
lower bound is more difï¬cult to prove. The result is very surprising, since
there are an inï¬nite number of programs that print x. From any program
it is possible to produce longer programs by padding the program with
irrelevant instructions. The theorem proves that although there are an
inï¬nite number of such programs, the universal probability is essentially
determined by the largest term, which is 2âK(x) . If PU (x) is large, K(x)
is small, and vice versa.
However, there is another way to look at the upper bound that makes
it less surprising. Consider any computable probability mass function on
strings p(x). Using this mass function, we can construct a ShannonâFano
code (Section 5.9) for the source and then describe each string by the
1
corresponding codeword, which will have length log p(x)
. Hence, for
any computable distribution, we can construct a description of a string
1
+ c bits, which is an upper bound on the
using not more than log p(x)
Kolmogorov complexity K(x). Even though PU (x) is not a computable
probability mass function, we are able to ï¬nesse the problem using the
rather involved tree construction procedure described below.

Proof: (of Theorem 14.11.1). The ï¬rst inequality is simple. Let p â be
the shortest program for x. Then
PU (x) =



â

2âl(p) â¥ 2âl(p ) = 2âK(x) ,

(14.91)

p:U (p)=x

as we wished to show.
We can rewrite the second inequality as
K(x) â¤ log

1
+ c.
PU (x)

(14.92)

Our objective in the proof is to ï¬nd a short program to describe the strings
that have high PU (x). An obvious idea is some kind of Huffman coding
based on PU (x), but PU (x) cannot be calculated effectively, hence a procedure using Huffman coding is not implementable on a computer. Similarly,
the process using the ShannonâFano code also cannot be implemented.
However, if we have the ShannonâFano code tree, we can reconstruct the

492

KOLMOGOROV COMPLEXITY

string by looking for the corresponding node in the tree. This is the basis
for the following tree construction procedure.
To overcome the problem of noncomputability of PU (x), we use a modiï¬ed approach, trying to construct a code tree directly. Unlike Huffman
coding, this approach is not optimal in terms of minimum expected codeword length. However, it is good enough for us to derive a code for which
each codeword for x has a length that is within a constant of log P 1(x) .
U
Before we get into the details of the proof, let us outline our approach.
We want to construct a code tree in such a way that strings with high
probability have low depth. Since we cannot calculate the probability of a
string, we do not know a priori the depth of the string on the tree. Instead,
we assign x successively to the nodes of the tree, assigning x to nodes
closer and closer to the root as our estimate of PU (x) improves. We want
the computer to be able to recreate the tree and use the lowest depth node
corresponding to the string x to reconstruct the string.
We now consider the set of programs and their corresponding outputs
{(p, x)}. We try to assign these pairs to the tree. But we immediately
come across a problemâthere are an inï¬nite number of programs for a
given string, and we do not have enough nodes of low depth. However,
as we shall show, if we trim the list of program-output pairs, we will be
able to deï¬ne a more manageable list that can be assigned to the tree.
Next, we demonstrate the existence of programs for x of length log P 1(x) .
U
Tree construction procedure: For the universal computer U, we simulate
all programs using the technique explained in Section 14.8. We list all
binary programs:
, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, . . . .

(14.93)

Then let the computer execute one clock cycle of  for the ï¬rst stage.
In the next stage, let the computer execute two clock cycles of  and
two clock cycles of the program 0. In the third stage, let the computer
execute three clock cycles of each of the ï¬rst three programs, and so on.
In this way, the computer will eventually run all possible programs and
run them for longer and longer times, so that if any program halts, it will
be discovered to halt eventually. We use this method to produce a list
of all programs that halt in the order in which they halt, together with
their associated outputs. For each program and its corresponding output,
(pk , xk ), we calculate nk , which is chosen so that it corresponds to the
current estimate of PU (x). Speciï¬cally,


1
,
(14.94)
nk = log
PÌU (xk )

14.11 KOLMOGOROV COMPLEXITY AND UNIVERSAL PROBABILITY

where

PÌU (xk ) =



2âl(pi ) .

493

(14.95)

(pi ,xi ):xi =xk ,iâ¤k

Note that PÌU (xk ) â PU (x) on the subsequence of times k such that xk = x.
We are now ready to construct a tree. As we add to the list of triplets,
(pk , xk , nk ), of programs that halt, we map some of them onto nodes of
a binary tree. For purposes of the construction, we must ensure that all
the ni âs corresponding to a particular xk are distinct. To ensure this, we
remove from the list all triplets that have the same x and n as a previous
triplet. This will ensure that there is at most one node at each level of the
tree that corresponds to a given x.
Let {(pi , xi , ni ) : i = 1, 2, 3, . . .} denote the new list. On the winnowed
list, we assign the triplet (pk , xk , nk ) to the ï¬rst available node at level
nk + 1. As soon as a node is assigned, all of its descendants become
unavailable for assignment. (This keeps the assignment preï¬x-free.)
We illustrate this by means of an example:
(p1 , x1 , n1 ) = (10111, 1110, 5), n1 = 5 because PU (x1 ) â¥ 2âl(p1 ) = 2â5
(p2 , x2 , n2 ) = (11, 10, 2),
n2 = 2 because PU (x2 ) â¥ 2âl(p2 ) = 2â2
(p3 , x3 , n3 ) = (0, 1110, 1),
n3 = 1 because PU (x3 ) â¥ 2âl(p3 ) + 2âl(p1 )
= 2â5 + 2â1
â¥ 2â1
(p4 , x4 , n4 ) = (1010, 1111, 4), n4 = 4 because PU (x4 ) â¥ 2âl(p4 ) = 2â4
(p5 , x5 , n5 ) = (101101, 1110, 1), n5 = 1 because PU (x5 ) â¥ 2â1 + 2â5 + 2â5
â¥ 2â1
(p6 , x6 , n6 ) = (100, 1, 3),
n6 = 3 because PU (x6 ) â¥ 2âl(p6 ) = 2â3 .
..
.
(14.96)

We note that the string x = (1110) appears in positions 1, 3 and 5 in
the list, but n3 = n5 . The estimate of the probability PÌU (1110) has not
jumped sufï¬ciently for (p5 , x5 , n5 ) to survive the cut. Thus the winnowed
list becomes
(p1 , x1 , n1 ) = (10111, 1110, 5),
(p2 , x2 , n2 ) = (11, 10, 2),
(p3 , x3 , n3 ) = (0, 1110, 1),
(p4 , x4 , n4 ) = (1010, 1111, 4),
(p5 , x5 , n5 ) = (100, 1, 3),
..
.

(14.97)

The assignment of the winnowed list to nodes of the tree is illustrated in
Figure 14.3.

494

KOLMOGOROV COMPLEXITY
(p1, x1, n1), x1 = 1110

(p4, x4, n4), x4 = 1111

(p5, x5, n5), x5 = 100

(p2, x2, n2), x2 = 10

(p3, x3, n3), x3 = 110

FIGURE 14.3. Assignment of nodes.

In the example, we are able to ï¬nd nodes at level nk + 1 to which
we can assign the triplets. Now we shall prove that there are always
enough nodes so that the assignment can be completed. We can perform
the assignment of triplets to nodes if and only if the Kraft inequality is
satisï¬ed.
We now drop the primes and deal only with the winnowed list illustrated
in (14.97). We start with the inï¬nite sum in the Kraft inequality and split
it according to the output strings:
â






xâ{0,1}â

k:xk =x

2â(nk +1) =

k=1

We then write the inner sum as


2â(nk +1) = 2â1
2ânk
k:xk =x

k:xk =x

2â(nk +1) .

(14.98)

(14.99)

14.11 KOLMOGOROV COMPLEXITY AND UNIVERSAL PROBABILITY

495

	


â¤ 2â1 2log PU (x) + 2log PU (x)â1 + 2log PU (x)â2 + Â· Â· Â·
(14.100)


1 1
= 2â1 2log PU (x) 1 + + + Â· Â· Â·
(14.101)
2 4
= 2â1 2log PU (x) 2

(14.102)

â¤ PU (x),

(14.103)

where (14.100) is true because there is at most one node at each level
that prints out a particular x. More precisely, the nk âs on the winnowed
list for a particular output string x are all different integers. Hence,

k

2â(nk +1) â¤

 
x k:xk =x

2â(nk +1) â¤



PU (x) â¤ 1,

(14.104)

x

and we can construct a tree with the nodes labeled by the triplets.
If we are given the tree constructed above, it is easy to identify a given
x by the path to the lowest depth node that prints x. Call this node pÌ.
(By construction, l(pÌ) â¤ log P 1(x) + 2.) To use this tree in a program
U
to print x, we specify pÌ and ask the computer to execute the foregoing
simulation of all programs. Then the computer will construct the tree as
described above and wait for the particular node pÌ to be assigned. Since
the computer executes the same construction as the sender, eventually the
node pÌ will be assigned. At this point, the computer will halt and print
out the x assigned to that node.
This is an effective (ï¬nite, mechanical) procedure for the computer to
reconstruct x. However, there is no effective procedure to ï¬nd the lowest
depth node corresponding to x. All that we have proved is that there is
an (inï¬nite) tree with a node corresponding to x at level log P 1(x)  + 1.
U
But this accomplishes our purpose.
With reference to the example, the description of x = 1110 is the path
to the node (p3 , x3 , n3 ) (i.e., 01), and the description of x = 1111 is the
path 00001. If we wish to describe the string 1110, we ask the computer
to perform the (simulation) tree construction until node 01 is assigned.
Then we ask the computer to execute the program corresponding to node
01 (i.e., p3 ). The output of this program is the desired string, x = 1110.
The length of the program to reconstruct x is essentially the length of
the description of the position of the lowest depth node pÌ corresponding

496

KOLMOGOROV COMPLEXITY

to x in the tree. The length of this program for x is l(pÌ) + c, where


1
l(pÌ) â¤ log
+ 1,
(14.105)
PU (x)
and hence the complexity of x satisï¬es


1
+ c.
K(x) â¤ log
PU (x)
14.12

(14.106)

KOLMOGOROV SUFFICIENT STATISTIC

Suppose that we are given a sample sequence from a Bernoulli(Î¸ ) process.
What are the regularities or deviations from randomness in this sequence?
One way to address the question is to ï¬nd the Kolmogorov complexity
K(x n |n), which we discover to be roughly nH0 (Î¸ ) + log n + c. Since,
for Î¸ 
= 12 , this is much less than n, we conclude that x n has structure
and is not randomly drawn Bernoulli( 12 ). But what is the structure? The
ï¬rst attempt to ï¬nd the structure is to investigate the shortest program p â
for x n . But the shortest description of p â is about as long as p â itself;
otherwise, we could further compress the description of x n , contradicting
the minimality of p â . So this attempt is fruitless.
A hint at a good approach comes from an examination of the way in
which p â describes x n . The program âThe sequence has k 1âs; of such
sequences, it is the ithâ is optimal to ï¬rst order for Bernoulli(Î¸ ) sequences.
We note that it is a two-stage description, and all of the structure of the
sequence is captured in the ï¬rst stage. Moreover, x n is maximally complex
given the ï¬rst stage of the description. The ï¬rst stage, the description
of

n
:
x
=
k}.
k, requires log(n + 1) bits and deï¬nes a	set
S
=
{x
â
{0,
1}
i


The second stage requires log |S| = log nk â nH0 (x n ) â nH0 (Î¸ ) bits and
reveals nothing extraordinary about x n .
We mimic this process for general sequences by looking for a simple
set S that contains x n . We then follow it with a brute-force description of
x n in S using log |S| bits. We begin with a deï¬nition of the smallest set
containing x n that is describable in no more than k bits.
Deï¬nition The Kolmogorov structure function Kk (x n |n) of a binary
string x â {0, 1}n is deï¬ned as
Kk (x n |n) =

min
log |S|.
p : l(p) â¤ k
U(p, n) = S
n
x â S â {0, 1}n

(14.107)

14.12

KOLMOGOROV SUFFICIENT STATISTIC

497

The set S is the smallest set that can be described with no more than
k bits and which includes x n . By U(p, n) = S, we mean that running the
program p with data n on the universal computer U will print out the
indicator function of the set S.
Deï¬nition For a given small constant c, let k â be the least k such that
Kk (x n |n) + k â¤ K(x n |n) + c.

(14.108)

Let S ââ be the corresponding set and let p ââ be the program that prints out
the indicator function of S ââ . Then we shall say that p ââ is a Kolmogorov
minimal sufï¬cient statistic for x n .
Consider the programs p â describing sets S â such that
Kk (x n |n) + k = K(x n |n).

(14.109)

All the programs p â are âsufï¬cient statisticsâ in that the complexity of
x n given S â is maximal. But the minimal sufï¬cient statistic is the shortest
âsufï¬cient statistic.â
The equality in the deï¬nition above is up to a large constant depending
on the computer U . Then k â corresponds to the least k for which the twostage description of x n is as good as the best single-stage description of
x n . The second stage of the description merely provides the index of x n
within the set S ââ ; this takes Kk (x n |n) bits if x n is conditionally maximally
complex given the set S ââ . Hence the set S ââ captures all the structure
within x n . The remaining description of x n within S ââ is essentially the
description of the randomness within the string. Hence S ââ or p ââ is called
the Kolmogorov sufï¬cient statistic for x n .
This is parallel to the deï¬nition of a sufï¬cient statistic in mathematical
statistics. A statistic T is said to be sufï¬cient for a parameter Î¸ if the
distribution of the sample given the sufï¬cient statistic is independent of
the parameter; that is,
Î¸ â T (X) â X

(14.110)

forms a Markov chain in that order. For the Kolmogorov sufï¬cient statistic,
the program p ââ is sufï¬cient for the âstructureâ of the string x n ; the
remainder of the description of x n is essentially independent of the âstructureâ of x n . In particular, x n is maximally complex given S ââ .
A typical graph of the structure function is illustrated in Figure 14.4.
When k = 0, the only set that can be described is the entire set {0, 1}n ,

498

KOLMOGOROV COMPLEXITY

Kk(x)

n

Slope = â1

k*

K(x)

k

FIGURE 14.4. Kolmogorov sufï¬cient statistic.

so that the corresponding log set size is n. As we increase k, the size of
the set drops rapidly until
k + Kk (x n |n) â K(x n |n).

(14.111)

After this, each additional bit of k reduces the set by half, and we proceed along the line of slope â1 until k = K(x n |n). For k â¥ K(x n |n), the
smallest set that can be described that includes x n is the singleton {x n },
and hence Kk (x n |n) = 0.
We will now illustrate the concept with a few examples.
1. Bernoulli(Î¸ ) sequence. Consider a sample of length n from a
Bernoulli sequence with an unknown parameter Î¸ . As	discussed
in


Example 14.2, we can describle this sequence with nH nk + 12 log n
bits using a two stage description where we describe k in the ï¬rst
stage (using log n bits) and then 	describe
the sequence within all
n

sequences with k ones (using log k bits). However, we can use an
even shorter ï¬rst stage description. Instead of describing k exactly,
we divide
of k into bins and describe k only to an accu the range
k nâk â
racy of n n n using 12 log n bits. Then we describe the actual

14.12

KOLMOGOROV SUFFICIENT STATISTIC

499

Kk(x)

n

Slope = â1

1
2 log

n

nH0(p) +

1
2 log

n

k

FIGURE 14.5. Kolmogorov sufï¬cient statistic for a Bernoulli sequence.

sequence among all sequences whose type is in the same
 binâas k.
n is
The size of the set of all sequences with l ones, l â k Â± nk nâk
n
	k 

nH n + o(n) by Stirlingâs formula, so the total description length
	 

is still nH nk + 12 log n + o(n), but the description length of the
Kolmogorov sufï¬cient statistics is k â â n1 log n.
2. Sample from a Markov chain. In the same vein as the preceding
example, consider a sample from a ï¬rst-order binary Markov chain.
In this case again, p ââ will correspond to describing the Markov type
of the sequence (the number of occurrences of 00âs, 01âs, 10âs, and
11âs in the sequence); this conveys all the structure in the sequence.
The remainder of the description will be the index of the sequence
in the set of all sequences of this Markov type. Hence, in this case,
k â â 2( 12 log n) = log n, corresponding to describing two elements
of the conditional joint type to appropriate accuracy. (The other
elements of the conditional joint type can be determined from these
two.)
3. Mona Lisa. Consider an image that consists of a gray circle on a
white background. The circle is not uniformly gray but Bernoulli
with parameter Î¸ . This is illustrated in Figure 14.6. In this case, the
best two-stage description is ï¬rst to describe the size and position of

500

KOLMOGOROV COMPLEXITY

FIGURE 14.6. Mona Lisa.

the circle and its average gray level and then to describe the index of
the circle among all the circles
with
â the same gray level. Assuming
â
an n-pixel image (of size n byâ n), there are about n + 1 possible
gray levels, and there are about ( n)3 distinguishable circles. Hence,
k â â 52 log n in this case.
14.13

MINIMUM DESCRIPTION LENGTH PRINCIPLE

A natural extension of Occamâs razor occurs when we need to describe
data drawn from an unknown distribution. Let X1 , X2 , . . . , Xn be drawn
i.i.d. according to probability mass function p(x). We assume that we
do not know p(x), but know that p(x) â P, a class of probability mass
functions. Given the data, we can estimate the probability mass function in
P that best ï¬ts the data. For simple classes P (e.g., if P has only ï¬nitely
many distributions), the problem is straightforward, and the maximum
likelihood procedure [i.e., ï¬nd pÌ â P that maximizes pÌ(X1 , X2 , . . . , Xn )]
works well. However, if the class P is rich enough, there is a problem
of overï¬tting the data. For example, if X1 , X2 , . . . , Xn are continuous
random variables, and if P is the set of all probability distributions, the
maximum likelihood estimator given X1 , X2 , . . . , Xn is a distribution that
places a single mass point of weight n1 at each observed value. Clearly, this
estimate is too closely tied to actual observed data and does not capture
any of the structure of the underlying distribution.
To get around this problem, various methods have been applied. In the
simplest case, the data are assumed to come from some parametric distribution (e.g., the normal distribution), and the parameters of the distribution
are estimated from the data. To validate this method, the data should be
tested to check whether the distribution âlooksâ normal, and if the data
pass the test, we could use this description of the data. A more general
procedure is to take the maximum likelihood estimate and smooth it out
to obtain a smooth density. With enough data, and appropriate smoothness

SUMMARY

501

conditions, it is possible to make good estimates of the original density.
This process is called kernel density estimation.
However, the theory of Kolmogorov complexity (or the Kolmogorov
sufï¬cient statistic) suggests a different procedure: Find the p â P that
minimizes
Lp (X1 , X2 , . . . , Xn ) = K(p) + log

1
.
p(X1 , X2 , . . . , Xn )

(14.112)

This is the length of a two-stage description of the data, where we ï¬rst
describe the distribution p and then, given the distribution, construct the
Shannon code and describe the data using log p(X1 ,X12 ,...,Xn ) bits. This procedure is a special case of what is termed the minimum description length
(MDL) principle: Given data and a choice of models, choose the model
such that the description of the model plus the conditional description of
the data is as short as possible.

SUMMARY
Deï¬nition. The Kolmogorov complexity K(x) of a string x is
K(x) =
K(x|l(x)) =

min

p: U (p)=x

l(p)

min

p: U (p,l(x))=x

l(p).

(14.113)
(14.114)

Universality of Kolmogorov complexity. There exists a universal
computer U such that for any other computer A,
KU (x) â¤ KA (x) + cA

(14.115)

for any string x, where the constant cA does not depend on x. If U and
A are universal, |KU (x) â KA (x)| â¤ c for all x.
Upper bound on Kolmogorov complexity
K(x|l(x)) â¤ l(x) + c
K(x) â¤ K(x|l(x)) + 2 log l(x) + c.

(14.116)
(14.117)

502

KOLMOGOROV COMPLEXITY

Kolmogorov complexity and entropy. If X1 , X2 , . . . are i.i.d. integervalued random variables with entropy H , there exists a constant c such
that for all n,
H â¤

1
log n c
EK(X n |n) â¤ H + |X|
+ .
n
n
n

(14.118)

Lower bound on Kolmogorov complexity. There are no more than
2k strings x with complexity K(x) < k. If X1 , X2 , . . . , Xn are drawn
according to a Bernoulli( 12 ) process,
Pr (K(X1 X2 . . . Xn |n) â¤ n â k) â¤ 2âk .

(14.119)

Deï¬nition A sequence x is said to be incompressible if
K(x1 x2 . . . xn |n)/n â 1.
Strong law of large numbers for incompressible sequences
1
1
K(x1 , x2 , . . . , xn )
â1â
xi â .
n
n
2
n

(14.120)

i=1

Deï¬nition The universal probability of a string x is

PU (x) =
2âl(p) = Pr( U(p) = x).

(14.121)

p: U (p)=x

Universality of PU (x). For every computer A,
PU (x) â¥ cA PA (x)

(14.122)

for every string x â {0, 1}â , where the constant cA depends only on U
and A.

Deï¬nition  = p: U (p) halts 2âl(p) = Pr( U(p) halts) is the probability that the computer halts when the input p to the computer is a
binary string drawn according to a Bernoulli( 12 ) process.
Properties of 
1.  is not computable.
2.  is a âphilosopherâs stoneâ.
3.  is algorithmically random (incompressible).

PROBLEMS



1
PU (x)

503



. There exists a constant c indeEquivalence of K(x) and log
pendent of x such that




log 1 â K(x) â¤ c
(14.123)


PU (x)
for all strings x. Thus, the universal probability of a string x is essentially determined by its Kolmogorov complexity.
Deï¬nition The Kolmogorov structure function Kk (x n |n) of a binary
string x n â {0, 1}n is deï¬ned as
Kk (x n |n) =

min
log |S|.
p : l(p) â¤ k
U(p, n) = S
xâS

(14.124)

Deï¬nition Let k â be the least k such that
Kkâ (x n |n) + k â = K(x n |n).

(14.125)

Let S ââ be the corresponding set and let p ââ be the program that prints
out the indicator function of S ââ . Then p ââ is the Kolmogorov minimal
sufï¬cient statistic for x.

PROBLEMS
Let x, y â {0, 1}â .

14.1

Kolmogorov complexity of two sequences.
Argue that K(x, y) â¤ K(x) + K(y) + c.

14.2

Complexity of the sum
(a) Argue that K(n) â¤ log n + 2 log log n + c.
(b) Argue that K(n1 + n2 ) â¤ K(n1 ) + K(n2 ) + c.
(c) Give an example in which n1 and n2 are complex but the sum
is relatively simple.

14.3

Images.
n2 bits.

Consider an n Ã n array x of 0âs and 1âs . Thus, x has

504

KOLMOGOROV COMPLEXITY

Find the Kolmogorov complexity K(x | n) (to ï¬rst order) if:
(a) x is a horizontal line.
(b) x is a square.
(c) x is the union of two lines, each line being vertical or horizontal.
14.4

Do computers reduce entropy? Feed a random program P into
an universal computer. What is the entropy of the corresponding
output? Speciï¬cally, let X = U(P ), where P is a Bernoulli( 12 )
sequence. Here the binary sequence X is either undeï¬ned or is
in {0, 1}â . Let H (X) be the Shannon entropy of X. Argue that
H (X) = â. Thus, although the computer turns nonsense into
sense, the output entropy is still inï¬nite.

14.5

Monkeys on a computer. Suppose that a random program is
typed into a computer. Give a rough estimate of the probability
that the computer prints the following sequence:
(a) 0n followed by any arbitrary sequence.
(b) Ï1 Ï2 . . . Ïn followed by any arbitrary sequence, where Ïi is
the ith bit in the expansion of Ï.
(c) 0n 1 followed by any arbitrary sequence.
(d) Ï1 Ï2 . . . Ïn followed by any arbitrary sequence.
(e) A proof of the four-color theorem.

14.6

Kolmogorov complexity and ternary programs. Suppose that the
input programs for a universal computer U are sequences in
{0, 1, 2}â (ternary inputs). Also, suppose that U prints ternary outputs. Let K(x|l(x)) = minU (p,l(x))=x l(p). Show that:
(a) K(x n |n) â¤ n + c.
(b) |x n â {0, 1}â : K(x n |n) < k| < 3k .

14.7

Law of large numbers. Using ternary inputs and outputs as in
Problem 14.14.6, outline an argument demonstrating that if a
sequence x is algorithmically random [i.e., if K(x|l(x)) â l(x)],
the proportion of 0âs, 1âs, and 2âs in x must each be near 13 . It
may be helpful to use Stirlingâs approximation n! â (n/e)n .

PROBLEMS

14.8

505

Image complexity. Consider two binary subsets A and B (of an
n Ã n grid): for example,

Find general upper and lower bounds, in terms of K(A|n) and
K(B|n), for:
(a) K(Ac |n).
(b) K(A âª B|n).
(c) K(A â© B|n).
14.9

14.10

Random program. Suppose that a random program (symbols
i.i.d. uniform over the symbol set) is fed into the nearest available
computer.
â To our surprise the ï¬rst n bits of the binary expansion
of 1/ 2 are printed out. Roughly what would you say the probability is that the next output
â bit will agree with the corresponding
bit in the expansion of 1/ 2 ?
Faceâvase illusion

(a) What is an upper bound on the complexity of a pattern on an
m Ã m grid that has mirror-image symmetry about a vertical
axis through the center of the grid and consists of horizontal
line segments?
(b) What is the complexity K if the image differs in one cell
from the pattern described above?
14.11

Kolmogorov complexity. Assume that n is very large and known.
Let all rectangles be parallel to the frame.

506

KOLMOGOROV COMPLEXITY

(a) What is the (maximal) Kolmogorov complexity of the union
of two rectangles on an n Ã n grid?

(b) What if the rectangles intersect at a corner?

(c) What if they have the same (unknown) shape?
(d) What if they have the same (unknown) area?
(e) What is the minimum Kolmogorov complexity of the union
of two rectangles? That is, what is the simplest union?
(f) What is the (maximal) Kolmogorov complexity over all images
(not necessarily rectangles) on an n Ã n grid?
14.12

Encrypted text. Suppose that English text x n is encrypted into
y n by a substitution cypher: a 1-to-1 reassignment of each of the
27 letters of the alphabet (AâZ, including the space character)
to itself. Suppose that the Kolmogorov complexity of the text
x n is K(x n ) = n4 . (This is about right for English text. Weâre
now assuming a 27-symbol programming language, instead of a
binary symbol-set for the programming language. So, the length
of the shortest program, using a 27-ary programming language,
that prints out a particular string of English text of length n, is
approximately n/4.)
(a) What is the Kolmogorov complexity of the encryption map?

HISTORICAL NOTES

507

(b) Estimate the Kolmogorov complexity of the encrypted text
yn.
(c) How high must n be before you would expect to be able to
decode y n ?
14.13

Kolmogorov complexity. Consider the Kolmogorov complexity
K(n) over the integers n. If a speciï¬c integer n1 has a low Kolmogorov complexity K(n1 ), by how much can the Kolmogorov
complexity K(n1 + k) for the integer n1 + k vary from K(n1 )?

14.14

Complexity of large numbers. Let A(n) be the set of positive
integers x for which a terminating program p of length less than
or equal to n bits exists that outputs x. Let B(n) be the complement of A(n) [i.e., B(n) is the set of integers x for which no
program of length less than or equal to n outputs x]. Let M(n)
be the maximum integer in A(n), and let S(n) be the minimum
integer in B(n). What is the Kolmogorov complexity K(M(n))
(approximately)? What is K(S(n)) (approximately)? Which is
larger (M(n) or S(n))? Give a reasonable lower bound on M(n)
and a reasonable upper bound on S(n).

HISTORICAL NOTES
The original ideas of Kolmogorov complexity were put forth independently and almost simultaneously by Kolmogorov [321, 322], Solomonoff
[504], and Chaitin [89]. These ideas were developed further by students of
Kolmogorov such as Martin-LoÌf [374], who deï¬ned the notion of algorithmically random sequences and algorithmic tests for randomness, and by
Levin and Zvonkin [353], who explored the ideas of universal probability
and its relationship to complexity. A series of papers by Chaitin [90]â[92]
developed the relationship between algorithmic complexity and mathematical proofs. C. P. Schnorr studied the universal notion of randomness and
related it to gambling in [466]â[468].
The concept of the Kolmogorov structure function was deï¬ned by Kolmogorov at a talk at the Tallin conference in 1973, but these results
were not published. Vâyugin pursues this in [549], where he shows that
there are some very strange sequences x n that reveal their structure arbitrarily slowly in the sense that Kk (x n |n) = n â k, k < K(x n |n). Zurek
[606]â[608] addresses the fundamental questions of Maxwellâs demon
and the second law of thermodynamics by establishing the physical consequences of Kolmogorov complexity.

508

KOLMOGOROV COMPLEXITY

Rissanenâs minimum description length (MDL) principle is very close
in spirit to the Kolmogorov sufï¬cient statistic. Rissanen [445, 446] ï¬nds a
low-complexity model that yields a high likelihood of the data. Barron and
Cover [32] argue that the density minimizing K(f ) + log  f1(Xi ) yields
consistent density estimation.
A nontechnical introduction to the various measures of complexity can
be found in a thought-provoking book by Pagels [412]. Additional references to work in the area can be found in a paper by Cover et al. [114] on
Kolmogorovâs contributions to information theory and algorithmic complexity. A comprehensive introduction to the ï¬eld, including applications
of the theory to analysis of algorithms and automata, may be found in the
book by Li and Vitanyi [354]. Additional coverage may be found in the
books by Chaitin [86, 93].

CHAPTER 15

NETWORK INFORMATION
THEORY

A system with many senders and receivers contains many new elements
in the communication problem: interference, cooperation, and feedback.
These are the issues that are the domain of network information theory.
The general problem is easy to state. Given many senders and receivers
and a channel transition matrix that describes the effects of the interference
and the noise in the network, decide whether or not the sources can be
transmitted over the channel. This problem involves distributed source
coding (data compression) as well as distributed communication (ï¬nding
the capacity region of the network). This general problem has not yet been
solved, so we consider various special cases in this chapter.
Examples of large communication networks include computer networks,
satellite networks, and the phone system. Even within a single computer,
there are various components that talk to each other. A complete theory
of network information would have wide implications for the design of
communication and computer networks.
Suppose that m stations wish to communicate with a common satellite
over a common channel, as shown in Figure 15.1. This is known as a
multiple-access channel. How do the various senders cooperate with each
other to send information to the receiver? What rates of communication are
achievable simultaneously? What limitations does interference among the
senders put on the total rate of communication? This is the best understood
multiuser channel, and the above questions have satisfying answers.
In contrast, we can reverse the network and consider one TV station
sending information to m TV receivers, as in Figure 15.2. How does
the sender encode information meant for different receivers in a common
signal? What are the rates at which information can be sent to the different
receivers? For this channel, the answers are known only in special cases.
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

509

510

NETWORK INFORMATION THEORY

FIGURE 15.1. Multiple-access channel.

FIGURE 15.2. Broadcast channel.

There are other channels, such as the relay channel (where there is
one source and one destination, but one or more intermediate senderâ
receiver pairs that act as relays to facilitate the communication between the
source and the destination), the interference channel (two senders and two
receivers with crosstalk), and the two-way channel (two senderâreceiver
pairs sending information to each other). For all these channels, we have
only some of the answers to questions about achievable communication
rates and the appropriate coding strategies.
All these channels can be considered special cases of a general communication network that consists of m nodes trying to communicate with
each other, as shown in Figure 15.3. At each instant of time, the ith node
sends a symbol xi that depends on the messages that it wants to send

NETWORK INFORMATION THEORY

511

(X1, Y1)

(Xm, Ym)
(X2, Y2)

FIGURE 15.3. Communication network.

and on past received symbols at the node. The simultaneous transmission of the symbols (x1 , x2 , . . . , xm ) results in random received symbols
(Y1 , Y2 , . . . , Ym ) drawn according to the conditional probability distribution p(y (1) , y (2) , . . . , y (m) |x (1) , x (2) , . . . , x (m) ). Here p(Â·|Â·) expresses the
effects of the noise and interference present in the network. If p(Â·|Â·) takes
on only the values 0 and 1, the network is deterministic.
Associated with some of the nodes in the network are stochastic data
sources, which are to be communicated to some of the other nodes in the
network. If the sources are independent, the messages sent by the nodes
are also independent. However, for full generality, we must allow the
sources to be dependent. How does one take advantage of the dependence
to reduce the amount of information transmitted? Given the probability
distribution of the sources and the channel transition function, can one
transmit these sources over the channel and recover the sources at the
destinations with the appropriate distortion?
We consider various special cases of network communication. We consider the problem of source coding when the channels are noiseless and
without interference. In such cases, the problem reduces to ï¬nding the set
of rates associated with each source such that the required sources can
be decoded at the destination with a low probability of error (or appropriate distortion). The simplest case for distributed source coding is the
SlepianâWolf source coding problem, where we have two sources that
must be encoded separately, but decoded together at a common node. We
consider extensions to this theory when only one of the two sources needs
to be recovered at the destination.
The theory of ï¬ow in networks has satisfying answers in such domains
as circuit theory and the ï¬ow of water in pipes. For example, for the
single-source single-sink network of pipes shown in Figure 15.4, the maximum ï¬ow from A to B can be computed easily from the FordâFulkerson
theorem . Assume that the edges have capacities Ci as shown. Clearly,

512

NETWORK INFORMATION THEORY

C1

C4

C3

A

C2

B

C5

C = min{C1 + C2, C2 + C3 + C4, C4 + C5, C1 + C5}

FIGURE 15.4. Network of water pipes.

the maximum ï¬ow across any cut set cannot be greater than the sum
of the capacities of the cut edges. Thus, minimizing the maximum ï¬ow
across cut sets yields an upper bound on the capacity of the network. The
FordâFulkerson theorem [214] shows that this capacity can be achieved.
The theory of information ï¬ow in networks does not have the same
simple answers as the theory of ï¬ow of water in pipes. Although we
prove an upper bound on the rate of information ï¬ow across any cut set,
these bounds are not achievable in general. However, it is gratifying that
some problems, such as the relay channel and the cascade channel, admit
a simple max-ï¬ow min-cut interpretation. Another subtle problem in the
search for a general theory is the absence of a sourceâchannel separation
theorem, which we touch on brieï¬y in Section 15.10. A complete theory
combining distributed source coding and network channel coding is still
a distant goal.
In the next section we consider Gaussian examples of some of the
basic channels of network information theory. The physically motivated
Gaussian channel lends itself to concrete and easily interpreted answers.
Later we prove some of the basic results about joint typicality that we use
to prove the theorems of multiuser information theory. We then consider
various problems in detail: the multiple-access channel, the coding of correlated sources (SlepianâWolf data compression), the broadcast channel,
the relay channel, the coding of a random variable with side information,
and the rate distortion problem with side information. We end with an
introduction to the general theory of information ï¬ow in networks. There
are a number of open problems in the area, and there does not yet exist a
comprehensive theory of information networks. Even if such a theory is
found, it may be too complex for easy implementation. But the theory will
be able to tell communication designers how close they are to optimality
and perhaps suggest some means of improving the communication rates.

15.1 GAUSSIAN MULTIPLE-USER CHANNELS

15.1

513

GAUSSIAN MULTIPLE-USER CHANNELS

Gaussian multiple-user channels illustrate some of the important features
of network information theory. The intuition gained in Chapter 9 on the
Gaussian channel should make this section a useful introduction. Here the
key ideas for establishing the capacity regions of the Gaussian multipleaccess, broadcast, relay, and two-way channels will be given without
proof. The proofs of the coding theorems for the discrete memoryless
counterparts to these theorems are given in later sections of the chapter.
The basic discrete-time additive white Gaussian noise channel with
input power P and noise variance N is modeled by
Yi = Xi + Zi ,

i = 1, 2, . . . ,

(15.1)

where the Zi are i.i.d. Gaussian random variables with mean 0 and variance N . The signal X = (X1 , X2 , . . . , Xn ) has a power constraint
1 2
Xi â¤ P .
n
n

(15.2)

i=1

The Shannon capacity C is obtained by maximizing I (X; Y ) over all
random variables X such that EX 2 â¤ P and is given (Chapter 9) by


P
1
bits per transmission.
(15.3)
C = log 1 +
2
N
In this chapter we restrict our attention to discrete-time memoryless channels; the results can be extended to continuous-time Gaussian channels.
15.1.1

Single-User Gaussian Channel

We ï¬rst review the single-user Gaussian channel studied in Chapter 9.
P
Here Y = X + Z. Choose a rate R < 12 log(1 + N
). Fix a good (2nR , n)
codebook of power P . Choose an index w in the set 2nR . Send the
wth codeword X(w) from the codebook generated above. The receiver
observes Y = X(w) + Z and then ï¬nds the index wÌ of the codeword
closest to Y. If n is sufï¬ciently large, the probability of error Pr(w = wÌ)
will be arbitrarily small. As can be seen from the deï¬nition of joint typicality, this minimum-distance decoding scheme is essentially equivalent
to ï¬nding the codeword in the codebook that is jointly typical with the
received vector Y.

514

15.1.2

NETWORK INFORMATION THEORY

Gaussian Multiple-Access Channel with m Users

We consider m transmitters, each with a power P . Let

Y =

m


Xi + Z.

(15.4)



1
P
= log 1 +
2
N

(15.5)

i=1

Let

C

P
N



denote the capacity of a single-user Gaussian channel with signal-to-noise
ratio P /N . The achievable rate region for the Gaussian channel takes on
the simple form given in the following equations:

Ri < C

Ri + Rj < C

Ri + Rj + Rk < C
..
.
m

i=1

Ri < C



P
N



2P
N
3P
N

(15.6)

(15.7)


mP
N

(15.8)
(15.9)


.

(15.10)

Note that when all the rates are the same, the last inequality dominates
the others.
Here we need m codebooks, the ith codebook having 2nRi codewords
of power P . Transmission is simple. Each of the independent transmitters
chooses an arbitrary codeword from its own codebook. The users send
these vectors simultaneously. The receiver sees these codewords added
together with the Gaussian noise Z.
Optimal decoding consists of looking for the m codewords, one from
each codebook, such that the vector sum is closest to Y in Euclidean
distance. If (R1 , R2 , . . . , Rm ) is in the capacity region given above, the
probability of error goes to 0 as n tends to inï¬nity.

15.1 GAUSSIAN MULTIPLE-USER CHANNELS

515

Remarks It is exciting to see in this problem that the sum of the rates
of the users C(mP /N ) goes to inï¬nity with m. Thus, in a cocktail party
with m celebrants of power P in the presence of ambient noise N , the
intended listener receives an unbounded amount of information as the
number of people grows to inï¬nity. A similar conclusion holds, of course,
for ground communications to a satellite. Apparently, the increasing interference as the number of senders m â â does not limit the total received
information.
It is also interesting to note that the optimal transmission scheme here
does not involve time-division multiplexing. In fact, each of the transmitters uses all of the bandwidth all of the time.
15.1.3

Gaussian Broadcast Channel

Here we assume that we have a sender of power P and two distant
receivers, one with Gaussian noise power N1 and the other with Gaussian
noise power N2 . Without loss of generality, assume that N1 < N2 . Thus,
receiver Y1 is less noisy than receiver Y2 . The model for the channel is
Y1 = X + Z1 and Y2 = X + Z2 , where Z1 and Z2 are arbitrarily correlated Gaussian random variables with variances N1 and N2 , respectively.
The sender wishes to send independent messages at rates R1 and R2 to
receivers Y1 and Y2 , respectively.
Fortunately, all scalar Gaussian broadcast channels belong to the class
of degraded broadcast channels discussed in Section 15.6.2. Specializing
that work, we ï¬nd that the capacity region of the Gaussian broadcast
channel is


Î±P
R1 < C
(15.11)
N1


(1 â Î±)P
,
(15.12)
R2 < C
Î±P + N2
where Î± may be arbitrarily chosen (0 â¤ Î± â¤ 1) to trade off rate R1 for
rate R2 as the transmitter wishes.
To encode the messages, the transmitter generates two codebooks, one
with power Î±P at rate R1 , and another codebook with power Î±P at rate
R2 , where R1 and R2 lie in the capacity region above. Then to send
an index w1 â {1, 2, . . . , 2nR1 } and w2 â {1, 2, . . . , 2nR2 } to Y1 and Y2 ,
respectively, the transmitter takes the codeword X(w1 ) from the ï¬rst codebook and codeword X(w2 ) from the second codebook and computes the
sum. He sends the sum over the channel.

516

NETWORK INFORMATION THEORY

The receivers must now decode the messages. First consider the bad
receiver Y2 . He merely looks through the second codebook to ï¬nd the closest codeword to the received vector Y2 . His effective signal-to-noise ratio is
Î±P /(Î±P + N2 ), since Y1 âs message acts as noise to Y2 . (This can be proved.)
The good receiver Y1 ï¬rst decodes Y2 âs codeword, which he can accomplish because of his lower noise N1 . He subtracts this codeword XÌ2 from
Y1 . He then looks for the codeword in the ï¬rst codebook closest to
Y1 â XÌ2 . The resulting probability of error can be made as low as desired.
A nice dividend of optimal encoding for degraded broadcast channels is
that the better receiver Y1 always knows the message intended for receiver
Y2 in addition to the message intended for himself.
15.1.4

Gaussian Relay Channel

For the relay channel, we have a sender X and an ultimate intended
receiver Y . Also present is the relay channel, intended solely to help the
receiver. The Gaussian relay channel (Figure 15.31 in Section 15.7) is
given by
Y1 = X + Z1 ,

(15.13)

Y = X + Z 1 + X1 + Z 2 ,

(15.14)

where Z1 and Z2 are independent zero-mean Gaussian random variables
with variance N1 and N2 , respectively. The encoding allowed by the relay
is the causal sequence
X1i = fi (Y11 , Y12 , . . . , Y1iâ1 ).

(15.15)

Sender X has power P and sender X1 has power P1 . The capacity is
â
 



P + P1 + 2 Î±P P1
Î±P
C = max min C
,C
,
(15.16)
0â¤Î±â¤1
N1 + N2
N1
where Î± = 1 â Î±. Note that if
P1
P
â¥
,
N2
N1

(15.17)

it can be seen that C = C(P /N1 ),which is achieved by Î± = 1. The channel
appears to be noise-free after the relay, and the capacity C(P /N1 ) from
X to the relay can be achieved. Thus, the rate C(P /(N1 + N2 )) without
the relay is increased by the presence of the relay to C(P /N1 ). For large

15.1 GAUSSIAN MULTIPLE-USER CHANNELS

517

N2 and for P1 /N2 â¥ P /N1 , we see that the increment in rate is from
C(P /(N1 + N2 )) â 0 to C(P /N1 ).
Let R1 < C(Î±P /N1 ). Two codebooks are needed. The ï¬rst codebook
has 2nR1 words of power Î±P . The second has 2nR0 codewords of power
Î±P . We shall use codewords from these codebooks successively to create the opportunity for cooperation by the relay. We start by sending a
codeword from the ï¬rst codebook. The relay now knows the index of
this codeword since R1 < C(Î±P /N1 ), but the intended receiver has a list
of possible codewords of size 2n(R1 âC(Î±P /(N1 +N2 ))) . This list calculation
involves a result on list codes.
In the next block, the transmitter and the relay wish to cooperate to
resolve the receiverâs uncertainty about the codeword sent previously that
is on the receiverâs list. Unfortunately, they cannot be sure what this list
is because they do not know the received signal Y . Thus, they randomly
partition the ï¬rst codebook into 2nR0 cells with an equal number of codewords in each cell. The relay, the receiver, and the transmitter agree on
this partition. The relay and the transmitter ï¬nd the cell of the partition
in which the codeword from the ï¬rst codebook lies and cooperatively
send the codeword from the second codebook with that index. That is, X
and X1 send the same designated codeword. The relay, of course, must
scale this codeword so that it meets his power constraint P1 . They now
transmit their codewords simultaneously. An important point to note here
is that the cooperative information sent by the relay and the transmitter
isâsent coherently.
So the power of the sum as seen by the receiver Y is
â
( Î±P + P1 )2 .
However, this does not exhaust what the transmitter does in the second
block. He also chooses a fresh codeword from the ï¬rst codebook, adds it
âon paperâ to the cooperative codeword from the second codebook, and
sends the sum over the channel.
The reception by the ultimate receiver Y in the second block involves
ï¬rst ï¬nding the cooperative index from the second codebook by looking
for the closest codeword in the second codebook. He subtracts the codeword from the received sequence and then calculates a list of indices of
size 2nR0 corresponding to all codewords of the ï¬rst codebook that might
have been sent in the second block.
Now it is time for the intended receiver to complete computing the
codeword from the ï¬rst codebook sent in the ï¬rst block. He takes his list
of possible codewords that might have been sent in the ï¬rst block and
intersects it with the cell of the partition that he has learned from the
cooperative relay transmission in the second block. The rates and powers
have been chosen so that it is highly probable that there is only one

518

NETWORK INFORMATION THEORY

codeword in the intersection. This is Y âs guess about the information sent
in the ï¬rst block.
We are now in steady state. In each new block, the transmitter and the
relay cooperate to resolve the list uncertainty from the previous block. In
addition, the transmitter superimposes some fresh information from his
ï¬rst codebook to this transmission from the second codebook and transmits the sum. The receiver is always one block behind, but for sufï¬ciently
many blocks, this does not affect his overall rate of reception.
15.1.5

Gaussian Interference Channel

The interference channel has two senders and two receivers. Sender 1
wishes to send information to receiver 1. He does not care what receiver
2 receives or understands; similarly with sender 2 and receiver 2. Each
channel interferes with the other. This channel is illustrated in Figure 15.5.
It is not quite a broadcast channel since there is only one intended receiver
for each sender, nor is it a multiple access channel because each receiver
is only interested in what is being sent by the corresponding transmitter.
For symmetric interference, we have
Y1 = X1 + aX2 + Z1

(15.18)

Y2 = X2 + aX1 + Z2 ,

(15.19)

where Z1 , Z2 are independent N(0, N) random variables. This channel has
not been solved in general even in the Gaussian case. But remarkably, in
the case of high interference, it can be shown that the capacity region of
this channel is the same as if there were no interference whatsoever.
To achieve this, generate two codebooks, each with power P and rate
C(P /N ). Each sender independently chooses a word from his book and
Z1 ~

(0,N )

X1

Y1
a
a

X2

Y2

Z2 ~

(0,N )

FIGURE 15.5. Gaussian interference channel.

15.1 GAUSSIAN MULTIPLE-USER CHANNELS

519

sends it. Now, if the interference a satisï¬es C(a 2 P /(P + N )) > C(P /N ),
the ï¬rst transmitter understands perfectly the index of the second transmitter. He ï¬nds it by the usual technique of looking for the closest codeword
to his received signal. Once he ï¬nds this signal, he subtracts it from
his waveform received. Now there is a clean channel between him and
his sender. He then searches the senderâs codebook to ï¬nd the closest
codeword and declares that codeword to be the one sent.
15.1.6

Gaussian Two-Way Channel

The two-way channel is very similar to the interference channel, with the
additional provision that sender 1 is attached to receiver 2 and sender 2
is attached to receiver 1, as shown in Figure 15.6. Hence, sender 1 can
use information from previous received symbols of receiver 2 to decide
what to send next. This channel introduces another fundamental aspect
of network information theory: namely, feedback. Feedback enables the
senders to use the partial information that each has about the otherâs
message to cooperate with each other.
The capacity region of the two-way channel is not known in general.
This channel was ï¬rst considered by Shannon [486], who derived upper
and lower bounds on the region (see Problem 15.15). For Gaussian channels, these two bounds coincide and the capacity region is known; in
fact, the Gaussian two-way channel decomposes into two independent
channels.
Let P1 and P2 be the powers of transmitters 1 and 2, respectively,
and let N1 and N2 be the noise variances of the two channels. Then
the rates R1 < C(P1 /N1 ) and R2 < C(P2 /N2 ) can be achieved by the
techniques described for the interference channel. In this case, we generate
two codebooks of rates R1 and R2 . Sender 1 sends a codeword from the
ï¬rst codebook. Receiver 2 receives the sum of the codewords sent by the

X1

X2
p(y1, y2|x1, x2)

Y1

Y2

FIGURE 15.6. Two-way channel.

520

NETWORK INFORMATION THEORY

two senders plus some noise. He simply subtracts out the codeword of
sender 2 and he has a clean channel from sender 1 (with only the noise of
variance N1 ). Hence, the two-way Gaussian channel decomposes into two
independent Gaussian channels. But this is not the case for the general
two-way channel; in general, there is a trade-off between the two senders
so that both of them cannot send at the optimal rate at the same time.
15.2

JOINTLY TYPICAL SEQUENCES

We have previewed the capacity results for networks by considering multiuser Gaussian channels. We begin a more detailed analysis in this section,
where we extend the joint AEP proved in Chapter 7 to a form that we
will use to prove the theorems of network information theory. The joint
AEP will enable us to calculate the probability of error for jointly typical
decoding for the various coding schemes considered in this chapter.
Let (X1 , X2 , . . . , Xk ) denote a ï¬nite collection of discrete random variables with some ï¬xed joint distribution, p(x (1) , x (2) , . . . , x (k) ), (x (1) , x (2) ,
. . . , x (k) ) â X1 Ã X2 Ã Â· Â· Â· Ã Xk . Let S denote an ordered subset of these
random variables and consider n independent copies of S. Thus,
Pr{S = s} =

n


Pr{Si = si },

s â Sn .

(15.20)

i=1

For example, if S = (Xj , Xl ), then


Pr{S = s} = Pr (Xj , Xl ) = (xj , xl )
=

n


p(xij , xil ).

(15.21)
(15.22)

i=1

To be explicit, we will sometimes use X(S) for S. By the law of large
numbers, for any subset S of random variables,
1
1
â log p(S1 , S2 , . . . , Sn ) = â
log p(Si ) â H (S),
n
n
n

(15.23)

i=1

where the convergence takes place with probability 1 for all 2k subsets,
S â {X (1) , X (2) , . . . , X (k) }.

15.2

JOINTLY TYPICAL SEQUENCES

521

Deï¬nition The set A(n)
of -typical n-sequences (x1 , x2 , . . . , xk ) is

deï¬ned by
(1)
(2)
(k)
A(n)
 (X , X , . . . , X )

= A(n)

	
	

	 1
	
	
= (x1 , x2 , . . . , xk ) : 	â log p(s) â H (S)		 < , âS â {X (1) , X (2) , . . . ,
n

(15.24)
X (k) } .
(n)
Let A(n)
 (S) denote the restriction of A to the coordinates of S. Thus,
if S = (X1 , X2 ), we have

A(n)
 (X1 , X2 ) = {(x1 , x2 ) :
	
	
	
	 1
	â log p(x1 , x2 ) â H (X1 , X2 )	 < ,
	
	 n
	
	
	 1
	
	â log p(x1 ) â H (X1 )	 < ,
	 n
	
	
	
	
	 1
	â log p(x2 ) â H (X2 )	 < }.
	
	 n

(15.25)

.

Deï¬nition We will use the notation an = 2n(bÂ±) to mean that
	
	
	1
	
	 log an â b	 < 
	n
	

(15.26)

for n sufï¬ciently large.
Theorem 15.2.1

For any  > 0, for sufï¬ciently large n,

1. P (A(n)
 (S)) â¥ 1 â ,

âS â {X (1) , X (2) , . . . , X (k) }.
.

n(H (S)Â±)
.
2. s â A(n)
 (S) â p(s) = 2
.

n(H (S)Â±2)
.
3. |A(n)
 (S)| = 2

(15.27)
(15.28)
(15.29)

522

NETWORK INFORMATION THEORY

4. Let S1 , S2 â {X (1) , X (2) , . . . , X (k) }. If (s1 , s2 ) â A(n)
 (S1 , S2 ), then
p(s1 |s2 ) = 2ân(H (S1 |S2 )Â±2) .
.

(15.30)

Proof
1. This follows from the law of large numbers for the random variables
in the deï¬nition of A(n)
 (S).
2. This follows directly from the deï¬nition of A(n)
 (S).
3. This follows from


1â¥

p(s)

(15.31)

2ân(H (S)+)

(15.32)

(n)
sâA (S)



â¥

(n)

sâA (S)
ân(H (S)+)
.
= |A(n)
 (S)|2

(15.33)

If n is sufï¬ciently large, we can argue that
1â â¤



p(s)

(15.34)

2ân(H (S)â)

(15.35)

(n)
sâA (S)

â¤



(n)
sâA (S)

ân(H (S)â)
.
= |A(n)
 (S)|2

(15.36)
.

n(H (S)Â±2) for
Combining (15.33) and (15.36), we have |A(n)
 (S)| = 2
sufï¬ciently large n.
.
ân(H (S1 )Â±) and
4. For (s1 , s2 ) â A(n)
 (S1 , S2 ), we have p(s1 ) = 2
.
p(s1 , s2 ) = 2ân(H (S1 ,S2 )Â±) . Hence,

p(s2 |s1 ) =

p(s1 , s2 ) . ân(H (S2 |S1 )Â±2)
=2
.
p(s1 )



(15.37)

The next theorem bounds the number of conditionally typical sequences
for a given typical sequence.

15.2

JOINTLY TYPICAL SEQUENCES

523

Theorem 15.2.2 Let S1 , S2 be two subsets of X (1) , X (2) , . . . , X (k) . For
any  > 0, deï¬ne A(n)
 (S1 |s2 ) to be the set of s1 sequences that are jointly typical with a particular s2 sequence. If s2 â A(n)
 (S2 ), then for sufï¬ciently
large n, we have
n(H (S1 |S2 )+2)
|A(n)
 (S1 |s2 )| â¤ 2

(15.38)

and
(1 â )2n(H (S1 |S2 )â2) â¤



p(s2 )|A(n)
 (S1 |s2 )|.

(15.39)

s2

Proof: As in part 3 of Theorem 15.2.1, we have
1â¥



p(s1 |s2 )

(15.40)

2ân(H (S1 |S2 )+2)

(15.41)

(n)
s1 âA (S1 |s2 )

â¥



(n)
s1 âA (S1 |s2 )

ân(H (S1 |S2 )+2)
.
= |A(n)
 (S1 |s2 )|2

(15.42)

If n is sufï¬ciently large, we can argue from (15.27) that
1â â¤



p(s2 )

s2

â¤


s2

=





p(s1 |s2 )

(15.43)

2ân(H (S1 |S2 )â2)

(15.44)

(n)
s1 âA (S1 |s2 )

p(s2 )



(n)
s1 âA (S1 |s2 )

ân(H (S1 |S2 )â2)
p(s2 )|A(n)
. 
 (S1 |s2 )|2

(15.45)

s2

To calculate the probability of decoding error, we need to know the
probability that conditionally independent sequences are jointly typical.
Let S1 , S2 , and S3 be three subsets of {X (1) , X (2) , . . . , X (k) }. If S1
 and
S2
 are conditionally independent given S3
 but otherwise share the same
pairwise marginals of (S1 , S2 , S3 ), we have the following probability of
joint typicality.

524

NETWORK INFORMATION THEORY

Theorem 15.2.3 Let A(n)
 denote the typical set for the probability mass
function p(s1 , s2 , s3 ), and let
P (S
1 = s1 , S
2 = s2 , S
3 = s3 ) =

n


p(s1i |s3i )p(s2i |s3i )p(s3i ).

(15.46)

i=1

Then
n(I (S1 ;S2 |S3 )Â±6)
P {(S
1 , S
2 , S
3 ) â A(n)
.
 } = 2
.

(15.47)

.

Proof: We use the = notation from (15.26) to avoid calculating the
upper and lower bounds separately. We have
P {(S
1 , S
2 , S
3 ) â A(n)
 }

p(s3 )p(s1 |s3 )p(s2 |s3 )
=

(15.48)

(n)
(s1 , s2 , s3 )âA

ân(H (S3 )Â±) ân(H (S1 |S3 )Â±2) ân(H (S2 |S3 )Â±2)
= |A(n)
2
2
(15.49)
 (S1 , S2 , S3 )|2
.

= 2n(H (S1 ,S2 ,S3 )Â±) 2ân(H (S3 )Â±) 2ân(H (S1 |S3 )Â±2) 2ân(H (S2 |S3 )Â±2)

(15.50)

= 2ân(I (S1 ;S2 |S3 )Â±6) . 

(15.51)

.
.

We will specialize this theorem to particular choices of S1 , S2 , and S3
for the various achievability proofs in this chapter.

15.3

MULTIPLE-ACCESS CHANNEL

The ï¬rst channel that we examine in detail is the multiple-access channel,
in which two (or more) senders send information to a common receiver.
The channel is illustrated in Figure 15.7. A common example of this
channel is a satellite receiver with many independent ground stations, or
a set of cell phones communicating with a base station. We see that the
senders must contend not only with the receiver noise but with interference
from each other as well.
Deï¬nition A discrete memoryless multiple-access channel consists of
three alphabets, X1 , X2 , and Y, and a probability transition matrix
p(y|x1 , x2 ).

525

15.3 MULTIPLE-ACCESS CHANNEL

W1

X1
p(y | x1, x2)

W2

Y

^

^

(W1, W2)

Y1

FIGURE 15.7. Multiple-access channel.

Deï¬nition A ((2nR1 , 2nR2 ), n) code for the multiple-access channel consists of two sets of integers W1 = {1, 2, . . . , 2nR1 } and W2 = {1, 2, . . . ,
2nR2 }, called the message sets, two encoding functions,
X1 : W1 â Xn1

(15.52)

X2 : W2 â Xn2 ,

(15.53)

g : Y n â W1 Ã W2 .

(15.54)

and

and a decoding function,

There are two senders and one receiver for this channel. Sender 1
chooses an index W1 uniformly from the set {1, 2, . . . , 2nR1 } and sends
the corresponding codeword over the channel. Sender 2 does likewise.
Assuming that the distribution of messages over the product set W1 Ã W2
is uniform (i.e., the messages are independent and equally likely), we
deï¬ne the average probability of error for the ((2nR1 , 2nR2 ), n) code as
follows:



1
Pr g(Y n ) = (w1 , w2 )|(w1 , w2 ) sent .
Pe(n) = n(R +R )
2 1 2
(w1 ,w2 )âW1 ÃW2

(15.55)
Deï¬nition A rate pair (R1 , R2 ) is said to be achievable for the multipleaccess channel if there exists a sequence of ((2nR1 , 2nR2 ), n) codes with
Pe(n) â 0.

526

NETWORK INFORMATION THEORY

Deï¬nition The capacity region of the multiple-access channel is the
closure of the set of achievable (R1 , R2 ) rate pairs.
An example of the capacity region for a multiple-access channel is
illustrated in Figure 15.8. We ï¬rst state the capacity region in the form of
a theorem.
Theorem 15.3.1 (Multiple-access channel capacity) The capacity of
a multiple-access channel (X1 Ã X2 , p(y|x1 , x2 ), Y) is the closure of the
convex hull of all (R1 , R2 ) satisfying
R1 < I (X1 ; Y |X2 ),

(15.56)

R2 < I (X2 ; Y |X1 ),

(15.57)

R1 + R2 < I (X1 , X2 ; Y )

(15.58)

for some product distribution p1 (x1 )p2 (x2 ) on X1 Ã X2 .
Before we prove that this is the capacity region of the multiple-access
channel, let us consider a few examples of multiple-access channels:
Example 15.3.1 (Independent binary symmetric channels) Assume
that we have two independent binary symmetric channels, one from sender
1 and the other from sender 2, as shown in Figure 15.9. In this case, it is
obvious from the results of Chapter 7 that we can send at rate 1 â H (p1 )
over the ï¬rst channel and at rate 1 â H (p2 ) over the second channel.
R2

C2

0

C1

R1

FIGURE 15.8. Capacity region for a multiple-access channel.

15.3 MULTIPLE-ACCESS CHANNEL

527

X2

Y

X2

FIGURE 15.9. Independent binary symmetric channels.

Since the channels are independent, there is no interference between the
senders. The capacity region in this case is shown in Figure 15.10.
Example 15.3.2 (Binary multiplier channel )
access channel with binary inputs and output
Y = X 1 X2 .

Consider a multiple(15.59)

Such a channel is called a binary multiplier channel. It is easy to see
that by setting X2 = 1, we can send at a rate of 1 bit per transmission
from sender 1 to the receiver. Similarly, setting X1 = 1, we can achieve
R2 = 1. Clearly, since the output is binary, the combined rates R1 + R2
of sender 1 and sender 2 cannot be more than 1 bit. By timesharing, we
can achieve any combination of rates such that R1 + R2 = 1. Hence the
capacity region is as shown in Figure 15.11.
Example 15.3.3 (Binary erasure multiple-access channel ) This
multiple-access channel has binary inputs, X1 = X2 = {0, 1}, and a
ternary output, Y = X1 + X2 . There is no ambiguity in (X1 , X2 ) if Y = 0
or Y = 2 is received; but Y = 1 can result from either (0,1) or (1,0).

528

NETWORK INFORMATION THEORY

R2

C2 = 1 â H(p2)

C1 = 1 â H(p1) R1

0

FIGURE 15.10. Capacity region for independent BSCs.

R2

C2 = 1

0

C1 = 1

R1

FIGURE 15.11. Capacity region for binary multiplier channel.

We now examine the achievable rates on the axes. Setting X2 = 0, we
can send at a rate of 1 bit per transmission from sender 1. Similarly, setting
X1 = 0, we can send at a rate R2 = 1. This gives us two extreme points of
the capacity region. Can we do better? Let us assume that R1 = 1, so that
the codewords of X1 must include all possible binary sequences; X1 would
look like a Bernoulli( 12 ) process. This acts like noise for the transmission
from X2 . For X2 , the channel looks like the channel in Figure 15.12.
This is the binary erasure channel of Chapter 7. Recalling the results, the

15.3 MULTIPLE-ACCESS CHANNEL
1
2

0

529

0

1
2
?

1
2

1

1

1
2

FIGURE 15.12. Equivalent single-user channel for user 2 of a binary erasure multipleaccess channel.

R2

C2 = 1

1
2

0

1
2

C1 = 1

R1

FIGURE 15.13. Capacity region for binary erasure multiple-access channel.

capacity of this channel is 12 bit per transmission. Hence when sending at
maximum rate 1 for sender 1, we can send an additional 12 bit from sender
2. Later, after deriving the capacity region, we can verify that these rates
are the best that can be achieved. The capacity region for a binary erasure
channel is illustrated in Figure 15.13.

530

NETWORK INFORMATION THEORY

15.3.1 Achievability of the Capacity Region for the
Multiple-Access Channel
We now prove the achievability of the rate region in Theorem 15.3.1;
the proof of the converse will be left until the next section. The proof
of achievability is very similar to the proof for the single-user channel.
We therefore only emphasize the points at which the proof differs from
the single-user case. We begin by proving the achievability of rate pairs
that satisfy (15.58) for some ï¬xed product distribution p(x1 )p(x2 ). In
Section 15.3.3 we extend this to prove that all points in the convex hull
of (15.58) are achievable.
Proof: (Achievability in Theorem 15.3.1). Fix p(x1 , x2 ) = p1 (x1 )p2 (x2 ).
Codebook generation: Generate 2nR1 independent codewords X1 (i),
i â
{1, 2, . . . , 2nR1 }, of length n, generating each element i.i.d.
â¼ ni=1 p1 (x1i ). Similarly, generate 2nR2 independent
codewords X2 (j ),
j â {1, 2, . . . , 2nR2 }, generating each element i.i.d. â¼ ni=1 p2 (x2i ). These
codewords form the codebook, which is revealed to the senders and the
receiver.
Encoding: To send index i, sender 1 sends the codeword X1 (i). Similarly, to send j , sender 2 sends X2 (j ).
Decoding: Let A(n)
 denote the set of typical (x1 , x2 , y) sequences. The
n
receiver Y chooses the pair (i, j ) such that
(x1 (i), x2 (j ), y) â A(n)


(15.60)

if such a pair (i, j ) exists and is unique; otherwise, an error is declared.
Analysis of the probability of error: By the symmetry of the random
code construction, the conditional probability of error does not depend on
which pair of indices is sent. Thus, the conditional probability of error
is the same as the unconditional probability of error. So, without loss of
generality, we can assume that (i, j ) = (1, 1) was sent.
We have an error if either the correct codewords are not typical with
the received sequence or there is a pair of incorrect codewords that are
typical with the received sequence. Deï¬ne the events
Eij = {(X1 (i), X2 (j ), Y) â A(n)
 }.

(15.61)

531

15.3 MULTIPLE-ACCESS CHANNEL

Then by the union of events bound,



c
âª(i,j )=(1,1)Eij
Pe(n) = P E11


c
â¤ P (E11
)+
P (Ei1 ) +
P (E1j )
+



i=1, j =1

(15.62)

i=1, j =1

P (Eij ),

(15.63)

i=1, j =1

where P is the conditional probability given that (1, 1) was sent. From the
c
AEP, P (E11
) â 0. By Theorems 15.2.1 and 15.2.3, for i = 1, we have
P (Ei1 ) = P ((X1 (i), X2 (1), Y) â A(n)
 )

=
p(x1 )p(x2 , y)

(15.64)
(15.65)

(n)
(x1 ,x2 ,y)âA

ân(H (X1 )â) ân(H (X2 ,Y )â)
â¤ |A(n)
2
 |2

(15.66)

â¤ 2ân(H (X1 )+H (X2 ,Y )âH (X1 ,X2 ,Y )â3)

(15.67)

= 2ân(I (X1 ;X2 ,Y )â3)

(15.68)

= 2ân(I (X1 ;Y |X2 )â3) ,

(15.69)

where the equivalence of (15.68) and (15.69) follows from the independence of X1 and X2 , and the consequent I (X1 ; X2 , Y ) = I (X1 ; X2 ) +
I (X1 ; Y |X2 ) = I (X1 ; Y |X2 ). Similarly, for j = 1,
P (E1j ) â¤ 2ân(I (X2 ;Y |X1 )â3) ,

(15.70)

P (Eij ) â¤ 2ân(I (X1 ,X2 ;Y )â4) .

(15.71)

and for i = 1, j = 1,

It follows that
c
) + 2nR1 2ân(I (X1 ;Y |X2 )â3) + 2nR2 2ân(I (X2 ;Y |X1 )â3)
Pe(n) â¤ P (E11

+2n(R1 +R2 ) 2ân(I (X1 ,X2 ;Y )â4) .

(15.72)

Since  > 0 is arbitrary, the conditions of the theorem imply that each
term tends to 0 as n â â. Thus, the probability of error, conditioned

532

NETWORK INFORMATION THEORY

on a particular codeword being sent, goes to zero if the conditions of the
theorem are met. The above bound shows that the average probability of
error, which by symmetry is equal to the probability for an individual
codeword, averaged over all choices of codebooks in the random code
construction, is arbitrarily small. Hence, there exists at least one code Câ
with arbitrarily small probability of error.
This completes the proof of achievability of the region in (15.58) for
a ï¬xed input distribution. Later, in Section 15.3.3, we show that timesharing allows any (R1 , R2 ) in the convex hull to be achieved, completing
the proof of the forward part of the theorem.

15.3.2 Comments on the Capacity Region for the
Multiple-Access Channel
We have now proved the achievability of the capacity region of the
multiple-access channel, which is the closure of the convex hull of the set
of points (R1 , R2 ) satisfying
R1 < I (X1 ; Y |X2 ),

(15.73)

R2 < I (X2 ; Y |X1 ),

(15.74)

R1 + R2 < I (X1 , X2 ; Y )

(15.75)

for some distribution p1 (x1 )p2 (x2 ) on X1 Ã X2 . For a particular
p1 (x1 )p2 (x2 ), the region is illustrated in Figure 15.14.
R2

I (X2;Y |X1)

D

C

I (X2;Y )

B

A
0

I(X1;Y )

I(X1;Y |X2)

R1

FIGURE 15.14. Achievable region of multiple-access channel for a ï¬xed input distribution.

15.3 MULTIPLE-ACCESS CHANNEL

533

Let us now interpret the corner points in the region. Point A corresponds
to the maximum rate achievable from sender 1 to the receiver when sender
2 is not sending any information. This is
max R1 =

I (X1 ; Y |X2 ).

(15.76)

p2 (x2 )I (X1 ; Y |X2 = x2 )

(15.77)

max

p1 (x1 )p2 (x2 )

Now for any distribution p1 (x1 )p2 (x2 ),
I (X1 ; Y |X2 ) =


x2

â¤ max I (X1 ; Y |X2 = x2 ),
x2

(15.78)

since the average is less than the maximum. Therefore, the maximum
in (15.76) is attained when we set X2 = x2 , where x2 is the value that
maximizes the conditional mutual information between X1 and Y . The
distribution of X1 is chosen to maximize this mutual information. Thus,
X2 must facilitate the transmission of X1 by setting X2 = x2 .
The point B corresponds to the maximum rate at which sender 2 can
send as long as sender 1 sends at his maximum rate. This is the rate that
is obtained if X1 is considered as noise for the channel from X2 to Y . In
this case, using the results from single-user channels, X2 can send at a
rate I (X2 ; Y ). The receiver now knows which X2 codeword was used and
can âsubtractâ its effect from the channel. We can consider the channel
now to be an indexed set of single-user channels, where the index is the
X2 symbol used. The X1 rate achieved in this case is the average mutual
information, where the average is over these channels, and each channel
occurs as many times as the corresponding X2 symbol appears in the
codewords. Hence, the rate achieved is


p(x2 )I (X1 ; Y |X2 = x2 ) = I (X1 ; Y |X2 ).

(15.79)

x2

Points C and D correspond to B and A, respectively, with the roles of the
senders reversed. The noncorner points can be achieved by time-sharing.
Thus, we have given a single-user interpretation and justiï¬cation for the
capacity region of a multiple-access channel.
The idea of considering other signals as part of the noise, decoding
one signal, and then âsubtractingâ it from the received signal is a very
useful one. We will come across the same concept again in the capacity
calculations for the degraded broadcast channel.

534

NETWORK INFORMATION THEORY

15.3.3 Convexity of the Capacity Region of the Multiple-Access
Channel
We now recast the capacity region of the multiple-access channel in order
to take into account the operation of taking the convex hull by introducing
a new random variable. We begin by proving that the capacity region is
convex.
Theorem 15.3.2 The capacity region C of a multiple-access channel
is convex [i.e., if (R1 , R2 ) â C and (R1
 , R2
 ) â C, then (Î»R1 + (1 â Î»)R1
 ,
Î»R2 + (1 â Î»)R2
 ) â C for 0 â¤ Î» â¤ 1].
Proof: The idea is time-sharing. Given two sequences of codes at different rates R = (R1 , R2 ) and R
 = (R1
 , R2
 ), we can construct a third
codebook at a rate Î»R + (1 â Î»)R
 by using the ï¬rst codebook for the
ï¬rst Î»n symbols and using the second codebook for the last (1 â Î»)n
symbols. The number of X1 codewords in the new code is




2nÎ»R1 2n(1âÎ»)R1 = 2n(Î»R1 +(1âÎ»)R1 ) ,

(15.80)

and hence the rate of the new code is Î»R + (1 â Î»)R
 . Since the overall
probability of error is less than the sum of the probabilities of error for
each of the segments, the probability of error of the new code goes to 0

and the rate is achievable.
We can now recast the statement of the capacity region for the multipleaccess channel using a time-sharing random variable Q. Before we prove
this result, we need to prove a property of convex sets deï¬ned by linear inequalities like those of the capacity region of the multiple-access
channel. In particular, we would like to show that the convex hull of two
such regions deï¬ned by linear constraints is the region deï¬ned by the
convex combination of the constraints. Initially, the equality of these two
sets seems obvious, but on closer examination, there is a subtle difï¬culty
due to the fact that some of the constraints might not be active. This is
best illustrated by an example. Consider the following two sets deï¬ned
by linear inequalities:
C1 = {(x, y) : x â¥ 0, y â¥ 0, x â¤ 10, y â¤ 10, x + y â¤ 100} (15.81)
C2 = {(x, y) : x â¥ 0, y â¥ 0, x â¤ 20, y â¤ 20, x + y â¤ 20}. (15.82)
In this case, the ( 12 , 12 ) convex combination of the constraints deï¬nes the
region
C = {(x, y) : x â¥ 0, y â¥ 0, x â¤ 15, y â¤ 15, x + y â¤ 60}.

(15.83)

15.3 MULTIPLE-ACCESS CHANNEL

535

It is not difï¬cult to see that any point in C1 or C2 has x + y < 20, so
any point in the convex hull of the union of C1 and C2 satisï¬es this
property. Thus, the point (15,15), which is in C, is not in the convex hull
of (C1 âª C2 ). This example also hints at the cause of the problemâin the
deï¬nition for C1 , the constraint x + y â¤ 100 is not active. If this constraint
were replaced by a constraint x + y â¤ a, where a â¤ 20, the above result
of the equality of the two regions would be true, as we now prove.
We restrict ourselves to the pentagonal regions that occur as components of the capacity region of a two-user multiple-access channel. In
this case, the capacity region for a ï¬xed p(x1 )p(x2 ) is deï¬ned by three
mutual informations, I (X1 ; Y |X2 ), I (X2 ; Y |X1 ), and I (X1 , X2 ; Y ), which
we shall call I1 , I2 , and I3 , respectively. For each p(x1 )p(x2 ), there is a
corresponding vector, I = (I1 , I2 , I3 ), and a rate region deï¬ned by
CI = {(R1 , R2 ) : R1 â¥ 0, R2 â¥ 0, R1 â¤ I1 , R2 â¤ I2 , R1 + R2 â¤ I3 }.
(15.84)
Also, since for any distribution p(x1 )p(x2 ), we have I (X2 ; Y |X1 ) =
H (X2 |X1 ) â H (X2 |Y, X1 ) = H (X2 ) â H (X2 |Y, X1 ) = I (X2 ; Y, X1 ) =
I (X2 ; Y ) + I (X2 ; X1 |Y ) â¥ I (X2 ; Y ), and therefore, I (X1 ; Y |X2 ) +
I (X2 ; Y |X1 ) â¥ I (X1 ; Y |X2 )+ I (X2 ; Y ) = I (X1 , X2 ; Y ), we have for all
vectors I that I1 + I2 â¥ I3 . This property will turn out to be critical for
the theorem.
Lemma 15.3.1 Let I1 , I2 â R3 be two vectors of mutual informations
that deï¬ne rate regions CI1 and CI2 , respectively, as given in (15.84).
For 0 â¤ Î» â¤ 1, deï¬ne IÎ» = Î»I1 + (1 â Î»)I2 , and let CIÎ» be the rate region
deï¬ned by IÎ» . Then
CIÎ» = Î»CI1 + (1 â Î»)CI2 .

(15.85)

Proof: We shall prove this theorem in two parts. We ï¬rst show that
any point in the (Î», 1 â Î») mix of the sets CI1 and CI2 satisï¬es the constraints IÎ» . But this is straightforward, since any point in CI1 satisï¬es the
inequalities for I1 and a point in CI2 satisï¬es the inequalities for I2 , so
the (Î», 1 â Î») mix of these points will satisfy the (Î», 1 â Î») mix of the
constraints. Thus, it follows that
Î»CI1 + (1 â Î»)CI2 â CIÎ» .

(15.86)

To prove the reverse inclusion, we consider the extreme points of the
pentagonal regions. It is not difï¬cult to see that the rate regions deï¬ned
in (15.84) are always in the form of a pentagon, or in the extreme case

536

NETWORK INFORMATION THEORY

when I3 = I1 + I2 , in the form of a rectangle. Thus, the capacity region
CI can be also deï¬ned as a convex hull of ï¬ve points:
(0, 0), (I1 , 0), (I1 , I3 â I1 ), (I3 â I2 , I2 ), (0, I2 ).

(15.87)

Consider the region deï¬ned by IÎ» ; it, too, is deï¬ned by ï¬ve points. Take
any one of the points, say (I3(Î») â I2(Î») , I2(Î») ). This point can be written as
the (Î», 1 â Î») mix of the points (I3(1) â I2(1) , I2(1) ) and (I3(2) â I2(2) , I2(2) ),
and therefore lies in the convex mixture of CI1 and CI2 . Thus, all extreme
points of the pentagon CIÎ» lie in the convex hull of CI1 and CI2 , or
CIÎ» â Î»CI1 + (1 â Î»)CI2 .
Combining the two parts, we have the theorem.

(15.88)


In the proof of the theorem, we have implicitly used the fact that all
the rate regions are deï¬ned by ï¬ve extreme points (at worst, some of
the points are equal). All ï¬ve points deï¬ned by the I vector were within
the rate region. If the condition I3 â¤ I1 + I2 is not satisï¬ed, some of the
points in (15.87) may be outside the rate region and the proof collapses.
As an immediate consequence of the above lemma, we have the following theorem:
Theorem 15.3.3 The convex hull of the union of the rate regions deï¬ned
by individual I vectors is equal to the rate region deï¬ned by the convex
hull of the I vectors.
These arguments on the equivalence of the convex hull operation on
the rate regions with the convex combinations of the mutual informations can be extended to the general m-user multiple-access channel. A
proof along these lines using the theory of polymatroids is developed in
Han [271].
Theorem 15.3.4 The set of achievable rates of a discrete memoryless
multiple-access channel is given by the closure of the set of all (R1 , R2 )
pairs satisfying
R1 < I (X1 ; Y |X2 , Q),
R2 < I (X2 ; Y |X1 , Q),
R1 + R2 < I (X1 , X2 ; Y |Q)

(15.89)

15.3 MULTIPLE-ACCESS CHANNEL

537

for some choice of the joint distribution p(q)p(x1 |q)p(x2 |q)p(y|x1 , x2 )
with |Q| â¤ 4.
Proof: We will show that every rate pair lying in the region deï¬ned in
(15.89) is achievable (i.e., it lies in the convex closure of the rate pairs
satisfying Theorem 15.3.1). We also show that every point in the convex
closure of the region in Theorem 15.3.1 is also in the region deï¬ned
in (15.89).
Consider a rate point R satisfying the inequalities (15.89) of the theorem. We can rewrite the right-hand side of the ï¬rst inequality as
I (X1 ; Y |X2 , Q) =

m


p(q)I (X1 ; Y |X2 , Q = q)

(15.90)

p(q)I (X1 ; Y |X2 )p1q ,p2q ,

(15.91)

q=1

=

m

q=1

where m is the cardinality of the support set of Q. We can expand the
other mutual informations similarly.
For simplicity in notation, we consider a rate pair as a vector and
denote a pair satisfying the inequalities in (15.58) for a speciï¬c input
product distribution p1q (x1 )p2q (x2 ) as Rp1 ,p2 as Rq . Speciï¬cally, let Rq =
(R1q , R2q ) be a rate pair satisfying
R1q < I (X1 ; Y |X2 )p1q (x1 )p2q (x2 ) ,

(15.92)

R2q < I (X2 ; Y |X1 )p1q (x1 )p2q (x2 ) ,

(15.93)

R1q + R2q < I (X1 , X2 ; Y )p1q (x1 )p2q (x2 ) .

(15.94)

Then by Theorem 15.3.1, Rq = (R1q , R2q ) is achievable. Then since R
satisï¬es (15.89) and we can expand the right-hand sides as in (15.91),
there exists a setof pairs Rq satisfying (15.94) such that
R=

m


p(q)Rq .

(15.95)

q=1

Since a convex combination of achievable rates is achievable, so is R.
Hence, we have proven the achievability of the region in the theorem.
The same argument can be used to show that every point in the convex
closure of the region in (15.58) can be written as the mixture of points
satisfying (15.94) and hence can be written in the form (15.89).

538

NETWORK INFORMATION THEORY

The converse is proved in the next section. The converse shows that
all achievable rate pairs are of the form (15.89), and hence establishes
that this is the capacity region of the multiple-access channel. The cardinality bound on the time-sharing random variable Q is a consequence of

CaratheÌodoryâs theorem on convex sets. See the discussion below.
The proof of the convexity of the capacity region shows that any convex
combination of achievable rate pairs is also achievable. We can continue
this process, taking convex combinations of more points. Do we need to
use an arbitrary number of points ? Will the capacity region be increased?
The following theorem says no.
Theorem 15.3.5 (CaratheÌodory) Any point in the convex closure of a
compact set A in a d-dimensional Euclidean space can be represented as
a convex combination of d + 1 or fewer points in the original set A.
Proof: The proof may be found in Eggleston [183] and GruÌnbaum
[263].

This theorem allows us to restrict attention to a certain ï¬nite convex
combination when calculating the capacity region. This is an important
property because without it, we would not be able to compute the capacity
region in (15.89), since we would never know whether using a larger
alphabet Q would increase the region.
In the multiple-access channel, the bounds deï¬ne a connected compact
set in three dimensions. Therefore, all points in its closure can be deï¬ned
as the convex combination of at most four points. Hence, we can restrict
the cardinality of Q to at most 4 in the above deï¬nition of the capacity
region.
Remark Many of the cardinality bounds may be slightly improved by
introducing other considerations. For example, if we are only interested
in the boundary of the convex hull of A as we are in capacity theorems,
a point on the boundary can be expressed as a mixture of d points of
A, since a point on the boundary lies in the intersection of A with a
(d â 1)-dimensional support hyperplane.
15.3.4

Converse for the Multiple-Access Channel

We have so far proved the achievability of the capacity region. In this
section we prove the converse.

15.3 MULTIPLE-ACCESS CHANNEL

539

Proof: (Converse to Theorems 15.3.1 and 15.3.4). We must show that
given any sequence of ((2nR1 , 2nR2 ), n) codes with Pe(n) â 0, the rates
must satisfy
R1 â¤ I (X1 ; Y |X2 , Q),
R2 â¤ I (X2 ; Y |X1 , Q),
R1 + R2 â¤ I (X1 , X2 ; Y |Q)

(15.96)

for some choice of random variable Q deï¬ned on {1, 2, 3, 4} and joint
distribution p(q)p(x1 |q)p(x2 |q)p(y|x1 , x2 ). Fix n. Consider the given
code of block length n. The joint distribution on W1 Ã W2 Ã Xn1 Ã Xn2 Ã
Y n is well deï¬ned. The only randomness is due to the random uniform
choice of indices W1 and W2 and the randomness induced by the channel.
The joint distribution is
p(w1 , w2 , x1n , x2n , y n ) =

1

1

2nR1 2nR2

p(x1n |w1 )p(x2n |w2 )

n


p(yi |x1i , x2i ),

i=1

(15.97)
where p(x1n |w1 ) is either 1 or 0, depending on whether x1n = x1 (w1 ), the
codeword corresponding to w1 , or not, and similarly, p(x2n |w2 ) = 1 or 0,
according to whether x2n = x2 (w2 ) or not. The mutual informations that
follow are calculated with respect to this distribution.
By the code construction, it is possible to estimate (W1 , W2 ) from the
received sequence Y n with a low probability of error. Hence, the conditional entropy of (W1 , W2 ) given Y n must be small. By Fanoâs inequality,


H (W1 , W2 |Y n ) â¤ n(R1 + R2 )Pe(n) + H (Pe(n) ) = nn .

(15.98)

It is clear that n â 0 as Pe(n) â 0. Then we have
H (W1 |Y n ) â¤ H (W1 , W2 |Y n ) â¤ nn ,

(15.99)

H (W2 |Y ) â¤ H (W1 , W2 |Y ) â¤ nn .

(15.100)

n

n

We can now bound the rate R1 as
nR1 = H (W1 )
= I (W1 ; Y n ) + H (W1 |Y n )
( a)
â¤ I (W1 ; Y n ) + nn

(15.101)
(15.102)
(15.103)

540

NETWORK INFORMATION THEORY

(b)

â¤ I (X1n (W1 ); Y n ) + nn

(15.104)

=
â
+ nn
( c)
â¤ H (X1n (W1 )|X2n (W2 )) â H (X1n (W1 )|Y n , X2n (W2 )) + nn

(15.105)

= I (X1n (W1 ); Y n |X2n (W2 )) + nn
= H (Y n |X2n (W2 )) â H (Y n |X1n (W1 ), X2n (W2 )) + nn
n

(d)
= H (Y n |X2n (W2 )) â
H (Yi |Y iâ1 , X1n (W1 ), X2n (W2 ))
i=1

(15.107)

H (X1n (W1 ))

H (X1n (W1 )|Y n )

(15.106)
(15.108)

+ nn
(15.109)

( e)

= H (Y n |X2n (W2 )) â

n


H (Yi |X1i , X2i ) + nn

(15.110)

i=1
(f)

n


(g)

i=1
n


â¤

â¤

H (Yi |X2n (W2 )) â

=

H (Yi |X1i , X2i ) + nn

(15.111)

i=1

H (Yi |X2i ) â

i=1
n


n


n


H (Yi |X1i , X2i ) + nn

(15.112)

i=1

I (X1i ; Yi |X2i ) + nn ,

(15.113)

i=1

where
(a) follows from Fanoâs inequality
(b) follows from the data-processing inequality
(c) follows from the fact that since W1 and W2 are independent,
so are X1n (W1 ) and X2n (W2 ), and hence H (X1n (W1 )|X2n (W2 )) =
H (X1n (W1 )), and H (X1n (W1 )|Y n , X2n (W2 )) â¤ H (X1n (W1 )|Y n ) by
conditioning
(d) follows from the chain rule
(e) follows from the fact that Yi depends only on X1i and X2i by the
memoryless property of the channel
(f) follows from the chain rule and removing conditioning
(g) follows from removing conditioning
Hence, we have
1
I (X1i ; Yi |X2i ) + n .
n
n

R1 â¤

i=1

(15.114)

15.3 MULTIPLE-ACCESS CHANNEL

541

Similarly, we have
1
I (X2i ; Yi |X1i ) + n .
n
n

R2 â¤

(15.115)

i=1

To bound the sum of the rates, we have
n(R1 + R2 ) = H (W1 , W2 )

(15.116)

= I (W1 , W2 ; Y n ) + H (W1 , W2 |Y n )
( a)

â¤ I (W1 , W2 ; Y n ) + nn

(b)

(15.117)
(15.118)

â¤ I (X1n (W1 ), X2n (W2 ); Y n ) + nn

(15.119)

= H (Y n ) â H (Y n |X1n (W1 ), X2n (W2 )) + nn

(15.120)

( c)

= H (Y ) â
n

n


H (Yi |Y iâ1 , X1n (W1 ), X2n (W2 )) + nn

i=1

(15.121)
(d)

= H (Y n ) â

n


H (Yi |X1i , X2i ) + nn

(15.122)

i=1
( e)

â¤

n


H (Yi ) â

i=1

=

n


n


H (Yi |X1i , X2i ) + nn

(15.123)

i=1

I (X1i , X2i ; Yi ) + nn ,

(15.124)

i=1

where
(a) follows from
(b) follows from
(c) follows from
(d) follows from
conditionally
(e) follows from

Fanoâs inequality
the data-processing inequality
the chain rule
the fact that Yi depends only on X1i and X2i and is
independent of everything else
the chain rule and removing conditioning

Hence, we have
1
I (X1i , X2i ; Yi ) + n .
n
n

R1 + R2 â¤

i=1

(15.125)

542

NETWORK INFORMATION THEORY

The expressions in (15.114), (15.115), and (15.125) are the averages of the
mutual informations calculated at the empirical distributions in column i
of the codebook. We can rewrite these equations with the new variable Q,
where Q = i â {1, 2, . . . , n} with probability n1 . The equations become
1
I (X1i ; Yi |X2i ) + n
n

(15.126)

1
I (X1q ; Yq |X2q , Q = i) + n
n

(15.127)

n

R1 â¤

i=1
n

=

i=1



= I (X1Q ; YQ |X2Q , Q) + n

(15.128)

= I (X1 ; Y |X2 , Q) + n ,

(15.129)





where X1 = X1Q , X2 = X2Q , and Y = YQ are new random variables
whose distributions depend on Q in the same way as the distributions
of X1i , X2i and Yi depend on i. Since W1 and W2 are independent, so are
X1i (W1 ) and X2i (W2 ), and hence
Pr (X1i (W1 ) = x1 , X2i (W2 ) = x2 )


= Pr{X1Q = x1 |Q = i} Pr{X2Q = x2 |Q = i}.

(15.130)

Hence, taking the limit as n â â, Pe(n) â 0, we have the following
converse:
R1 â¤ I (X1 ; Y |X2 , Q),
R2 â¤ I (X2 ; Y |X1 , Q),
R1 + R2 â¤ I (X1 , X2 ; Y |Q)

(15.131)

for some choice of joint distribution p(q)p(x1 |q)p(x2 |q)p(y|x1 , x2 ). As
in Section 15.3.3, the region is unchanged if we limit the cardinality of
Q to 4.

This completes the proof of the converse.
Thus, the achievability of the region of Theorem 15.3.1 was proved in
Section 15.3.1. In Section 15.3.3 we showed that every point in the region
deï¬ned by (15.96) was also achievable. In the converse, we showed that
the region in (15.96) was the best we can do, establishing that this is
indeed the capacity region of the channel. Thus, the region in (15.58)

15.3 MULTIPLE-ACCESS CHANNEL

543

cannot be any larger than the region in (15.96), and this is the capacity
region of the multiple-access channel.
15.3.5

m-User Multiple-Access Channels

We will now generalize the result derived for two senders to m senders,
m â¥ 2. The multiple-access channel in this case is shown in Figure 15.15.
We send independent indices w1 , w2 , . . . , wm over the channel from
the senders 1, 2, . . . , m, respectively. The codes, rates, and achievability
are all deï¬ned in exactly the same way as in the two-sender case.
Let S
â {1, 2, . . . , m}. Let S c denote the complement of S. Let
R(S) = iâS Ri , and let X(S) = {Xi : i â S}. Then we have the following theorem.
Theorem 15.3.6 The capacity region of the m-user multiple-access
channel is the closure of the convex hull of the rate vectors satisfying
R(S) â¤ I (X(S); Y |X(S c ))

for all S â {1, 2, . . . , m}

(15.132)

for some product distribution p1 (x1 )p2 (x2 ) Â· Â· Â· pm (xm ).
Proof: The proof contains no new ideas. There are now 2m â 1 terms in
the probability of error in the achievability proof and an equal number of
inequalities in the proof of the converse. Details are left to the reader. 
In general, the region in (15.132) is a beveled box.

X1
X2
.
.
.

p(y|x1,x2,...,xm)

Y

Xm

FIGURE 15.15. m-user multiple-access channel.

544

NETWORK INFORMATION THEORY

15.3.6

Gaussian Multiple-Access Channels

We now discuss the Gaussian multiple-access channel of Section 15.1.2
in somewhat more detail.
Two senders, X1 and X2 , communicate to the single receiver, Y . The
received signal at time i is
Yi = X1i + X2i + Zi ,

(15.133)

where {Zi } is a sequence of independent, identically distributed, zeromean Gaussian random variables with variance N (Figure 15.16). We
assume that there is a power constraint Pj on sender j ; that is, for each
sender, for all messages, we must have
1 2
xj i (wj ) â¤ Pj ,
n
n

wj â {1, 2, . . . , 2nRj },

j = 1, 2. (15.134)

i=1

Just as the proof of achievability of channel capacity for the discrete
case (Chapter 7) was extended to the Gaussian channel (Chapter 9), we
can extend the proof for the discrete multiple-access channel to the Gaussian multiple-access channel. The converse can also be extended similarly,
so we expect the capacity region to be the convex hull of the set of rate
pairs satisfying
R1 â¤ I (X1 ; Y |X2 ),

(15.135)

R2 â¤ I (X2 ; Y |X1 ),

(15.136)

R1 + R2 â¤ I (X1 , X2 ; Y )

(15.137)

for some input distribution f1 (x1 )f2 (x2 ) satisfying EX12 â¤ P1 and
EX22 â¤ P2 .
Zn
W1

X n1
P1
Yn

W2

^

^

(W1, W2)

X n2
P2

FIGURE 15.16. Gaussian multiple-access channel.

15.3 MULTIPLE-ACCESS CHANNEL

545

Now, we can expand the mutual information in terms of relative
entropy, and thus
I (X1 ; Y |X2 ) = h(Y |X2 ) â h(Y |X1 , X2 )

(15.138)

= h(X1 + X2 + Z|X2 ) â h(X1 + X2 + Z|X1 , X2 )
(15.139)
= h(X1 + Z|X2 ) â h(Z|X1 , X2 )

(15.140)

= h(X1 + Z|X2 ) â h(Z)

(15.141)

= h(X1 + Z) â h(Z)

(15.142)

1
log(2Ï e)N
2
1
1
â¤ log(2Ï e)(P1 + N ) â log(2Ï e)N
2
2


1
P1
,
= log 1 +
2
N
= h(X1 + Z) â

(15.143)
(15.144)
(15.145)

where (15.141) follows from the fact that Z is independent of X1 and
X2 , (15.142) from the independence of X1 and X2 , and (15.144) from
the fact that the normal maximizes entropy for a given second moment.
Thus, the maximizing distribution is X1 â¼ N(0, P1 ) and X2 â¼ N(0, P2 )
with X1 and X2 independent. This distribution simultaneously maximizes
the mutual information bounds in (15.135)â(15.137).
Deï¬nition We deï¬ne the channel capacity function


C(x) =

1
log(1 + x),
2

(15.146)

corresponding to the channel capacity of a Gaussian white-noise channel
with signal-to-noise ratio x (Figure 15.17). Then we write the bound on
R1 as
 
P1
.
(15.147)
R1 â¤ C
N
Similarly,


R2 â¤ C

P2
N


(15.148)

546

NETWORK INFORMATION THEORY

R2

C

C

P2
N

D

C

P2
P1 + N

B

A
0

C

P1
P2 + N

C

P1
N

R1

FIGURE 15.17. Gaussian multiple-access channel capacity.



and
R1 + R2 â¤ C


P1 + P2
.
N

(15.149)

These upper bounds are achieved when X1 â¼ N(0, P1 ) and X2 =
N(0, P2 ) and deï¬ne the capacity region. The surprising fact about

 these
+P2
,
inequalities is that the sum of the rates can be as large as C P1N
which is that rate achieved by a single transmitter sending with a power
equal to the sum of the powers.
The interpretation of the corner points is very similar to the interpretation of the achievable rate pairs for a discrete multiple-access channel
for a ï¬xed input distribution. In the case of the Gaussian channel, we can
consider decoding as a two-stage process: In the ï¬rst stage, the receiver
decodes the second sender, considering the ï¬rst sender as part of the noise.
2
). After
This decoding will have low probability of error if R2 < C( P1P+N
the second sender has been decoded successfully, it can be subtracted out
and the ï¬rst sender can be decoded correctly if R1 < C( PN1 ). Hence, this
argument shows that we can achieve the rate pairs at the corner points
of the capacity region by means of single-user operations. This process,
called onion-peeling, can be extended to any number of users.
If we generalize this to m senders with equal power, the total rate
is C mP
N , which goes to â as m â â. The average rate per sender,
1
mP
m C( N ), goes to 0. Thus, when the total number of senders is very large,

15.3 MULTIPLE-ACCESS CHANNEL

547

so that there is a lot of interference, we can still send a total amount of
information that is arbitrarily large even though the rate per individual
sender goes to 0.
The capacity region described above corresponds to code-division multiple access (CDMA), where separate codes are used for the different
senders and the receiver decodes them one by one. In many practical situations, though, simpler schemes, such as frequency-division multiplexing
or time-division multiplexing, are used. With frequency-division multiplexing, the rates depend on the bandwidth allotted to each sender. Consider
the case of two senders with powers P1 and P2 using nonintersecting
frequency bands with bandwidths W1 and W2 , where W1 + W2 = W (the
total bandwidth). Using the formula for the capacity of a single-user bandlimited channel, the following rate pair is achievable:


P1
,
(15.150)
R1 = W1 log 1 +
N W1


P2
R2 = W2 log 1 +
.
(15.151)
N W2
As we vary W1 and W2 , we trace out the curve as shown in Figure 15.18.
This curve touches the boundary of the capacity region at one point,
which corresponds to allotting bandwidth to each channel proportional to
the power in that channel. We conclude that no allocation of frequency
bands to radio stations can be optimal unless the allocated powers are
proportional to the bandwidths.
In time-division multiple access (TDMA), time is divided into slots,
and each user is allotted a slot during which only that user will transmit
and every other user remains quiet. If there are two users, each of power
P , the rate that each sends when the other is silent is C(P /N ). Now if
time is divided into equal-length slots, and every odd slot is allocated
to user 1 and every even slot to user 2, the average rate that each user
achieves is 12 C(P /N ). This system is called naive time-division multiple
access (TDMA). However, it is possible to do better if we notice that since
user 1 is sending only half the time, it is possible for him to use twice
the power during his transmissions and still maintain the same average
power constraint. With this modiï¬cation, it is possible for each user to
send information at a rate 12 C(2P /N). By varying the lengths of the
slots allotted to each sender (and the instantaneous power used during the
slot), we can achieve the same capacity region as FDMA with different
bandwidth allocations.
As Figure 15.18 illustrates, in general the capacity region is larger than
that achieved by time- or frequency-division multiplexing. But note that

548

NETWORK INFORMATION THEORY

R2

C

C

P2
N

P2
P1 + N

0

C

P1
P2 + N

C

P1
N

R1

FIGURE 15.18. Gaussian multiple-access channel capacity with FDMA and TDMA.

the multiple-access capacity region derived above is achieved by use of
a common decoder for all the senders. However, it is also possible to
achieve the capacity region by onion-peeling, which removes the need
for a common decoder and instead, uses a sequence of single-user codes.
CDMA achieves the entire capacity region, and in addition, allows new
users to be added easily without changing the codes of the current users.
On the other hand, TDMA and FDMA systems are usually designed for
a ï¬xed number of users and it is possible that either some slots are empty
(if the actual number of users is less than the number of slots) or some
users are left out (if the number of users is greater than the number
of slots). However, in many practical systems, simplicity of design is
an important consideration, and the improvement in capacity due to the
multiple-access ideas presented earlier may not be sufï¬cient to warrant
the increased complexity.
For a Gaussian multiple-access system with m sources with powers
P1 , P2 , . . . , Pm and ambient noise of power N , we can state the equivalent
of Gaussâs law for any set S in the form


Ri = total rate of information ï¬ow from S

iâS


â¤C

iâS

N

Pi

(15.152)


.

(15.153)

549

15.4 ENCODING OF CORRELATED SOURCES

15.4

ENCODING OF CORRELATED SOURCES

We now turn to distributed data compression. This problem is in many
ways the data compression dual to the multiple-access channel problem.
We know how to encode a source X. A rate R > H (X) is sufï¬cient. Now
suppose that there are two sources (X, Y ) â¼ p(x, y). A rate H (X, Y )
is sufï¬cient if we are encoding them together. But what if the X and
Y sources must be described separately for some user who wishes to
reconstruct both X and Y ? Clearly, by separately encoding X and Y , it is
seen that a rate R = Rx + Ry > H (X) + H (Y ) is sufï¬cient. However, in
a surprising and fundamental paper by Slepian and Wolf [502], it is shown
that a total rate R = H (X, Y ) is sufï¬cient even for separate encoding of
correlated sources.
Let (X1 , Y1 ), (X2 , Y2 ), . . . be a sequence of jointly distributed random
variables i.i.d. â¼ p(x, y). Assume that the X sequence is available at a
location A and the Y sequence is available at a location B. The situation
is illustrated in Figure 15.19.
Before we proceed to the proof of this result, we will give a few
deï¬nitions.
Deï¬nition A ((2nR1 , 2nR2 ), n) distributed source code for the joint
source (X, Y ) consists of two encoder maps,
f1 : X n â {1, 2, . . . , 2nR1 },

(15.154)

f2 : Y n â {1, 2, . . . , 2nR2 },

(15.155)

X
Encoder

R1

(X, Y )

Decoder

Y
Encoder

R2

FIGURE 15.19. SlepianâWolf coding.

^

^

(X, Y )

550

NETWORK INFORMATION THEORY

and a decoder map,
g : {1, 2, . . . , 2nR1 } Ã {1, 2, . . . , 2nR2 } â X n Ã Y n .

(15.156)

Here f1 (X n ) is the index corresponding to X n , f2 (Y n ) is the index corresponding to Y n , and (R1 , R2 ) is the rate pair of the code.
Deï¬nition The probability of error for a distributed source code is
deï¬ned as
Pe(n) = P (g(f1 (X n ), f2 (Y n )) = (X n , Y n )).

(15.157)

Deï¬nition A rate pair (R1 , R2 ) is said to be achievable for a distributed
source if there exists a sequence of ((2nR1 , 2nR2 ), n) distributed source
codes with probability of error Pe(n) â 0. The achievable rate region is
the closure of the set of achievable rates.
Theorem 15.4.1 (SlepianâWolf ) For the distributed source coding
problem for the source (X, Y ) drawn i.i.d â¼ p(x, y), the achievable rate
region is given by
R1 â¥ H (X|Y ),

(15.158)

R2 â¥ H (Y |X),

(15.159)

R1 + R2 â¥ H (X, Y ).

(15.160)

Let us illustrate the result with some examples.
Example 15.4.1 Consider the weather in Gotham and Metropolis. For
the purposes of our example, we assume that Gotham is sunny with probability 0.5 and that the weather in Metropolis is the same as in Gotham
with probability 0.89. The joint distribution of the weather is given as
follows:
Metropolis
p(x, y)

Rain

Gotham
Rain
Shine

0.445
0.055

Shine
0.055
0.445

15.4 ENCODING OF CORRELATED SOURCES

551

Assume that we wish to transmit 100 days of weather information to the
National Weather Service headquarters in Washington. We could send all
the 100 bits of the weather in both places, making 200 bits in all. If we
decided to compress the information independently, we would still need
100H (0.5) = 100 bits of information from each place, for a total of 200
bits. If, instead, we use SlepianâWolf encoding, we need only H (X) +
H (Y |X) = 100H (0.5) + 100H (0.89) = 100 + 50 = 150 bits total.
Example 15.4.2

Consider the following joint distribution:
p(u, v)

0

1

0
1

1
3

1
3
1
3

0

In this case, the total rate required for the transmission of this
source is H (U ) + H (V |U ) = log 3 = 1.58 bits rather than the 2 bits that
would be needed if the sources were transmitted independently without
SlepianâWolf encoding.
15.4.1

Achievability of the SlepianâWolf Theorem

We now prove the achievability of the rates in the SlepianâWolf theorem.
Before we proceed to the proof, we introduce a new coding procedure
using random bins. The essential idea of random bins is very similar to
hash functions: We choose a large random index for each source sequence.
If the set of typical source sequences is small enough (or equivalently, the
range of the hash function is large enough), then with high probability,
different source sequences have different indices, and we can recover the
source sequence from the index.
Let us consider the application of this idea to the encoding of a single
source. In Chapter 3 the method that we considered was to index all
elements of the typical set and not bother about elements outside the
typical set. We will now describe the random binning procedure, which
indexes all sequences but rejects untypical sequences at a later stage.
Consider the following procedure: For each sequence X n , draw an index
at random from {1, 2, . . . , 2nR }. The set of sequences X n which have the
same index are said to form a bin, since this can be viewed as ï¬rst laying
down a row of bins and then throwing the X n âs at random into the bins.
For decoding the source from the bin index, we look for a typical X n
sequence in the bin. If there is one and only one typical X n sequence
in the bin, we declare it to be the estimate XÌ n of the source sequence;
otherwise, an error is declared.

552

NETWORK INFORMATION THEORY

The above procedure deï¬nes a source code. To analyze the probability
of error for this code, we will now divide the X n sequences into two
types, typical sequences and nontypical sequences. If the source sequence
is typical, the bin corresponding to this source sequence will contain at
least one typical sequence (the source sequence itself). Hence there will
be an error only if there is more than one typical sequence in this bin. If
the source sequence is nontypical, there will always be an error. But if
the number of bins is much larger than the number of typical sequences,
the probability that there is more than one typical sequence in a bin is
very small, and hence the probability that a typical sequence will result
in an error is very small.
Formally, let f (X n ) be the bin index corresponding to X n . Call the
decoding function g. The probability of error (averaged over the random
choice ofcodes f ) is
P (g(f (X)) = X) â¤ P (X â
/ A(n)
 )+




P (âx
 = x : x
 â A(n)
 , f (x )

x

= f (x))p(x)


â¤+

P (f (x
 ) = f (x))p(x) (15.161)

x
 â A(n)

x
 = x
 
â¤+
2ânR p(x)
x

x x
 âA(n)


=+



2ânR



p(x)

(15.163)

x

(n)
x
 âA

â¤+



(15.162)

2ânR

(15.164)

(n)
x
 âA

â¤  + 2n(H (X)+) 2ânR

(15.165)

â¤ 2

(15.166)

if R > H (X) +  and n is sufï¬ciently large. Hence, if the rate of the code
is greater than the entropy, the probability of error is arbitrarily small and
the code achieves the same results as the code described in Chapter 3.
The above example illustrates the fact that there are many ways to
construct codes with low probabilities of error at rates above the entropy
of the source; the universal source code is another example of such a code.

15.4 ENCODING OF CORRELATED SOURCES

553

Note that the binning scheme does not require an explicit characterization
of the typical set at the encoder; it is needed only at the decoder. It is
this property that enables this code to continue to work in the case of a
distributed source, as illustrated in the proof of the theorem.
We now return to the consideration of the distributed source coding and
prove the achievability of the rate region in the SlepianâWolf theorem.
Proof: (Achievability in Theorem 15.4.1). The basic idea of the proof is
to partition the space of Xn into 2nR1 bins and the space of Y n into 2nR2
bins.
Random code generation: Assign every x â X n to one of 2nR1 bins
independently according to a uniform distribution on {1, 2, . . . , 2nR1 }.
Similarly, randomly assign every y â Y n to one of 2nR2 bins. Reveal
the assignments f1 and f2 to both the encoder and the decoder.
Encoding: Sender 1 sends the index of the bin to which X belongs.
Sender 2 sends the index of the bin to which Y belongs.
Decoding: Given the received index pair (i0 , j0 ), declare (xÌ, yÌ) = (x, y)
if there is one and only one pair of sequences (x, y) such that f1 (x) = i0 ,
f2 (y) = j0 and (x, y) â A(n)
 . Otherwise, declare an error. The scheme
is illustrated in Figure 15.20. The set of X sequences and the set of Y
sequences are divided into bins in such a way that the pair of indices
speciï¬es a product bin.

xn

2nR1 bins

2nR2 bins

yn

2nH(X, Y )
jointly typical pairs
(x n,y n )

FIGURE 15.20. SlepianâWolf encoding: the jointly typical pairs are isolated by the product
bins.

554

NETWORK INFORMATION THEORY

Probability of error: Let (Xi , Yi ) â¼ p(x, y). Deï¬ne the events
/ A(n)
E0 = {(X, Y) â
 },

(15.167)

E1 = {âx
 = X : f1 (x
 ) = f1 (X) and (x
 , Y) â A(n)
 },

(15.168)

E2 = {ây
 = Y : f2 (y
 ) = f2 (Y) and (X, y
 ) â A(n)
 },

(15.169)

and
E12 = {â(x
 , y
 ) : x
 = X, y
 = Y, f1 (x
 )
= f1 (X), f2 (y
 ) = f2 (Y) and (x
 , y
 ) â A(n)
 }.

(15.170)

Here X, Y, f1 , and f2 are random. We have an error if (X, Y) is not in
A(n)
 or if there is another typical pair in the same bin. Hence by the union
of events bound,
Pe(n) = P (E0 âª E1 âª E2 âª E12 )
â¤ P (E0 ) + P (E1 ) + P (E2 ) + P (E12 ).

(15.171)
(15.172)

First consider E0 . By the AEP, P (E0 ) â 0 and hence for n sufï¬ciently
large, P (E0 ) < . To bound P (E1 ), we have
P (E1 ) = P {âx
 = X : f1 (x
 ) = f1 (X), and (x
 , Y) â A(n)
(15.173)
 }

=
p(x, y)P {âx
 = x : f1 (x
 ) = f1 (x), (x
 , y) â A(n)
 }
(x,y)

â¤


(x,y)

(15.174)



p(x, y)

P (f1 (x
 ) = f1 (x))

(15.175)



x = x
(x , y) â A(n)



=



p(x, y)2ânR1 |A (X|y)|

(15.176)

(x,y)

â¤ 2ânR1 2n(H (X|Y )+)

(by Theorem 15.2.2 ),

(15.177)

which goes to 0 if R1 > H (X|Y ). Hence for sufï¬ciently large n, P (E1 ) <
. Similarly, for sufï¬ciently large n, P (E2 ) <  if R2 > H (Y |X) and
P (E12 ) <  if R1 + R2 > H (X, Y ). Since the average probability of error
is < 4, there exists at least one code (f1â , f2â , g â ) with probability of error
< 4. Thus, we can construct a sequence of codes with Pe(n) â 0, and

the proof of achievability is complete.

15.4 ENCODING OF CORRELATED SOURCES

15.4.2

555

Converse for the SlepianâWolf Theorem

The converse for the SlepianâWolf theorem follows obviously from the
results for a single source, but we will provide it for completeness.
Proof: (Converse to Theorem 15.4.1). As usual, we begin with Fanoâs
inequality. Let f1 , f2 , g be ï¬xed. Let I0 = f1 (X n ) and J0 = f2 (Y n ). Then
H (X n , Y n |I0 , J0 ) â¤ Pe(n) n(log |X| + log |Y|) + 1 = nn ,

(15.178)

where n â 0 as n â â. Now adding conditioning, we also have
H (X n |Y n , I0 , J0 ) â¤ nn ,

(15.179)

H (Y n |X n , I0 , J0 ) â¤ nn .

(15.180)

and

We can write a chain of inequalities
(a)
n(R1 + R2 ) â¥ H (I0 , J0 )

= I (X n , Y n ; I0 , J0 ) + H (I0 , J0 |X n , Y n )
(b)

(15.181)
(15.182)

= I (X n , Y n ; I0 , J0 )

(15.183)

= H (X n , Y n ) â H (X n , Y n |I0 , J0 )

(15.184)

(c)

â¥ H (X n , Y n ) â nn

(d)

= nH (X, Y ) â nn ,

(15.185)
(15.186)

where
(a) follows from the fact that I0 â {1, 2, . . . , 2nR1 } and J0 â
{1, 2, . . . , 2nR2 }
(b) follows from the fact the I0 is a function of X n and J0 is a function
of Y n
(c) follows from Fanoâs inequality (15.178)
(d) follows from the chain rule and the fact that (Xi , Yi ) are i.i.d.

556

NETWORK INFORMATION THEORY

Similarly, using (15.179), we have
( a)
nR1 â¥ H (I0 )

(15.187)

â¥ H (I0 |Y n )

(15.188)

= I (X n ; I0 |Y n ) + H (I0 |X n , Y n )

(15.189)

(b)

= I (X n ; I0 |Y n )

(15.190)

= H (X |Y ) â H (X |I0 , J0 , Y )
n

n

n

n

( c)

â¥ H (X n |Y n ) â nn

(d)

= nH (X|Y ) â nn ,

(15.191)
(15.192)
(15.193)

where the reasons are the same as for the equations above. Similarly, we
can show that
nR2 â¥ nH (Y |X) â nn .

(15.194)

Dividing these inequalities by n and taking the limit as n â â, we have
the desired converse.

The region described in the SlepianâWolf theorem is illustrated in
Figure 15.21.
15.4.3

SlepianâWolf Theorem for Many Sources

The results of Section 15.4.2 can easily be generalized to many sources.
The proof follows exactly the same lines.
Theorem 15.4.2 Let (X1i , X2i , . . . , Xmi ) be i.i.d. â¼ p(x1 , x2 , . . . , xm ).
Then the set of rate vectors achievable for distributed source coding with
separate encoders and a common decoder is deï¬ned by
R(S) > H (X(S)|X(S c ))

(15.195)

for all S â {1, 2, . . . , m}, where
R(S) =


iâS

and X(S) = {Xj : j â S}.

Ri

(15.196)

15.4 ENCODING OF CORRELATED SOURCES

557

R2

H(Y )

H(Y |X )

0

H(X |Y )

H(X )

R1

FIGURE 15.21. Rate region for SlepianâWolf encoding.

Proof: The proof is identical to the case of two variables and is
omitted.

The achievability of SlepianâWolf encoding has been proved for an
i.i.d. correlated source, but the proof can easily be extended to the case
of an arbitrary joint source that satisï¬es the AEP; in particular, it can
be extended to the case of any jointly ergodic source [122]. In these
cases the entropies in the deï¬nition of the rate region are replaced by the
corresponding entropy rates.
15.4.4

Interpretation of SlepianâWolf Coding

We consider an interpretation of the corner points of the rate region in
SlepianâWolf encoding in terms of graph coloring. Consider the point
with rate R1 = H (X), R2 = H (Y |X). Using nH (X) bits, we can encode
X n efï¬ciently, so that the decoder can reconstruct X n with arbitrarily low
probability of error. But how do we code Y n with nH (Y |X) bits? Looking
at the picture in terms of typical sets, we see that associated with every
X n is a typical âfanâ of Y n sequences that are jointly typical with the
given X n as shown in Figure 15.22.
If the Y encoder knows X n , the encoder can send the index of the Y n
within this typical fan. The decoder, also knowing X n , can then construct
this typical fan and hence reconstruct Y n . But the Y encoder does not
know X n . So instead of trying to determine the typical fan, he randomly

558

NETWORK INFORMATION THEORY

xn

yn

FIGURE 15.22. Jointly typical fans.

colors all Y n sequences with 2nR2 colors. If the number of colors is high
enough, then with high probability all the colors in a particular fan will
be different and the color of the Y n sequence will uniquely deï¬ne the
Y n sequence within the X n fan. If the rate R2 > H (Y |X), the number of
colors is exponentially larger than the number of elements in the fan and
we can show that the scheme will have an exponentially small probability
of error.
15.5 DUALITY BETWEEN SLEPIANâWOLF ENCODING
AND MULTIPLE-ACCESS CHANNELS
With multiple-access channels, we considered the problem of sending
independent messages over a channel with two inputs and only one output.
With SlepianâWolf encoding, we considered the problem of sending a
correlated source over a noiseless channel, with a common decoder for
recovery of both sources. In this section we explore the duality between
the two systems.
In Figure 15.23, two independent messages are to be sent over the
channel as X1n and X2n sequences. The receiver estimates the messages
from the received sequence. In Figure 15.24 the correlated sources are
encoded as âindependentâ messages i and j . The receiver tries to estimate
the source sequences from knowledge of i and j .
In the proof of the achievability of the capacity region for the multipleaccess channel, we used a random map from the set of messages to the

15.5

W1

X1
Y

p(y|x1, x2)

W2

559

SLEPIANâWOLF ENCODING AND MULTIPLE-ACCESS CHANNELS

^

^

(W1, W2)

X2

FIGURE 15.23. Multiple-access channels.

X
Encoder

R1

(X, Y )

^

Decoder

Y
Encoder

^

(X, Y )

R2

FIGURE 15.24. Correlated source encoding.

sequences X1n and X2n . In the proof for SlepianâWolf coding, we used a
random map from the set of sequences X n and Y n to a set of messages.
In the proof of the coding theorem for the multiple-access channel, the
probability of error was bounded by
Pe(n) â¤  +



Pr(codeword jointly typical with sequence received)

codewords

=+


2nR1 terms

2ânI1 +


2nR2 terms

2ânI2 +



(15.197)
2ânI3 ,

2n(R1 +R2 ) terms

(15.198)

560

NETWORK INFORMATION THEORY

where  is the probability the sequences are not typical, Ri are the rates
corresponding to the number of codewords that can contribute to the
probability of error, and Ii is the corresponding mutual information that
corresponds to the probability that the codeword is jointly typical with
the received sequence.
In the case of SlepianâWolf encoding, the corresponding expression
for the probability of error is


Pe(n) â¤  +

Pr( have the same codeword)

jointly typical sequences

=+



2nH1 terms



2ânR1 +

2nH2 terms

2ânR2 +



(15.199)

2ân(R1 +R2 ) ,

2nH3 terms

(15.200)
where again the probability that the constraints of the AEP are not satisï¬ed
is bounded by , and the other terms refer to the various ways in which
another pair of sequences could be jointly typical and in the same bin as
the given source pair.
The duality of the multiple-access channel and correlated source encoding is now obvious. It is rather surprising that these two systems are duals
of each other; one would have expected a duality between the broadcast
channel and the multiple-access channel.

15.6

BROADCAST CHANNEL

The broadcast channel is a communication channel in which there is one
sender and two or more receivers. It is illustrated in Figure 15.25. The
basic problem is to ï¬nd the set of simultaneously achievable rates for
communication in a broadcast channel. Before we begin the analysis, let
us consider some examples.
Example 15.6.1 (TV station) The simplest example of the broadcast
channel is a radio or TV station. But this example is slightly degenerate
in the sense that normally the station wants to send the same information to everybody who is tuned in; the capacity is essentially maxp(x)
mini I (X; Yi ), which may be less than the capacity of the worst receiver.
But we may wish to arrange the information in such a way that the better receivers receive extra information, which produces a better picture
or sound, while the worst receivers continue to receive more basic information. As TV stations introduce high-deï¬nition TV (HDTV), it may be
necessary to encode the information so that bad receivers will receive the

15.6

(W1, W2)

X
Encoder

BROADCAST CHANNEL

561

Y1

Decoder

W1

Y2

Decoder

W2

^

p(y1, y2|x)

^

FIGURE 15.25. Broadcast channel.

regular TV signal, while good receivers will receive the extra information for the high-deï¬nition signal. The methods to accomplish this will
be explained in the discussion of the broadcast channel.
Example 15.6.2 (Lecturer in classroom) A lecturer in a classroom is
communicating information to the students in the class. Due to differences
among the students, they receive various amounts of information. Some of
the students receive most of the information; others receive only a little. In
the ideal situation, the lecturer would be able to tailor his or her lecture in
such a way that the good students receive more information and the poor
students receive at least the minimum amount of information. However, a
poorly prepared lecture proceeds at the pace of the weakest student. This
situation is another example of a broadcast channel.
Example 15.6.3 (Orthogonal broadcast channels) The simplest broadcast channel consists of two independent channels to the two receivers.
Here we can send independent information over both channels, and we
can achieve rate R1 to receiver 1 and rate R2 to receiver 2 if R1 < C1 and
R2 < C2 . The capacity region is the rectangle shown in Figure 15.26.
Example 15.6.4 (Spanish and Dutch speaker) To illustrate the idea of
superposition, we will consider a simpliï¬ed example of a speaker who can
speak both Spanish and Dutch. There are two listeners: One understands
only Spanish and the other understands only Dutch. Assume for simplicity
that the vocabulary of each language is 220 words and that the speaker
speaks at the rate of 1 word per second in either language. Then he

562

NETWORK INFORMATION THEORY
R2

C2

0

C1

R1

FIGURE 15.26. Capacity region for two orthogonal broadcast channels.

can transmit 20 bits of information per second to receiver 1 by speaking
to her all the time; in this case, he sends no information to receiver 2.
Similarly, he can send 20 bits per second to receiver 2 without sending
any information to receiver 1. Thus, he can achieve any rate pair with
R1 + R2 = 20 by simple time-sharing. But can he do better?
Recall that the Dutch listener, even though he does not understand
Spanish, can recognize when the word is Spanish. Similarly, the Spanish
listener can recognize when Dutch occurs. The speaker can use this to
convey information; for example, if the proportion of time he uses each
language is 50%, then of a sequence of 100 words, about 50 will be
Dutch and about 50 will be Spanish. But there are many ways to order the
1
 
Spanish and Dutch words; in fact, there are about 100 â 2100H ( 2 ) ways
50

to order the words. Choosing one of these orderings conveys information
to both listeners. This method enables the speaker to send information at
a rate of 10 bits per second to the Dutch receiver, 10 bits per second to
the Spanish receiver, and 1 bit per second of common information to both
receivers, for a total rate of 21 bits per second, which is more than that
achievable by simple timesharing. This is an example of superposition of
information.
The results of the broadcast channel can also be applied to the case
of a single-user channel with an unknown distribution. In this case, the
objective is to get at least the minimum information through when the
channel is bad and to get some extra information through when the channel
is good. We can use the same superposition arguments as in the case of
the broadcast channel to ï¬nd the rates at which we can send information.

15.6

15.6.1

BROADCAST CHANNEL

563

Deï¬nitions for a Broadcast Channel

Deï¬nition A broadcast channel consists of an input alphabet X and
two output alphabets, Y1 and Y2 , and a probability transition function
p(y1 , y2 |x). The
broadcast channel will be said to be memoryless if
p(y1n , y2n |x n ) = ni=1 p(y1i , y2i |xi ).
We deï¬ne codes, probability of error, achievability, and capacity regions
for the broadcast channel as we did for the multiple-access channel. A
((2nR1 , 2nR2 ), n) code for a broadcast channel with independent information consists of an encoder,
X : ({1, 2, . . . , 2nR1 } Ã {1, 2, . . . , 2nR2 }) â X n ,

(15.201)

and two decoders,
g1 : Y1n â {1, 2, . . . , 2nR1 }

(15.202)

g2 : Y2n â {1, 2, . . . , 2nR2 }.

(15.203)

and

We deï¬ne the average probability of error as the probability that the
decoded message is not equal to the transmitted message; that is,
Pe(n) = P (g1 (Y1n ) = W1

or

g2 (Y2n ) = W2 ),

(15.204)

where (W1 , W2 ) are assumed to be uniformly distributed over 2nR1 Ã 2nR2 .
Deï¬nition A rate pair (R1 , R2 ) is said to be achievable for the broadcast channel if there exists a sequence of ((2nR1 , 2nR2 ), n) codes with
Pe(n) â 0.
We will now deï¬ne the rates for the case where we have common
information to be sent to both receivers. A ((2nR0 , 2nR1 , 2nR2 ), n) code
for a broadcast channel with common information consists of an encoder,
X : ({1, 2, . . . , 2nR0 } Ã {1, 2, . . . , 2nR1 } Ã {1, 2, . . . , 2nR2 }) â X n ,
(15.205)
and two decoders,

and

g1 : Y1n â {1, 2, . . . , 2nR0 } Ã {1, 2, . . . , 2nR1 }

(15.206)

g2 : Y2n â {1, 2, . . . , 2nR0 } Ã {1, 2, . . . , 2nR2 }.

(15.207)

564

NETWORK INFORMATION THEORY

Assuming that the distribution on (W0 , W1 , W2 ) is uniform, we can deï¬ne
the probability of error as the probability that the decoded message is not
equal to the transmitted message:
Pe(n) = P (g1 (Y1n ) = (W0 , W1 ) or g2 (Z n ) = (W0 , W2 )).

(15.208)

Deï¬nition A rate triple (R0 , R1 , R2 ) is said to be achievable for the
broadcast channel with common information if there exists a sequence of
((2nR0 , 2nR1 , 2nR2 ), n) codes with Pe(n) â 0.
Deï¬nition The capacity region of the broadcast channel is the closure
of the set of achievable rates.
We observe that an error for receiver Y1n depends only the distribution
p(x n , y1n ) and not on the joint distribution p(x n , y1n , y2n ). Thus, we have
the following theorem:
Theorem 15.6.1 The capacity region of a broadcast channel depends
only on the conditional marginal distributions p(y1 |x) and p(y2 |x).


Proof: See the problems.
15.6.2

Degraded Broadcast Channels

Deï¬nition A broadcast channel is said to be physically degraded if
p(y1 , y2 |x) = p(y1 |x)p(y2 |y1 ).
Deï¬nition A broadcast channel is said to be stochastically degraded if
its conditional marginal distributions are the same as that of a physically
degraded broadcast channel; that is, if there exists a distribution p 
 (y2 |y1 )
such that
p(y2 |x) =



p(y1 |x)p 
 (y2 |y1 ).

(15.209)

y1

Note that since the capacity of a broadcast channel depends only on the
conditional marginals, the capacity region of the stochastically degraded
broadcast channel is the same as that of the corresponding physically
degraded channel. In much of the following, we therefore assume that the
channel is physically degraded.

15.6

15.6.3

BROADCAST CHANNEL

565

Capacity Region for the Degraded Broadcast Channel

We now consider sending independent information over a degraded broadcast channel at rate R1 to Y1 and rate R2 to Y2 .
Theorem 15.6.2 The capacity region for sending independent information over the degraded broadcast channel X â Y1 â Y2 is the convex
hull of the closure of all (R1 , R2 ) satisfying
R2 â¤ I (U ; Y2 ),

(15.210)

R1 â¤ I (X; Y1 |U )

(15.211)

for some joint distribution p(u)p(x|u)p(y1 , y2 |x), where the auxiliary random variable U has cardinality bounded by |U| â¤ min{|X|, |Y1 |, |Y2 |}.
Proof: (The cardinality bounds for the auxiliary random variable U are
derived using standard methods from convex set theory and are not dealt
with here.) We ï¬rst give an outline of the basic idea of superposition
coding for the broadcast channel. The auxiliary random variable U will
serve as a cloud center that can be distinguished by both receivers Y1
and Y2 . Each cloud consists of 2nR1 codewords X n distinguishable by the
receiver Y1 . The worst receiver can only see the clouds, while the better
receiver can see the individual codewords within the clouds. The formal
proof of the achievability of this region uses a random coding argument:
Fix p(u) and p(x|u).
Random codebook generation: Generate 2nR2 independent

 codewords
of length n, U(w2 ), w2 â {1, 2, . . . , 2nR2 }, according to ni=1 p(ui ). For
nR1
independent codewords X(w1 , w2 )
each codeword
2 ), generate 2

U(w
n
according to i=1 p(xi |ui (w2 )). Here u(i) plays the role of the cloud
center understandable to both Y1 and Y2 , while x(i, j ) is the j th satellite
codeword in the ith cloud.
Encoding: To send the pair (W1 , W2 ), send the corresponding codeword
X(W1 , W2 ).
Ë such that (U(WÌ
Ë ),
Decoding: Receiver 2 determines the unique WÌ
2
2
(n)
Y2 ) â A . If there are none such or more than one such, an error is
declared.
Receiver 1 looks for the unique (WÌ1 , WÌ2 ) such that (U(WÌ2 ), X(WÌ1 , WÌ2 ),
Y1 ) â A(n)
 . If there are none such or more than one such, an error is
declared.
Analysis of the probability of error: By the symmetry of the code generation, the probability of error does not depend on which codeword was

566

NETWORK INFORMATION THEORY

sent. Hence, without loss of generality, we can assume that the message pair (W1 , W2 ) = (1, 1) was sent. Let P (Â·) denote the conditional
probability of an event given that (1,1) was sent.
Since we have essentially a single-user channel from U to Y2 , we will
be able to decode the U codewords with a low probability of error if
R2 < I (U ; Y2 ). To prove this, we deï¬ne the events
EY i = {(U(i), Y2 ) â A(n)
 }.

(15.212)

Then the probability of error at receiver 2 is
Pe(n) (2) = P (EYc 1



EY i )

(15.213)

i=1

â¤ P (EYc 1 ) +



P (EY i )

(15.214)

i=1

â¤  + 2nR2 2ân(I (U ;Y2 )â2)

(15.215)

â¤ 2

(15.216)

if n is large enough and R2 < I (U ; Y2 ), where (15.215) follows from the
AEP. Similarly, for decoding for receiver 1, we deï¬ne the events
EÌY i = {(U(i), Y1 ) â A(n)
 },

(15.217)

EÌY ij = {(U(i), X(i, j ), Y1 ) â A(n)
 },

(15.218)

where the tilde refers to events deï¬ned at receiver 1. Then we can bound
the probability of error as
ï£«
Pe(n) (1) = P ï£­EÌYc 1



EÌYc 11



EÌY i

i=1

â¤ P (EÌYc 1 ) + P (EÌYc 11 ) +


i=1



ï£¶
EÌY 1j ï£¸

j =1

P (EÌY i ) +



(15.219)

P (EÌY 1j ). (15.220)

j =1

By the same arguments as for receiver 2, we can bound P (EÌY i ) â¤
2ân(I (U ;Y1 )â3) . Hence, the third term goes to 0 if R2 < I (U ; Y1 ). But
by the data-processing inequality and the degraded nature of the channel, I (U ; Y1 ) â¥ I (U ; Y2 ), and hence the conditions of the theorem imply

15.6

BROADCAST CHANNEL

567

that the third term goes to 0. We can also bound the fourth term in the
probability of error as
P (EÌY 1j ) = P ((U(1), X(1, j ), Y1 ) â A(n)
 )

=
P ((U(1), X(1, j ), Y1 ))

(15.221)
(15.222)

(n)
(U,X,Y1 )âA



=

P (U(1))P (X(1, j )|U(1))P (Y1 |U(1)) (15.223)

(n)
(U,X,Y1 )âA



â¤

2ân(H (U )â) 2ân(H (X|U )â) 2ân(H (Y1 |U )â)
(n)

(U,X,Y1 )âA

(15.224)

n(H (U,X,Y1 )+) ân(H (U )â) ân(H (X|U )â) ân(H (Y1 |U )â)

â¤2

2

= 2ân(I (X;Y1 |U )â4) .

2

2

(15.225)
(15.226)

Hence, if R1 < I (X; Y1 |U ), the fourth term in the probability of error
goes to 0. Thus, we can bound the probability of error
Pe(n) (1) â¤  +  + 2nR2 2ân(I (U ;Y1 )â3) + 2nR1 2ân(I (X;Y1 |U )â4) (15.227)
â¤ 4

(15.228)

if n is large enough and R2 < I (U ; Y1 ) and R1 < I (X; Y1 |U ). The above
bounds show that we can decode the messages with total probability
of error that goes to 0. Hence, there exists a sequence of good ((2nR1 ,
2nR2 ), n) codes Cân with probability of error going to 0. With this, we complete the proof of the achievability of the capacity region for the degraded
broadcast channel. Gallagerâs proof [225] of the converse is outlined in
Problem 15.11.

So far we have considered sending independent information to each
receiver. But in certain situations, we wish to send common information
to both receivers. Let the rate at which we send common information be
R0 . Then we have the following obvious theorem:
Theorem 15.6.3 If the rate pair (R1 , R2 ) is achievable for a broadcast
channel with independent information, the rate triple (R0 , R1 â R0 , R2 â
R0 ) with a common rate R0 is achievable, provided that R0 â¤
min(R1 , R2 ).

568

NETWORK INFORMATION THEORY

In the case of a degraded broadcast channel, we can do even better.
Since by our coding scheme the better receiver always decodes all the
information that is sent to the worst receiver, one need not reduce the
amount of information sent to the better receiver when we have common
information. Hence, we have the following theorem:
Theorem 15.6.4 If the rate pair (R1 , R2 ) is achievable for a degraded
broadcast channel, the rate triple (R0 , R1 , R2 â R0 ) is achievable for the
channel with common information, provided that R0 < R2 .
We end this section by considering the example of the binary symmetric
broadcast channel.
Example 15.6.5 Consider a pair of binary symmetric channels with
parameters p1 and p2 that form a broadcast channel as shown in Figure 15.27. Without loss of generality in the capacity calculation, we can
recast this channel as a physically degraded channel. We assume that
p1 < p2 < 12 . Then we can express a binary symmetric channel with
parameter p2 as a cascade of a binary symmetric channel with parameter
p1 with another binary symmetric channel. Let the crossover probability
of the new channel be Î±. Then we must have
p1 (1 â Î±) + (1 â p1 )Î± = p2

(15.229)
0

Y1
1

0

X
1

0

Y2
1

FIGURE 15.27. Binary symmetric broadcast channel.

15.6

1âb

1 â p1

U

a

X
b

1âa

p1

b

569

BROADCAST CHANNEL

Y1
p1

Y2
a

1 â p1

1âb

1âa

FIGURE 15.28. Physically degraded binary symmetric broadcast channel.

or
Î±=

p2 â p1
.
1 â 2p1

(15.230)

We now consider the auxiliary random variable in the deï¬nition of the
capacity region. In this case, the cardinality of U is binary from the bound
of the theorem. By symmetry, we connect U to X by another binary
symmetric channel with parameter Î², as illustrated in Figure 15.28.
We can now calculate the rates in the capacity region. It is clear by symmetry that the distribution on U that maximizes the rates is the uniform
distribution on {0, 1}, so that
I (U ; Y2 ) = H (Y2 ) â H (Y2 |U )
= 1 â H (Î² â p2 ),
where

(15.231)
(15.232)

Î² â p2 = Î²(1 â p2 ) + (1 â Î²)p2 .

(15.233)

I (X; Y1 |U ) = H (Y1 |U ) â H (Y1 |X, U )

(15.234)

Similarly,

where

= H (Y1 |U ) â H (Y1 |X)

(15.235)

= H (Î² â p1 ) â H (p1 ),

(15.236)

Î² â p1 = Î²(1 â p1 ) + (1 â Î²)p1 .

(15.237)

Plotting these points as a function of Î², we obtain the capacity region
in Figure 15.29. When Î² = 0, we have maximum information transfer
to Y2 [i.e., R2 = 1 â H (p2 ) and R1 = 0]. When Î² = 12 , we have maximum information transfer to Y1 [i.e., R1 = 1 â H (p1 )] and no information
transfer to Y2 . These values of Î² give us the corner points of the rate
region.

570

NETWORK INFORMATION THEORY

R2

I â H(p2)

I â H(p1)

R1

FIGURE 15.29. Capacity region of binary symmetric broadcast channel.

Z1 ~

Z 2â² ~

(0,N1)

(0,N2 â N1)

Y1

X

Y2

FIGURE 15.30. Gaussian broadcast channel.

Example 15.6.6 (Gaussian broadcast channel ) The Gaussian broadcast channel is illustrated in Figure 15.30. We have shown it in the case
where one output is a degraded version of the other output. Based on
the results of Problem 15.10, it follows that all scalar Gaussian broadcast
channels are equivalent to this type of degraded channel.
Y1 = X + Z1 ,

(15.238)

Y2 = X + Z2 = Y1 + Z2
 ,

(15.239)

where Z1 â¼ N(0, N1 ) and Z2
 â¼ N(0, N2 â N1 ).
Extending the results of this section to the Gaussian case, we can show
that the capacity region of this channel is given by

R1 < C

R2 < C

Î±P
N1



(1 â Î±)P
Î±P + N2

(15.240)

,

(15.241)

15.7 RELAY CHANNEL

571

where Î± may be arbitrarily chosen (0 â¤ Î± â¤ 1). The coding scheme that
achieves this capacity region is outlined in Section 15.1.3.
15.7

RELAY CHANNEL

The relay channel is a channel in which there is one sender and one
receiver with a number of intermediate nodes that act as relays to help
the communication from the sender to the receiver. The simplest relay
channel has only one intermediate or relay node. In this case the channel
consists of four ï¬nite sets X, X1 , Y, and Y1 and a collection of probability
mass functions p(y, y1 |x, x1 ) on Y Ã Y1 , one for each (x, x1 ) â X Ã X1 .
The interpretation is that x is the input to the channel and y is the output
of the channel, y1 is the relayâs observation, and x1 is the input symbol
chosen by the relay, as shown in Figure 15.31. The problem is to ï¬nd the
capacity of the channel between the sender X and the receiver Y .
The relay channel combines a broadcast channel (X to Y and Y1 ) and a
multiple-access channel (X and X1 to Y ). The capacity is known for the
special case of the physically degraded relay channel. We ï¬rst prove an
outer bound on the capacity of a general relay channel and later establish
an achievable region for the degraded relay channel.
Deï¬nition A (2nR , n) code for a relay channel consists of a set of
integers W = {1, 2, . . . , 2nR }, an encoding function
X : {1, 2, . . . , 2nR } â X n ,

(15.242)

a set of relay functions {fi }ni=1 such that
x1i = fi (Y11 , Y12 , . . . , Y1iâ1 ),

1 â¤ i â¤ n,

(15.243)

and a decoding function,
g : Y n â {1, 2, . . . , 2nR }.

(15.244)

Y1 : X1

X

Y

FIGURE 15.31. Relay channel.

572

NETWORK INFORMATION THEORY

Note that the deï¬nition of the encoding functions includes the nonanticipatory condition on the relay. The relay channel input is allowed to
depend only on the past observations y11 , y12 , . . . , y1iâ1 . The channel is
memoryless in the sense that (Yi , Y1i ) depends on the past only through
the current transmitted symbols (Xi , X1i ). Thus, for any choice p(w),
w â W, and code choice X : {1, 2, . . . , 2nR } â Xin and relay functions
{fi }ni=1 , the joint probability mass function on W Ã X n Ã X1n Ã Y n Ã Y1n
is given by
p(w, x, x1 , y, y1 ) = p(w)

n


p(xi |w)p(x1i |y11 , y12 , . . . , y1iâ1 )

i=1

Ã p(yi , y1i |xi , x1i ).

(15.245)

If the message w â [1, 2nR ] is sent, let
Î»(w) = Pr{g(Y) = w|w sent}

(15.246)

denote the conditional probability of error. We deï¬ne the average probability of error of the code as
Pe(n) =

1 
Î»(w).
2nR w

(15.247)

The probability of error is calculated under the uniform distribution over
the codewords w â {1, . . . , 2nR }. The rate R is said to be achievable
by the relay channel if there exists a sequence of (2nR , n) codes with
Pe(n) â 0. The capacity C of a relay channel is the supremum of the set
of achievable rates.
We ï¬rst give an upper bound on the capacity of the relay channel.
Theorem 15.7.1 For any relay channel (X Ã X1 , p(y, y1 |x, x1 ), Y Ã
Y1 ), the capacity C is bounded above by
C â¤ sup min {I (X, X1 ; Y ), I (X; Y, Y1 |X1 )} .

(15.248)

p(x,x1 )

Proof: The proof is a direct consequence of a more general max-ï¬ow

min-cut theorem given in Section 15.10.
This upper bound has a nice max-ï¬ow min-cut interpretation. The ï¬rst
term in (15.248) upper bounds the maximum rate of information transfer

15.7 RELAY CHANNEL

573

from senders X and X1 to receiver Y . The second terms bound the rate
from X to Y and Y1 .
We now consider a family of relay channels in which the relay receiver
is better than the ultimate receiver Y in the sense deï¬ned below. Here the
max-ï¬ow min-cut upper bound in the (15.248) is achieved.
Deï¬nition The relay channel (X Ã X1 , p(y, y1 |x, x1 ), Y Ã Y1 ) is said
to be physically degraded if p(y, y1 |x, x1 ) can be written in the form
p(y, y1 |x, x1 ) = p(y1 |x, x1 )p(y|y1 , x1 ).

(15.249)

Thus, Y is a random degradation of the relay signal Y1 .
For the physically degraded relay channel, the capacity is given by the
following theorem.
Theorem 15.7.2
is given by

The capacity C of a physically degraded relay channel

C = sup min {I (X, X1 ; Y ), I (X; Y1 |X1 )} ,

(15.250)

p(x,x1 )

where the supremum is over all joint distributions on X Ã X1 .
Proof:
Converse: The proof follows from Theorem 15.7.1 and by degradedness,
since for the degraded relay channel, I (X; Y, Y1 |X1 ) = I (X; Y1 |X1 ).
Achievability: The proof of achievability involves a combination
of the following basic techniques: (1) random coding, (2) list codes,
(3) SlepianâWolf partitioning, (4) coding for the cooperative multipleaccess channel, (5) superposition coding, and (6) block Markov encoding
at the relay and transmitter. We provide only an outline of the proof.
Outline of achievability: We consider B blocks of transmission, each of
n symbols. A sequence of B â 1 indices, wi â {1, . . . , 2nR }, i = 1, 2, . . . ,
B â 1, will be sent over the channel in nB transmissions. (Note that as
B â â for a ï¬xed n, the rate R(B â 1)/B is arbitrarily close to R.)
We deï¬ne a doubly indexed set of codewords:
C = {x(w|s), x1 (s)} : w â {1, 2nR }, s â {1, 2nR0 }, x â X n , x1 â X1n .
(15.251)
We will also need a partition
S = {S1 , S2 , . . . , S2nR0 } of W = {1, 2, . . . , 2nR }

(15.252)

574

NETWORK INFORMATION THEORY

into 2nR0 cells, with Si â© Sj = Ï, i = j , and âªSi = W. The partition will
enable us to send side information to the receiver in the manner of Slepian
and Wolf [502].
Generation of random code: Fix p(x1 )p(x|x1 ).
First generate at random 

2nR0 i.i.d. n-sequences in X1n , each
drawn according to p(x1 ) = ni=1 p(x1i ). Index them as x1 (s), s â
{1, 2, . . . , 2nR0 }. For each x1 (s), generate 2nR conditionally independent
n-sequences x(w|s), 

w â {1, 2, . . . , 2nR }, drawn independently according to p(x|x1 (s)) = ni=1 p(xi |x1i (s)). This deï¬nes the random codebook C = {x(w|s), x1 (s)}. The random partition S = {S1 , S2 , . . . , S2nR0 } of
{1, 2, . . . , 2nR } is deï¬ned as follows. Let each integer w â {1, 2, . . . , 2nR }
be assigned independently, according to a uniform distribution over the
indices s = 1, 2, . . . , 2nR0 , to cells Ss .
Encoding: Let wi â {1, 2, . . . , 2nR } be the new index to be sent in block
i, and let si be deï¬ned as the partition corresponding to wiâ1 (i.e., wiâ1 â
Ë iâ1 of the
Ssi ). The encoder sends x(wi |si ). The relay has an estimate wÌ
previous index wiâ1 . (This will be made precise in the decoding section.)
Ë iâ1 â S Ë . The relay encoder sends x1 (sËËi ) in block i.
Assume that wÌ
sËi
Decoding: We assume that at the end of block i â 1, the receiver
knows (w1 , w2 , . . . , wiâ2 ) and (s1 , s2 , . . . , siâ1 ) and the relay knows (w1 ,
w2 , . . . , wiâ1 ) and consequently, (s1 , s2 , . . . , si ). The decoding procedures
at the end of block i are as follows:
1. Knowing si and upon receiving y1 (i), the relay receiver estimates
the message of the transmitter wÌË i = w if and only if there exists
a unique w such that (x(w|si ), x1 (si ), y1 (i)) are jointly -typical.
Using Theorem 15.2.3, it can be shown that wÌË i = wi with an arbitrarily small probability of error if
R < I (X; Y1 |X1 )

(15.253)

and n is sufï¬ciently large.
2. The receiver declares that sËi = s was sent iff there exists one and
only one s such that (x1 (s), y(i)) are jointly -typical. From Theorem 15.2.1 we know that si can be decoded with arbitrarily small
probability of error if
R0 < I (X1 ; Y )
and n is sufï¬ciently large.

(15.254)

15.8 SOURCE CODING WITH SIDE INFORMATION

575

3. Assuming that si is decoded correctly at the receiver, the receiver
constructs a list Å(y(i â 1)) of indices that the receiver considers to
be jointly typical with y(i â 1) in the (i â 1)th block. The receiver
then declares wÌiâ1 = w as the index sent in block i â 1 if there is
a unique w in Ssi â© Å(y(i â 1)). If n is sufï¬ciently large and if
R < I (X; Y |X1 ) + R0 ,

(15.255)

then wÌiâ1 = wiâ1 with arbitrarily small probability of error. Combining the two constraints (15.254) and (15.255), R0 drops out,
leaving
R < I (X; Y |X1 ) + I (X1 ; Y ) = I (X, X1 ; Y ).

(15.256)

For a detailed analysis of the probability of error, the reader is
referred to Cover and El Gamal [127].

Theorem 15.7.2 can also shown to be the capacity for the following
classes of relay channels:
1. Reversely degraded relay channel, that is,
p(y, y1 |x, x1 ) = p(y|x, x1 )p(y1 |y, x1 ).

(15.257)

2. Relay channel with feedback
3. Deterministic relay channel,
y1 = f (x, x1 ),
15.8

y = g(x, x1 ).

(15.258)

SOURCE CODING WITH SIDE INFORMATION

We now consider the distributed source coding problem where two random
variables X and Y are encoded separately but only X is to be recovered.We
now ask how many bits R1 are required to describe X if we are allowed
R2 bits to describe Y . If R2 > H (Y ), then Y can be described perfectly,
and by the results of SlepianâWolf coding, R1 = H (X|Y ) bits sufï¬ce
to describe X. At the other extreme, if R2 = 0, we must describe X
without any help, and R1 = H (X) bits are then necessary to describe X. In
general, we use R2 = I (Y ; YÌ ) bits to describe an approximate version of
Y . This will allow us to describe X using H (X|YÌ ) bits in the presence of
side information YÌ . The following theorem is consistent with this intuition.

576

NETWORK INFORMATION THEORY

Theorem 15.8.1 Let (X, Y ) â¼ p(x, y). If Y is encoded at rate R2 and
X is encoded at rate R1 , we can recover X with an arbitrarily small probability of error if and only if
R1 â¥ H (X|U ),

(15.259)

R2 â¥ I (Y ; U )

(15.260)

for some joint probability mass function p(x, y)p(u|y), where |U| â¤
|Y| + 2.
We prove this theorem in two parts. We begin with the converse, in
which we show that for any encoding scheme that has a small probability
of error, we can ï¬nd a random variable U with a joint probability mass
function as in the theorem.
Proof: (Converse). Consider any source code for Figure 15.32. The
source code consists of mappings fn (X n ) and gn (Y n ) such that the rates of
fn and gn are less than R1 and R2 , respectively, and a decoding mapping
hn such that
Pe(n) = Pr{hn (fn (X n ), gn (Y n )) = X n } < .

(15.261)

Deï¬ne new random variables S = fn (X n ) and T = gn (Y n ). Then since
we can recover X n from S and T with low probability of error, we have,
by Fanoâs inequality,
H (X n |S, T ) â¤ nn .
Then

(15.262)

(a)
nR2 â¥ H (T )

(15.263)

(b)

â¥ I (Y n ; T )

X

Encoder

Y

Encoder

R1

(15.264)

Decoder

R2

FIGURE 15.32. Encoding with side information.

^

X

15.8 SOURCE CODING WITH SIDE INFORMATION

=

n


I (Yi ; T |Y1 , . . . , Yiâ1 )

577

(15.265)

i=1
(c)

=

n


I (Yi ; T , Y1 , . . . , Yiâ1 )

(15.266)

I (Yi ; Ui )

(15.267)

i=1
(d)

=

n

i=1

where
(a) follows from the fact that the range of gn is {1, 2, . . . , 2nR2 }
(b) follows from the properties of mutual information
(c) follows from the chain rule and the fact that Yi is independent of
Y1 , . . . , Yiâ1 and hence I (Yi ; Y1 , . . . , Yiâ1 ) = 0
(d) follows if we deï¬ne Ui = (T , Y1 , . . . , Yiâ1 )
We also have another chain for R1 ,
(a)
nR1 â¥ H (S)

(15.268)

(b)

â¥ H (S|T )

(15.269)

= H (S|T ) + H (X |S, T ) â H (X |S, T )
n

n

(c)

â¥ H (X n , S|T ) â nn

(d)

= H (X n |T ) â nn

(e)

=

n


(15.270)
(15.271)
(15.272)

H (Xi |T , X1 , . . . , Xiâ1 ) â nn

(15.273)

H (Xi |T , X iâ1 , Y iâ1 ) â nn

(15.274)

i=1
(f)

â¥

n

i=1

(g)

=

n


H (Xi |T , Y iâ1 ) â nn

(15.275)

H (Xi |Ui ) â nn ,

(15.276)

i=1
(h)

=

n

i=1

578

NETWORK INFORMATION THEORY

where
(a) follows from the fact that the range of S is {1, 2, . . . , 2nR1 }
(b) follows from the fact that conditioning reduces entropy
(c) follows from Fanoâs inequality
(d) follows from the chain rule and the fact that S is a function of X n
(e) follows from the chain rule for entropy
(f) follows from the fact that conditioning reduces entropy
(g) follows from the (subtle) fact that Xi â (T , Y iâ1 ) â X iâ1 forms
a Markov chain since Xi does not contain any information about
X iâ1 that is not there in Y iâ1 and T
(h) follows from the deï¬nition of U
Also, since Xi contains no more information about Ui than is present
in Yi , it follows that Xi â Yi â Ui forms a Markov chain. Thus we have
the following inequalities:
1
H (Xi |Ui ),
R1 â¥
n
n

(15.277)

i=1

1
I (Yi ; Ui ).
n
n

R2 â¥

(15.278)

i=1

We now introduce a timesharing random variable Q so that we can rewrite
these equations as
1
H (Xi |Ui , Q = i) = H (XQ |UQ , Q),
n

(15.279)

1
I (Yi ; Ui |Q = i) = I (YQ ; UQ |Q).
n

(15.280)

n

R1 â¥

i=1
n

R2 â¥

i=1

Now since Q is independent of YQ (the distribution of Yi does not depend
on i), we have
I (YQ ; UQ |Q) = I (YQ ; UQ , Q) â I (YQ ; Q) = I (YQ ; UQ , Q). (15.281)
Now XQ and YQ have the joint distribution p(x, y) in the theorem. Deï¬ning U = (UQ , Q), X = XQ , and Y = YQ , we have shown the existence
of a random variable U such that
R1 â¥ H (X|U ),

(15.282)

R2 â¥ I (Y ; U )

(15.283)

15.8 SOURCE CODING WITH SIDE INFORMATION

579

for any encoding scheme that has a low probability of error. Thus, the

converse is proved.
Before we proceed to the proof of the achievability of this pair of rates,
we will need a new lemma about strong typicality and Markov chains.
Recall the deï¬nition of strong typicality for a triple of random variables
X, Y , and Z. A triplet of sequences x n , y n , zn is said to be -strongly
typical if
	
	
	
	1

	 N (a, b, c|x n , y n , zn ) â p(a, b, c)	 <
(15.284)
	 |X||Y||Z| .
	n
In particular, this implies that (x n , y n ) are jointly strongly typical and
that (y n , zn ) are also jointly strongly typical. But the converse is not true:
n n
â(n)
The fact that (x n , y n ) â Aâ(n)
 (X, Y ) and (y , z ) â A (Y, Z) does not
n
n
n
â(n)
in general imply that (x , y , z ) â A (X, Y, Z). But if X â Y â Z
forms a Markov chain, this implication is true. We state this as a lemma
without proof [53, 149].
Lemma 15.8.1 Let (X, Y, Z) form a Markov chain X â Y â Z [i.e.,
n
p(x, y, z) 

= p(x, y)p(z|y)]. If for a given (y n , zn ) â Aâ(n)
 (Y, Z), X is
n
n
n n
â(n)
drawn â¼ i=1 p(xi |yi ), then Pr{(X , y , z ) â A (X, Y, Z)} > 1 â 
for n sufï¬ciently large.
Remark

 The theorem is true from the strong law of large numbers if
X n â¼ ni=1 p(xi |yi , zi ). The Markovity of X â Y â Z is used to show
that X n â¼ p(xi |yi ) is sufï¬cient for the same conclusion.
We now outline the proof of achievability in Theorem 15.8.1.
Proof:
(Achievability in Theorem 15.8.1). Fix p(u|y). Calculate p(u) =

y p(y)p(u|y).
Generation of codebooks: Generate 2nR2 independent

n codewords of
nR2
length n, U(w2 ), w2 â {1, 2, . . . , 2 } according to i=1 p(ui ). Randomly bin all the X n sequences into 2nR1 bins by independently generating
an index b distributed uniformly on {1, 2, . . . , 2nR1 } for each X n . Let B(i)
denote the set of X n sequences allotted to bin i.
Encoding: The X sender sends the index i of the bin in which X n falls.
The Y sender looks for an index s such that (Y n , U n (s)) â Aâ(n)
 (Y, U ).
If there is more than one such s, it sends the least. If there is no such
U n (s) in the codebook, it sends s = 1.
Decoding: The receiver looks for a unique X n â B(i) such that (X n ,
U n (s)) â Aâ(n)
 (X, U ). If there is none or more than one, it declares an
error.

580

NETWORK INFORMATION THEORY

Analysis of the probability of error: The various sources of error are as
follows:
1. The pair (X n , Y n ) generated by the source is not typical. The probability of this is small if n is large. Hence, without loss of generality,
we can condition on the event that the source produces a particular
typical sequence (x n , y n ) â Aâ(n)
 .
n
2. The sequence Y is typical, but there does not exist a U n (s) in the
codebook that is jointly typical with it. The probability of this is
small from the arguments of Section 10.6, where we showed that if
there are enough codewords; that is, if
R2 > I (Y ; U ),

(15.285)

we are very likely to ï¬nd a codeword that is jointly strongly typical
with the given source sequence.
3. The codeword U n (s) is jointly typical with y n but not with x n . But
by Lemma 15.8.1, the probability of this is small since X â Y â U
forms a Markov chain.
4. We also have an error if there exists another typical X n â B(i) which
is jointly typical with U n (s). The probability that any other X n is
jointly typical with U n (s) is less than 2ân(I (U ;X)â3) , and therefore
the probability of this kind of error is bounded above by
ân(I (X;U )â3)
|B(i) â© Aâ(n)
â¤ 2n(H (X)+) 2ânR1 2ân(I (X;U )â3) ,
 (X)|2
(15.286)
which goes to 0 if R1 > H (X|U ).

Hence, it is likely that the actual source sequence X n is jointly typical
with U n (s) and that no other typical source sequence in the same bin is
also jointly typical with U n (s). We can achieve an arbitrarily low probability of error with an appropriate choice of n and , and this completes

the proof of achievability.
15.9

RATE DISTORTION WITH SIDE INFORMATION

We know that R(D) bits are sufï¬cient to describe X within distortion D.
We now ask how many bits are required given side information Y .
We begin with a few deï¬nitions. Let (Xi , Yi ) be i.i.d. â¼p(x, y) and
encoded as shown in Figure 15.33.

15.9 RATE DISTORTION WITH SIDE INFORMATION

Encoder

X

R

581

^

Decoder

X

^

Ed (X, X ) = D

Y

FIGURE 15.33. Rate distortion with side information.

Deï¬nition The rate distortion function with side information RY (D)
is deï¬ned as the minimum rate required to achieve distortion D if the
side information Y is available to the decoder. Precisely, RY (D) is the
inï¬mum of rates R such that there exist maps in : X n â {1, . . . , 2nR },
gn : Y n Ã {1, . . . , 2nR } â XË n such that
lim sup Ed(X n , gn (Y n , in (X n ))) â¤ D.

(15.287)

nââ

Clearly, since the side information can only help, we have RY (D) â¤
R(D). For the case of zero distortion, this is the SlepianâWolf problem
and we will need H (X|Y ) bits. Hence, RY (0) = H (X|Y ). We wish to
determine the entire curve RY (D). The result can be expressed in the
following theorem.
Theorem 15.9.1 (Rate distortion with side information (Wyner and Ziv))
Let (X, Y ) be drawn i.i.d. â¼ p(x, y) and let d(x n , xÌ n )
= n1 ni=1 d(xi , xÌi ) be given. The rate distortion function with side information is
RY (D) = min min (I (X; W ) â I (Y ; W ))
p(w|x) f

(15.288)

where the minimization is over all functions f : Y Ã W â XÌ and conditional probability mass functions p(w|x), |W| â¤ |X| + 1, such that

p(x, y)p(w|x)d(x, f (y, w)) â¤ D.
(15.289)
x

w

y

The function f in the theorem corresponds to the decoding map that
maps the encoded version of the X symbols and the side information Y to
the output alphabet. We minimize over all conditional distributions on W
and functions f such that the expected distortion for the joint distribution
is less than D.
We ï¬rst prove the converse after considering some of the properties of
the function RY (D) deï¬ned in (15.288).

582

NETWORK INFORMATION THEORY

Lemma 15.9.1
The rate distortion function with side information
RY (D) deï¬ned in (15.288) is a nonincreasing convex function of D.
Proof: The monotonicity of RY (D) follows immediately from the fact
that the domain of minimization in the deï¬nition of RY (D) increases with
D. As in the case of rate distortion without side information, we expect
RY (D) to be convex. However, the proof of convexity is more involved
because of the double rather than single minimization in the deï¬nition of
RY (D) in (15.288). We outline the proof here.
Let D1 and D2 be two values of the distortion and let W1 , f1 and
W2 , f2 be the corresponding random variables and functions that achieve
the minima in the deï¬nitions of RY (D1 ) and RY (D2 ), respectively. Let
Q be a random variable independent of X, Y, W1 , and W2 which takes on
the value 1 with probability Î» and the value 2 with probability 1 â Î».
Deï¬ne W = (Q, WQ ) and let f (W, Y ) = fQ (WQ , Y ). Speciï¬cally,
f (W, Y ) = f1 (W1 , Y ) with probability Î» and f (W, Y ) = f2 (W2 , Y ) with
probability 1 â Î». Then the distortion becomes
D = Ed(X, XÌ)

(15.290)

= Î»Ed(X, f1 (W1 , Y )) + (1 â Î»)Ed(X, f2 (W2 , Y ))

(15.291)

= Î»D1 + (1 â Î»)D2 ,

(15.292)

and (15.288) becomes
I (W ; X) â I (W ; Y ) = H (X) â H (X|W ) â H (Y ) + H (Y |W )
(15.293)
= H (X) â H (X|WQ , Q) â H (Y ) + H (Y |WQ , Q)
(15.294)
= H (X) â Î»H (X|W1 ) â (1 â Î»)H (X|W2 )
â H (Y ) + Î»H (Y |W1 ) + (1 â Î»)H (Y |W2 )
(15.295)
= Î» (I (W1 , X) â I (W1 ; Y ))
+ (1 â Î») (I (W2 , X) â I (W2 ; Y )) , (15.296)
and hence
RY (D) = min (I (U ; X) â I (U ; Y ))

(15.297)

â¤ I (W ; X) â I (W ; Y )

(15.298)

U :Edâ¤D

15.9 RATE DISTORTION WITH SIDE INFORMATION

583

= Î» (I (W1 , X) â I (W1 ; Y )) + (1 â Î») (I (W2 , X) â I (W2 ; Y ))
= Î»RY (D1 ) + (1 â Î»)RY (D2 ),
proving the convexity of RY (D).

(15.299)


We are now in a position to prove the converse to the conditional rate
distortion theorem.
Proof: (Converse to Theorem 15.9.1). Consider any rate distortion code
with side information. Let the encoding function be fn : X n â {1, 2, . . . ,
2nR }. Let the decoding function be gn : Y n Ã {1, 2, . . . , 2nR } â XË n , and
let gni : Y n Ã {1, 2, . . . , 2nR } â XÌ denote the ith symbol produced by the
decoding function. Let T = fn (X n ) denote the encoded version of X n .
We must show that if Ed(X n , gn (Y n , fn (X n ))) â¤ D, then R â¥ RY (D).
We have the following chain of inequalities:
(a)
nR â¥ H (T )
(b)

(15.300)

â¥ H (T |Y n )

(15.301)

â¥ I (X n ; T |Y n )

(15.302)

(c)

=

n


I (Xi ; T |Y n , X iâ1 )

(15.303)

H (Xi |Y n , X iâ1 ) â H (Xi |T , Y n , X iâ1 )

(15.304)

i=1

=

n

i=1

(d)

=

n


n
H (Xi |Yi ) â H (Xi |T , Y iâ1 , Yi , Yi+1
, X iâ1 )

(15.305)

n
H (Xi |Yi ) â H (Xi |T , Y iâ1 , Yi , Yi+1
)

(15.306)

H (Xi |Yi ) â H (Xi |Wi , Yi )

(15.307)

i=1
(e)

â¥

n

i=1

(f)

=

n

i=1

(g)

=

n

i=1

I (Xi ; Wi |Yi )

(15.308)

584

NETWORK INFORMATION THEORY

=

n


H (Wi |Yi ) â H (Wi |Xi , Yi )

(15.309)

i=1
(h)

=

n


H (Wi |Yi ) â H (Wi |Xi )

(15.310)

H (Wi ) â H (Wi |Xi ) â H (Wi ) + H (Wi |Yi )

(15.311)

I (Wi ; Xi ) â I (Wi ; Yi )

(15.312)


RY (Ed(Xi , gni
(Wi , Yi )))

(15.313)

i=1

=

n

i=1

=

n

i=1

(i)

â¥

n

i=1

1

=n
RY (Ed(Xi , gni
(Wi , Yi )))
n
i=1

 n
(j)
1

â¥ nRY
Ed(Xi , gni
(Wi , Yi ))
n
n

(15.314)

(15.315)

i=1

(k)

â¥ nRY (D) ,

(15.316)

where
(a) follows from the fact that the range of T is {1, 2, . . . , 2nR }
(b) follows from the fact that conditioning reduces entropy
(c) follows from the chain rule for mutual information
(d) follows from the fact that Xi is independent of the past and future
Y âs and Xâs given Yi
(e) follows from the fact that conditioning reduces entropy
n
(f) follows by deï¬ning Wi = (T , Y iâ1 , Yi+1
)
(g) follows from the deï¬nition of mutual information
(h) follows from the fact that since Yi depends only on Xi and is conditionally independent of T and the past and future Y âs, Wi â Xi â
Yi forms a Markov chain
(i) follows from the deï¬nition of the (information) conditional
 
rate distortion function since XÌi = gni (T , Y n ) = gni
(Wi , Yi ),
and hence I (Wi ; Xi ) â I (Wi ; Yi ) â¥ minW :Ed(X,XÌ)â¤Di I (W ; X) â
I (W ; Y ) = RY (Di )

15.9 RATE DISTORTION WITH SIDE INFORMATION

585

(j) follows from Jensenâs inequality and the convexity of the conditional
rate distortion function (Lemma 15.9.1) 
(k) follows from the deï¬nition of D = E[ n1 ni=1 d(Xi , XÌi )] 
It is easy to see the parallels between this converse and the converse
for rate distortion without side information (Section 10.4). The proof of
achievability is also parallel to the proof of the rate distortion theorem
using strong typicality. However, instead of sending the index of the
codeword that is jointly typical with the source, we divide these codewords
into bins and send the bin index instead. If the number of codewords in
each bin is small enough, the side information can be used to isolate
the particular codeword in the bin at the receiver. Hence again we are
combining random binning with rate distortion encoding to ï¬nd a jointly
typical reproduction codeword. We outline the details of the proof below.
Proof: (Achievability of Theorem
15.9.1). Fix p(w|x) and the function

f (w, y). Calculate p(w) = x p(x)p(w|x).
Generation of codebook:
Let R1 = I (X; W ) + . Generate 2nR i.i.d.


n
codewords W n (s) â¼ i=1 p(wi ), and index them by s â {1, 2, . . . , 2nR1 }.
Let R2 = I (X; W ) â I (Y ; W ) + 5. Randomly assign the indices s â
{1, 2, . . . , 2nR1 } to one of 2nR2 bins using a uniform distribution over
the bins. Let B(i) denote the indices assigned to bin i. There are approximately 2n(R1 âR2 ) indices in each bin.
Encoding: Given a source sequence X n , the encoder looks for a coden
word W n (s) such that (X n , W n (s)) â Aâ(n)
 . If there is no such W , the
encoder sets s = 1. If there is more than one such s, the encoder uses the
lowest s. The encoder sends the index of the bin in which s belongs.
Decoding: The decoder looks for a W n (s) such that s â B(i) and
n
(W n (s), Y n ) â Aâ(n)
 . If he ï¬nds a unique s, he then calculates XÌ , where
XÌi = f (Wi , Yi ). If he does not ï¬nd any such s or more than one such s,
he sets XÌ n = xÌ n , where xÌ n is an arbitrary sequence in XË n . It does not
matter which default sequence is used; we will show that the probability
of this event is small.
Analysis of the probability of error: As usual, we have various error
events:
1. The pair (X n , Y n ) â
/ Aâ(n)
 . The probability of this event is small for
large enough n by the weak law of large numbers.
2. The sequence X n is typical, but there does not exist an s such that
(X n , W n (s)) â Aâ(n)
 . As in the proof of the rate distortion theorem,

586

NETWORK INFORMATION THEORY

the probability of this event is small if
R1 > I (W ; X).

(15.317)

3. The pair of sequences (X n , W n (s)) â Aâ(n)
but (W n (s), Y n ) â
/ Aâ(n)


n
(i.e., the codeword is not jointly typical with the Y sequence). By
the Markov lemma (Lemma 15.8.1), the probability of this event is
small if n is large enough.
4. There exists another s 
 with the same bin index such that (W n (s 
 ),
n
Y n ) â Aâ(n)
 . Since the probability that a randomly chosen W is
jointly typical with Y n is â 2ânI (Y ;W ) , the probability that there is
another W n in the same bin that is typical with Y n is bounded by
the number of codewords in the bin times the probability of joint
typicality, that is,
n(R1 âR2 ) ân(I (W ;Y )â3)
Pr(âs 
 â B(i) : (W n (s 
 ), Y n ) â Aâ(n)
2
,
 )â¤2
(15.318)
which goes to zero since R1 â R2 < I (Y ; W ) â 3.
5. If the index s is decoded correctly, (X n , W n (s)) â Aâ(n)
 . By item 1
.
Thus,
by
the
Markov
lemma,
we can assume that (X n , Y n ) â Aâ(n)

n
n
n
â(n)
we have (X , Y , W ) â A and therefore the empirical joint distribution is close to the original distribution p(x, y)p(w|x) that we
started with, and hence (X n , XÌ n ) will have a joint distribution that
is close to the distribution that achieves distortion D.

Hence with high probability, the decoder will produce XÌ n such that the
distortion between X n and XÌ n is close to nD. This completes the proof

of the theorem.
The reader is referred to Wyner and Ziv [574] for details of the proof.
After the discussion of the various situations of compressing distributed
data, it might be expected that the problem is almost completely solved,
but unfortunately, this is not true. An immediate generalization of all
the above problems is the rate distortion problem for correlated sources,
illustrated in Figure 15.34. This is essentially the SlepianâWolf problem
with distortion in both X and Y . It is easy to see that the three distributed source coding problems considered above are all special cases
of this setup. Unlike the earlier problems, though, this problem has not
yet been solved and the general rate distortion region remains
unknown.

15.10

Xn

GENERAL MULTITERMINAL NETWORKS

587

Encoder 1

i(x n) â 2nR1

Decoder

Yn

Encoder 2

^

^

(X n, Y n )

j(y n) â 2nR2

FIGURE 15.34. Rate distortion for two correlated sources.

15.10

GENERAL MULTITERMINAL NETWORKS

We conclude this chapter by considering a general multiterminal network
of senders and receivers and deriving some bounds on the rates achievable
for communication in such a network. A general multiterminal network
is illustrated in Figure 15.35. In this section, superscripts denote node
indices and subscripts denote time indices. There are m nodes, and node
i has an associated transmitted variable X (i) and a received variable Y (i) .

S

Sc

(X (m),Y (m))
(X (1),Y (1))

FIGURE 15.35. General multiterminal network.

588

NETWORK INFORMATION THEORY

The node i sends information at rate R (ij ) to node j . We assume that all
the messages W (ij ) being sent from node i to node j are independent and
(ij )
uniformly distributed over their respective ranges {1, 2, . . . , 2nR }.
The channel is represented by the channel transition function
p(y (1) , . . . , y (m) |x (1) , . . . , x (m) ), which is the conditional probability mass
function of the outputs given the inputs. This probability transition function captures the effects of the noise and the interference in the network.
The channel is assumed to be memoryless (i.e., the outputs at any time
instant depend only the current inputs and are conditionally independent
of the past inputs).
Corresponding to each transmitterâreceiver node pair is a message
(ij )
W (ij ) â {1, 2, . . . , 2nR }. The input symbol X (i) at node i depends on
W (ij ) , j â {1, . . . , m} and also on the past values of the received symbol
Y (i) at node i. Hence, an encoding scheme of block length n consists of
a set of encoding and decoding functions, one for each node:
â¢

â¢

(i)
Encoders: Xk(i) (W (i1) , W (i2) , . . . , W (im) , Y1(i) , Y2(i) , . . . , Ykâ1
), k = 1,
. . . , n. The encoder maps the messages and past received symbols
into the symbol Xk(i) transmitted at time k.


Decoders: WÌ (j i) Y1(i) , . . . , Yn(i) , W (i1) , . . . , W (im) , j = 1, 2, . . . , m.
The decoder j at node i maps the received symbols in each block and
his own transmitted information to form estimates of the messages
intended for him from node j , j = 1, 2, . . . , m.

Associated with every pair of nodes is a rate and a corresponding
probability of error that the message will not be decoded correctly,
Pe(n)

(ij )





= Pr WÌ (ij ) Y(j ) , W (j 1) , . . . , W (j m) = W (ij ) ,

(ij )

(15.319)

is deï¬ned under the assumption that all the messages are
where Pe(n)
independent and distributed uniformly over their respective ranges.
A set of rates {R (ij ) } is said to be achievable if there exist encoders and
(ij )
decoders with block length n with Pe(n) â 0 as n â â for all i, j â
{1, 2, . . . , m}. We use this formulation to derive an upper bound on the
ï¬ow of information in any multiterminal network. We divide the nodes
into two sets, S and the complement S c . We now bound the rate of ï¬ow
of information from nodes in S to nodes in S c . See [514]

15.10

GENERAL MULTITERMINAL NETWORKS

589

Theorem 15.10.1 If the information rates {R (ij ) } are achievable, there
exists some joint probability distribution p(x (1) , x (2) , . . . , x (m) ) such that


c

c

R (ij ) â¤ I (X (S); Y (S ) |X (S ) )

(15.320)

iâS,j âS c

for all S â {1, 2, . . . , m}. Thus, the total rate of ï¬ow of information across
cut sets is bounded by the conditional mutual information.
Proof: The proof follows the same lines as the proof of the converse
for the multiple access channel. Let T = {(i, j ) : i â S, j â S c } be the set
of links that cross from S to S c , and let T c be all the other links in the
network. Then


n

R (ij )

(15.321)

iâS,j âS c
( a)

=





H W (ij )

(15.322)

iâS,j âS c



= H W (T )


c
( c)
= H W (T ) |W (T )


c
c
c
= I W (T ) ; Y1(S ) , . . . , Yn(S ) |W (T )


(T ) (S c )
(S c )
(T c )
+ H W |Y1 , . . . , Yn , W
(b)

(15.323)
(15.324)
(15.325)
(15.326)



(S c )
(T )
(S c )
(T c )
â¤ I W ; Y1 , . . . , Yn |W
+ nn

(15.327)

n



c
c
c
(S c )
=
I W (T ) ; Yk(S ) |Y1(S ) , . . . , Ykâ1
, W (T ) + nn

(15.328)

(d)

( e)

k=1
(f)

=

n



 c
c
c
(S c )
H Yk(S ) |Y1(S ) , . . . , Ykâ1
, W (T )

k=1

 c

c
c
(S c )
â H Yk(S ) |Y1(S ) , . . . , Ykâ1
, W (T ) , W (T ) + nn

(15.329)

590
(g)

â¤

NETWORK INFORMATION THEORY

n


 c

c
c
c
(S c )
H Yk(S ) |Y1(S ) , . . . , Ykâ1
, W (T ) , Xk(S )

k=1

 c

c
c
c
(S c )
â H Yk(S ) |Y1(S ) , . . . , Ykâ1
, W (T ) , W (T ) , Xk(S) , Xk(S ) + nn

n

 c

 c
(h) 
c
c
â¤
H Yk(S ) |Xk(S ) â H Yk(S ) |Xk(S ) , Xk(S) + nn

(15.330)
(15.331)

k=1

=

n



c
c
I Xk(S) ; Yk(S ) |Xk(S ) + nn

(15.332)

k=1


1   (S) (S c ) (S c )
=n
I XQ ; YQ |XQ , Q = k + nn
n

(i)

n

(15.333)

k=1



c
(S)
(S c )
= nI XQ
; YQ(S ) |XQ
, Q + nn
(15.334)
  c

 c


(S c )
(S)
(S c )
= n H YQ(S ) |XQ
, Q â H YQ(S ) |XQ
, XQ
, Q + nn (15.335)

(j)


 c
  c


(S c )
(S)
(S c )
â H YQ(S ) |XQ
â¤ n H YQ(S ) |XQ
, XQ
, Q + nn

(k)

  c

 c


(S c )
(S)
(S c )
= n H YQ(S ) |XQ
, XQ
â H YQ(S ) |XQ
+ nn


c
(S)
(S c )
+ nn ,
; YQ(S ) |XQ
= nI XQ

(l)

(15.336)
(15.337)
(15.338)

where
(a) follows from the fact that the messages W (ij ) are uniformly dis(ij )
tributed over their respective ranges {1, 2, . . . , 2nR }
(b) follows from the deï¬nition of W (T ) = {W (ij ) : i â S, j â S c } and
the fact that the messages are independent
(c) follows from the independence of the messages for T and T c
(d) follows from Fanoâs inequality since the messages W (T ) can be
c
decoded from Y (S) and W (T )
(e) is the chain rule for mutual information
(f) follows from the deï¬nition of mutual information
c
(g) follows from the fact that Xk(S ) is a function of the past received
c
c
symbols Y (S ) and the messages W (T ) and the fact that adding
conditioning reduces the second term

15.10

GENERAL MULTITERMINAL NETWORKS

591

c

(h) follows from the fact that Yk(S ) depends only on the current input
c
symbols Xk(S) and Xk(S )
(i) follows after we introduce a new timesharing random variable Q
distributed uniformly on {1, 2, . . . , n}
(j) follows from the deï¬nition of mutual information
(k) follows from the fact that conditioning reduces entropy
c
(S)
(l) follows from the fact that YQ(S ) depends only on the inputs XQ
and
(S
XQ

c)

and is conditionally independent of Q
c

Thus, there exist random variables X (S) and X (S ) with some arbitrary

joint distribution that satisfy the inequalities of the theorem.
The theorem has a simple max-ï¬ow min-cut interpretation. The rate of
ï¬ow of information across any boundary is less than the mutual information between the inputs on one side of the boundary and the outputs on
the other side, conditioned on the inputs on the other side.
The problem of information ï¬ow in networks would be solved if the
bounds of the theorem were achievable. But unfortunately, these bounds
are not achievable even for some simple channels. We now apply these
bounds to a few of the channels that we considered earlier.
â¢

â¢

Multiple-access channel. The multiple access channel is a network
with many input nodes and one output node. For the case of a two-user
multiple-access channel, the bounds of Theorem 15.10.1 reduce to
R1 â¤ I (X1 ; Y |X2 ),

(15.339)

R2 â¤ I (X2 ; Y |X1 ),

(15.340)

R1 + R2 â¤ I (X1 , X2 ; Y )

(15.341)

for some joint distribution p(x1 , x2 )p(y|x1 , x2 ). These bounds coincide with the capacity region if we restrict the input distribution to
be a product distribution and take the convex hull (Theorem 15.3.1).
Relay channel. For the relay channel, these bounds give the upper
bound of Theorem 15.7.1 with different choices of subsets as shown
in Figure 15.36. Thus,
C â¤ sup min {I (X, X1 ; Y ), I (X; Y, Y1 |X1 )} .

(15.342)

p(x,x1 )

This upper bound is the capacity of a physically degraded relay channel and for the relay channel with feedback [127].

592

NETWORK INFORMATION THEORY

S1

Y1 : X1

S2

X

Y

FIGURE 15.36. Relay channel.

U

X1
p(y |x1, x2)

V

Y

^ ^

(U, V )

X2

FIGURE 15.37. Transmission of correlated sources over a multiple-access channel.

To complement our discussion of a general network, we should mention
two features of single-user channels that do not apply to a multiuser
network.
â¢

Sourceâchannel separation theorem. In Section 7.13 we discussed
the sourceâchannel separation theorem, which proves that we can
transmit the source noiselessly over the channel if and only if the
entropy rate is less than the channel capacity. This allows us to characterize a source by a single number (the entropy rate) and the channel
by a single number (the capacity). What about the multiuser case?
We would expect that a distributed source could be transmitted over
a channel if and only if the rate region for the noiseless coding of the
source lay within the capacity region of the channel. To be speciï¬c,
consider the transmission of a distributed source over a multipleaccess channel, as shown in Figure 15.37. Combining the results of
SlepianâWolf encoding with the capacity results for the multipleaccess channel, we can show that we can transmit the source over
the channel and recover it with a low probability of error if
H (U |V ) â¤ I (X1 ; Y |X2 , Q),

(15.343)

15.10

â¢

GENERAL MULTITERMINAL NETWORKS

593

H (V |U ) â¤ I (X2 ; Y |X1 , Q),

(15.344)

H (U, V ) â¤ I (X1 , X2 ; Y |Q)

(15.345)

for some distribution p(q)p(x1 |q)p(x2 |q)p(y|x1 , x2 ). This condition
is equivalent to saying that the SlepianâWolf rate region of the source
has a nonempty intersection with the capacity region of the multipleaccess channel.
But is this condition also necessary? No, as a simple example illustrates. Consider the transmission of the source of Example 15.4.2
over the binary erasure multiple-access channel (Example 15.3.3).
The SlepianâWolf region does not intersect the capacity region, yet
it is simple to devise a scheme that allows the source to be transmitted over the channel. We just let X1 = U and X2 = V , and the value
of Y will tell us the pair (U, V ) with no error. Thus, the conditions
(15.345) are not necessary.
The reason for the failure of the sourceâchannel separation theorem
lies in the fact that the capacity of the multiple-access channel
increases with the correlation between the inputs of the channel.
Therefore, to maximize the capacity, one should preserve the correlation between the inputs of the channel. SlepianâWolf encoding,
on the other hand, gets rid of the correlation. Cover et al. [129] proposed an achievable region for transmission of a correlated source
over a multiple access channel based on the idea of preserving the
correlation. Han and Costa [273] have proposed a similar region for
the transmission of a correlated source over a broadcast channel.
Capacity regions with feedback. Theorem 7.12.1 shows that feedback
does not increase the capacity of a single-user discrete memoryless
channel. For channels with memory, on the other hand, feedback
enables the sender to predict something about the noise and to combat
it more effectively, thus increasing capacity.
What about multiuser channels? Rather surprisingly, feedback does
increase the capacity region of multiuser channels, even when the
channels are memoryless. This was ï¬rst shown by Gaarder and Wolf
[220], who showed how feedback helps increase the capacity of the
binary erasure multiple-access channel. In essence, feedback from the
receiver to the two senders acts as a separate channel between the two
senders. The senders can decode each otherâs transmissions before the
receiver does. They then cooperate to resolve the uncertainty at the
receiver, sending information at the higher cooperative capacity rather
than the noncooperative capacity. Using this scheme, Cover and
Leung [133] established an achievable region for a multiple-access

594

NETWORK INFORMATION THEORY

channel with feedback. Willems [557] showed that this region was
the capacity for a class of multiple-access channels that included the
binary erasure multiple-access channel. Ozarow [410] established the
capacity region for a two-user Gaussian multiple-access channel. The
problem of ï¬nding the capacity region for a multiple-access channel
with feedback is closely related to the capacity of a two-way channel
with a common output.
There is as yet no uniï¬ed theory of network information ï¬ow. But there
can be no doubt that a complete theory of communication networks would
have wide implications for the theory of communication and computation.

SUMMARY
Multiple-access channel. The capacity of a multiple-access channel
(X1 Ã X2 , p(y|x1 , x2 ), Y) is the closure of the convex hull of all (R1 , R2 )
satisfying
R1 < I (X1 ; Y |X2 ),

(15.346)

R2 < I (X2 ; Y |X1 ),

(15.347)

R1 + R2 < I (X1 , X2 ; Y )

(15.348)

for some distribution p1 (x1 )p2 (x2 ) on X1 Ã X2 .
The capacity region of the m-user multiple-access channel is the closure
of the convex hull of the rate vectors satisfying
R(S) â¤ I (X(S); Y |X(S c ))

for all S â {1, 2, . . . , m}

(15.349)

for some product distribution p1 (x1 )p2 (x2 ) Â· Â· Â· pm (xm ).
Gaussian multiple-access channel. The capacity region of a two-user
Gaussian multiple-access channel is
 
P1
R1 â¤ C
,
(15.350)
N
 
P2
R2 â¤ C
,
(15.351)
N

SUMMARY


R1 + R2 â¤ C
where
C(x) =


P1 + P2
,
N

1
log(1 + x).
2

595

(15.352)

(15.353)

SlepianâWolf coding. Correlated sources X and Y can be described
separately at rates R1 and R2 and recovered with arbitrarily low probability of error by a common decoder if and only if
R1 â¥ H (X|Y ),

(15.354)

R2 â¥ H (Y |X),

(15.355)

R1 + R2 â¥ H (X, Y ).

(15.356)

Broadcast channels. The capacity region of the degraded broadcast
channel X â Y1 â Y2 is the convex hull of the closure of all (R1 , R2 )
satisfying
R2 â¤ I (U ; Y2 ),

(15.357)

R1 â¤ I (X; Y1 |U )

(15.358)

for some joint distribution p(u)p(x|u)p(y1 , y2 |x).
Relay channel. The capacity C of the physically degraded relay channel p(y, y1 |x, x1 ) is given by
C = sup min {I (X, X1 ; Y ), I (X; Y1 |X1 )} ,

(15.359)

p(x,x1 )

where the supremum is over all joint distributions on X Ã X1 .
Source coding with side information. Let (X, Y ) â¼ p(x, y). If Y is
encoded at rate R2 and X is encoded at rate R1 , we can recover X with
an arbitrarily small probability of error iff
R1 â¥ H (X|U ),

(15.360)

R2 â¥ I (Y ; U )

(15.361)

for some distribution p(y, u) such that X â Y â U .

596

NETWORK INFORMATION THEORY

Rate distortion with side information. Let (X, Y ) â¼ p(x, y). The
rate distortion function with side information is given by
RY (D) = min

min

p(w|x) f :Y ÃW âXÌ

I (X; W ) â I (Y ; W ),

(15.362)

where the minimization is over all functions f and conditional distributions p(w|x), |W| â¤ |X| + 1, such that

p(x, y)p(w|x)d(x, f (y, w)) â¤ D.
(15.363)
x

w

y

PROBLEMS
15.1

Cooperative capacity of a multiple-access channel

X1
(W1, W2)

p(y |x1, x2)

Y

^

^

(W1, W2)

X2

(a) Suppose that X1 and X2 have access to both indices W1 â
{1, 2nR }, W2 â {1, 2nR2 }. Thus, the codewords X1 (W1 ,
W2 ), X2 (W1 , W2 ) depend on both indices. Find the capacity
region.
(b) Evaluate this region for the binary erasure multiple access
channel Y = X1 + X2 , Xi â {0, 1}. Compare to the noncooperative region.
15.2

Capacity of multiple-access channels. Find the capacity region
for each of the following multiple-access channels:
(a) Additive modulo 2 multiple-access channel. X1 â {0, 1},
X2 â {0, 1}, Y = X1 â X2 .
(b) Multiplicative multiple-access channel. X1 â {â1, 1},
X2 â {â1, 1}, Y = X1 Â· X2 .

PROBLEMS

15.3

597

Cut-set interpretation of capacity region of multiple-access channel . For the multiple-access channel we know that (R1 , R2 ) is
achievable if
R1 < I (X1 ; Y | X2 ),

(15.364)

R2 < I (X2 ; Y | X1 ),

(15.365)

R1 + R2 < I (X1 , X2 ; Y )

(15.366)

for X1 , X2 independent. Show, for X1 , X2 independent that
I (X1 ; Y | X2 ) = I (X1 ; Y, X2 ).

S1
S3

X1

Y

X2
S2

Interpret the information bounds as bounds on the rate of ï¬ow
across cut sets S1 , S2 , and S3 .
15.4

Gaussian multiple-access channel capacity. For the AWGN
multiple-access channel, prove, using typical sequences, the
achievability of any rate pairs (R1 , R2 ) satisfying


P1
1
,
(15.367)
R1 < log 1 +
2
N


1
P2
R2 < log 1 +
,
(15.368)
2
N


P 1 + P2
1
.
(15.369)
R1 + R2 < log 1 +
2
N

598

NETWORK INFORMATION THEORY

The proof extends the proof for the discrete multiple-access channel in the same way as the proof for the single-user Gaussian
channel extends the proof for the discrete single-user channel.
15.5

Converse for the Gaussian multiple-access channel . Prove the
converse for the Gaussian multiple-access channel by extending
the converse in the discrete case to take into account the power
constraint on the codewords.

15.6

Unusual multiple-access channel . Consider the following
multiple-access channel: X1 = X2 = Y = {0, 1}. If (X1 , X2 ) =
(0, 0), then Y = 0. If (X1 , X2 ) = (0, 1), then Y = 1. If (X1 , X2 ) =
(1, 0), then Y = 1. If (X1 , X2 ) = (1, 1), then Y = 0 with probability 12 and Y = 1 with probability 12 .
(a) Show that the rate pairs (1,0) and (0,1) are achievable.
(b) Show that for any nondegenerate distribution p(x1 )p(x2 ), we
have I (X1 , X2 ; Y ) < 1.
(c) Argue that there are points in the capacity region of this
multiple-access channel that can only be achieved by timesharing; that is, there exist achievable rate pairs (R1 , R2 ) that
lie in the capacity region for the channel but not in the region
deï¬ned by
R1 â¤ I (X1 ; Y |X2 ),

(15.370)

R2 â¤ I (X2 ; Y |X1 ),

(15.371)

R1 + R2 â¤ I (X1 , X2 ; Y )

(15.372)

for any product distribution p(x1 )p(x2 ). Hence the operation
of convexiï¬cation strictly enlarges the capacity region. This
channel was introduced independently by CsiszaÌr and KoÌrner
[149] and Bierbaum and Wallmeier [59].
15.7

Convexity of capacity region of broadcast channel . Let C â R2
be the capacity region of all achievable rate pairs R = (R1 , R2 )
for the broadcast channel. Show that C is a convex set by using
a time-sharing argument. Speciï¬cally, show that if R(1) and R(2)
are achievable, Î»R(1) + (1 â Î»)R(2) is achievable for 0 â¤ Î» â¤ 1.

15.8

SlepianâWolf for deterministically related sources. Find and
sketch the SlepianâWolf rate region for the simultaneous data
compression of (X, Y ), where y = f (x) is some deterministic
function of x.

PROBLEMS

599

15.9

SlepianâWolf . Let Xi be i.i.d. Bernoulli(p). Let Zi be i.i.d. â¼
Bernoulli(r), and let Z be independent of X. Finally, let Y =
X â Z (mod 2 addition). Let X be described at rate R1 and Y
be described at rate R2 . What region of rates allows recovery of
X, Y with probability of error tending to zero?

15.10

Broadcast capacity depends only on the conditional marginals.
Consider the general broadcast channel (X, Y1 Ã Y2 , p(y1 , y2 | x)).
Show that the capacity region depends only on p(y1 | x) and p(y2 |
x). To do this, for any given ((2nR1 , 2nR2 ), n) code, let
P1(n) = P {WÌ1 (Y1 ) = W1 },

(15.373)

P2(n) = P {WÌ2 (Y2 ) = W2 },

(15.374)

P (n) = P {(WÌ1 , WÌ2 ) = (W1 , W2 )}.

(15.375)

Then show that
max{P1(n) , P2(n) } â¤ P (n) â¤ P1(n) + P2(n) .
The result now follows by a simple argument. (Remark: The
probability of error P (n) does depend on the conditional joint
distribution p(y1 , y2 | x). But whether or not P (n) can be driven
to zero [at rates (R1 , R2 )] does not [except through the conditional
marginals p(y1 | x), p(y2 | x)] .)
15.11

Converse for the degraded broadcast channel. The following
chain of inequalities proves the converse for the degraded discrete memoryless broadcast channel. Provide reasons for each of
the labeled inequalities.
Setup for converse for degraded broadcast channel capacity:
(W1 , W2 )indep. â X n (W1 , W2 ) â Y1n â Y2n .
â¢
â¢

Encoding fn : 2nR1 Ã 2nR2 â X n
Decoding:
gn : Y1n â 2nR1 , hn : Y2n â 2nR2 .
iâ1
(W2 , Y1 ). Then
Â·

nR2 â¤Fano I (W2 ; Y2n )
( a)

=

n

i=1

I (W2 ; Y2i | Y2iâ1 )

Let

Ui =

(15.376)
(15.377)

600

NETWORK INFORMATION THEORY

(b)

=


(H (Y2i | Y2iâ1 ) â H (Y2i | W2 , Y2iâ1 ))

(15.378)

i
( c) 
â¤
(H (Y2i ) â H (Y2i | W2 , Y2iâ1 , Y1iâ1 ))

(15.379)

i

=


(H (Y2i ) â H (Y2i | W2 , Y1iâ1 ))

( e)

n


(d)

(15.380)

i

=

I (Ui ; Y2i ).

(15.381)

i=1

Continuation of converse: Give reasons for the labeled inequalities:
Â·

nR1 â¤Fano I (W1 ; Y1n )
(f)
â¤ I (W1 ; Y1n , W2 )
(g)

â¤ I (W1 ; Y1n | W2 )

(h)

=

n


(15.382)
(15.383)
(15.384)

I (W1 ; Y1i | Y1iâ1 , W2 )

(15.385)

I (Xi ; Y1i | Ui ).

(15.386)

iâ1
( i)

â¤

n

i=1

Now let Q be a time-sharing random variable with Pr(Q = i) =
1/n, i = 1, 2, . . . , n. Justify the following:
R1 â¤ I (XQ ; Y1Q |UQ , Q),

(15.387)

R2 â¤ I (UQ ; Y2Q |Q)

(15.388)

for some distribution p(q)p(u|q)p(x|u, q)p(y1 , y2 |x). By appropriately redeï¬ning U , argue that this region is equal to the convex
closure of regions of the form
R1 â¤ I (X; Y1 |U ),

(15.389)

R2 â¤ I (U ; Y2 )

(15.390)

for some joint distribution p(u)p(x|u)p(y1 , y2 |x).

PROBLEMS

15.12

601

Capacity points.
(a) For the degraded broadcast channel X â Y1 â Y2 , ï¬nd the
points a and b where the capacity region hits the R1 and R2
axes.
R2
b

a

R1

(b) Show that b â¤ a.
15.13

Degraded broadcast channel . Find the capacity region for the
degraded broadcast channel shown below.
1âp

1âa

a

p
X

p

1âp

15.14

Y2

Y1
a

1âa

Channels with unknown parameters. We are given a binary
symmetric channel with parameter p. The capacity is C = 1 â
H (p). Now we change the problem slightly. The receiver knows
only that p â {p1 , p2 } (i.e., p = p1 or p = p2 , where p1 and p2
are given real numbers). The transmitter knows the actual value
of p. Devise two codes for use by the transmitter, one to be used
if p = p1 , the other to be used if p = p2 , such that transmission
to the receiver can take place at rate â C(p1 ) if p = p1 and at
rate â C(p2 ) if p = p2 . (Hint: Devise a method for revealing
p to the receiver without affecting the asymptotic rate. Preï¬xing
the codeword by a sequence of 1âs of appropriate length should
work.)

602

15.15

NETWORK INFORMATION THEORY

Two-way channel . Consider the two-way channel shown in
Figure 15.6. The outputs Y1 and Y2 depend only on the current
inputs X1 and X2 .
(a) By using independently generated codes for the two senders,
show that the following rate region is achievable:
R1 < I (X1 ; Y2 |X2 ),

(15.391)

R2 < I (X2 ; Y1 |X1 )

(15.392)

for some product distribution p(x1 )p(x2 )p(y1 , y2 |x1 , x2 ).
(b) Show that the rates for any code for a two-way channel with
arbitrarily small probability of error must satisfy
R1 â¤ I (X1 ; Y2 |X2 ),

(15.393)

R2 â¤ I (X2 ; Y1 |X1 )

(15.394)

for some joint distribution p(x1 , x2 )p(y1 , y2 |x1 , x2 ).
The inner and outer bounds on the capacity of the two-way
channel are due to Shannon [486]. He also showed that the inner
bound and the outer bound do not coincide in the case of the
binary multiplying channel X1 = X2 = Y1 = Y2 = {0, 1}, Y1 =
Y2 = X1 X2 . The capacity of the two-way channel is still an open
problem.
15.16

Multiple-access channel .
channel be given by

Let the output Y of a multiple-access

Y = X1 + sgn(X2 ),
where X1 , X2 are both real and power limited,
E(X12 ) â¤ P1 ,
E(X22 ) â¤ P2 ,


1, x > 0,
â1, x â¤ 0.
Note that there is interference but no noise in this channel.
(a) Find the capacity region.
(b) Describe a coding scheme that achieves the capacity region.

and sgn(x) =

PROBLEMS

15.17

SlepianâWolf .
tion p(x, y):

603

Let (X, Y ) have the joint probability mass funcp(x, y)

1

2

1
2
3

Î±
Î²
Î²

Î²
Î±
Î²

3
Î²
Î²
Î±

where Î² = 16 â Î±2 . (Note: This is a joint, not a conditional, probability mass function.)
(a) Find the SlepianâWolf rate region for this source.
(b) What is Pr{X = Y } in terms of Î±?
(c) What is the rate region if Î± = 13 ?
(d) What is the rate region if Î± = 19 ?
15.18

Square channel .
access channel?

What is the capacity of the following multipleX1 â {â1, 0, 1},
X2 â {â1, 0, 1},
Y = X12 + X22 .

(a) Find the capacity region.
(b) Describe p â (x1 ), p â (x2 ) achieving a point on the boundary of
the capacity region.
15.19

SlepianâWolf . Two senders know random variables U1 and U2 ,
respectively. Let the random variables (U1 , U2 ) have the following
joint distribution:
U1 \U2

0

1

2

Â·Â·Â·

mâ1

0
1
2
..
.

Î±

Î²
mâ1

Î²
mâ1

Î²
mâ1

..
.

0
0
..
.

0
0
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

0

0

Â·Â·Â·

0

mâ1

Î³
mâ1
Î³
mâ1
Î³
mâ1

0
0
..
.

where Î± + Î² + Î³ = 1. Find the region of rates (R1 , R2 ) that would
allow a common receiver to decode both random variables reliably.

604

15.20

NETWORK INFORMATION THEORY

Multiple access
(a) Find the capacity region for the multiple-access channel
X

Y = X1 2 ,
where
X1 {2, 4} , X2 {1, 2}.
(b) Suppose that the range of X1 is {1, 2}. Is the capacity region
decreased? Why or why not?
15.21

Broadcast channel .
channel.
0

1 â a1

Consider the following degraded broadcast

0

a1

1 â a2
a2
1

E
a1
1
X

1 â a1

0

E

a2
1
Y1

1 â a2

1
Y2

(a) What is the capacity of the channel from X to Y1 ?
(b) What is the channel capacity from X to Y2 ?
(c) What is the capacity region of all (R1 , R2 ) achievable for this
broadcast channel? Simplify and sketch.
15.22

Stereo. The sum and the difference of the right and left ear signals are to be individually compressed for a common receiver. Let
Z1 be Bernoulli (p1 ) and Z2 be Bernoulli (p2 ) and suppose that
Z1 and Z2 are independent. Let X = Z1 + Z2 , and Y = Z1 â Z2 .
(a) What is the SlepianâWolf rate region of achievable (RX , RY )?
X

RX
Decoder

Y

RY

^

^

( X, Y )

PROBLEMS

605

(b) Is this larger or smaller than the rate region of (RZ1 , RZ2 )?
Why?
Z1

RZ1
^

Z2

^

(Z1, Z2)

Decoder

RZ2

There is a simple way to do this part.
15.23

Multiplicative multiple-access channel . Find and sketch the capacity region of the following multiplicative multiple-access channel:
X1
Y
X2

with X1 â {0, 1}, X2 â {1, 2, 3}, and Y = X1 X2 .
15.24

Distributed data compression. Let Z1 , Z2 , Z3 be independent
Bernoulli(p). Find the SlepianâWolf rate region for the description
of (X1 , X2 , X3 ), where
X1 = Z 1
X2 = Z 1 + Z 2
X3 = Z 1 + Z 2 + Z 3 .
X1

X2

X3

^

^

^

(X1, X2, X3)

606

NETWORK INFORMATION THEORY

15.25

Noiseless multiple-access channel. Consider the following
multiple-access channel with two binary inputs X1 , X2 â {0, 1}
and output Y = (X1 , X2 ).
(a) Find the capacity region. Note that each sender can send at
capacity.
(b) Now consider the cooperative capacity region, R1 â¥ 0, R2 â¥
0, R1 + R2 â¤ maxp(x1 ,x2 ) I (X1 , X2 ; Y ). Argue that the
throughput R1 + R2 does not increase but the capacity region
increases.

15.26

Inï¬nite bandwidth multiple-access channel . Find the capacity
region for the Gaussian multiple-access channel with inï¬nite bandwidth. Argue that all senders can send at their individual capacities
(i.e., inï¬nite bandwidth eliminates interference).

15.27

Multiple-access identity. Let C(x) = 12 log(1 + x) denote the
channel capacity of a Gaussian channel with signal-to-noise ratio
x. Show that
 




P1
P2
P1 + P2
C
+C
=C
.
N
P1 + N
N
This suggests that two independent users can send information as
well as if they had pooled their power.

15.28

Frequency-division multiple access (FDMA). Maximize the
throughput
R1 +R2 = W1 log(1 + NPW1 1 ) + (W â W1 ) log(1 +
P2
N (W âW1 ) ) over W1 to show that bandwidth should be proportional
to transmitted power for FDMA.

15.29

Trilingual-speaker broadcast channel . A speaker of Dutch,
Spanish, and French wishes to communicate simultaneously to
three people: D, S, and F . D knows only Dutch but can distinguish when a Spanish word is being spoken as distinguished from
a French word; similarly for the other two, who know only Spanish and French, respectively, but can distinguish when a foreign
word is spoken and which language is being spoken. Suppose
that each language, Dutch, Spanish, and French, has M words: M
words of Dutch, M words of French, and M words of Spanish.
(a) What is the maximum rate at which the trilingual speaker can
speak to D?
(b) If he speaks to D at the maximum rate, what is the maximum
rate at which he can speak simultaneously to S?

PROBLEMS

607

(c) If he is speaking to D and S at the joint rate in part (b), can
he also speak to F at some positive rate? If so, what is it? If
not, why not?
15.30

Parallel Gaussian channels from a mobile telephone. Assume
that a sender X is sending to two ï¬xed base stations. Assume that
the sender sends a signal X that is constrained to have average
power P . Assume that the two base stations receive signals Y1
and Y2 , where
Y1 = Î±1 X + Z1
Y2 = Î±2 X + Z2 ,
where Zi â¼ N(0, N1 ), Z2 â¼ N(0, N2 ), and Z1 and Z2 are independent. We will assume the Î±âs are constant over a transmitted
block.
(a) Assuming that both signals Y1 and Y2 are available at a common decoder Y = (Y1 , Y2 ), what is the capacity of the channel
from the sender to the common receiver?
(b) If, instead, the two receivers Y1 and Y2 each decode their signals independently, this becomes a broadcast channel. Let R1
be the rate to base station 1 and R2 be the rate to base station
2. Find the capacity region of this channel.

15.31

Gaussian multiple access. A group of m users, each with power
P , is using a Gaussian multiple-access channel at capacity, so that


m

mP
,
(15.395)
Ri = C
N
i=1

where C(x) = 12 log(1 + x) and N is the receiver noise power. A
new user of power P0 wishes to join in.
(a) At what rate can he send without disturbing the other users?
(b) What should his power P0 be so that the new usersâ rate is
equal to the combined communication rate C(mP /N ) of all
the other users?
15.32

Converse for deterministic broadcast channel. A deterministic
broadcast channel is deï¬ned by an input X and two outputs, Y1
and Y2 , which are functions of the input X. Thus, Y1 = f1 (X) and
Y2 = f2 (X). Let R1 and R2 be the rates at which information can
be sent to the two receivers. Prove that
R1 â¤ H (Y1 )

(15.396)

608

NETWORK INFORMATION THEORY

R2 â¤ H (Y2 )

(15.397)

R1 + R2 â¤ H (Y1 , Y2 ).

(15.398)

15.33

Multiple-access channel . Consider the multiple-access channel
Y = X1
+ X2 (mod 4), where X1 â {0, 1, 2, 3}, X2 â {0, 1}.
(a) Find the capacity region (R1 , R2 ).
(b) What is the maximum throughput R1 + R2 ?

15.34

Distributed source compression.

Z1 =

Z2 =

Let
1, p
0, q,
1, p
0, q,

and let U = Z1 Z2 , V = Z1 + Z2 . Assume that Z1 and Z2 are
independent. This induces a joint distribution on (U, V ). Let
(Ui , Vi ) be i.i.d. according to this distribution. Sender 1 describes
U n at rate R1 , and sender 2 describes V n at rate R2 .
(a) Find the SlepianâWolf rate region for recovering (U n , V n )
at the receiver.
(b) What is the residual uncertainty (conditional entropy) that
the receiver has about (X n , Y n ).
15.35

Multiple-access channel capacity with costs. The cost of using
symbol
x is r(x). The cost of a codeword x n is r(x n ) =
1 n
nR
n i=1 r(xi ). A (2 , n) codebook satisï¬es cost constraint r if
n
1
nR
i=1 r(xi (w)) â¤ r for all w â 2 .
n
(a) Find an expression for the capacity C(r) of a discrete memoryless channel with cost constraint r.
(b) Find an expression for the multiple-access channel capacity
region for (X1 Ã X2 , p(y|x1 , x2 ), Y) if sender X1 has cost constraint r1 and sender X2 has cost constraint r2 .
(c) Prove the converse for part (b).

15.36

SlepianâWolf . Three cards from a three-card deck are dealt, one
to sender X1 , one to sender X2 , and one to sender X3 . At what
rates do X1 , X2 , and X3 need to communicate to some receiver
so that their card information can be recovered?

HISTORICAL NOTES

X n1

i(X n1)

X n2

j(X n2)

X n3

k(X n3)

Decoder

^

^

609

^

(X n1, X n2, X n3)

Assume that (X1i , X2i , X3i ) are drawn i.i.d. from a uniform distribution over the permutations of {1, 2, 3}.
HISTORICAL NOTES
This chapter is based on the review in El Gamal and Cover [186]. The
two-way channel was studied by Shannon [486] in 1961. He derived inner
and outer bounds on the capacity region. Dueck [175] and Schalkwijk
[464, 465] suggested coding schemes for two-way channels that achieve
rates exceeding Shannonâs inner bound; outer bounds for this channel
were derived by Zhang et al. [596] and Willems and Hekstra [558].
The multiple-access channel capacity region was found by Ahlswede
[7] and Liao [355] and was extended to the case of the multiple-access
channel with common information by Slepian and Wolf [501]. Gaarder
and Wolf [220] were the ï¬rst to show that feedback increases the capacity of a discrete memoryless multiple-access channel. Cover and Leung
[133] proposed an achievable region for the multiple-access channel with
feedback, which was shown to be optimal for a class of multiple-access
channels by Willems [557]. Ozarow [410] has determined the capacity
region for a two-user Gaussian multiple-access channel with feedback.
Cover et al. [129] and Ahlswede and Han [12] have considered the problem of transmission of a correlated source over a multiple-access channel.
The SlepianâWolf theorem was proved by Slepian and Wolf [502] and was
extended to jointly ergodic sources by a binning argument in Cover [122].
Superposition coding for broadcast channels was suggested by Cover
in 1972 [119]. The capacity region for the degraded broadcast channel
was determined by Bergmans [55] and Gallager [225]. The superposition codes for the degraded broadcast channel are also optimal for the
less noisy broadcast channel (KoÌrner and Marton [324]), the more capable broadcast channel (El Gamal [185]), and the broadcast channel with
degraded message sets (KoÌrner and Marton [325]). Van der Meulen [526]
and Cover [121] proposed achievable regions for the general broadcast
channel. The capacity of a deterministic broadcast channel was found by
Gelfand and Pinsker [242, 243, 423] and Marton [377]. The best known

610

NETWORK INFORMATION THEORY

achievable region for the broadcast channel is due to Marton [377]; a simpler proof of Martonâs region was given by El Gamal and Van der Meulen
[188]. El Gamal [184] showed that feedback does not increase the capacity of a physically degraded broadcast channel. Dueck [176] introduced
an example to illustrate that feedback can increase the capacity of a memoryless broadcast channel; Ozarow and Leung [411] described a coding
procedure for the Gaussian broadcast channel with feedback that increased
the capacity region.
The relay channel was introduced by Van der Meulen [528]; the capacity region for the degraded relay channel was determined by Cover and
El Gamal [127]. Carleial [85] introduced the Gaussian interference channel with power constraints and showed that very strong interference is
equivalent to no interference at all. Sato and Tanabe [459] extended the
work of Carleial to discrete interference channels with strong interference.
Sato [457] and Benzel [51] dealt with degraded interference channels. The
best known achievable region for the general interference channel is due
to Han and Kobayashi [274]. This region gives the capacity for Gaussian
interference channels with interference parameters greater than 1, as was
shown in Han and Kobayashi [274] and Sato [458]. Carleial [84] proved
new bounds on the capacity region for interference channels.
The problem of coding with side information was introduced by Wyner
and Ziv [573] and Wyner [570]; the achievable region for this problem
was described in Ahlswede and KoÌrner [13], Gray and Wyner [261], and
Wyner [571],[572]. The problem of ï¬nding the rate distortion function
with side information was solved by Wyner and Ziv [574]. The channel
capacity counterpart of rate distortion with side information was solved by
Gelfand and Pinsker [243]; the duality between the two results is explored
in Cover and Chiang [113]. The problem of multiple descriptions is treated
in El Gamal and Cover [187].
The special problem of encoding a function of two random variables
was discussed by KoÌrner and Marton [326], who described a simple
method to encode the modulo 2 sum of two binary random variables.
A general framework for the description of source networks may be
found in CsiszaÌr and KoÌrner [148],[149]. A common model that includes
SlepianâWolf encoding, coding with side information, and rate distortion with side information as special cases was described by Berger and
Yeung [54].
In 1989, Ahlswede and Dueck [17] introduced the problem of identiï¬cation via communication channels, which can be viewed as a problem
where the sender sends information to the receivers but each receiver only
needs to know whether or not a single message was sent. In this case, the
set of possible messages that can be sent reliably is doubly exponential in

HISTORICAL NOTES

611
nC

the block length, and the key result of this paper was to show that 22
messages could be identiï¬ed for any noisy channel with capacity C. This
problem spawned a set of papers [16, 18, 269, 434], including extensions
to channels with feedback and multiuser channels.
Another active area of work has been the analysis of MIMO (multipleinput multiple-output) systems or space-time coding, which use multiple
antennas at the transmitter and receiver to take advantage of the diversity
gains from multipath for wireless systems. The analysis of these multiple
antenna systems by Foschini [217], Teletar [512], and Rayleigh and Ciofï¬
[246] show that the capacity gains from the diversity obtained using multiple antennas in fading environments can be substantial relative to the
single-user capacity achieved by traditional equalization and interleaving techniques. A special issue of the IEEE Transactions in Information
Theory [70] has a number of papers covering different aspects of this
technology.
Comprehensive surveys of network information theory may be found
in El Gamal and Cover [186], Van der Meulen [526â528], Berger [53],
CsiszaÌr and KoÌrner [149], Verdu [538], Cover [111], and Ephremides and
Hajek [197].

CHAPTER 16

INFORMATION THEORY
AND PORTFOLIO THEORY

The duality between the growth rate of wealth in the stock market and
the entropy rate of the market is striking. In particular, we shall ï¬nd the
competitively optimal and growth rate optimal portfolio strategies. They
are the same, just as the Shannon code is optimal both competitively and
in the expected description rate. We also ï¬nd the asymptotic growth rate
of wealth for an ergodic stock market process. We end with a discussion
of universal portfolios that enable one to achieve the same asymptotic
growth rate as the best constant rebalanced portfolio in hindsight.
In Section 16.8 we provide a âsandwichâ proof of the asymptotic
equipartition property for general ergodic processes that is motivated by
the notion of optimal portfolios for stationary ergodic stock markets.
16.1

THE STOCK MARKET: SOME DEFINITIONS

A stock market is represented as a vector of stocks X = (X1 , X2 , . . . , Xm ),
Xi â¥ 0, i = 1, 2, . . . , m, where m is the number of stocks and the price
relative Xi is the ratio of the price at the end of the day to the price at the
beginning of the day. So typically, Xi is near 1. For example, Xi = 1.03
means that the ith stock went up 3 percent that day.
Let X â¼ F (x), where F (x) is the joint distribution of
the vector of
price relatives. A portfolio b = (b1 , b2 , . . . , bm ), bi â¥ 0, bi = 1, is an
allocation of wealth across the stocks. Here bi is the fraction of oneâs
wealth invested in stock i. If one uses a portfolio b and the stock vector
is X, the wealth relative (ratio of the wealth at theend of the day to the
wealth at the beginning of the day) is S = bt X = m
i=1 bi Xi .
We wish to maximize S in some sense. But S is a random variable,
the distribution of which depends on portfolio b, so there is controversy
Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

613

614

INFORMATION THEORY AND PORTFOLIO THEORY

Mean

Efficient
frontier

Risk-free asset

Variance

FIGURE 16.1. SharpeâMarkowitz theory: set of achievable meanâvariance pairs.

over the choice of the best distribution for S. The standard theory of
stock market investment is based on consideration of the ï¬rst and second
moments of S. The objective is to maximize the expected value of S
subject to a constraint on the variance. Since it is easy to calculate these
moments, the theory is simpler than the theory that deals with the entire
distribution of S.
The meanâvariance approach is the basis of the SharpeâMarkowitz
theory of investment in the stock market and is used by business analysts and others. It is illustrated in Figure 16.1. The ï¬gure illustrates the
set of achievable meanâvariance pairs using various portfolios. The set
of portfolios on the boundary of this region corresponds to the undominated portfolios: These are the portfolios that have the highest mean for
a given variance. This boundary is called the efï¬cient frontier, and if one
is interested only in mean and variance, one should operate along this
boundary.
Normally, the theory is simpliï¬ed with the introduction of a risk-free
asset (e.g., cash or Treasury bonds, which provide a ï¬xed interest rate
with zero variance). This stock corresponds to a point on the Y axis
in the ï¬gure. By combining the risk-free asset with various stocks, one
obtains all points below the tangent from the risk-free asset to the efï¬cient
frontier. This line now becomes part of the efï¬cient frontier.
The concept of the efï¬cient frontier also implies that there is a true
price for a stock corresponding to its risk. This theory of stock prices,
called the capital asset pricing model (CAPM), is used to decide whether
the market price for a stock is too high or too low. Looking at the mean
of a random variable gives information about the long-term behavior of

16.1

THE STOCK MARKET: SOME DEFINITIONS

615

the sum of i.i.d. versions of the random variable. But in the stock market,
one normally reinvests every day, so that the wealth at the end of n days
is the product of factors, one for each day of the market. The behavior of
the product is determined not by the expected value but by the expected
logarithm. This leads us to deï¬ne the growth rate as follows:
Deï¬nition The growth rate of a stock market portfolio b with respect
to a stock distribution F (x) is deï¬ned as



(16.1)
W (b, F ) = log bt x dF (x) = E log bt X .
If the logarithm is to base 2, the growth rate is also called the doubling
rate.
Deï¬nition The optimal growth rate W â (F ) is deï¬ned as
W â (F ) = max W (b, F ),

(16.2)

b

where the maximum is over all possible portfolios bi â¥ 0,


i

bi = 1.

Deï¬nition A portfolio bâ that achieves the maximum of W (b, F ) is
called a log-optimal portfolio or growth optimal portfolio.
The deï¬nition of growth rate is justiï¬ed by the following theorem,
â
which shows that wealth grows as 2nW .
Theorem 16.1.1

Let X1 , X2 , . . . , Xn be i.i.d. according to F (x). Let
Snâ

=

n


bât Xi

(16.3)

i=1

be the wealth after n days using the constant rebalanced portfolio bâ . Then
1
log Snâ â W â
n

with probability 1.

(16.4)

Proof: By the strong law of large numbers,
1
1
log bât Xi
log Snâ =
n
n
n

âW
.

â

Hence, Snâ = 2nW .

i=1
â

with probability 1.

(16.5)
(16.6)


616

INFORMATION THEORY AND PORTFOLIO THEORY

We now consider some of the properties of the growth rate.
Lemma 16.1.1
convex in F .

W (b, F ) is concave in b and linear in F . W â (F ) is

Proof: The growth rate is
W (b, F ) =


log bt x dF (x).

(16.7)

Since the integral is linear in F , so is W (b, F ). Since
log(Î»b1 + (1 â Î»)b2 )t X â¥ Î» log bt1 X + (1 â Î») log bt2 X,

(16.8)

by the concavity of the logarithm, it follows, by taking expectations, that
W (b, F ) is concave in b. Finally, to prove the convexity of W â (F ) as a
function of F , let F1 and F2 be two distributions on the stock market and
let the corresponding optimal portfolios be bâ (F1 ) and bâ (F2 ), respectively. Let the log-optimal portfolio corresponding to Î»F1 + (1 â Î»)F2 be
bâ (Î»F1 + (1 â Î»)F2 ). Then by linearity of W (b, F ) with respect to F ,
we have
W â (Î»F1 + (1 â Î»)F2 )
= W (bâ (Î»F1 + (1 â Î»)F2 ), Î»F1 + (1 â Î»)F2 )

(16.9)

= Î»W (bâ (Î»F1 + (1 â Î»)F2 ), F1 )
+ (1 â Î»)W (bâ (Î»F1 + (1 â Î»)F2 ), F2 )
â¤ Î»W (bâ (F1 ), F1 ) + (1 â Î»)W â (bâ (F2 ), F2 ),

(16.10)

since bâ (F1 ) maximizes W (b, F1 ) and bâ (F2 ) maximizes W (b, F2 ).



Lemma 16.1.2 The set of log-optimal portfolios with respect to a given
distribution is convex.
Proof: Suppose that b1 and b2 are log-optimal (i.e., W (b1 , F ) = W (b2 , F )
= W â (F )). By the concavity of W (b, F ) in b, we have
W (Î»b1 + (1 â Î»)b2 , F ) â¥ Î»W (b1 , F ) + (1 â Î»)W (b2 , F ) = W â (F ).
(16.11)
Thus, Î»b1 + (1 â Î»)b2 is also log-optimal.



In the next section we use these properties to characterize the logoptimal portfolio.

16.2 KUHNâTUCKER CHARACTERIZATION OF THE LOG-OPTIMAL PORTFOLIO

617

16.2 KUHNâTUCKER CHARACTERIZATION
OF THE LOG-OPTIMAL PORTFOLIO

Let B = {b â Rm : bi â¥ 0, m
i=1 bi = 1} denote the set of allowed portfolios. The determination of bâ that achieves W â (F ) is a problem of
maximization of a concave function W (b, F ) over a convex set B. The
maximum may lie on the boundary. We can use the standard KuhnâTucker
conditions to characterize the maximum. Instead, we derive these conditions from ï¬rst principles.
Theorem 16.2.1 The log-optimal portfolio bâ for a stock market X â¼ F
(i.e., the portfolio that maximizes the growth rate W (b, F )) satisï¬es the
following necessary and sufï¬cient conditions:


Xi
E
=1
if biâ > 0,
bât X
(16.12)
â¤1
if biâ = 0.
Proof: The growth rate W (b) = E(ln bt X) is concave in b, where b
ranges over the simplex of portfolios. It follows that bâ is log-optimum
iff the directional derivative of W (Â·) in the direction from bâ to any
alternative portfolio b is nonpositive. Thus, letting bÎ» = (1 â Î»)bâ + Î»b
for 0 â¤ Î» â¤ 1, we have
	
d
	
â¤ 0,
b â B.
(16.13)
W (bÎ» )	
Î»=0+
dÎ»
These conditions reduce to (16.12) since the one-sided derivative at Î» =
0+ of W (bÎ» ) is
	
d
	
t
E(ln(bÎ» X))	
Î»=0+
dÎ»
 

(1 â Î»)bât X + Î»bt X
1
(16.14)
= lim E ln
Î»â0 Î»
bât X

 t


bX
1
â1
(16.15)
= E lim ln 1 + Î»
Î»â0 Î»
bât X
 t 
bX
=E
â 1,
(16.16)
bât X
where the interchange of limit and expectation can be justiï¬ed using the
dominated convergence theorem [39]. Thus, (16.13) reduces to
 t 
bX
â1â¤0
(16.17)
E
bât X

618

INFORMATION THEORY AND PORTFOLIO THEORY

for all b â B. If the line segment from b to bâ can be extended beyond bâ
in the simplex, the two-sided derivative at Î» = 0 of W (bÎ» ) vanishes and
(16.17) holds with equality. If the line segment from b to bâ cannot be
extended because of the inequality constraint on b, we have an inequality
in (16.17).
The KuhnâTucker conditions will hold for all portfolios b â B if they
hold for all extreme points of the simplex B since E(bt X/bât X) is linear
in b. Furthermore, the line segment from the j th extreme point (b : bj =
1, bi = 0, i = j ) to bâ can be extended beyond bâ in the simplex iff bjâ >
0. Thus, the KuhnâTucker conditions that characterize the log-optimum
bâ are equivalent to the following necessary and sufï¬cient conditions:


Xi
=1
if biâ > 0,
E
bât X
(16.18)
â¤1
if biâ = 0. 
This theorem has a few immediate consequences. One useful equivalence is expressed in the following theorem.
Theorem 16.2.2 Let S â = bât X be the random wealth resulting from
the log-optimal portfolio bâ . Let S = bt X be the wealth resulting from any
other portfolio b. Then
E ln

S
S
â¤ 0 for all S â E â â¤ 1 for all S.
â
S
S

(16.19)

Proof: From Theorem 16.2.1 it follows that for a log-optimal portfolio
bâ ,


Xi
E
â¤1
(16.20)
bât X
for all i. Multiplying this equation by bi and summing over i, we have
 

m
m

Xi
â¤
bi E
bi = 1,
(16.21)
bât X
i=1

i=1

which is equivalent to
E

S
bt X
= E â â¤ 1.
ât
b X
S

(16.22)

The converse follows from Jensenâs inequality, since
E log

S
S
â¤ log E â â¤ log 1 = 0.
â
S
S



(16.23)

16.3 ASYMPTOTIC OPTIMALITY OF THE LOG-OPTIMAL PORTFOLIO

619

Maximizing the expected logarithm was motivated by the asymptotic
growth rate. But we have just shown that the log-optimal portfolio, in
addition to maximizing the asymptotic growth rate, also âmaximizesâ the
expected wealth relative E(S/S â ) for one day. We shall say more about
the short-term optimality of the log-optimal portfolio when we consider
the game-theoretic optimality of this portfolio.
Another consequence of the KuhnâTucker characterization of the logoptimal portfolio is the fact that the expected proportion of wealth in
each stock under the log-optimal portfolio is unchanged from day to day.
Consider the stocks at the end of the ï¬rst day. The initial allocation of
wealth is bâ . The proportion of the wealth in stock i at the end of the day
bâ X
is biât Xi , and the expected value of this proportion is
E

biâ Xi
Xi
= biâ E ât = biâ .
ât
b X
b X

(16.24)

Hence, the proportion of wealth in stock i expected at the end of the
day is the same as the proportion invested in stock i at the beginning of
the day. This is a counterpart to Kelly proportional gambling, where one
invests in proportions that remain unchanged in expected value after the
investment period.
16.3 ASYMPTOTIC OPTIMALITY OF THE LOG-OPTIMAL
PORTFOLIO
In Section 16.2 we introduced the log-optimal portfolio and explained its
motivation in terms of the long-term behavior of a sequence of investments
in a repeated independent versions of the stock market. In this section we
expand on this idea and prove that with probability 1, the conditionally
log-optimal investor will not do any worse than any other investor who
uses a causal investment strategy.
We ï¬rst consider an i.i.d. stock market (i.e., X1 , X2 , . . . , Xn are i.i.d.
according to F (x)). Let
n

Sn =
bti Xi
(16.25)
i=1

be the wealth after n days for an investor who uses portfolio bi on day i.
Let
W â = max W (b, F ) = max E log bt X
b

b

(16.26)

620

INFORMATION THEORY AND PORTFOLIO THEORY

be the maximal growth rate, and let bâ be a portfolio that achieves the
maximum growth rate. We only allow alternative portfolios bi that depend
causally on the past and are independent of the future values of the stock
market.
Deï¬nition A nonanticipating or causal portfolio strategy is a sequence
of mappings bi : Rm(iâ1) â B, with the interpretation that portfolio bi (x1 ,
. . . , xiâ1 ) is used on day i.
From the deï¬nition of W â , it follows immediately that the log-optimal
portfolio maximizes the expected log of the ï¬nal wealth. This is stated in
the following lemma.
Lemma 16.3.1 Let Snâ be the wealth after n days using the log-optimal
strategy bâ on i.i.d. stocks, and let Sn be the wealth using a causal portfolio
strategy bi . Then
E log Snâ = nW â â¥ E log Sn .

(16.27)

Proof
max

b1 ,b2 ,...,bn

E log Sn =
=

max

b1 ,b2 ,...,bn
n

i=1

=

n


E

n


log bti Xi

(16.28)

i=1

max

bi (X1 ,X2 ,...,Xiâ1 )

E log bti (X1 , X2 , . . . , Xiâ1 )Xi
(16.29)

E log bât Xi

(16.30)

i=1

= nW â ,
and the maximum is achieved by a constant portfolio strategy bâ .

(16.31)


So far, we have proved two simple consequences of the deï¬nition of
log-optimal portfolios: that bâ (satisfying (16.12)) maximizes the expected
â
log wealth, and that the resulting wealth Snâ is equal to 2nW to ï¬rst order
in the exponent, with high probability.
Now we prove a much stronger result, which shows that Snâ exceeds
the wealth (to ï¬rst order in the exponent) of any other investor for almost
every sequence of outcomes from the stock market.
Theorem 16.3.1 (Asymptotic optimality of the log-optimal portfolio)
Let X1 , X2 , . . . , Xn be a sequence of i.i.d. stock vectors drawn according

16.4 SIDE INFORMATION AND THE GROWTH RATE

621



to F (x). 

Let Snâ = ni=1 bât Xi , where bâ is the log-optimal portfolio, and
let Sn = ni=1 bti Xi be the wealth resulting from any other causal portfolio.
Then
lim sup
nââ

1
Sn
log â â¤ 0
n
Sn

with probability 1.

(16.32)

Proof: From the KuhnâTucker conditions and the log optimality of Snâ ,
we have
Sn
(16.33)
E â â¤ 1.
Sn
Hence by Markovâs inequality, we have




Sn
1
â
Pr Sn > tn Sn = Pr
> tn < .
â
Sn
tn
Hence,


1
Sn
1
1
Pr
log â > log tn â¤ .
n
Sn
n
tn
Setting tn = n2 and summing over n, we have
 

â
â

Sn
1
2 log n
Ï2
1
log â >
â¤
.
Pr
=
n
Sn
n
n2
6
n=1

(16.34)

(16.35)

(16.36)

n=1

Then, by the BorelâCantelli lemma,


1
Sn
2 log n
log â >
, inï¬nitely often = 0.
Pr
n
Sn
n

(16.37)

This implies that for almost every sequence from the stock market, there
n
exists an N such that for all n > N , n1 log SSnâ < 2 log
n . Thus,
n

lim sup

1
Sn
log â â¤ 0
n
Sn

with probability 1. 

(16.38)

The theorem proves that the log-optimal portfolio will perform as well
as or better than any other portfolio to ï¬rst order in the exponent.
16.4

SIDE INFORMATION AND THE GROWTH RATE

We showed in Chapter 6 that side information Y for the horse race X can
be used to increase the growth rate by the mutual information I (X; Y ).

622

INFORMATION THEORY AND PORTFOLIO THEORY

We now extend this result to the stock market. Here, I (X; Y ) is an upper
bound on the increase in the growth rate, with equality if X is a horse
race. We ï¬rst consider the decrease in growth rate incurred by believing
in the wrong distribution.
Theorem 16.4.1 Let X â¼ f (x). Let bf be a log-optimal portfolio corresponding to f (x), and let bg be a log-optimal portfolio corresponding
to some other density g(x). Then the increase in growth rate W by using
bf instead of bg is bounded by
W = W (bf , F ) â W (bg , F ) â¤ D(f ||g).

(16.39)

Proof: We have



W =

f (x) log bft x


=

f (x) log


=

f (x) log


=

f (x) log

(a)



â¤ log

f (x)


= log

g(x)

â

f (x) log bgt x

bft x

(16.41)

bgt x
bft x g(x) f (x)
bgt x f (x) g(x)
bft x g(x)
bgt x f (x)
bft x g(x)
bgt x f (x)

bft x
bgt x

(16.40)

+ D(f ||g)

(16.43)

+ D(f ||g)

(16.44)

+ D(f ||g)

(b)

(16.42)

(16.45)

â¤ log 1 + D(f ||g)

(16.46)

= D(f ||g),

(16.47)

where (a) follows from Jensenâs inequality and (b) follows from the

KuhnâTucker conditions and the fact that bg is log-optimal for g.
Theorem 16.4.2 The increase W in growth rate due to side information Y is bounded by
W â¤ I (X; Y ).

(16.48)

16.5

INVESTMENT IN STATIONARY MARKETS

623

Proof: Let (X, Y ) â¼ f (x, y), where X is the market vector and Y is the
related side information. Given side information Y = y, the log-optimal
investor uses the conditional log-optimal portfolio for the conditional
distribution f (x|Y = y). Hence, conditional on Y = y, we have, from
Theorem 16.4.1,

f (x|Y = y)
dx.
WY =y â¤ D(f (x|Y = y)||f (x)) = f (x|Y = y) log
f (x)
x
(16.49)
Averaging this over possible values of Y , we have


f (x|Y = y)
dx dy
(16.50)
W â¤ f (y) f (x|Y = y) log
f (x)
y
x
 
f (x|Y = y) f (y)
=
f (y)f (x|Y = y) log
dx dy (16.51)
f (x)
f (y)
y x
 
f (x, y)
dx dy
(16.52)
f (x, y) log
=
f (x)f (y)
y x
= I (X; Y ).

(16.53)

Hence, the increase in growth rate is bounded above by the mutual infor
mation between the side information Y and the stock market X.
16.5

INVESTMENT IN STATIONARY MARKETS

We now extend some of the results of Section 16.4 from i.i.d. markets
to time-dependent market processes. Let X1 , X2 , . . . , Xn , . . . be a vectorvalued stochastic process with Xi â¥ 0. We consider investment strategies
that depend on the past values of the market in a causal fashion (i.e., bi
may depend on X1 , X2 , . . . , Xiâ1 ). Let
Sn =

n


bti (X1 , X2 , . . . , Xiâ1 )Xi .

(16.54)

i=1

Our objective is to maximize E log Sn over all such causal portfolio strategies {bi (Â·)}. Now
n

max
E log bti Xi
(16.55)
max E log Sn =
b1 ,b2 ,...,bn

i=1

=

n

i=1

bi (X1 ,X2 ,...,Xiâ1 )

E log bât
i Xi ,

(16.56)

624

INFORMATION THEORY AND PORTFOLIO THEORY

where bâi is the log-optimal portfolio for the conditional distribution of Xi
given the past values of the stock market; that is, bâi (x1 , x2 , . . . , xiâ1 ) is
the portfolio that achieves the conditional maximum, which is denoted by
max E[ log bt Xi |(X1 , X2 , . . . , Xiâ1 ) = (x1 , x2 , . . . , xiâ1 )]
b

= W â (Xi |x1 , x2 , . . . , xiâ1 ). (16.57)
Taking the expectation over the past, we write


W â (Xi |X1 , X2 , . . . , Xiâ1 ) = E max E log bt Xi |X1 , X2 , . . . , Xiâ1
b

(16.58)
as the conditional optimal growth rate, where the maximum is over all
portfolio-valued functions b deï¬ned on X1 , . . . , Xiâ1 . Thus, the highest expected log return is achieved by using the conditional log-optimal
portfolio at each stage. Let
W â (X1 , X2 , . . . , Xn ) =

max

b1 ,b2 ,...,bn

E log Sn ,

(16.59)

where themaximum is over all causal portfolio strategies. Then since
ât
â
log Snâ = m
i=1 log bi Xi , we have the following chain rule for W :
â

W (X1 , X2 , . . . , Xn ) =

n


W â (Xi |X1 , X2 , . . . , Xiâ1 ).

(16.60)

i=1

This chain rule is formally the same as the chain rule for H . In some
ways, W is the dual of H . In particular, conditioning reduces H but
increases W . We now deï¬ne the counterpart of the entropy rate for timedependent stochastic processes.
â is deï¬ned as
Deï¬nition The growth rate Wâ

W â (X1 , X2 , . . . , Xn )
nââ
n

â
Wâ
= lim

(16.61)

if the limit exists.
Theorem 16.5.1
equal to

For a stationary market, the growth rate exists and is

â
= lim W â (Xn |X1 , X2 , . . . , Xnâ1 ).
Wâ
nââ

(16.62)

16.5

INVESTMENT IN STATIONARY MARKETS

625

Proof: By stationarity, W â (Xn |X1 , X2 , . . . , Xnâ1 ) is nondecreasing in n.
Hence, it must have a limit, possibly inï¬nity. Since
W â (X1 , X2 , . . . , Xn )
1 â
W (Xi |X1 , X2 , . . . , Xiâ1 ),
=
n
n
n

(16.63)

i=1

it follows by the theorem of the CesaÌro mean (Theorem 4.2.3) that the
left-hand side has the same limit as the limit of the terms on the right-hand
â
side. Hence, Wâ
exists and
W â (X1 , X2 , . . . , Xn )
= lim W â (Xn |X1 , X2 , . . . , Xnâ1 ). 
nââ
nââ
n
(16.64)

â
Wâ
= lim

We can now extend the asymptotic optimality property to stationary
markets. We have the following theorem.
Theorem 16.5.2 Consider an arbitrary stochastic process {Xi }, Xi â
â
iâ1
Rm
) and wealth Snâ . Let Sn
+ , conditionally log-optimal portfolios, bi (X
be the wealth generated by any other causal portfolio strategy bi (X iâ1 ).
Then Sn /Snâ is a positive supermartingale with respect to the sequence of
Ï -ï¬elds generated by the past X1 , X2 , . . . , Xn . Consequently, there exists
a random variable V such that
Sn
âV
Snâ

with probability 1

EV â¤ 1

(16.65)
(16.66)




1
Sn
Pr sup â â¥ t â¤ .
t
n Sn

and

(16.67)

Proof: Sn /Snâ is a positive supermartingale because
	 
	 
 t
(bn+1 Xn+1 )Sn (X n ) 	 n
Sn+1 (X n+1 ) 		 n
	X
E â
X =E
â
n 	
Sn+1 (X n+1 ) 	
(bât
n+1 Xn+1 )Sn (X )
	 
 t
bn+1 Xn+1 	 n
Sn (X n )
	X
= â n E ât
Sn (X )
bn+1 Xn+1 	


â¤

Sn (X n )
,
Snâ (X n )

(16.68)
(16.69)
(16.70)

626

INFORMATION THEORY AND PORTFOLIO THEORY

by the KuhnâTucker condition on the conditionally log-optimal portfolio.
Thus, by the martingale convergence theorem, Sn /Snâ has a limit, call it
V , and EV â¤ E(S0 /S0â ) = 1. Finally, the result for sup(Sn /Snâ ) follows
from Kolmogorovâs inequality for positive martingales.

We remark that (16.70) shows how strong the competitive optimality
of Snâ is. Apparently, the probability is less than 1/10 that Sn (X n ) will
ever be 10 times as large as Snâ (X n ). For a stationary ergodic market, we
can extend the asymptotic equipartition property to prove the following
theorem.
Theorem 16.5.3 (AEP for the stock market) Let X1 , X2 , . . . , Xn be a
stationary ergodic vector-valued stochastic process. Let Snâ be the wealth
at time n for the conditionally log-optimal strategy, where
Snâ
Then

=

n


bât
i (X1 , X2 , . . . , Xiâ1 )Xi .

(16.71)

i=1

1
â
log Snâ â Wâ
n

with probability 1.

(16.72)

Proof: The proof involves a generalization of the sandwich argument
[20] used to prove the AEP in Section 16.8. The details of the proof (in

Algoet and Cover [21]) are omitted.
Finally, we consider the example of the horse race once again. The
horse race is a special case of the stock market in which there are m
stocks corresponding to the m horses in the race. At the end of the race,
the value of the stock for horse i is either 0 or oi , the value of the odds
for horse i. Thus, X is nonzero only in the component corresponding to
the winning horse.
In this case, the log-optimal portfolio is proportional betting, known as
Kelly gambling (i.e., biâ = pi ), and in the case of uniform fair odds (i.e.,
oi = m, for all i),
W â = log m â H (X).

(16.73)

When we have a sequence of correlated horse races, the optimal portfolio
is conditional proportional betting and the asymptotic growth rate is
â
Wâ
= log m â H (X),

(16.74)

16.6

COMPETITIVE OPTIMALITY OF THE LOG-OPTIMAL PORTFOLIO

627

where H (X) = lim n1 H (X1 , X2 , . . . , Xn ) if the limit exists. Then Theorem 16.5.3 asserts that
â
.
Snâ = 2nW ,
(16.75)
in agreement with the results in chapter 6.
16.6 COMPETITIVE OPTIMALITY OF THE LOG-OPTIMAL
PORTFOLIO
We now ask whether the log-optimal portfolio outperforms alternative
portfolios at a given ï¬nite time n. As a direct consequence of the
KuhnâTucker conditions, we have
E

Sn
â¤ 1,
Snâ

(16.76)

and hence by Markovâs inequality,
1
Pr(Sn > tSnâ ) â¤ .
t

(16.77)

This result is similar to the result derived in Chapter 5 for the competitive
optimality of Shannon codes.
By considering examples, it can be seen that it is not possible to get
a better bound on the probability that Sn > Snâ . Consider a stock market
with two stocks and two possible outcomes,
ï£± 

1
ï£²
with probability 1 â ,
1,
(X1 , X2 ) =
(16.78)
1â
ï£³
(1, 0)
with probability .
In this market the log-optimal portfolio invests all the wealth in the ï¬rst
stock. [It is easy to verify that b = (1, 0) satisï¬es the KuhnâTucker conditions.] However, an investor who puts all his wealth in the second stock
earns more money with probability 1 â . Hence, it is not true that with
high probability the log-optimal investor will do better than any other
investor.
The problem with trying to prove that the log-optimal investor does
best with a probability of at least 12 is that there exist examples like the
one above, where it is possible to beat the log-optimal investor by a
small amount most of the time. We can get around this by allowing each
investor an additional fair randomization, which has the effect of reducing
the effect of small differences in the wealth.

628

INFORMATION THEORY AND PORTFOLIO THEORY

Theorem 16.6.1 (Competitive optimality) Let S â be the wealth at the
end of one period of investment in a stock market X with the log-optimal
portfolio, and let S be the wealth induced by any other portfolio. Let U â be
a random variable independent of X uniformly distributed on [0, 2], and
let V be any other random variable independent of X and U â with V â¥ 0
and EV = 1. Then
1
Pr(V S â¥ U â S â ) â¤ .
2

(16.79)

Remark Here U â and V correspond to initial âfairâ randomizations of
the initial wealth. This exchange of initial wealth S0 = 1 for âfairâ wealth
U â can be achieved in practice by placing a fair bet. The effect of the
fair randomization is to randomize small differences, so that only the
signiï¬cant deviations of the ratio S/S â affect the probability of winning.
Proof: We have



VS
Pr(V S â¥ U S ) = Pr
â¥ Uâ
Sâ
â â

= Pr(W â¥ U â ),
where W =

VS
Sâ


(16.80)
(16.81)

is a non-negative-valued random variable with mean
 
Sn
â¤1
(16.82)
EW = E(V )E
Snâ

by the independence of V from X and the KuhnâTucker conditions. Let
F be the distribution function of W . Then since U â is uniform on [0, 2],
 2
â
Pr(W > w)fU â (w) dw
(16.83)
Pr(W â¥ U ) =
0


=

2

0


=

0


â¤

0

2

1
Pr(W > w) dw
2

(16.84)

1 â F (w)
dw
2

(16.85)

â

1 â F (w)
dw
2

1
= EW
2
1
â¤ ,
2

(16.86)
(16.87)
(16.88)

16.7 UNIVERSAL PORTFOLIOS

using the easily proved fact (by integrating by parts) that
 â
EW =
(1 â F (w)) dw

629

(16.89)

0

for a positive random variable W . Hence, we have
1
Pr(V S â¥ U â S â ) = Pr(W â¥ U â ) â¤ . 
2

(16.90)

Theorem 16.6.1 provides a short-term justiï¬cation for the use of the
log-optimal portfolio. If the investorâs only objective is to be ahead of his
opponent at the end of the day in the stock market, and if fair randomization is allowed, Theorem 16.6.1 says that the investor should exchange his
wealth for a uniform [0, 2] wealth and then invest using the log-optimal
portfolio. This is the game-theoretic solution to the problem of gambling
competitively in the stock market.
16.7

UNIVERSAL PORTFOLIOS

The development of the log-optimal portfolio strategy in Section 16.1
relies on the assumption that we know the distribution of the stock vectors
and can therefore calculate the optimal portfolio bâ . In practice, though,
we often do not know the distribution. In this section we describe a causal
portfolio that performs well on individual sequences. Thus, we make no
statistical assumptions about the market sequence. We assume that the
stock market can be represented by a sequence of vectors x1 , x2 , . . . â Rm
+,
where xij is the price relative for stock j on day i and xi is the vector
of price relatives for all stocks on day i. We begin with a ï¬nite-horizon
problem, where we have n vectors x1 , . . . , xn . We later extend the results
to the inï¬nite-horizon case.
Given this sequence of stock market outcomes, what is the best we
can do? A realistic target is the growth achieved by the best constant
rebalanced portfolio strategy in hindsight (i.e., the best constant rebalanced portfolio on the known sequence of stock market vectors). Note
that constant rebalanced portfolios are optimal against i.i.d. stock market sequences with known distribution, so that this set of portfolios is
reasonably natural.
Let us assume that we have a number of mutual funds, each of which
follows a constant rebalanced portfolio strategy chosen in advance. Our
objective is to perform as well as the best of these funds. In this section
we show that we can do almost as well as the best constant rebalanced

630

INFORMATION THEORY AND PORTFOLIO THEORY

portfolio without advance knowledge of the distribution of the stock
market vectors.
One approach is to distribute the wealth among a continuum of fund
managers, each of which follows a different constantly rebalanced portfolio strategy. Since one of the managers will do exponentially better than
the others, the total wealth after n days will be dominated by the largest
term. We will show that we can achieve a performance of the best fund
mâ1
manager within a factor of n 2 . This is the essence of the argument for
the inï¬nite-horizon universal portfolio strategy.
A second approach to this problem is as a game against a malicious
opponent or nature who is allowed to choose the sequence of stock
market vectors. We deï¬ne a causal (nonanticipating) portfolio strategy
bÌi (xiâ1 , . . . , x1 ) that depends only on the past values of the stock market
sequence. Then nature, with knowledge of the strategy bÌi (xiâ1 ), chooses a
sequence of vectors xi to make the strategy perform as poorly as possible
relative to the best constantly rebalanced portfolio for that stock sequence.
Let bâ (xn ) be the best constantly rebalanced portfolio for a stock market
sequence xn . Note that bâ (xn ) depends only on the empirical distribution
of the sequence, not on the order in which the vectors occur. At the end
of n days, a constantly rebalanced portfolio b achieves wealth:
Sn (b, x ) =
n

n


bt xi ,

(16.91)

i=1

and the best constant portfolio bâ (xn ) achieves a wealth
Snâ (xn ) = max
b

n


bt xi ,

(16.92)

i=1

whereas the nonanticipating portfolio bÌi (xiâ1 ) strategy achieves
SÌn (x ) =
n

n


bÌti (xiâ1 )xi .

(16.93)

i=1

Our objective is to ï¬nd a nonanticipating portfolio strategy bÌ(Â·) = (bÌ1 ,
bÌ2 (x1 ), . . . , bÌi (xiâ1 )) that does well in the worst case in terms of the ratio
of SÌn to Snâ . We will ï¬nd the optimal universal strategy and show that this
strategy for each stock sequence achieves wealth SÌn that is within a factor
mâ1

Vn â nâ 2 of the wealth Snâ achieved by the best constantly rebalanced
portfolio on that sequence. This strategy depends on n, the horizon of

16.7 UNIVERSAL PORTFOLIOS

631

the game. Later we describe some horizon-free results that have the same
worst-case asymptotic performance as that of the ï¬nite-horizon game.
16.7.1

Finite-Horizon Universal Portfolios

We begin by analyzing a stock market of n periods, where n is known
in advance, and attempt to ï¬nd a portfolio strategy that does well against
all possible sequences of n stock market vectors. The main result can be
stated in the following theorem.
Theorem 16.7.1 For a stock market sequence xn = x1 , . . . , xn , xi â
â n
Rm
+ of length n with m assets, let Sn (x ) be the wealth achieved by the
optimal constantly rebalanced portfolio on xn , and let SÌn (xn ) be the wealth
achieved by any causal portfolio strategy bÌi (Â·) on xn ; then
max min

bÌi (Â·) x1 ,...,xn

where

Vn =



n1 +Â·Â·Â·+nm =n

SÌn (xn )
= Vn ,
Snâ (xn )

â1

n1
nm
n
.
2ânH ( n ,..., n )
n1 , n2 , . . . , n m

(16.94)



(16.95)

Using Stirlingâs approximation, we can show that Vn is on the order
mâ1
of nâ 2 , and therefore the growth rate for the universal portfolio on
the worst sequence differs from the growth rate of the best constantly
rebalanced portfolio on that sequence by at most a polynomial factor.
The logarithm of the ratio of growth of wealth of the universal portfolio
bÌ to the growth of wealth of the best constant portfolio behaves like
the redundancy of a universal source code. (See Shtarkov [496], where
log Vn appears as the minimax individual sequence redundancy in data
compression.)
We ï¬rst illustrate the main results by means of an example for n = 1.
Consider the case of two stocks and a single day. Let the stock vector for
the day be x = (x1 , x2 ). If x1 > x2 , the best portfolio is one that puts all
its money on stock 1, and if x2 > x1 , the best portfolio puts all its money
on stock 2. (If x1 = x2 , all portfolios are equivalent.)
Now assume that we must choose a portfolio in advance and our opponent can choose the stock market sequence after we have chosen our
portfolio to make us do as badly as possible relative to the best portfolio.
Given our portfolio, the opponent can ensure that we do as badly as possible by making the stock on which we have put more weight equal to 0
and the other stock equal to 1. Our best strategy is therefore to put equal

632

INFORMATION THEORY AND PORTFOLIO THEORY

weight on both stocks, and with this, we will achieve a growth factor at
least equal to half the growth factor of the best stock, and hence we will
achieve at least half the gain of the best constantly rebalanced portfolio.
It is not hard to calculate that Vn = 2 when n = 1 and m = 2 in equation
(16.94).
However, this result seems misleading, since it appears to suggest that
for n days, we would use a constant uniform portfolio, putting half our
money on each stock every day. If our opponent then chose the stock
sequence so that only the ï¬rst stock was 1 (and the other was 0) every
day, this uniform strategy would achieve a wealth of 1/2n , and we would
achieve a wealth only within a factor of 2n of the best constant portfolio,
which puts all the money on the ï¬rst stock for all time.
The result of the theorem shows that we can do signiï¬cantly better.
The main part of the argument is to reduce a sequence of stock vectors to
the extreme cases where only one of the stocks is nonzero for each day.
If we can ensure that we do well on such sequences, we can guarantee
that we do well on any sequence of stock vectors, and achieve the bounds
of the theorem.
Before we prove the theorem, we need the following lemma.
Lemma 16.7.1

For p1 , p2 , . . . , pm â¥ 0 and q1 , q2 , . . . , qm â¥ 0,
m
pi
pi
i=1
â¥ min .
(16.96)
m
i qi
i=1 qi

Proof: Let I denote the index i that minimizes the right-hand side in
(16.96). Assume that pI > 0 (if pI = 0, the lemma is trivially true). Also,
if qI = 0, both sides of (16.96) are inï¬nite (all the other qi âs must also
be zero), and again the inequality holds. Therefore, we can also assume
that qI > 0. Then

m
pI
pI 1 + i=I (pi /pI )
i=1 pi

m
â¥
=
(16.97)
qI 1 + i=I (qi /qI )
qI
i=1 qi
because
pI
pi
qi
pi
â¥
ââ
â¥
qi
qI
pI
qI

(16.98)


for all i.

First consider the case when n = 1. The wealth at the end of the ï¬rst
day is
SÌ1 (x) = bÌt x,

(16.99)

S1 (x) = bt x

(16.100)

16.7 UNIVERSAL PORTFOLIOS

and

 

bÌi xi
bÌi
SÌ1 (x)
=
â¥ min
.
S1 (x)
bi
bi xi

We wish to ï¬nd maxbÌ minb,x

bÌt x
bt x .

633

(16.101)

Nature should choose x = ei , where ei is

the ith basis vector with 1 in the component i that minimizes

bÌi
,
biâ

and the

investor should choose bÌ to maximize this minimum. This is achieved by
choosing bÌ = ( m1 , m1 , . . . , m1 ).
The important point to realize is that

n
t
SÌn (xn )
i=1 bÌi xi


= n
(16.102)
t
Sn (xn )
i=1 bi xi
can also be rewritten in the form of a ratio of terms
bÌt x
SÌn (xn )
=
,
Sn (xn )
bt x

(16.103)

n

n
where bÌ, b, x â Rm
+ . Here the m components of the constantly rebaln n
nm . One wishes
anced portfolios b are all of the product form b1 1 b2 2 Â· Â· Â· bm
to ï¬nd a universal bÌ that is uniformly close to the bâs corresponding to
constantly rebalanced portfolios.
We can now prove the main theorem (Theorem 16.7.1).

Proof of Theorem 16.7.1: We will prove the theorem for m = 2. The
proof extends in a straightforward fashion to the case m > 2. Denote the
stocks by 1 and 2. The key idea is to express the wealth at time n,
Sn (x ) =
n

n


bti xi ,

(16.104)

i=1

which is a product of sums, into a sum of products. Each term in the sum
corresponds to a sequence of stock price relatives for stock 1 or stock
2 times the proportion bi1 or bi2 that the strategy places on stock 1 or
stock 2 at time i. We can therefore view the wealth Sn as a sum over
all 2n possible n-sequences of 1âs and 2âs of the product of the portfolio
proportions times the stock price relatives:
Sn (x ) =
n



n


j n â{1,2}n i=1

biji xiji =



n


j n â{1,2}n i=1

biji

n

i=1

xiji .

(16.105)

634

INFORMATION THEORY AND PORTFOLIO THEORY



If we let w(j n ) denote the product ni=1 biji , the total fraction of wealth
invested in the sequence j n , and let
x(j n ) =

n


xiji

(16.106)

i=1

be the corresponding return for this sequence, we can write

Sn (xn ) =
w(j n )x(j n ).

(16.107)

j n â{1,2}n

Similar expressions apply to both the best constantly rebalanced portfolio
and the universal portfolio strategy. Thus, we have

n
n
SÌn (xn )
j n â{1,2}n wÌ(j )x(j )

=
,
(16.108)
â n
n
Snâ (xn )
j n â{1,2}n w (j )x(j )
where wÌ n is the amount of wealth placed on the sequence j n by the
universal nonanticipating strategy, and w â (j n ) is the amount placed by the
best constant rebalanced portfolio strategy. Now applying Lemma 16.7.1,
we have
SÌn (xn )
wÌ(j n )x(j n )
wÌ(j n )
â¥
min
=
min
.
j n w â (j n )x(j n )
j n w â (j n )
Snâ (xn )

(16.109)

Thus, the problem of maximizing the performance ratio SÌn /Snâ is reduced
to ensuring that the proportion of money bet on a sequence of stocks by
the universal portfolio is uniformly close to the proportion bet by bâ . As
might be obvious by now, this formulation of Sn reduces the n-period
stock market to a special case of a single-period stock marketâthere are
n
2n stocks, one invests w(j n ) in stock
a return x(j n ) for
j andn receives
n
n
stock j , and the total wealth Sn is j n w(j )x(j ).
We ï¬rst calculate the weight w â (j n ) associated with the best constant
rebalanced portfolio bâ . We observe that a constantly rebalanced portfolio
b results in
w(j n ) =

n


biji = bk (1 â b)nâk ,

(16.110)

i=1

where k is the number of times 1 appears in the sequence j n . Thus, w(j n )
depends only on k, the number of 1âs in j n . Fixing attention on j n , we

16.7 UNIVERSAL PORTFOLIOS

635

ï¬nd by differentiating with respect to b that the maximum value
w â (j n ) = max bk (1 â b)nâk

(16.111)

0â¤bâ¤1


 k 
n â k nâk
k
,
=
n
n

(16.112)


k nâk
,
.
n
n

(16.113)

which is achieved by
â



b =

 â n
Note that
w (j ) > 1, reï¬ecting the fact that the amount âbetâ on
j n is chosen in hindsight, thus relieving the hindsight investor of the
responsibility of allocating his investments w â (j n ) to sum to 1. The causal
investor has no suchluxury. How can the causal investor choose initial
wÌ(j n ) = 1, to protect himself from all possible j n
investments wÌ(j n ),
and hindsight-determined w â (j n )? The answer will be to choose wÌ(j n )
proportional to w â (j n ). Then the worst-case ratio of wÌ(j n )/w â (j n ) will
be maximized. To proceed, we deï¬ne Vn by
  k(j n ) k(j n )  n â k(j n ) nâk(j n )
1
=
(16.114)
Vn
n
n
n
j

=


n    k 

k
n
n â k nâk
k=0

k

n

(16.115)

n

and let

wÌ(j ) = Vn
n

k(j n )
n

k(j n ) 

n â k(j n )
n

nâk(j n )
.

(16.116)

of wealth over the 2n
It is clear that wÌ(j n ) is a legitimate distribution

n
n
stock sequences (i.e., wÌ(j ) â¥ 0 and
j n wÌ(j ) = 1). Here Vn is the
n
normalization factor that makes wÌ(j ) a probability mass function. Also,
from (16.109) and (16.113), for all sequences xn ,
SÌn (xn )
wÌ(j n )
â¥
min
j n w â (j n )
Snâ (xn )
nâk
Vn ( nk )k ( nâk
n )
= min âk
k
b (1 â bâ )nâk
â¥ Vn ,

(16.117)
(16.118)
(16.119)

636

INFORMATION THEORY AND PORTFOLIO THEORY

where (16.117) follows from (16.109) and (16.119) follows from (16.112).
Consequently, we have
max min
n
bÌ

x

SÌn (xn )
â¥ Vn .
Snâ (xn )

(16.120)

We have thus demonstrated a portfolio on the 2n possible sequences of
length n that achieves wealth SÌn (xn ) within a factor Vn of the wealth
Snâ (xn ) achieved by the best constant rebalanced portfolio in hindsight. To
complete the proof of the theorem, we show that this is the best possible,
that is, that any nonanticipating portfolio bi (xiâ1 ) cannot do better than
a factor Vn in the worst case (i.e., for the worst choice of xn ). To prove
this, we construct a set of extremal stock market sequences and show that
the performance of any nonanticipating portfolio strategy is bounded by
Vn for at least one of these sequences, proving the worst-case bound.
For each j n â {1, 2}n , we deï¬ne the corresponding extremal stock market vector xn (j n ) as

(1, 0)t if ji = 1,
xi (ji ) =
(16.121)
(0, 1)t if ji = 2,
Let e1 = (1, 0)t , e2 = (0, 1)t be standard basis vectors. Let
K = {x(j n ) : j n â {1, 2}n , xiji = eji }

(16.122)

be the set of extremal sequences. There are 2n such extremal sequences,
and for each sequence at each time, there is only one stock that yields
a nonzero return. The wealth invested in the other stock is lost. Therefore, the wealth at the end of n periods for extremal sequence xn (j n )
is the product

of the amounts invested in the stocks j1 , j2 , . . . , jn , [i.e.,
Sn (xn (j n )) = i bji = w(j n )]. Again, we can view this as an investment
on sequences of length n, and given the 0â1 nature of the return, it is
easy to see for xn â K that


Sn (xn (j n )) = 1.

(16.123)

jn

For any extremal sequence xn (j n ) â K, the best constant rebalanced portfolio is


n
n t
bâ (xn (j n )) = n1 (j ) , n2 (j ) ,
(16.124)
n
n

16.7 UNIVERSAL PORTFOLIOS

637

where n1 (j n ) is the number of occurrences of 1 in the sequence j n . The
corresponding wealth at the end of n periods is
Snâ (xn (j n ))


=

n1 (j n )
n

n1 (j n ) 

n2 (j n )
n

n2 (j n )

=

wÌ(j n )
,
Vn

(16.125)

from (16.116) and it therefore follows that


1 
1
wÌ(j n ) =
.
Vn n
Vn

Snâ (xn ) =

xn âK

(16.126)

j

We then have the following inequality for any portfolio sequence {bi }ni=1 ,
with Sn (xn ) deï¬ned as in (16.104):

Sn (xn )
Snâ (xÌn )
Sn (xÌn )

â¤
â n
â n
âK Snâ (xn )
xn âK Sn (x ) Sn (xÌ )
n

min
n

x

(16.127)

xÌ âK

=


xÌn âK

Sn (xÌn )
â n
xn âK Sn (x )



1
â n
xn âK Sn (x )

(16.128)

=

(16.129)

= Vn ,

(16.130)

where the inequality follows from the fact that the minimum is less than
the average. Thus,
Sn (xn )
â¤ Vn . 
âK Snâ (xn )

max min
n
b

x

(16.131)

The strategy described in the theorem puts mass on all sequences of
length n and is clearly dependent on n. We can recast the strategy in
incremental terms (i.e., in terms of the amount bet on stock 1 and stock
2 at time 1), then, conditional on the outcome at time 1, the amount bet
on each of the two stocks at time 2, and so on. Consider the weight
bÌi,1 assigned by the algorithm to stock 1 at time i given the previous
sequence of stock vectors xiâ1 . We can calculate this by summing over
all sequences j n that have a 1 in position i, giving

iâ1
1)x(j iâ1 )
j iâ1 âM iâ1 wÌ(j
iâ1

,
(16.132)
bÌi,1 (x ) =
i
iâ1 )
j i âM i wÌ(j )x(j

638

INFORMATION THEORY AND PORTFOLIO THEORY

where
wÌ(j i ) =



w(j n )

(16.133)

j n :j i âj n

is the weight put on all sequences j n that start with j i , and
x(j

iâ1

)=

iâ1


xkjk

(16.134)

k=1

is the return on those sequences as deï¬ned in (16.106).
Investigation of the asymptotics of Vn reveals [401, 496] that
 mâ1
â
2
Vn â¼
(m/2)/ Ï
(16.135)
n
for m assets. In particular, for m = 2 assets,

2
Vn â¼
Ïn

(16.136)

and
2
1
â¤ Vn â¤ â
â
2 n+1
n+1

(16.137)

for all n [400]. Consequently, for m = 2 stocks, the causal portfolio strategy bÌi (xiâ1 ) given in (16.132) achieves wealth SÌn (x n ) such that
SÌn (x n )
1
â¥ Vn â¥ â
â
n
Sn (x )
2 n+1

(16.138)

for all market sequences x n .
16.7.2

Horizon-Free Universal Portfolios

We describe the horizon-free strategy in terms of a weighting of different
portfolio strategies. As described earlier, each constantly rebalanced portfolio b can be viewed as corresponding to a mutual fund that rebalances
the m assets according to b. Initially, we distribute the wealth among
these funds according to a distribution Âµ(b), where dÂµ(b) is the amount
of wealth invested in portfolios in the neighborhood db of the constantly
rebalanced portfolio b.

16.7 UNIVERSAL PORTFOLIOS

Let
Sn (b, xn ) =

n


bt xi

639

(16.139)

i=1

be the wealth generated by a constant rebalanced portfolio b on the stock
sequence xn . Recall that
Snâ (xn ) = max Sn (b, xn )
bâB

(16.140)

is the wealth of the best constant rebalanced portfolio in hindsight.
We investigate the causal portfolio deï¬ned by

bSi (b, xi ) dÂµ(b)
i
.
(16.141)
bÌi+1 (x ) = B
i
B Si (b, x ) dÂµ(b)
We note that

bÌti+1 (xi )xi+1

B

=

=

bt xi+1 Si (b, xi ) dÂµ(b)

i
B Si (b, x ) dÂµ(b)

i+1
) dÂµ(b)
BSi+1 (b, x
.
i
B Si (b, x ) dÂµ(b)

(16.142)
(16.143)


 t
Thus, the product
bÌi xi telescopes and we see that the wealth SÌn (xn )
resulting from this portfolio is given by
SÌn (x ) =
n

n


bÌti (xiâ1 )xi

(16.144)

i=1


=

bâB

Sn (b, xn ) dÂµ(b).

(16.145)

There is another way to interpret (16.145). The amount given to portfolio manager b is dÂµ(b), the resulting growth factor for the manager
rebalancing to b is S(b, xn ), and the total wealth of this batch of investments is

Sn (b, xn ) dÂµ(b).
(16.146)
SÌn (xn ) =
B

Then bÌi+1 , deï¬ned in (16.141), is the performance-weighted total âbuy
orderâ of the individual portfolio manager b.

640

INFORMATION THEORY AND PORTFOLIO THEORY

So far, we have not speciï¬ed what distribution Âµ(b) we use to apportion
the initial wealth. We now use a distribution Âµ that puts mass on all
possible portfolios, so that we approximate the performance of the best
portfolio for the actual distribution of stock price vectors.
In the next lemma, we bound SÌn /Snâ as a function of the initial wealth
distribution Âµ(b).
Lemma 16.7.2 Let Snâ (xn ) in 16.140 be the wealth achieved by the best
constant rebalanced portfolio and let SÌn (xn ) in (16.144) be the wealth
achieved by the universal mixed portfolio bÌ(Â·), given by

bSi (b, xi ) dÂµ(b)
i
.
(16.147)
bÌi+1 (x ) = 
Si (b, xi ) dÂµ(b)
Then
SÌn (xn )
â¥ min
jn
Snâ (xn )

 
n
i=1 bji dÂµ(b)
B


.
n
â
i=1 bji

(16.148)

Proof: As before, we can write

w â (j n )x(j n ),
Snâ (xn ) =

(16.149)

jn



where w â (j n ) = ni=1 bjâi is the amount invested on the sequence j n and


x(j n ) = ni=1 xiji is the corresponding return. Similarly, we can write
 
n
n
SÌn (x ) =
bt xi dÂµ(b)
(16.150)
i=1

=

n
 
jn

=
where wÌ(j n ) =

 
n



bji xiji dÂµ(b)

(16.151)

i=1

wÌ(j n )x(j n ),

(16.152)

jn

i=1 bji

dÂµ(b). Now applying Lemma 16.7.1, we have

n
n
SÌn (xn )
j n wÌ(j )x(j )

=
(16.153)
â n
n
Snâ (xn )
j n w (j )x(j )
wÌ(j n )x(j n )
j
w â (j n )x(j n )
 
n
i=1 bji dÂµ(b)
B


.
= min
n
â
n
j
i=1 bji
â¥ min
n

(16.154)


(16.155)

16.7 UNIVERSAL PORTFOLIOS

641

We now apply this lemma when Âµ(b) is the Dirichlet( 12 ) distribution.
Theorem 16.7.2 For the causal universal portfolio bÌi ( ), i = 1, 2, . . .,
given in (16.141), with m = 2 stocks and dÂµ(b) the Dirichlet( 12 , 12 ) distribution, we have
SÌn (x n )
1
,
â¥ â
â
n
Sn (x )
2 n+1
for all n and all stock sequences x n .
Proof: As in the discussion preceding (16.112), we can show that the
weight put by the best constant portfolio bâ on the sequence j n is
n

i=1

bjâi


 k 
k
n â k nâk
=
= 2ânH (k/n) ,
n
n

(16.156)

where k is the number of indices where ji = 1. We can also explicitly
calculate the integral in the numerator of (16.148) in Lemma 16.7.2 for
the Dirichlet( 12 ) density, deï¬ned for m variables as
m
( m ) 
â1
bj 2 db,
dÂµ(b) =   12m
 2
j =1

(16.157)

â
where (x) = 0 eât t xâ1 dt denotes the gamma function. For simplicity,
we consider the case of two stocks, in which case
dÂµ(b) =

1
1
db,
â
Ï b(1 â b)

0 â¤ b â¤ 1,

(16.158)

where b is the fraction of wealth invested in stock 1. Now consider any
sequence j n â {1, 2}n , and consider the amount invested in that sequence,
b(j n ) =

n


bji = bl (1 â b)nâl ,

(16.159)

i=1

where l is the number of indices where ji = 1. Then


1
1
n
b(j ) dÂµ(b) = bl (1 â b)nâl â
db
Ï b(1 â b)

(16.160)

642

INFORMATION THEORY AND PORTFOLIO THEORY



1
=
Ï

1

1

blâ 2 (1 â b)nâlâ 2 db

(16.161)



1
1
1
= B l + ,n â l +
,
Ï
2
2

(16.162)



where B(Î»1 , Î»2 ) is the beta function, deï¬ned as
 1
B(Î»1 , Î»2 ) =
x Î»1 â1 (1 â x)Î»2 â1 dx

(16.163)

0

(Î»1 )(Î»2 )
(Î»1 + Î»2 )

=



and

â

(Î») =

x Î»â1 eâx dx.

(16.164)

(16.165)

0

â
Note that for any integer n, (n + 1) = n! and (n + 12 ) = 1Â·3Â·5Â·Â·Â·(2nâ1)
Ï.
2n
1
1
We can calculate B(l + 2 , n â l + 2 ) by means of simple recursion
using integration by parts. Alternatively, using (16.164), we obtain
  
2n n


1
Ï
1
l
n
(16.166)
= 2n   .
B l + ,n â l +
2n
2
2
2
2l
Combining all the results with Lemma 16.7.2, we have
 
n
SÌn (xn )
i=1 bji dÂµ(b)
B


â¥ min
n
â
â
n
n
j
Sn (x )
i=1 bji
â¥ min

1
Ï B(l

l

+ 12 , n â l + 12 )
2ânH (l/n)

1
â¥ â
,
2 n+1
using the results in [135, Theorem 2].

(16.167)

(16.168)
(16.169)


It follows for m = 2 stocks that
1
SÌn
â¥ â Vn
â
Sn
2Ï

(16.170)

643

16.7 UNIVERSAL PORTFOLIOS

for all n and all market sequences x1 , x2 , . . . , xnâ. Thus, good minimax performance for all n costs at most an extra factor 2Ï over the ï¬xed horizon
minimax portfolio. The cost of universality is Vn , which is asymptotically
negligible in the growth rate in the sense that
Vn
1
1
1
â 0.
ln SÌn (xn ) â ln Snâ (xn ) â¥ ln â
n
n
n
2Ï

(16.171)

Thus, the universal causal portfolio achieves the same asymptotic growth
rate of wealth as the best hindsight portfolio.
Letâs now consider how this portfolio algorithm performs on two real
stocks. We consider a 14-year period (ending in 2004) and two stocks,
Hewlett-Packard and Altria (formerly, Phillip Morris), which are both
components of the Dow Jones Index. Over these 14 years, HP went up by
a factor of 11.8, while Altria went up by a factor of 11.5. The performance
of the different constantly rebalanced portfolios that contain HP and Altria
are shown in Figure 16.2. The best constantly rebalanced portfolio (which
can be computed only in hindsight) achieves a growth of a factor of 18.7
using a mixture of about 51% HP and 49% Altria. The universal portfolio
strategy described in this section achieves a growth factor of 15.7 without
foreknowledge.

20

Sn*

Value Sn (b) of initial investment

18
16
14
12
10
8
6
4
2
0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Proportion b of wealth in HPQ

FIGURE 16.2. Performance of different constant rebalanced portfolios b for HP and Altria.

644

INFORMATION THEORY AND PORTFOLIO THEORY

16.8 SHANNONâMCMILLANâBREIMAN THEOREM
(GENERAL AEP)
The AEP for ergodic processes has come to be known as the Shannon âMcMillan âBreiman theorem. In Chapter 3 we proved the AEP for
i.i.d. processes. In this section we offer a proof of the theorem for a
general ergodic process. We prove the convergence of n1 log p(X n ) by
sandwiching it between two ergodic sequences.
In a sense, an ergodic process is the most general dependent process for
which the strong law of large numbers holds. For ï¬nite alphabet processes,
ergodicity is equivalent to the convergence of the kth-order empirical
distributions to their marginals for all k.
The technical deï¬nition requires some ideas from probability theory. To
be precise, an ergodic source is deï¬ned on a probability space (, B, P ),
where B is a Ï -algebra of subsets of  and P is a probability measure.
A random variable X is deï¬ned as a function X(Ï), Ï â , on the probability space. We also have a transformation T :  â , which plays
the role of a time shift. We will say that the transformation is stationary
if P (T A) = P (A) for all A â B. The transformation is called ergodic if
every set A such that T A = A, a.e., satisï¬es P (A) = 0 or 1. If T is stationary and ergodic, we say that the process deï¬ned by Xn (Ï) = X(T n Ï) is
stationary and ergodic. For a stationary ergodic source, Birkhoffâs ergodic
theorem states that
1
Xi (Ï) â EX =
n
n


X dP

with probability 1.

(16.172)

i=1

Thus, the law of large numbers holds for ergodic processes.
We wish to use the ergodic theorem to conclude that
1
1
log p(Xi |X0iâ1 )
â log p(X0 , X1 , . . . , Xnâ1 ) = â
n
n
nâ1
i=0

â lim E[â log p(Xn |X0nâ1 )].
nââ

(16.173)

But the stochastic sequence p(Xi |X0iâ1 ) is not ergodic. However, the
iâ1
iâ1
closely related quantities p(Xi |Xiâk
) and p(Xi |Xââ
) are ergodic and
have expectations easily identiï¬ed as entropy rates. We plan to sandwich
p(Xi |X0iâ1 ) between these two more tractable processes.

16.8 SHANNONâMCMILLANâBREIMAN THEOREM (GENERAL AEP)

645

We deï¬ne the kth-order entropy H k as
H k = E {â log p(Xk |Xkâ1 , Xkâ2 , . . . , X0 )}
= E {â log p(X0 |Xâ1 , Xâ2 , . . . , Xâk )} ,

(16.174)
(16.175)

where the last equation follows from stationarity. Recall that the entropy
rate is given by
H = lim H k
(16.176)
kââ

1 k
H .
= lim
nââ n
nâ1

(16.177)

k=0

Of course, H k  H by stationarity and the fact that conditioning does
not increase entropy. It will be crucial that H k  H = H â , where
H â = E {â log p(X0 |Xâ1 , Xâ2 , . . .)} .

(16.178)

The proof that H â = H involves exchanging expectation and limit.
The main idea in the proof goes back to the idea of (conditional) proportional gambling. A gambler receiving uniform odds with the knowledge of
the k past will have a growth rate of wealth log |X| â H k , while a gambler
with a knowledge of the inï¬nite past will have a growth rate of wealth
of log |X| â H â . We donât know the wealth growth rate of a gambler
with growing knowledge of the past X0n , but it is certainly sandwiched
between log |X| â H k and log |X| â H â . But H k  H = H â . Thus, the
sandwich closes and the growth rate must be log |X| â H .
We will prove the theorem based on lemmas that will follow the proof.
Theorem 16.8.1 (AEP: ShannonâMcMillanâBreiman Theorem)
If
H is the entropy rate of a ï¬nite-valued stationary ergodic process {Xn },
then
1
â log p(X0 , . . . , Xnâ1 ) â H with probability 1.
(16.179)
n
Proof: We prove this for ï¬nite alphabet X; this proof and the proof for
countable alphabets and densities is given in Algoet and Cover [20]. We
argue that the sequence of random variables â n1 log p(X0nâ1 ) is asymptotically sandwiched between the upper bound H k and the lower bound H â
for all k â¥ 0. The AEP will follow since H k â H â and H â = H . The
kth-order Markov approximation to the probability is deï¬ned for n â¥ k as
p k (X0nâ1 ) = p(X0kâ1 )

nâ1

i=k

iâ1
p(Xi |Xiâk
).

(16.180)

646

INFORMATION THEORY AND PORTFOLIO THEORY

From Lemma 16.8.3 we have
lim sup
nââ

p k (X0nâ1 )
1
â¤ 0,
log
n
p(X0nâ1 )

which we rewrite, taking the existence of the limit
account (Lemma 16.8.1), as
lim sup
nââ

(16.181)
1
n

log p k (X0n ) into

1
1
1
1
log
â¤ lim log
= Hk
nâ1
nâ1
k
nââ
n
n
p(X0 )
p (X0 )

(16.182)

for k = 1, 2, . . . . Also, from Lemma 16.8.3, we have
p(X0nâ1 )
1
lim sup log
â¤ 0,
â1
nââ n
p(X0nâ1 |Xââ
)

(16.183)

which we rewrite as
lim inf

1
1
1
1
log
â¥ lim log
= Hâ
nâ1
nâ1
â1
n
n
p(X0 )
p(X0 |Xââ )

(16.184)

from the deï¬nition of H â in Lemma 16.8.1.
Putting together (16.182) and (16.184), we have
1
1
H â â¤ lim inf â log p(X0nâ1 ) â¤ lim sup â log p(X0nâ1 )
n
n
â¤ Hk

for all k.

(16.185)

But by Lemma 16.8.2, H k â H â = H . Consequently,
1
lim â log p(X0n ) = H. 
n

(16.186)

We now prove the lemmas that were used in the main proof. The ï¬rst
lemma uses the ergodic theorem.
Lemma 16.8.1 (Markov approximations)
stochastic process {Xn },
1
â log p k (X0nâ1 ) â H k
n
1
â1
â log p(X0nâ1 |Xââ
) â Hâ
n

For a stationary ergodic

with probability 1,

(16.187)

with probability 1. (16.188)

16.8 SHANNONâMCMILLANâBREIMAN THEOREM (GENERAL AEP)

647

n ) of ergodic processes {X } are ergodic
Proof: Functions Yn = f (Xââ
i
nâ1
processes. Thus, log p(Xn |Xnâk ) and log p(Xn |Xnâ1 , Xnâ2 , . . . , ) are also
ergodic processes, and

1
1
1
iâ1
log p(Xi |Xiâk
) (16.189)
â log p k (X0nâ1 ) = â log p(X0kâ1 ) â
n
n
n
nâ1
i=k

â0+H

k

with probability 1,

(16.190)

by the ergodic theorem. Similarly, by the ergodic theorem,
1
1
log p(Xi |Xiâ1 , Xiâ2 , . . .)
â log p(X0nâ1 |Xâ1 , Xâ2 , . . .) = â
n
n
nâ1
i=0

(16.191)
âH

Lemma 16.8.2

â

with probability 1.  (16.192)

H k  H â and H = H â .

(No gap)

Proof: We know that for stationary processes, H k  H , so it remains
to show that H k  H â , thus yielding H = H â . Levyâs martingale
convergence theorem for conditional probabilities asserts that
â1
â1
) â p(x0 |Xââ
)
p(x0 |Xâk

with probability 1

(16.193)

for all x0 â X. Since X is ï¬nite and p log p is bounded and continuous in
p for all 0 â¤ p â¤ 1, the bounded convergence theorem allows interchange
of expectation and limit, yielding

lim H = lim E â
k

kââ

kââ


=E â

x0 âX


x0 âX

= H â.
Thus, H k  H = H â .



â1
â1
p(x0 |Xâk
) log p(x0 |Xâk
)

â1
â1
p(x0 |Xââ
) log p(x0 |Xââ
)


(16.194)


(16.195)
(16.196)


648

INFORMATION THEORY AND PORTFOLIO THEORY

Lemma 16.8.3

(Sandwich)
lim sup
nââ

lim sup

p k (X0nâ1 )
1
log
â¤ 0,
n
p(X0nâ1 )

(16.197)

p(X0nâ1 )
1
â¤ 0.
log
â1
n
p(X0nâ1 |Xââ
)

(16.198)

Proof: Let A be the support set of p(X0nâ1 ). Then

E

p k (X0nâ1 )



p(X0nâ1 )



=

p(x0nâ1 )

x0nâ1 âA



=

p k (x0nâ1 )

(16.199)

p(x0nâ1 )

p k (x0nâ1 )

(16.200)

x0nâ1 âA

= p k (A)

(16.201)

â¤ 1.

(16.202)

â1
â1
Similarly, let B(Xââ
) denote the support set of p(Â·|Xââ
). Then we have


E

p(X0nâ1 )

â1
p(X0nâ1 |Xââ
)



 
=E E
ï£®
ï£¯
= Eï£°
ï£®
ï£¯
= Eï£°

	

	
	 â1
X
â1 		 ââ
p(X0nâ1 |Xââ
)
p(X0nâ1 )


â1
x n âB(Xââ
)



p(x n )
â1
p(x n |Xââ
)

(16.203)
ï£¹

â1 ï£º
p(x n |Xââ
)ï£» (16.204)

ï£¹
ï£º
p(x n )ï£»

(16.205)

â1
x n âB(Xââ
)

â¤ 1.

(16.206)

By Markovâs inequality and (16.202), we have

Pr

p k (X0nâ1 )
p(X0nâ1 )


â¥ tn

â¤

1
tn

(16.207)

SUMMARY

649

or


p k (X0nâ1 )
1
1
Pr
â¥ log tn
log
nâ1
n
n
p(X0 )


â¤

1
.
tn

(16.208)

â 1
Letting tn = n2 and noting that
n=1 n2 < â, we see by the
BorelâCantelli lemma that the event


p k (X0nâ1 )
1
1
log
â¥ log tn
(16.209)
n
n
p(X0nâ1 )
occurs only ï¬nitely often with probability 1. Thus,
lim sup

p k (X0nâ1 )
1
â¤0
log
n
p(X0nâ1 )

with probability 1.

(16.210)

Applying the same arguments using Markovâs inequality to (16.206), we
obtain
lim sup

p(X0nâ1 )
1
â¤0
log
â1
n
p(X0nâ1 |Xââ
)

with probability 1,

(16.211)


proving the lemma.

The arguments used in the proof can be extended to prove the AEP for
the stock market (Theorem 16.5.3).

SUMMARY
Growth rate. The growth rate of a stock market portfolio b with
respect to a distribution F (x) is deï¬ned as



W (b, F ) = log bt x dF (x) = E log bt x .
(16.212)
Log-optimal portfolio. The optimal growth rate with respect to a distribution F (x) is
W â (F ) = max W (b, F ).
b

(16.213)

650

INFORMATION THEORY AND PORTFOLIO THEORY

The portfolio bâ that achieves the maximum of W (b, F ) is called the
log-optimal portfolio.
Concavity. W (b, F ) is concave in b and linear in F . W â (F ) is convex
in F .
Optimality conditions. The portfolio bâ is log-optimal if and only if


Xi
E
= 1 if biâ > 0,
bât X
â¤1
Expected ratio optimality. If Snâ =
E

Sn
â¤1
Snâ

if and only if

if biâ = 0.


n

i=1 b

E ln

ât

Xi , Sn =

(16.214)

n

t
i=1 bi Xi ,

Sn
â¤ 0.
Snâ

then

(16.215)

Growth rate (AEP)
1
log Snâ â W â (F )
n

with probability 1.

(16.216)

with probability 1.

(16.217)

Asymptotic optimality
lim sup
nââ

Sn
1
log â â¤ 0
n
Sn

Wrong information. Believing g when f is true loses
W = W (bâf , F ) â W (bâg , F ) â¤ D(f ||g).

(16.218)

Side information Y
W â¤ I (X; Y ).

(16.219)

Chain rule
W â (Xi |X1 , X2 , . . . , Xiâ1 ) =
W â (X1 , X2 , . . . , Xn ) =

n

i=1

max

bi (x1 ,x2 ,...,xiâ1 )

E log bti Xi

W â (Xi |X1 , X2 , . . . , Xiâ1 ).

(16.220)

(16.221)

SUMMARY

651

Growth rate for a stationary market.
â
= lim
Wâ

W â (X1 , X2 , . . . , Xn )
n

1
â
log Snâ â Wâ
.
n

(16.222)
(16.223)

Competitive optimality of log-optimal portfolios.
1
Pr(V S â¥ U â S â ) â¤ .
2

(16.224)


n
t iâ1
)xi
i=1 bÌi (x


max min
= Vn ,
n
t
n
bÌi (Â·) x ,b
i=1 b xi

(16.225)

Universal portfolio.

where

Vn =



n1 +Â·Â·Â·+nm =n

â1

n
.
2ânH (n1 /n,...,nm /n)
n 1 , n 2 , . . . , nm



For m = 2,

Vn â¼

$

2/Ï n

(16.226)

(16.227)

The causal universal portfolio

bSi (b, xi ) dÂµ(b)
bÌi+1 (x ) = 
Si (b, xi ) dÂµ(b)

(16.228)

SÌn (xn )
1
â¥ â
Snâ (xn )
2 n+1

(16.229)

i

achieves

for all n and all x n .
AEP. If {Xi } is stationary ergodic, then
1
â log p(X1 , X2 , . . . , Xn ) â H (X)
n

with probability 1. (16.230)

652

INFORMATION THEORY AND PORTFOLIO THEORY

PROBLEMS
16.1

Growth rate. Let
ï£±
ï£´
ï£² (1, a)
X=
ï£´
ï£³ (1, 1/a)

1
2
1
with probability
2

with probability

where a > 1. This vector X represents a stock market vector of
cash vs. a hot stock. Let
W (b, F ) = E log bt X
and

W â = max W (b, F )
b

be the growth rate.
(a) Find the log optimal portfolio bâ .
(b) Find the growth rate W â .
(c) Find the asymptotic behavior of
Sn =

n


bt Xi

i=1

for all b.
16.2

Side information.

Suppose, in Problem 16.1, that

1 if (X1 , X2 ) â¥ (1, 1),
Y=
0 if (X1 , X2 ) â¤ (1, 1).

Let the portfolio b depend on Y. Find the new growth rate W ââ
and verify that W = W ââ â W â satisï¬es
W â¤ I (X; Y ).
16.3

Stock dominance.

Consider a stock market vector
X = (X1 , X2 ).

Suppose that X1 = 2 with probability 1. Thus an investment in
the ï¬rst stock is doubled at the end of the day.

PROBLEMS

653

(a) Find necessary and sufï¬cient conditions on the distribution of
stock X2 such that the log-optimal portfolio bâ invests all the
wealth in stock X2 [i.e., bâ = (0, 1)].
(b) Argue for any distribution on X2 that the growth rate satisï¬es
W â â¥ 1.
16.4

Including experts and mutual funds. Let X â¼ F (x), x â Rm
+ , be
the vector of price relatives for a stock market. Suppose that an
âexpertâ suggests a portfolio b. This would result in a wealth
factor bt X. We add this to the stock alternatives to form XÌ =
(X1 , X2 , . . . , Xm , bt X). Show that the new growth rate,

â
ln(bt xÌ) dF (xÌ),
max
WÌ =
(16.231)
b1 ,...,bm ,bm+1

is equal to the old growth rate,

â
ln(bt x) dF (x).
W = max
b1 ,...,bm

(16.232)

16.5

Growth rate for symmetric distribution. Consider a stock vector X â¼ F (x), X â Rm , X â¥ 0, where the component stocks
are exchangeable. Thus, F (x1 , x2 , . . . , xm ) = F (xÏ (1) , xÏ (2) , . . . ,
xÏ (m) ) for all permutations Ï .
(a) Find the portfolio bâ optimizing the growth rate and establish
its optimality.
Now assume that X has been normalized so

X
that m1 m
i=1 i = 1, and F is symmetric as before.
(b) Again assuming X to be normalized, show that all symmetric
distributions F have the same growth rate against bâ .
(c) Find this growth rate.

16.6

Convexity. We are interested in the set of stock market densities
that yield the same optimal porfolio. Let Pb0 be the set of all
probability
on Rm
+ for which b0 is optimal. Thus, Pb0 =
 densities
t
{p(x) : ln(b x)p(x) dx is maximized by b = b0 }. Show that Pb0
is a convex set. It may be helpful to use Theorem 16.2.2.

16.7

Short selling. Let

X=

(1, 2),
(1, 12 ),

p,
1 â p.

Let B = {(b1 , b2 ) : b1 + b2 = 1}. Thus, this set of portfolios B
does not include the constraint bi â¥ 0. (This allows short selling.)

654

INFORMATION THEORY AND PORTFOLIO THEORY

(a) Find the log optimal portfolio bâ (p).
(b) Relate the growth rate W â (p) to the entropy rate H (p).
16.8

Normalizing x. Suppose that we deï¬ne the log-optimal portfolio
bâ to be the portfolio maximizing the relative growth rate

ln

1
m

bt x
m

i=1 xi

dF (x1 , . . . , xm ).


The virtue of the normalization m1
Xi , which can be viewed as
the wealth associated with a uniform portfolio, is that
 the relative
growth rate is ï¬nite even when the growth rate ln bt xdF (x)
is not. This matters, for example, if X has a St. Petersburg-like
distribution. Thus, the log-optimal portfolio bâ is deï¬ned for all
distributions F , even those with inï¬nite growth rates W â (F ).

(a) Show that if b maximizes ln(bt x) dF (x), it also maximizes
 bt x
ln ut x dF (x), where u = ( m1 , m1 , . . . , m1 ).
(b) Find the log optimal portfolio bâ for

X=

k

k

(22 +1 , 22 ),
k
k
(22 , 22 +1 ),

2â(k+1) ,
2â(k+1) ,

where k = 1, 2, . . . .
(c) Find EX and W â .
(d) Argue that bâ is competitively better than any portfolio b in
the sense that Pr{bt X > cbât X} â¤ 1c .
16.9

Universal portfolio. We examine the ï¬rst n = 2 steps of the
implementation of the universal portfolio in (16.7.2) for Âµ(b) uniform for m = 2 stocks. Let the stock vectors for days 1 and 2 be
x1 = (1, 12 ), and x2 = (1, 2). Let b = (b, 1 â b) denote a portfolio.


(a) Graph S2 (b) = 2i=1 bt xi , 0 â¤ b â¤ 1.
(b) Calculate S2â = maxb S2 (b).
(c) Argue that log S2 (b) is concave in b.
1
(d) Calculate the (universal) wealth SÌ2 = 0 S2 (b)db.
(e) Calculate the universal portfolio at times n = 1 and n = 2:


1

bÌ1 =

b db
0

HISTORICAL NOTES

655

1

bS1 (b) db
bÌ2 (x1 ) = 0 1
.
S
(b)
db
1
0
(f) Which of S2 (b), S2â , SÌ2 , bÌ2 are unchanged if we permute the
order of appearance of the stock vector outcomes [i.e., if the
sequence is now (1, 2), (1, 12 )]?
16.10

Growth optimal . Let X1 , X2 â¥ 0, be price relatives of two independent stocks. Suppose that EX1 > EX2 . Do you always want
some of X1 in a growth rate optimal portfolio S(b) = bX1 + bX2 ?
Prove or provide a counterexample.

16.11

Cost of universality. In the discussion of ï¬nite-horizon universal
portfolios, it was shown that the loss factor due to universality is

n    k 

k
1
n
n â k nâk
=
.
k
Vn
n
n

(16.233)

k=0

Evaluate Vn for n = 1, 2, 3.
16.12

Convex families. This problem generalizes Theorem 16.2.2. We
say that S is a convex family of random variables if S1 , S2 â S
implies that Î»S1 + (1 â Î»)S2 â S. Let S be a closed convex family
of random variables. Show that there is a random variable S â â S
such that
 
S
E ln
â¤0
(16.234)
Sâ
for all S â S if and only if

E

S
Sâ


â¤1

(16.235)

for all S â S.
HISTORICAL NOTES
There is an extensive literature on the meanâvariance approach to investment in the stock market. A good introduction is the book by Sharpe
[491]. Log-optimal portfolios were introduced by Kelly [308] and LataneÌ
[346], and generalized by Breiman [75]. The bound on the increase in the

656

INFORMATION THEORY AND PORTFOLIO THEORY

growth rate in terms of the mutual information is due to Barron and Cover
[31]. See Samuelson [453, 454] for a criticism of log-optimal investment.
The proof of the competitive optimality of the log-optimal portfolio
is due to Bell and Cover [39, 40]. Breiman [75] investigated asymptotic
optimality for random market processes.
The AEP was introduced by Shannon. The AEP for the stock market and the asymptotic optimality of log-optimal investment are given
in Algoet and Cover [21]. The relatively simple sandwich proof for the
AEP is due to Algoet and Cover [20]. The AEP for real-valued ergodic
processes was proved in full generality by Barron [34] and Orey [402].
The universal portfolio was deï¬ned in Cover [110] and the proof of
universality was given in Cover [110] and more exactly in Cover and
Ordentlich [135]. The ï¬xed-horizon exact calculation of the cost of universality Vn is given in Ordentlich and Cover [401]. The quantity Vn also
appears in data compression in the work of Shtarkov [496].

CHAPTER 17

INEQUALITIES IN
INFORMATION THEORY

This chapter summarizes and reorganizes the inequalities found throughout
this book. A number of new inequalities on the entropy rates of subsets
and the relationship of entropy and Lp norms are also developed. The
intimate relationship between Fisher information and entropy is explored,
culminating in a common proof of the entropy power inequality and the
BrunnâMinkowski inequality. We also explore the parallels between the
inequalities in information theory and inequalities in other branches of
mathematics, such as matrix theory and probability theory.
17.1

BASIC INEQUALITIES OF INFORMATION THEORY

Many of the basic inequalities of information theory follow directly from
convexity.
Deï¬nition A function f is said to be convex if
f (Î»x1 + (1 â Î»)x2 ) â¤ Î»f (x1 ) + (1 â Î»)f (x2 )

(17.1)

for all 0 â¤ Î» â¤ 1 and all x1 and x2 .
Theorem 17.1.1
then

(Theorem 2.6.2: Jensenâs inequality)
f (EX) â¤ Ef (X).

Lemma 17.1.1
0 < x < â.

If f is convex,

(17.2)

The function log x is concave and x log x is convex, for

Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

657

658

INEQUALITIES IN INFORMATION THEORY

Theorem 17.1.2 (Theorem 2.7.1: Log sum inequality)
numbers a1 , a2 , . . . , an and b1 , b2 , . . . , bn ,
 n 
n
n


ai
ai
ai log â¥
ai log i=1
n
bi
i=1 bi
i=1

with equality iff

ai
bi

For positive

(17.3)

i=1

= constant.

We recall the following properties of entropy from Section 2.1.
Deï¬nition The entropy H (X) of a discrete random variable X is deï¬ned by

H (X) = â
p(x) log p(x).
(17.4)
xâX

Theorem 17.1.3

(Lemma 2.1.1, Theorem 2.6.4: Entropy bound )
0 â¤ H (X) â¤ log |X|.

(17.5)

Theorem 17.1.4 (Theorem 2.6.5: Conditioning reduces entropy)
any two random variables X and Y ,
H (X|Y ) â¤ H (X),

For
(17.6)

with equality iff X and Y are independent.
Theorem 17.1.5

(Theorem 2.5.1 with Theorem 2.6.6: Chain rule)

H (X1 , X2 , . . . , Xn ) =

n


H (Xi |Xiâ1 , . . . , X1 ) â¤

i=1

n


H (Xi ),

(17.7)

i=1

with equality iff X1 , X2 , . . . , Xn are independent.
Theorem 17.1.6

(Theorem 2.7.3)

H (p) is a concave function of p.

We now state some properties of relative entropy and mutual information (Section 2.3).
Deï¬nition The relative entropy or KullbackâLeibler distance between
two probability mass functions p(x) and q(x) is deï¬ned by
D(p||q) =


xâX

p(x) log

p(x)
.
q(x)

(17.8)

659

17.1 BASIC INEQUALITIES OF INFORMATION THEORY

Deï¬nition The mutual information between two random variables X
and Y is deï¬ned by
I (X; Y ) =



p(x, y) log

xâX yâY

p(x, y)
= D(p(x, y)||p(x)p(y)).
p(x)p(y)
(17.9)

The following basic information inequality can be used to prove many
of the other inequalities in this chapter.
Theorem 17.1.7 (Theorem 2.6.3: Information inequality)
two probability mass functions p and q,
D(p||q) â¥ 0

For any
(17.10)

with equality iff p(x) = q(x) for all x â X.
Corollary For any two random variables X and Y ,
I (X; Y ) = D(p(x, y)||p(x)p(y)) â¥ 0

(17.11)

with equality iff p(x, y) = p(x)p(y) (i.e., X and Y are independent).
Theorem 17.1.8 (Theorem 2.7.2: Convexity
D(p||q) is convex in the pair (p, q).
Theorem 17.1.9

of

relative

entropy)

(Theorem 2.4.1 )
I (X; Y ) = H (X) â H (X|Y ).

(17.12)

I (X; Y ) = H (Y ) â H (Y |X).

(17.13)

I (X; Y ) = H (X) + H (Y ) â H (X, Y ).

(17.14)

I (X; X) = H (X).

(17.15)

Theorem 17.1.10

(Section 4.4)

For a Markov chain:

1. Relative entropy D(Âµn ||Âµn ) decreases with time.
2. Relative entropy D(Âµn ||Âµ) between a distribution and the stationary
distribution decreases with time.
3. Entropy H (Xn ) increases if the stationary distribution is uniform.
4. The conditional entropy H (Xn |X1 ) increases with time for a stationary Markov chain.

660

INEQUALITIES IN INFORMATION THEORY

Theorem 17.1.11 Let X1 , X2 , . . . , Xn be i.i.d. â¼ p(x). Let pÌn be the
empirical probability mass function of X1 , X2 , . . . , Xn . Then
ED(pÌn ||p) â¤ ED(pÌnâ1 ||p).
17.2

(17.16)

DIFFERENTIAL ENTROPY

We now review some of the basic properties of differential entropy
(Section 8.1).
Deï¬nition The differential entropy h(X1 , X2 , . . . , Xn ), sometimes written h(f ), is deï¬ned by

(17.17)
h(X1 , X2 , . . . , Xn ) = â f (x) log f (x) dx.
The differential entropy for many common densities is given in
Table 17.1.
Deï¬nition The relative entropy between probability densities f and
g is

D(f ||g) = f (x) log (f (x)/g(x)) dx.
(17.18)
The properties of the continuous version of relative entropy are identical to the discrete version. Differential entropy, on the other hand, has
some properties that differ from those of discrete entropy. For example,
differential entropy may be negative.
We now restate some of the theorems that continue to hold for differential entropy.
Theorem 17.2.1 (Theorem 8.6.1: Conditioning reduces
h(X|Y ) â¤ h(X), with equality iff X and Y are independent.
Theorem 17.2.2

entropy)

(Theorem 8.6.2: Chain rule)

h(X1 , X2 , . . . , Xn ) =

n


h(Xi |Xiâ1 , Xiâ2 , . . . , X1 ) â¤

i=1

n


h(Xi )

i=1

(17.19)
with equality iff X1 , X2 , . . . , Xn are independent.
Lemma 17.2.1

If X and Y are independent, then h(X + Y ) â¥ h(X).

Proof: h(X + Y ) â¥ h(X + Y |Y ) = h(X|Y ) = h(X).



17.2 DIFFERENTIAL ENTROPY

661

TABLE 17.1 Differential Entropiesa
Distribution
Name

Density

Entropy (nats)

x pâ1 (1 â x)qâ1
f (x) =
,
B(p, q)

ln B(p, q) â (p â 1)
Ã[Ï(p) â Ï(p + q)]
â(q â 1)[Ï(q) â Ï(p + q)]

Beta
0 â¤ x â¤ 1, p, q > 0
f (x) =
Cauchy

Î»
1
,
Ï Î»2 + x 2

ln(4Ï Î»)

ââ < x < â, Î» > 0
2

f (x) =

x
2
â
x nâ1 e 2Ï 2 ,
n/2
n
2 Ï (n/2)

Chi

ln
x > 0, n > 0
f (x) =

x
n
1
â
2 â 1 e 2Ï 2 ,
x
2n/2 Ï n (n/2)

Chi-squared
x > 0, n > 0
f (x) =
Erlang

Î²n
x nâ1 eâÎ²x ,
(n â 1)!

1 âx
e Î» , x, Î» > 0
Î»
n1

F

x(

n1
2 )

(n)
+n
Î²

1 + ln Î»

ln
â1

n1 +n2
n1 x) 2

n

2


n n
n
â 1â
Ï
+
2
2
2

n2

n2 n2
f (x) = 1n1 2n2
B( 2 , 2 )
Ã

ln 2Ï 2 

(1 â n)Ï(n) + ln

x, Î² > 0, n > 0
Exponential f (x) =

Ï (n/2) n â 1  n  n
Ï
+
â
â
2
2
2
2

n1  n1 n2 
B
,
n2
2 2
n 

1
n 	
+ 1 â 21 Ï
2

n2   n2 
â 1â
Ï
2 
 2

n1 + n2
n1 + n2
+
Ï
2
2

,

(n2 +
x > 0, n1 , n2 > 0

âx

Gamma

Laplace

x Î±â1 e Î²
, x, Î±, Î² > 0
Î² Î± (Î±)
1 â |xâÎ¸ |
f (x) =
e Î» ,
2Î»
ââ < x, Î¸ < â, Î» > 0

f (x) =

f (x) =
Logistic

eâx

(1+eâx )2

ln(Î²(Î±)) + (1 â Î±)Ï(Î±) + Î±

1 + ln 2Î»

,

ââ < x < â

2

662

INEQUALITIES IN INFORMATION THEORY

TABLE 17.1 (continued )
Distribution
Name

Density
f (x) =

Lognormal

â
1
â
e
Ï x 2Ï

Entropy (nats)

ln(xâm)2
2Ï 2

,

x > 0, ââ < m < â, Ï > 0
1

3

Maxwellâ
f (x) = 4Ï â 2 Î² 2 x 2 eâÎ²x ,
Boltzmann
x, Î² > 0

m+

1
2

ln(2Ï eÏ 2 )

2

1 Ï
1
ln + Î³ â
2 Î²
2

2

(xâÂµ)
1
â
f (x) = â
e 2Ï 2 ,
2Ï Ï 2

Normal

ââ < x, Âµ < â, Ï > 0

1
ln(2Ï eÏ 2 )
2

Î±

Generalized
normal

2Î² 2 Î±â1 âÎ²x 2
f (x) =
,
x e
( Î±2 )

ln

x, Î±, Î² > 0
Pareto

f (x) =

ak a
, x â¥ k > 0, a > 0
x a+1

Rayleigh

f (x) =

x â x2
e 2b ,
b2

f (x) =

(1 + x 2 /n)â(n+1)/2
,
â
nB( 12 , n2 )

2

Studentâs t

x, b > 0

ââ < x < â, n > 0

a

ln

( Î±2 )
1
2Î² 2

â

Î± â 1 Î±  Î±
Ï
+
2
2
2

k
1
+1+
a
a

Î²
Î³
1 + ln â +
2
2

n
n+1
âÏ
2
2



â
1 n
+ ln nB
,
2 2

n+1
Ï
2




Triangular

ï£±
2x
ï£´
ï£²
,
a
f (x) =
2(1 â x)
ï£´
ï£³
,
1âa

Uniform

f (x) =

1
, Î±â¤xâ¤Î²
Î² âÎ±

ln(Î² â Î±)

Weibull

f (x) =

c câ1 â x c
x e Î±,
Î±

(c â 1)Î³
Î±c
+ ln
+1
c
c

All entropies are in nats; (z) =

0.57721566 . . . .
Source: Lazo and Rathie [543].

â
0

0â¤xâ¤a
aâ¤xâ¤1

1
â ln 2
2

1

x, c, Î± > 0

eât t zâ1 dt; Ï(z) =

d
ln (z); Î³ = Eulerâs constant =
dz

17.3 BOUNDS ON ENTROPY AND RELATIVE ENTROPY

663

Theorem 17.2.3 (Theorem 8.6.5) Let the random vector X â Rn have
zero mean and covariance K = EXXt (i.e., Kij = EXi Xj , 1 â¤ i, j â¤ n).
Then
1
(17.20)
h(X) â¤ log(2Ï e)n |K|
2
with equality iff X â¼ N(0, K).
17.3

BOUNDS ON ENTROPY AND RELATIVE ENTROPY

In this section we revisit some of the bounds on the entropy function. The
most useful is Fanoâs inequality, which is used to bound away from zero
the probability of error of the best decoder for a communication channel
at rates above capacity.
Theorem 17.3.1 (Theorem 2.10.1: Fanoâs inequality) Given two random variables X and Y , let XÌ = g(Y ) be any estimator of X given Y and
let Pe = Pr(X = XÌ) be the probability of error. Then
H (Pe ) + Pe log |X| â¥ H (X|XÌ) â¥ H (X|Y ).

(17.21)

Consequently, if H (X|Y ) > 0, then Pe > 0.
A similar result is given in the following lemma.
Lemma 17.3.1
H (X)

(Lemma 2.10.1)

If X and X  are i.i.d. with entropy

Pr(X = X  ) â¥ 2âH (X)

(17.22)

with equality if and only if X has a uniform distribution.
The continuous analog of Fanoâs inequality bounds the mean-squared
error of an estimator.
Theorem 17.3.2 (Theorem 8.6.6 ) Let X be a random variable with
differential entropy h(X). Let XÌ be an estimate of X, and let E(X â XÌ)2
be the expected prediction error. Then
E(X â XÌ)2 â¥

1 2h(X)
e
.
2Ï e

(17.23)

Given side information Y and estimator XÌ(Y ),
E(X â XÌ(Y ))2 â¥

1 2h(X|Y )
e
.
2Ï e

(17.24)

664

INEQUALITIES IN INFORMATION THEORY

Theorem 17.3.3 (L1 bound on entropy) Let p and q be two probability mass functions on X such that

1
|p(x) â q(x)| â¤ .
(17.25)
||p â q||1 =
2
xâX

Then
|H (p) â H (q)| â¤ â||p â q||1 log

||p â q||1
.
|X|

(17.26)

Proof: Consider the function f (t) = ât log t shown in Figure 17.1. It
can be veriï¬ed by differentiation that the function f (Â·) is concave. Also,
f (0) = f (1) = 0. Hence the function is positive between 0 and 1. Consider the chord of the function from t to t + Î½ (where Î½ â¤ 12 ). The
maximum absolute slope of the chord is at either end (when t = 0 or
1 â Î½). Hence for 0 â¤ t â¤ 1 â Î½, we have
|f (t) â f (t + Î½)| â¤ max{f (Î½), f (1 â Î½)} = âÎ½ log Î½.

(17.27)

Let r(x) = |p(x) â q(x)|. Then






(âp(x) log p(x) + q(x) log q(x))
|H (p) â H (q)| = 


xâX

|(âp(x) log p(x) + q(x) log q(x))|
â¤

(17.28)
(17.29)

xâX
f(t ) = ât In t

0.4
0.35
0.3

ât In t

0.25
0.2
0.15
0.1
0.05
0

0

0.2

0.6

0.4

t

FIGURE 17.1. Function f (t) = ât ln t.

0.8

1

17.4 INEQUALITIES FOR TYPES

â¤



âr(x) log r(x)

xâX



(17.30)

r(x)
r(x)
log
||p â q||1
||p â q||1
||p â q||1
xâX



r(x)
= â||p â q||1 log ||p â q||1 + ||p â q||1 H
||p â q||1
= ||p â q||1

665

â

â¤ â||p â q||1 log ||p â q||1 + ||p â q||1 log |X|,

(17.31)
(17.32)
(17.33)


where (17.30) follows from (17.27).

Finally, relative entropy is stronger than the L1 norm in the following
sense:
Lemma 17.3.2

(Lemma 11.6.1)

1
(17.34)
||p1 â p2 ||21 .
2 ln 2
The relative entropy between two probability mass functions P (x) and
Q(x) is zero when P = Q. Around this point, the relative entropy has
a quadratic behavior, and the ï¬rst term in the Taylor series expansion of
the relative entropy D(P ||Q) around the point P = Q is the chi-squared
distance between the distributions P and Q. Let
D(p1 ||p2 ) â¥

Ï 2 (P , Q) =

 (P (x) â Q(x))2
x

Lemma 17.3.3

Q(x)

(17.35)

For P near Q,
1
D(P 	 Q) = Ï 2 + Â· Â· Â· .
2

(17.36)


Proof: See Problem 11.2.
17.4

.

INEQUALITIES FOR TYPES

The method of types is a powerful tool for proving results in large deviation theory and error exponents. We repeat the basic theorems.
Theorem 17.4.1 (Theorem 11.1.1)
inator n is bounded by

The number of types with denom-

|Pn | â¤ (n + 1)|X | .

(17.37)

666

INEQUALITIES IN INFORMATION THEORY

Theorem 17.4.2 (Theorem 11.1.2) If X1 , X2 , . . . , Xn are drawn i.i.d.
according to Q(x), the probability of x n depends only on its type and is
given by
Qn (x n ) = 2ân(H (Px n )+D(Px n ||Q)) .
Theorem 17.4.3
type P â Pn ,

(Theorem 11.1.3: Size of a type class T (P ))
1
2nH (P ) â¤ |T (P )| â¤ 2nH (P ) .
|
X
|
(n + 1)

(17.38)
For any

(17.39)

Theorem 17.4.4 (Theorem 11.1.4) For any P â Pn and any distribution Q, the probability of the type class T (P ) under Qn is 2ânD(P ||Q) to
ï¬rst order in the exponent. More precisely,
1
2ânD(P ||Q) â¤ Qn (T (P )) â¤ 2ânD(P ||Q) .
(n + 1)|X |

(17.40)

17.5

COMBINATORIAL BOUNDS ON ENTROPY
	
We give tight bounds on the size of nk when k is not 0 or n using the
result of Wozencraft and Reiffen [568]:
Lemma 17.5.1

For 0 < p < 1, q = 1 â p, such that np is an integer,

 
n ânH (p)
1
1
â¤
2
.
(17.41)
â¤â
â
np
Ï npq
8npq

Proof: We begin with a strong form of Stirlingâs approximation [208],
which states that
 n n
 n n 1
â
â
2Ï n
â¤ n! â¤ 2Ï n
e 12n .
(17.42)
e
e
Applying this to ï¬nd an upper bound, we obtain
1
â

2Ï n( ne )n e 12n
n
â¤â
â
np 2Ï nq( nq )nq
np
2Ï np( np
e )
e




1
1
1
e 12n
=â
np
nq
2Ï npq p q

(17.43)
(17.44)

17.6 ENTROPY RATES OF SUBSETS

1
<â
2nH (p) ,
Ï npq

667

(17.45)

1
â
1
since e 12n < e 12 = 1.087 < 2, hence proving the upper bound.
The lower bound is obtained similarly. Using Stirlingâs formula, we
obtain


1
1
â
â
+

 
2Ï n( ne )n e 12np 12nq
n
â¥â
(17.46)
â
np
nq
np
2Ï np( np
2Ï nq( nq
e )
e )


1
1
â 12np + 12nq
1
1
(17.47)
e
=â
2Ï npq p np q nq


1
1
â
+
1
2nH (p) e 12np 12nq .
=â
(17.48)
2Ï npq

If np â¥ 1, and nq â¥ 3, then


â
1
1
â 12np + 12nq
Ï
â 91
= 0.8862,
(17.49)
â¥ e = 0.8948 >
e
2
and the lower bound follows directly from substituting this into the equation. The exceptions to this condition are the cases where np = 1, nq = 1
or 2, and np = 2, nq = 2 (the case when np â¥ 3, nq = 1 or 2 can be
handled by ï¬ipping the roles of p and q). In each of these cases
n	
np = 1, nq = 1 â n = 2, p = 12 ,
np = 2, bound = 2
n	
np = 1, nq = 2 â n = 3, p = 13 ,
np = 3, bound = 2.92

n	
np = 2, nq = 2 â n = 4, p = 12 ,
np = 6, bound = 5.66.
Thus, even in these special cases, the bound is valid, and hence the lower
bound is valid for all p = 0, 1. Note that the lower bound blows up when

p = 0 or p = 1, and is therefore not valid.
17.6

ENTROPY RATES OF SUBSETS

We now generalize the chain rule for differential entropy. The chain rule
provides a bound on the entropy rate of a collection of random variables
in terms of the entropy of each random variable:
h(X1 , X2 , . . . , Xn ) â¤

n

i=1

h(Xi ).

(17.50)

668

INEQUALITIES IN INFORMATION THEORY

We extend this to show that the entropy per element of a subset of a set of
random variables decreases as the size of the subset increases. This is not
true for each subset but is true on the average over subsets, as expressed
in Theorem 17.6.1.
Deï¬nition Let (X1 , X2 , . . . , Xn ) have a density, and for every S â
{1, 2, . . . , n}, denote by X(S) the subset {Xi : i â S). Let
1  h(X(S))
.
h(n)
k = n	
k
k

(17.51)

S: |S|=k

Here h(n)
k is the average entropy in bits per symbol of a randomly drawn
k-element subset of {X1 , X2 , . . . , Xn }.
The following theorem by Han [270] says that the average entropy
decreases monotonically in the size of the subset.
Theorem 17.6.1
(n)
(n)
h(n)
1 â¥ h2 â¥ Â· Â· Â· â¥ hn .

(17.52)

(n)
Proof: We ï¬rst prove the last inequality, h(n)
n â¤ hnâ1 . We write

h(X1 , X2 , . . . , Xn ) = h(X1 , X2 , . . . , Xnâ1 )+h(Xn |X1 , X2 , . . . , Xnâ1 ),
h(X1 , X2 , . . . , Xn ) = h(X1 , X2 , . . . , Xnâ2 , Xn )
+ h(Xnâ1 |X1 , X2 , . . . , Xnâ2 , Xn ),
â¤ h(X1 , X2 , . . . , Xnâ2 , Xn )
+ h(Xnâ1 |X1 , X2 , . . . , Xnâ2 ),
..
.
h(X1 , X2 , . . . , Xn ) â¤ h(X2 , X3 , . . . , Xn ) + h(X1 ).

Adding these n inequalities and using the chain rule, we obtain
n h(X1 , X2 , . . . , Xn ) â¤

n


h(X1 , X2 , . . . , Xiâ1 , Xi+1 , . . . , Xn )

i=1

+ h(X1 , X2 , . . . , Xn )
or

(17.53)

1  h(X1 , X2 , . . . , Xiâ1 , Xi+1 , . . . , Xn )
1
h(X1 , X2 , . . . , Xn ) â¤
,
n
n
nâ1
n

i=1

(17.54)

17.6 ENTROPY RATES OF SUBSETS

669

(n)
(n)
(n)
which is the desired result h(n)
n â¤ hnâ1 . We now prove that hk â¤ hkâ1
for all k â¤ n by ï¬rst conditioning on a k-element subset, and then taking
a uniform choice over its (k â 1)-element subsets. For each k-element
(k)
subset, h(k)
k â¤ hkâ1 , and hence the inequality remains true after taking
the expectation over all k-element subsets chosen uniformly from the n

elements.

Theorem 17.6.2

Let r > 0, and deï¬ne
1  r h(X(S))
tk(n) = n	
e k .

(17.55)

k S: |S|=k

Then
t1(n) â¥ t2(n) â¥ Â· Â· Â· â¥ tn(n) .

(17.56)

Proof: Starting from (17.54), we multiply both sides by r, exponentiate,
and then apply the arithmetic mean geometric mean inequality, to obtain
1

e n rh(X1 , X2 , . . . , Xn )
rh(X1 ,X2 ,...,Xiâ1 ,Xi+1 ,...,Xn )
1 n
i=1
n
(nâ1)
â¤e
iâ1 ,Xi+1 ,...,Xn )
1  rh(X1 ,X2 ,...,X
(nâ1)
e
n

(17.57)

n

â¤

for all r â¥ 0,

(17.58)

i=1

(n)
. Now we use the same arguments as
which is equivalent to tn(n) â¤ tnâ1
in Theorem 17.6.1, taking an average over all subsets to prove the result
(n)
.

that for all k â¤ n, tk(n) â¤ tkâ1

Deï¬nition The average conditional entropy rate per element for all
subsets of size k is the average of the above quantities for k-element
subsets of {1, 2, . . . , n}:
1  h(X(S)|X(S c ))
.
gk(n) = n	
k
k

(17.59)

S:|S|=k

Here gk (S) is the entropy per element of the set S conditional on the
elements of the set S c . When the size of the set S increases, one can
expect a greater dependence among the elements of the set S, which
explains Theorem 17.6.1.

670

INEQUALITIES IN INFORMATION THEORY

In the case of the conditional entropy per element, as k increases, the
size of the conditioning set S c decreases and the entropy of the set S
increases. The increase in entropy per element due to the decrease in
conditioning dominates the decrease due to additional dependence among
the elements, as can be seen from the following theorem due to Han [270].
Note that the conditional entropy ordering in the following theorem is the
reverse of the unconditional entropy ordering in Theorem 17.6.1.
Theorem 17.6.3
g1(n) â¤ g2(n) â¤ Â· Â· Â· â¤ gn(n) .

(17.60)

Proof: The proof proceeds on lines very similar to the proof of the
theorem for the unconditional entropy per element for a random subset.
(n)
We ï¬rst prove that gn(n) â¥ gnâ1
and then use this to prove the rest of
the inequalities. By the chain rule, the entropy of a collection of random
variables is less than the sum of the entropies:
h(X1 , X2 , . . . , Xn ) â¤

n


h(Xi ).

(17.61)

i=1

Subtracting both sides of this inequality from nh(X1 , X2 , . . . , Xn ), we
have
(n â 1)h(X1 , X2 , . . . , Xn ) â¥

n


(h(X1 , X2 , . . . , Xn ) â h(Xi )) (17.62)

i=1

=

n


h(X1 , . . . , Xiâ1 , Xi+1 , . . . , Xn |Xi ).

i=1

(17.63)

Dividing this by n(n â 1), we obtain

1  h(X1 , X2 , . . . , Xiâ1 , Xi+1 , . . . , Xn |Xi )
h(X1 , X2 , . . . , Xn )
â¥
,
n
n
nâ1
n

i=1

(17.64)
(n)
which is equivalent to
â¥
We now prove that
â¥ gkâ1
for
all k â¤ n by ï¬rst conditioning on a k-element subset and then taking
a uniform choice over its (k â 1)-element subsets. For each k-element
(k)
subset, gk(k) â¥ gkâ1
, and hence the inequality remains true after taking
the expectation over all k-element subsets chosen uniformly from the n

elements.
gn(n)

(n)
gnâ1
.

gk(n)

17.7 ENTROPY AND FISHER INFORMATION

Theorem 17.6.4

671

Let
1  I (X(S); X(S c ))
fk(n) = n	
.
k
k

(17.65)

f1(n) â¥ f2(n) â¥ Â· Â· Â· â¥ fn(n) .

(17.66)

S:|S|=k

Then

Proof: The theorem follows from the identity I (X(S); X(S c )) =
h(X(S)) â h(X(S)|X(S c )) and Theorems 17.6.1 and 17.6.3.

17.7

ENTROPY AND FISHER INFORMATION

The differential entropy of a random variable is a measure of its descriptive
complexity. The Fisher information is a measure of the minimum error
in estimating a parameter of a distribution. In this section we derive a
relationship between these two fundamental quantities and use this to
derive the entropy power inequality.
Let X be any random variable with density f (x). We introduce a location parameter Î¸ and write the density in a parametric form as f (x â Î¸ ).
The Fisher information (Section 11.10) with respect to Î¸ is given by

2
 â
â
f (x â Î¸ )
(17.67)
ln f (x â Î¸ ) dx.
J (Î¸ ) =
âÎ¸
ââ
In this case, differentiation with respect to x is equivalent to differentiation
with respect to Î¸ . So we can write the Fisher information as

2
 â
â
J (X) =
f (x â Î¸ )
ln f (x â Î¸ ) dx
âx
ââ
2

 â
â
ln f (x) dx,
f (x)
(17.68)
=
âx
ââ
which we can rewrite as

2
 â
â
f
(x)
J (X) =
f (x) âx
dx.
(17.69)
f (x)
ââ
We will call this the Fisher information of the distribution of X. Notice
that like entropy, it is a function of the density.
The importance of Fisher information is illustrated in the following
theorem.

672

INEQUALITIES IN INFORMATION THEORY

Theorem 17.7.1
(Theorem 11.10.1: CrameÌrâRao inequality) The
mean-squared error of any unbiased estimator T (X) of the parameter Î¸ is
lower bounded by the reciprocal of the Fisher information:
1
.
(17.70)
var(T ) â¥
J (Î¸ )
We now prove a fundamental relationship between the differential
entropy and the Fisher information:
Theorem 17.7.2 (de Bruijnâs identity: entropy and Fisher information)
Let X be any random variable with a ï¬nite variance with a density f (x).
Let Z be an independent normally distributed random variable with zero
mean and unit variance. Then
â
â
â
1
(17.71)
he (X + tZ) = J (X + tZ),
ât
2
where he is the differential entropy to base e. In particular, if the limit
exists as t â 0,

â 
â
1
he (X + tZ)
(17.72)
= J (X).
ât
2
t=0
â
Proof: Let Yt = X + tZ. Then the density of Yt is
 â
(yâx)2
1
eâ 2t dx.
f (x) â
(17.73)
gt (y) =
2Ï t
ââ
Then


 â
(yâx)2
â
â
1
eâ 2t
dx
(17.74)
f (x)
â
gt (y) =
ât
ât
2Ï t
ââ

 â
(yâx)2
1 1
=
f (x) â â
eâ 2t
2t 2Ï t
ââ

(yâx)2
(y â x)2 1
eâ 2t
dx.
(17.75)
+
â
2t 2
2Ï t
We also calculate



(yâx)2
â
â 2t
f (x) â
dx
e
2Ï t ây
ââ


 â
2
y â x â (yâx)
1
2t
â
dx
f (x) â
e
=
t
2Ï t
ââ

â
gt (y) =
ây



â

1

(17.76)

(17.77)

17.7 ENTROPY AND FISHER INFORMATION

and

673



2
â
y â x â (yâx)
2t
e
f (x) â
dx
(17.78)
â
t
2Ï t ây
ââ


 â
2
2
1
(y â x)2 â (yâx)
1 â (yâx)
2t
2t
f (x) â
+
e
dx.
=
â e
t
t2
2Ï t
ââ
(17.79)

â2
gt (y) =
ây 2



â

1

Thus,
1 â2
â
gt (y) =
gt (y).
ât
2 ây 2

(17.80)

We will use this relationship to calculate the derivative of the entropy of
Yt , where the entropy is given by
 â
he (Yt ) = â
gt (y) ln gt (y) dy.
(17.81)
ââ

Differentiating, we obtain
 â
 â
â
â
â
he (Yt ) = â
gt (y) dy â
gt (y) ln gt (y) dy
(17.82)
ât
ââ ât
ââ ât
 â

â
1 â â2
=â
gt (y) dy â
gt (y) ln gt (y) dy. (17.83)
ât ââ
2 ââ ây 2

The ï¬rst term is zero since gt (y) dy = 1. The second term can be integrated by parts to obtain

â
2


1 âgt (y)
1 â â
1
â
he (Yt ) = â
ln gt (y)
gt (y)
dy.
+
ât
2
ây
2 ââ ây
gt (y)
ââ
(17.84)
The second term in (17.84) is 12 J (Yt ). So the proof will be complete if
we show that the ï¬rst term in (17.84) is zero. We can rewrite the ï¬rst
term as
 âgt (y) 

 

âgt (y)
ây
ln gt (y) = â
(17.85)
2 gt (y) ln gt (y) .
ây
gt (y)
The square of the ï¬rst factor integrates to the Fisher information and
hence must be bounded as y â Â±â. The second factor goes to zero since
x ln x â 0 as x â 0 and gt (y) â 0 as y â Â±â. Hence, the ï¬rst term in

674

INEQUALITIES IN INFORMATION THEORY

(17.84) goes to 0 at both limits and the theorem is proved. In the proof, we
have exchanged integration and differentiation in (17.74), (17.76), (17.78),
and (17.82). Strict justiï¬cation of these exchanges requires the application
of the bounded convergence and mean value theorems; the details may

be found in Barron [30].
This theorem can be used to prove the entropy power inequality, which
gives a lower bound on the entropy of a sum of independent random
variables.
Theorem 17.7.3 (Entropy power inequality) If X and Y are independent random n-vectors with densities, then
2 n h(X + Y) â¥ 2 n h(X) + 2 n h(Y) .
2

2

2

(17.86)

We outline the basic steps in the proof due to Stam [505] and Blachman
[61]. A different proof is given in Section 17.8.
Stamâs proof of the entropy powerâinequality is based on
â a perturbation
argument. Let n = 1. Let Xt = X + f (t)Z1 , Yt = Y + g(t)Z2 , where
Z1 and Z2 are independent N(0, 1) random variables. Then the entropy
power inequality for n = 1 reduces to showing that s(0) â¤ 1, where we
deï¬ne
22h(Xt ) + 22h(Yt )
.
(17.87)
22h(Xt +Yt )
If f (t) â â and g(t) â â as t â â, it is easy to show that s(â) = 1.
If, in addition, s  (t) â¥ 0 for t â¥ 0, this implies that s(0) â¤ 1. The proof
of the fact that s  (t) â¥ 0 involves a clever choice of the functions f (t)
and g(t), an application of Theorem 17.7.2 and the use of a convolution
inequality for Fisher information,
s(t) =

1
1
1
â¥
+
.
J (X + Y )
J (X) J (Y )

(17.88)

The entropy power inequality can be extended to the vector case by
induction. The details may be found in the papers by Stam [505] and
Blachman [61].
17.8 ENTROPY POWER INEQUALITY AND
BRUNNâMINKOWSKI INEQUALITY
The entropy power inequality provides a lower bound on the differential
entropy of a sum of two independent random vectors in terms of their
individual differential entropies. In this section we restate and outline an

675

17.8 ENTROPY POWER INEQUALITY AND BRUNNâMINKOWSKI INEQUALITY

alternative proof of the entropy power inequality. We also show how the
entropy power inequality and the BrunnâMinkowski inequality are related
by means of a common proof.
We can rewrite the entropy power inequality for dimension n = 1 in
a form that emphasizes its relationship to the normal distribution. Let
X and Y be two independent random variables with densities, and let
X  and Y  be independent normals with the same entropy as X and

Y , respectively. Then 22h(X) = 22h(X ) = (2Ï e)ÏX2  and similarly, 22h(Y ) =
(2Ï e)ÏY2 . Hence the entropy power inequality can be rewritten as




22h(X+Y ) â¥ (2Ï e)(ÏX2  + ÏY2 ) = 22h(X +Y ) ,

(17.89)

since X  and Y  are independent. Thus, we have a new statement of the
entropy power inequality.
Theorem 17.8.1 (Restatement of the entropy power inequality)
two independent random variables X and Y ,
h(X + Y ) â¥ h(X  + Y  ),

For

(17.90)

where X  and Y  are independent normal random variables with h(X  ) =
h(X) and h(Y  ) = h(Y ).
This form of the entropy power inequality bears a striking resemblance
to the BrunnâMinkowski inequality, which bounds the volume of set
sums.
Deï¬nition The set sum A + B of two sets A, B â Rn is deï¬ned as the
set {x + y : x â A, y â B}.
Example 17.8.1
radius 2.

The set sum of two spheres of radius 1 is a sphere of

Theorem 17.8.2 (BrunnâMinkowski inequality) The volume of the set
sum of two sets A and B is greater than the volume of the set sum of two
spheres A and B  with the same volume as A and B, respectively:
V (A + B) â¥ V (A + B  ),

(17.91)

where A and B  are spheres with V (A ) = V (A) and V (B  ) = V (B).
The similarity between the two theorems was pointed out in [104].
A common proof was found by Dembo [162] and Lieb, starting from a

676

INEQUALITIES IN INFORMATION THEORY

strengthened version of Youngâs inequality. The same proof can be used to
prove a range of inequalities which includes the entropy power inequality
and the BrunnâMinkowski inequality as special cases. We begin with a
few deï¬nitions.
Deï¬nition Let f and g be two densities over Rn and let f â g denote
the convolution of the two densities. Let the Lr norm of the density be
deï¬ned by


1
r
r
f (x) dx
.
(17.92)
||f ||r =
Lemma 17.8.1 (Strengthened Youngâs inequality) For any two densities f and g over Rn ,


n
Cp Cq 2
||f ||p ||g||q ,
(17.93)
||f â g||r â¤
Cr
where

1
1
1
= + â1
r
p q

and

1

Cp =

pp
p

1
 p

,

1
1
+  = 1.
p p

(17.94)

(17.95)

Proof: The proof of this inequality may be found in [38] and [73]. 
We deï¬ne a generalization of the entropy.
Deï¬nition The Renyi entropy hr (X) of order r is deï¬ned as


1
r
hr (X) =
log
f (x) dx
1âr

(17.96)

for 0 < r < â, r = 1. If we take the limit as r â 1, we obtain the Shannon entropy function,

h(X) = h1 (X) = â f (x) log f (x) dx.
(17.97)
If we take the limit as r â 0, we obtain the logarithm of the volume of
the support set,
h0 (X) = log (Âµ{x : f (x) > 0}) .

(17.98)

17.8 ENTROPY POWER INEQUALITY AND BRUNNâMINKOWSKI INEQUALITY

677

Thus, the zeroth-order Renyi entropy gives the logarithm of the measure
of the support set of the density f , and the Shannon entropy h1 gives the
logarithm of the size of the âeffectiveâ support set (Theorem 8.2.2). We
now deï¬ne the equivalent of the entropy power for Renyi entropies.
Deï¬nition The Renyi entropy power Vr (X) of order r is deï¬ned as
ï£±
â 2 r 
 r
ï£´
ï£´
ï£²
0 < r â¤ â , r = 1 , 1r + r1 = 1
f (x) dx n r ,
2
Vr (X) =
exp [ n h(X)],
r=1
ï£´
ï£´
2
ï£³
Âµ({x : f (x) > 0}) n , r = 0
(17.99)
Theorem 17.8.3 For two independent random variables X and Y and
any 0 â¤ r < â and any 0 â¤ Î» â¤ 1, we have
log Vr (X + Y ) â¥ Î» log Vp (X) + (1 â Î») log Vq (Y ) + H (Î»)
 

 r 	
r+Î»(1âr)
+ 1+r
(17.100)
H
â
H
1âr
1+r
1+r ,
r
where p = (r+Î»(1âr))
,q=
log(1 â Î»).

r
(r+(1âÎ»)(1âr))

and H (Î») = âÎ» log Î» â (1 â Î»)

Proof: If we take the logarithm of Youngâs inequality (17.93), we obtain
1
1
1
log Vr (X + Y ) â¥  log Vp (X) +  log Vq (Y ) + log Cr

r
p
q
â log Cp â log Cq .
Setting Î» = r  /p  and using (17.94), we have 1 â Î» = r  /q  , p =
r
and q = r+(1âÎ»)(1âr)
. Thus, (17.101) becomes

(17.101)
r
r+Î»(1âr)

r
log r â log r 
r
r
r
r
r
â log p +  log p  â log q +  log q 
p
p
q
q
(17.102)

log Vr (X + Y ) â¥ Î» log Vp (X) + (1 â Î») log Vq (Y ) +

= Î» log Vp (X) + (1 â Î») log Vq (Y )
r
log r â (Î» + 1 â Î») log r 
r
r
r
â log p + Î» log p  â log q + (1 â Î») log q 
p
q
(17.103)
+

678

INEQUALITIES IN INFORMATION THEORY

= Î» log Vp (X) + (1 â Î») log Vq (Y ) +

1
log r + H (Î»)
r â1

r + Î»(1 â r)
r
log
r â1
r + Î»(1 â r)
r + (1 â Î»)(1 â r)
r
log
â
r â1
r + (1 â Î»)(1 â r)
(17.104)

â

= Î» log Vp (X) + (1 â Î») log Vq (Y ) + H (Î»)
 





r + Î»(1 â r)
r
1+r
+
H
âH
,
1âr
1+r
1+r
(17.105)
where the details of the algebra for the last step are omitted.



The BrunnâMinkowski inequality and the entropy power inequality
can then be obtained as special cases of this theorem.
â¢

The entropy power inequality. Taking the limit of (17.100) as r â 1
and setting
Î»=
we obtain

â¢

V1 (X)
,
V1 (X) + V1 (Y )

(17.106)

V1 (X + Y ) â¥ V1 (X) + V1 (Y ),

(17.107)

which is the entropy power inequality.
The BrunnâMinkowski inequality. Similarly, letting r â 0 and choosing
â
V0 (X)
,
(17.108)
Î»= â
â
V0 (X) + V0 (Y )
we obtain



V0 (X + Y ) â¥



V0 (X) +



V0 (Y ).

(17.109)

Now let A be the support set of X and B be the support set of Y .
Then A + B is the support set of X + Y , and (17.109) reduces to
1

1

1

[Âµ(A + B)] n â¥ [Âµ(A)] n + [Âµ(B)] n ,
which is the BrunnâMinkowski inequality.

(17.110)

17.9

INEQUALITIES FOR DETERMINANTS

The general theorem uniï¬es the entropy power inequality
BrunnâMinkowski inequality and introduces a continuum
inequalities that lie between the entropy power inequality
BrunnâMinkowski inequality. This further strengthens the
between entropy power and volume.
17.9

679

and the
of new
and the
analogy

INEQUALITIES FOR DETERMINANTS

Throughout the remainder of this chapter, we assume that K is a nonnegative deï¬nite symmetric n Ã n matrix. Let |K| denote the determinant of
K.
We ï¬rst give an information-theoretic proof of a result due to Ky Fan
[199].
Theorem 17.9.1

log |K| is concave.

Proof: Let X1 and X2 be normally distributed n-vectors, Xi â¼ N(0, Ki ),
i = 1, 2. Let the random variable Î¸ have the distribution
Pr{Î¸ = 1} = Î»,

(17.111)

Pr{Î¸ = 2} = 1 â Î»

(17.112)

for some 0 â¤ Î» â¤ 1. Let Î¸ , X1 , and X2 be independent, and let Z =
XÎ¸ . Then Z has covariance KZ = Î»K1 + (1 â Î»)K2 . However, Z will
not be multivariate normal. By ï¬rst using Theorem 17.2.3, followed by
Theorem 17.2.1, we have
1
log(2Ï e)n |Î»K1 + (1 â Î»)K2 | â¥ h(Z)
2
â¥ h(Z|Î¸ )

(17.113)
(17.114)

1
= Î» log(2Ï e)n |K1 |
2
1
+ (1 â Î») log(2Ï e)n |K2 |.
2
Thus,
|Î»K1 + (1 â Î»)K2 | â¥ |K1 |Î» |K2 |1âÎ» ,
as desired.

(17.115)


We now give Hadamardâs inequality using an information-theoretic
proof [128].

680

INEQUALITIES IN INFORMATION THEORY

Theorem 17.9.2
0, i = j .

|K| â¤ 
Kii , with equality iff Kij =

(Hadamard)

Proof: Let X â¼ N(0, K). Then

1
1
h(Xi ) =
log(2Ï e)n |K| = h(X1 , X2 , . . . , Xn ) â¤
log 2Ï e|Kii |,
2
2
n

i=1

(17.116)
 j ).
with equality iff X1 , X2 , . . . , Xn are independent (i.e., Kij = 0, i =
We now prove a generalization of Hadamardâs inequality due to Szasz
[391]. Let K(i1 , i2 , . . . , ik ) be the k Ã k principal submatrix of K formed
by the rows and columns with indices i1 , i2 , . . . , ik .
Theorem 17.9.3 (Szasz ) If K is a positive deï¬nite n Ã n matrix and
Pk denotes the product of the determinants of all the principal k-rowed
minors of K, that is,

Pk =
|K(i1 , i2 , . . . , ik )|,
(17.117)
1â¤i1 <i2 <Â·Â·Â·<ik â¤n

then

1

1

(nâ1
1 )

(nâ1
2 )

P1 â¥ P2

â¥ P3

â¥ Â· Â· Â· â¥ Pn .

(17.118)

Proof: Let X â¼ N(0, K). Then the theorem follows directly from
1
1
Theorem 17.6.1, with the identiï¬cation h(n)
k = 2n(nâ1) log Pk + 2 log 2Ï e.
kâ1

We can also prove a related theorem.
Let K be a positive deï¬nite n Ã n matrix and let

Theorem 17.9.4
1
Sk(n) = n	



1

|K(i1 , i2 , . . . , ik )| k .

(17.119)

k 1â¤i1 <i2 <Â·Â·Â·<ik â¤n

Then
1
1
tr(K) = S1(n) â¥ S2(n) â¥ Â· Â· Â· â¥ Sn(n) = |K| n .
n

(17.120)

Proof: This follows directly from the corollary to Theorem 17.6.1, with

the identiï¬cation tk(n) = (2Ï e)Sk(n) and r = 2.

17.9

Theorem 17.9.5

INEQUALITIES FOR DETERMINANTS

681

Let
ï£«
Qk = ï£­



S:|S|=k

ï£¶
|K| ï£¸
|K(S c )|

1
k (nk)

.

(17.121)

Then
 n


1

n

Ïi2

1

= Q1 â¤ Q2 â¤ Â· Â· Â· â¤ Qnâ1 â¤ Qn = |K| n .

(17.122)

i=1

Proof: The theorem follows immediately from Theorem 17.6.3 and the
identiï¬cation
1
|K|
h(X(S)|X(S c )) = log(2Ï e)k
. 
(17.123)
2
|K(S c )|
The outermost inequality, Q1 â¤ Qn , can be rewritten as
|K| â¥

n


Ïi2 ,

(17.124)

i=1

where
|K|
(17.125)
|K(1, 2 . . . , i â 1, i + 1, . . . , n)|
is the minimum mean-squared error in the linear prediction of Xi from
the remaining Xâs. Thus, Ïi2 is the conditional variance of Xi given the
remaining Xj âs if X1 , X2 , . . . , Xn are jointly normal. Combining this with
Hadamardâs inequality gives upper and lower bounds on the determinant
of a positive deï¬nite matrix.
Ïi2 =

Corollary


i

Kii â¥ |K| â¥



Ïi2 .

(17.126)

i

Hence, the determinant of a covariance matrix lies between the product
of the unconditional variances Kii of the random variables Xi and the
product of the conditional variances Ïi2 .
We now prove a property of Toeplitz matrices, which are important as
the covariance matrices of stationary random processes. A Toeplitz matrix
K is characterized by the property that Kij = Krs if |i â j | = |r â s|.
Let Kk denote the principal minor K(1, 2, . . . , k). For such a matrix, the
following property can be proved easily from the properties of the entropy
function.

682

INEQUALITIES IN INFORMATION THEORY

Theorem 17.9.6

If the positive deï¬nite n Ã n matrix K is Toeplitz, then
1

1

1

|K1 | â¥ |K2 | 2 â¥ Â· Â· Â· â¥ |Knâ1 | (nâ1) â¥ |Kn | n

(17.127)

and |Kk |/|Kkâ1 | is decreasing in k, and
|Kn |
.
nââ |Knâ1 |

1

lim |Kn | n = lim

nââ

(17.128)

Proof: Let (X1 , X2 , . . . , Xn ) â¼ N(0, Kn ). We observe that
h(Xk |Xkâ1 , . . . , X1 ) = h(X k ) â h(X kâ1 )
1
|Kk |
log(2Ï e)
.
2
|Kkâ1 |

=

(17.129)
(17.130)

Thus, the monotonicity of |Kk |/|Kkâ1 | follows from the monotonocity of
h(Xk |Xkâ1 , . . . , X1 ), which follows from
h(Xk |Xkâ1 , . . . , X1 ) = h(Xk+1 |Xk , . . . , X2 )
â¥ h(Xk+1 |Xk , . . . , X2 , X1 ),

(17.131)
(17.132)

where the equality follows from the Toeplitz assumption and the inequality
from the fact that conditioning reduces entropy. Since h(Xk |Xkâ1 , . . . , X1 )
is decreasing, it follows that the running averages
1
1
h(X1 , . . . , Xk ) =
h(Xi |Xiâ1 , . . . , X1 )
k
k
k

(17.133)

i=1

are decreasing in k. Then (17.127) follows from h(X1 , X2 , . . . , Xk ) =
1
k

2 log(2Ï e) |Kk |.
Finally, since h(Xn |Xnâ1 , . . . , X1 ) is a decreasing sequence, it has a
limit. Hence by the theorem of the CesaÌro mean,
h(X1 , X2 , . . . , Xn )
1
h(Xk |Xkâ1 , . . . , X1 )
lim
= lim
nââ
nââ n
n
n

k=1

= lim h(Xn |Xnâ1 , . . . , X1 ).
nââ

(17.134)

17.10 INEQUALITIES FOR RATIOS OF DETERMINANTS

683

Translating this to determinants, one obtains
1

lim |Kn | n = lim

nââ

Theorem 17.9.7

nââ

|Kn |
.
|Knâ1 |

(17.135)

(Minkowski inequality [390] )
|K1 + K2 |1/n â¥ |K1 |1/n + |K2 |1/n .

(17.136)

Proof: Let X1 , X2 be independent with Xi â¼ N(0, Ki ). Noting that X1 +
X2 â¼ N(0, K1 + K2 ) and using the entropy power inequality (Theorem
17.7.3) yields
(2Ï e)|K1 + K2 |1/n = 2 n h(X1 + X2 )
2
2

â¥ 2 n h(X1 ) + 2 n h(X2 )
2

(17.137)
(17.138)

= (2Ï e)|K1 |1/n + (2Ï e)|K2 |1/n . (17.139)
17.10

INEQUALITIES FOR RATIOS OF DETERMINANTS

We now prove similar inequalities for ratios of determinants. Before developing the next theorem, we make an observation about minimum meansquared-error linear prediction. If (X1 , X2 , . . . , Xn ) â¼ N(0, Kn ), we know
that the conditional density of Xn given (X1 , X2 , . . . , Xnâ1 ) is univariate
normal with mean linear in X1 , X2 , . . . , Xnâ1 and conditional variance
Ïn2 . Here Ïn2 is the minimum mean squared error E(Xn â XÌn )2 over all
linear estimators XÌn based on X1 , X2 , . . . , Xnâ1 .
Ïn2 = |Kn |/|Knâ1 |.

Lemma 17.10.1

Proof: Using the conditional normality of Xn , we have
1
(17.140)
log 2Ï eÏn2 = h(Xn |X1 , X2 , . . . , Xnâ1 )
2
= h(X1 , X2 , . . . , Xn ) â h(X1 , X2 , . . . , Xnâ1 ) (17.141)
1
1
log(2Ï e)n |Kn | â log(2Ï e)nâ1 |Knâ1 | (17.142)
2
2
1
(17.143)
= log 2Ï e|Kn |/|Knâ1 |. 
2
=

684

INEQUALITIES IN INFORMATION THEORY

Minimization of Ïn2 over a set of allowed covariance matrices {Kn } is
aided by the following theorem. Such problems arise in maximum entropy
spectral density estimation.
Theorem 17.10.1
in Kn .

(BergstrÃ¸m [42] ) log(|Kn |/|Knâp |) is concave

Proof: We remark that Theorem 17.9.1 cannot be used because
log(|Kn |/|Knâp |) is the difference of two concave functions. Let Z = XÎ¸ ,
where X1 â¼ N(0, Sn ), X2 â¼ N(0, Tn ), Pr{Î¸ = 1} = Î» = 1 â Pr{Î¸ = 2},
and let X1 , X2 , Î¸ be independent. The covariance matrix Kn of Z is
given by
Kn = Î»Sn + (1 â Î»)Tn .

(17.144)

The following chain of inequalities proves the theorem:
1
1
Î» log(2Ï e)p |Sn |/|Snâp | + (1 â Î») log(2Ï e)p |Tn |/|Tnâp |
2
2
(a)

= Î»h(X1,n , X1,nâ1 , . . . , X1,nâp+1 |X1,1 , . . . , X1,nâp )
+ (1 â Î»)h(X2,n , X2,nâ1 , . . . , X2,nâp+1 |X2,1 , . . . , X2,nâp )
(17.145)

= h(Zn , Znâ1 , . . . , Znâp+1 |Z1 , . . . , Znâp , Î¸ )
(b)

â¤ h(Zn , Znâ1 , . . . , Znâp+1 |Z1 , . . . , Znâp )

(c)

â¤

|Kn |
1
log(2Ï e)p
,
2
|Knâp |

(17.146)
(17.147)
(17.148)

where (a) follows from h(Xn , Xnâ1 , . . . , Xnâp+1 |X1 , . . . , Xnâp ) =
h(X1 , . . . , Xn ) â h(X1 , . . . , Xnâp ), (b) follows from the conditioning
lemma, and (c) follows from a conditional version of Theorem 17.2.3. 
Theorem 17.10.2

(BergstrÃ¸m [42] )

|Kn |/|Knâ1 | is concave in Kn .

Proof: Again we use the properties of Gaussian random variables. Let
us assume that we have two independent Gaussian random n-vectors,
X â¼ N(0, An ) and Y â¼ N(0, Bn ). Let Z = X + Y. Then
1
|An + Bn | (a)
log 2Ï e
= h(Zn |Znâ1 , Znâ2 , . . . , Z1 )
2
|Anâ1 + Bnâ1 |

(17.149)

17.10 INEQUALITIES FOR RATIOS OF DETERMINANTS

685

(b)

â¥ h(Zn |Znâ1 , Znâ2 , . . . , Z1 , Xnâ1 , Xnâ2 , . . . , X1 , Ynâ1 , Ynâ2 , . . . , Y1 )
(17.150)

(c)

= h(Xn + Yn |Xnâ1 , Xnâ2 , . . . , X1 , Ynâ1 , Ynâ2 , . . . , Y1 )

(17.151)


1
= E log 2Ï e Var(Xn + Yn |Xnâ1 , Xnâ2 , . . . , X1 , Ynâ1 ,
2

Ynâ2 , . . . , Y1 )

(17.152)

(d)


1
= E log 2Ï e(Var(Xn |Xnâ1 , Xnâ2 , . . . , X1 )
2

+ Var(Yn |Ynâ1 , Ynâ2 , . . . , Y1 ))





|An |
|Bn |
1
(f)
+
= E log 2Ï e
2
|Anâ1 | |Bnâ1 |





1
|Bn |
|An |
,
= log 2Ï e
+
2
|Anâ1 | |Bnâ1 |
(e)

(17.153)
(17.154)
(17.155)

where
(a)
(b)
(c)
(d)

follows from Lemma 17.10.1
follows from the fact that the conditioning decreases entropy
follows from the fact that Z is a function of X and Y
follows since Xn + Yn is Gaussian conditioned on X1 , X2 , . . . ,
Xnâ1 , Y1 , Y2 , . . . , Ynâ1 , and hence we can express its entropy in
terms of its variance
(e) follows from the independence of Xn and Yn conditioned on the
past X1 , X2 , . . . , Xnâ1 , Y1 , Y2 , . . . , Ynâ1
(f) follows from the fact that for a set of jointly Gaussian random
variables, the conditional variance is constant, independent of the
conditioning variables (Lemma 17.10.1)
Setting A = Î»S and B = Î»T , we obtain
|Tn |
|Sn |
|Î»Sn + Î»Tn |
+Î»
â¥Î»
|Snâ1 |
|Tnâ1 |
|Î»Snâ1 + Î»Tnâ1 |

(17.156)

(i.e., |Kn |/|Knâ1 | is concave). Simple examples show that |Kn |/
|Knâp | is not necessarily concave for p â¥ 2.

A number of other determinant inequalities can be proved by these
techniques. A few of them are given as problems.

686

INEQUALITIES IN INFORMATION THEORY

OVERALL SUMMARY
Entropy. H (X) = â



p(x) log p(x).

Relative entropy. D(p||q) =



Mutual information. I (X; Y ) =

p(x) log p(x)
q(x) .


p(x,y)
.
p(x, y) log p(x)p(y)

Information inequality. D(p||q) â¥ 0.
Asymptotic equipartition property. â n1 log p(X1 , X2 , . . . , Xn ) â
H (X ).
Data compression. H (X) â¤ Lâ < H (X) + 1.
Kolmogorov complexity. K(x) = minU (p)=x l(p).
Universal probability. log P

1

U (x)

â K(x).

Channel capacity. C = maxp(x) I (X; Y ).
Data transmission
â¢
â¢

R < C: Asymptotically error-free communication possible
R > C: Asymptotically error-free communication not possible

Gaussian channel capacity. C =

1
2

log(1 +

P
N ).

Rate distortion. R(D) = min I (X; XÌ) over all p(xÌ|x) such that
Ep(x)p(xÌ|x) d(X, XÌ) â¤ D.
Growth rate for investment. W â = maxbâ E log bt X.

PROBLEMS
17.1

Sum of positive deï¬nite matrices. For any two positive deï¬nite
matrices, K1 and K2 , show that |K1 + K2 | â¥ |K1 |.

HISTORICAL NOTES

17.2

687

Fanâs inequality [200] for ratios of determinants. For all 1 â¤ p â¤
n, for a positive deï¬nite K = K(1, 2, . . . , n), show that
 |K(i, p + 1, p + 2, . . . , n)|
|K|
â¤
.
|K(p + 1, p + 2, . . . , n)|
|K(p + 1, p + 2, . . . , n)|
p

i=1

(17.157)
17.3

Convexity of determinant ratios. For positive deï¬nite matrices K,
K0 , show that ln(|K + K0 |/|K|) is convex in K.

17.4

Data-processing inequality. Let random variable X1 , X2 , X3 , and
X4 form a Markov chain X1 â X2 â X3 â X4 . Show that
I (X1 ; X3 ) + I (X2 ; X4 ) â¤ I (X1 ; X4 ) + I (X2 ; X3 ).

17.5

(17.158)

Markov chains. Let random variables X, Y, Z, and W form a
Markov chain so that X â Y â (Z, W ) [i.e., p(x, y, z, w) =
p(x)p(y|x)p(z, w|y)]. Show that
I (X; Z) + I (X; W ) â¤ I (X; Y ) + I (Z; W ).

(17.159)

HISTORICAL NOTES
The entropy power inequality was stated by Shannon [472]; the ï¬rst formal proofs are due to Stam [505] and Blachman [61]. The uniï¬ed proof
of the entropy power and BrunnâMinkowski inequalities is in Dembo
et al.[164].
Most of the matrix inequalities in this chapter were derived using
information-theoretic methods by Cover and Thomas [118]. Some of the
subset inequalities for entropy rates may be found in Han [270].

BIBLIOGRAPHY

[1] J. Abrahams. Code and parse trees for lossless source encoding. Proc.
Compression and Complexity of Sequences 1997, pages 145â171, 1998.
[2] N. Abramson. The ALOHA systemâanother alternative for computer communications. AFIPS Conf. Proc., pages 281â285, 1970.
[3] N. M. Abramson. Information Theory and Coding. McGraw-Hill, New York,
1963.
[4] Y. S. Abu-Mostafa. Information theory. Complexity, pages 25â28, Nov.
1989.
[5] R. L. Adler, D. Coppersmith, and M. Hassner. Algorithms for sliding block
codes: an application of symbolic dynamics to information theory. IEEE
Trans. Inf. Theory, IT-29(1):5â22, 1983.
[6] R. Ahlswede. The capacity of a channel with arbitrary varying Gaussian
channel probability functions. Trans. 6th Prague Conf. Inf. Theory, pages
13â21, Sept. 1971.
[7] R. Ahlswede. Multi-way communication channels. In Proc. 2nd Int.
Symp. Inf. Theory (Tsahkadsor, Armenian S.S.R.), pages 23â52. Hungarian
Academy of Sciences, Budapest, 1971.
[8] R. Ahlswede. The capacity region of a channel with two senders and two
receivers. Ann. Prob., 2:805â814, 1974.
[9] R. Ahlswede. Elimination of correlation in random codes for arbitrarily
varying channels. Z. Wahrscheinlichkeitstheorie und verwandte Gebiete,
33:159â175, 1978.
[10] R. Ahlswede. Coloring hypergraphs: A new approach to multiuser source
coding. J. Comb. Inf. Syst. Sci., pages 220â268, 1979.
[11] R. Ahlswede. A method of coding and an application to arbitrarily varying
channels. J. Comb. Inf. Syst. Sci., pages 10â35, 1980.
[12] R. Ahlswede and T. S. Han. On source coding with side information via
a multiple access channel and related problems in multi-user information
theory. IEEE Trans. Inf. Theory, IT-29:396â412, 1983.

Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

689

690

BIBLIOGRAPHY

[13] R. Ahlswede and J. KoÌrner. Source coding with side information and a
converse for the degraded broadcast channel. IEEE Trans. Inf. Theory,
IT-21:629â637, 1975.
[14] R. F. Ahlswede. Arbitrarily varying channels with states sequence known
to the sender. IEEE Trans. Inf. Theory, pages 621â629, Sept. 1986.
[15] R. F. Ahlswede. The maximal error capacity of arbitrarily varying channels
for constant list sizes (corresp.). IEEE Trans. Inf. Theory, pages 1416â1417,
July 1993.
[16] R. F. Ahlswede and G. Dueck. Identiï¬cation in the presence of feedback: a
discovery of new capacity formulas. IEEE Trans. Inf. Theory, pages 30â36,
Jan. 1989.
[17] R. F. Ahlswede and G. Dueck. Identiï¬cation via channels. IEEE Trans. Inf.
Theory, pages 15â29, Jan. 1989.
[18] R. F. Ahlswede, E. H. Yang, and Z. Zhang. Identiï¬cation via compressed
data. IEEE Trans. Inf. Theory, pages 48â70, Jan. 1997.
[19] H. Akaike. Information theory and an extension of the maximum likelihood
principle. Proc. 2nd Int. Symp. Inf. Theory, pages 267â281, 1973.
[20] P. Algoet and T. M. Cover.
A sandwich proof of the Shannonâ
McMillanâBreiman theorem. Ann. Prob., 16(2):899â909, 1988.
[21] P. Algoet and T. M. Cover. Asymptotic optimality and asymptotic equipartition property of log-optimal investment. Ann. Prob., 16(2):876â898, 1988.
[22] S. Amari. Differential-Geometrical Methods in Statistics. Springer-Verlag,
New York, 1985.
[23] S. I. Amari and H. Nagaoka. Methods of Information Geometry. Oxford
University Press, Oxford, 1999.
[24] V. Anantharam and S. Verdu. Bits through queues. IEEE Trans. Inf. Theory,
pages 4â18, Jan. 1996.
[25] S. Arimoto. An algorithm for calculating the capacity of an arbitrary discrete
memoryless channel. IEEE Trans. Inf. Theory, IT-18:14â20, 1972.
[26] S. Arimoto. On the converse to the coding theorem for discrete memoryless
channels. IEEE Trans. Inf. Theory, IT-19:357â359, 1973.
[27] R. B. Ash. Information Theory. Interscience, New York, 1965.
[28] J. AczeÌl and Z. DaroÌczy. On Measures of Information and Their Characterization. Academic Press, New York, 1975.
[29] L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv. Optimal decoding of linear
codes for minimizing symbol error rate (corresp.). IEEE Trans. Inf. Theory,
pages 284â287, March 1974.
[30] A. Barron. Entropy and the central limit theorem. Ann. Prob., 14(1):
336â342, 1986.
[31] A. Barron and T. M. Cover. A bound on the ï¬nancial value of information.
IEEE Trans. Inf. Theory, IT-34:1097â1100, 1988.

BIBLIOGRAPHY

691

[32] A. Barron and T. M. Cover. Minimum complexity density estimation. IEEE
Trans. Inf. Theory, 37(4):1034â1054, July 1991.
[33] A. R. Barron. Logically smooth density estimation. Ph.D. thesis, Department
of Electrical Engineering, Stanford University, Stanford, CA, 1985.
[34] A. R. Barron. The strong ergodic theorem for densities: generalized ShannonâMcMillanâBreiman theorem. Ann. Prob., 13:1292â1303, 1985.
[35] A. R. Barron. Are Bayesâ rules consistent in information? Prob. Commun.
Computation, pages 85â91, 1987.
[36] A. R. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. IEEE Trans. Inf. Theory, pages 2743â2760,
Oct. 1998.
[37] E. B. Baum. Neural net algorithms that learn in polynomial time from
examples and queries. IEEE Trans. Neural Networks, pages 5â19, 1991.
[38] W. Beckner. Inequalities in Fourier analysis. Ann. Math., 102:159â182,
1975.
[39] R. Bell and T. M. Cover. Competitive optimality of logarithmic investment.
Math. Oper. Res., 5(2):161â166, May 1980.
[40] R. Bell and T. M. Cover. Game-theoretic optimal portfolios. Manage. Sci.,
34(6):724â733, 1988.
[41] T. C. Bell, J. G. Cleary, and I. H. Witten. Text Compression. Prentice-Hall,
Englewood Cliffs, NJ, 1990.
[42] R. Bellman. Notes on matrix theory. IV: An inequality due to BergstrÃ¸m.
Am. Math. Monthly, 62:172â173, 1955.
[43] C. H. Bennett and G. Brassard. Quantum cryptography: public key distribution and coin tossing. Proc. IEEE Int. Conf. Comput., pages 175â179,
1984.
[44] C. H. Bennett, D. P. DiVincenzo, J. Smolin, and W. K. Wootters. Mixed
state entanglement and quantum error correction. Phys. Rev. A, pages
3824â3851, 1996.
[45] C. H. Bennett, D. P. DiVincenzo, and J. A. Smolin. Capacities of quantum
erasure channels. Phys. Rev. Lett., pages 3217â3220, 1997.
[46] C. H. Bennett and S. J. Wiesner. Communication via one- and two-particle
operators on EinsteinâpodolskyâRosen states. Phys. Rev. Lett., pages
2881â2884, 1992.
[47] C. H. Bennett.
Demons, engines and the second law.
Sci. Am.,
259(5):108â116, Nov. 1987.
[48] C. H. Bennett and R. Landauer. The fundamental physical limits of computation. Sci. Am., 255(1):48â56, July 1985.
[49] C. H. Bennett and P. W. Shor. Quantum information theory. IEEE Trans.
Inf. Theory, IT-44:2724â2742, Oct. 1998.
[50] J. Bentley, D. Sleator, R. Tarjan, and V. Wei. Locally adaptive data
compression scheme. Commun. ACM, pages 320â330, 1986.

692

BIBLIOGRAPHY

[51] R. Benzel. The capacity region of a class of discrete additive degraded
interference channels. IEEE Trans. Inf. Theory, IT-25:228â231, 1979.
[52] T. Berger.
Rate Distortion Theory: A Mathematical Basis for Data
Compression. Prentice-Hall, Englewood Cliffs, NJ, 1971.
[53] T. Berger. Multiterminal source coding. In G. Longo (Ed.), The Information
Theory Approach to Communications. Springer-Verlag, New York, 1977.
[54] T. Berger and R. W. Yeung. Multiterminal source encoding with one distortion criterion. IEEE Trans. Inf. Theory, IT-35:228â236, 1989.
[55] P. Bergmans. Random coding theorem for broadcast channels with degraded
components. IEEE Trans. Inf. Theory, IT-19:197â207, 1973.
[56] E. R. Berlekamp. Block Coding with Noiseless Feedback. Ph.D. thesis, MIT,
Cambridge, MA, 1964.
[57] C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit errorcorrecting coding and decoding: Turbo codes. Proc. 1993 Int. Conf. Commun., pages 1064â1070, May 1993.
[58] D. Bertsekas and R. Gallager. Data Networks, 2nd ed.. Prentice-Hall, Englewood Cliffs, NJ, 1992.
[59] M. Bierbaum and H. M. Wallmeier. A note on the capacity region of the
multiple access channel. IEEE Trans. Inf. Theory, IT-25:484, 1979.
[60] E. Biglieri, J. Proakis, and S. Shamai. Fading channels: information-theoretic
and communications aspects. IEEE Trans. Inf. Theory, pages 2619â2692,
October 1998.
[61] N. Blachman. The convolution inequality for entropy powers. IEEE Trans.
Inf. Theory, IT-11:267â271, Apr. 1965.
[62] D. Blackwell, L. Breiman, and A. J. Thomasian. Proof of Shannonâs transmission theorem for ï¬nite-state indecomposable channels. Ann. Math. Stat.,
pages 1209â1220, 1958.
[63] D. Blackwell, L. Breiman, and A. J. Thomasian. The capacity of a class of
channels. Ann. Math. Stat., 30:1229â1241, 1959.
[64] D. Blackwell, L. Breiman, and A. J. Thomasian. The capacities of certain channel classes under random coding. Ann. Math. Stat., 31:558â567,
1960.
[65] R. Blahut. Computation of channel capacity and rate distortion functions.
IEEE Trans. Inf. Theory, IT-18:460â473, 1972.
[66] R. E. Blahut. Information bounds of the FanoâKullback type. IEEE Trans.
Inf. Theory, IT-22:410â421, 1976.
[67] R. E. Blahut. Principles and Practice of Information Theory. AddisonWesley, Reading, MA, 1987.
[68] R. E. Blahut. Hypothesis testing and information theory. IEEE Trans. Inf.
Theory, IT-20:405â417, 1974.
[69] R. E. Blahut. Theory and Practice of Error Control Codes. Addison-Wesley,
Reading, MA, 1983.

BIBLIOGRAPHY

693

[70] B. M. Hochwald, G. Caire, B. Hassibi, and T. L. Marzetta (Eds.). IEEE
Trans. Inf. Theory, Special Issue on Space-Time Transmission, Reception,
Coding and Signal-Processing, Vol. 49, Oct. 2003.
[71] L. Boltzmann.
Beziehung Zwischen dem zweiten Hauptsatze der
mechanischen WaÌrmertheorie und der Wahrscheilichkeitsrechnung respektive den Saetzen uber das WaÌrmegleichgwicht. Wien. Ber., pages 373â435,
1877.
[72] R. C. Bose and D. K. Ray-Chaudhuri. On a class of error correcting binary
group codes. Inf. Control, 3:68â79, Mar. 1960.
[73] H. J. Brascamp and E. J. Lieb. Best constants in Youngâs inequality, its
converse and its generalization to more than three functions. Adv. Math.,
20:151â173, 1976.
[74] L. Breiman. The individual ergodic theorems of information theory. Ann.
Math. Stat., 28:809â811, 1957. With correction made in 31:809-810.
[75] L. Breiman. Optimal gambling systems for favourable games. In Fourth
Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1,
pages 65â78. University of California Press, Berkeley, CA, 1961.
[76] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiï¬cation
and Regression Trees. Wadsworth & Brooks, Paciï¬c Grove, CA, 1984.
[77] L. Brillouin. Science and Information Theory. Academic Press, New York,
1962.
[78] J. A. Bucklew. The source coding theorem via Sanovâs theorem. IEEE
Trans. Inf. Theory, pages 907â909, Nov. 1987.
[79] J. A. Bucklew. Large Deviation Techniques in Decision, Simulation, and
Estimation. Wiley, New York, 1990.
[80] J. P. Burg. Maximum entropy spectral analysis. Ph.D. thesis, Department of
Geophysics, Stanford University, Stanford, CA, 1975.
[81] M. Burrows and D. J. Wheeler. A Block-Sorting Lossless Data Compression
Algorithm (Tech. Rept. 124). Digital Systems Research Center, Palo Alto,
CA, May 1994.
[82] A. R. Calderbank. The art of signaling: ï¬fty years of coding theory. IEEE
Trans. Inf. Theory, pages 2561â2595, Oct. 1998.
[83] A. R. Calderbank and P. W. Shor. Good quantum error-correcting codes
exist. Phys. Rev. A, pages 1098â1106, 1995.
[84] A. Carleial. Outer bounds on the capacity of the interference channel. IEEE
Trans. Inf. Theory, IT-29:602â606, 1983.
[85] A. B. Carleial. A case where interference does not reduce capacity. IEEE
Trans. Inf. Theory, IT-21:569â570, 1975.
[86] G. Chaitin. Information-Theoretic Incompleteness. World Scientiï¬c, Singapore, 1992.
[87] G. J. Chaitin. On the length of programs for computing binary sequences.
J. ACM, pages 547â569, 1966.

694

BIBLIOGRAPHY

[88] G. J. Chaitin. The limits of mathematics. J. Universal Comput. Sci.,
2(5):270â305, 1996.
[89] G. J. Chaitin. On the length of programs for computing binary sequences.
J. ACM, 13:547â569, 1966.
[90] G. J. Chaitin. Information theoretical limitations of formal systems. J. ACM,
21:403â424, 1974.
[91] G. J. Chaitin. Randomness and mathematical proof. Sci. Am., 232(5):47â52,
May 1975.
[92] G. J. Chaitin. Algorithmic information theory. IBM J. Res. Dev., 21:350â359,
1977.
[93] G. J. Chaitin. Algorithmic Information Theory. Cambridge University Press,
Cambridge, 1987.
[94] C. S. Chang and J. A. Thomas. Huffman algebras for independent random
variables. Discrete Event Dynam. Syst., 4:23â40, 1994.
[95] C. S. Chang and J. A. Thomas. Effective bandwidth in high speed digital
networks. IEEE J. Select. Areas Commun., 13:1091â1114, Aug. 1995.
[96] R. Chellappa. Markov Random Fields: Theory and Applications. Academic
Press, San Diego, CA, 1993.
[97] H. Chernoff. A measure of the asymptotic efï¬ciency of tests of a hypothesis based on a sum of observations. Ann. Math. Stat., 23:493â507,
1952.
[98] B. S. Choi and T. M. Cover. An information-theoretic proof of Burgâs
maximum entropy spectrum. Proc. IEEE, 72:1094â1095, 1984.
[99] N. Chomsky. Three models for the description of language. IEEE Trans.
Inf. Theory, pages 113â124, Sept. 1956.
[100] P. A. Chou, M. Effros, and R. M. Gray. A vector quantization approach to
universal noiseless coding and quantization. IEEE Trans. Inf. Theory, pages
1109â1138, July 1996.
[101] K. L. Chung. A note on the ergodic theorem of information theory. Ann.
Math. Stat., 32:612â614, 1961.
[102] B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of Bayesâ
methods. IEEE Trans. Inf. Theory, pages 453â471, May 1990.
[103] B. S. Clarke and A. R. Barron. Jeffreysâ prior is asymptotically least favorable under entropy risk. J. Stat. Planning Inf., pages 37â60, Aug. 1994.
[104] M. Costa and T. M. Cover. On the similarity of the entropy power inequality and the BrunnâMinkowski inequality. IEEE Trans. Inf. Theory, IT30:837â839, 1984.
[105] M. H. M. Costa. On the Gaussian interference channel. IEEE Trans. Inf.
Theory, pages 607â615, Sept. 1985.
[106] M. H. M. Costa and A. A. El Gamal. The capacity region of the discrete
memoryless interference channel with strong interference. IEEE Trans. Inf.
Theory, pages 710â711, Sept. 1987.

BIBLIOGRAPHY

695

[107] T. M. Cover. Geometrical and statistical properties of systems of linear
inequalities with applications to pattern recognition. IEEE Trans. Electron.
Computation, pages 326â334, 1965.
[108] T. M. Cover. Universal Gambling Schemes and the Complexity Measures of
Kolmogorov and Chaitin (Tech. Rept. 12). Department of Statistics, Stanford
University, Stanford, CA, Oct. 1974.
[109] T. M. Cover. Open problems in information theory. Proc. Moscow Inf.
Theory Workshop, pages 35â36, 1975.
[110] T. M. Cover. Universal portfolios. Math. Finance, pages 1â29, Jan. 1991.
[111] T. M. Cover. Comments on broadcast channels. IEEE Trans. Inf. Theory,
pages 2524â2530, Oct. 1998.
[112] T. M. Cover. Shannon and investment. IEEE Inf. Theory Newslett (Special
Golden Jubilee Issue), pp. 10â11, June 1998.
[113] T. M. Cover and M. S. Chiang. Duality between channel capacity and
rate distortion with two-sided state information. IEEE Trans. Inf. Theory,
IT-48(6):1629â1638, June 2002.
[114] T. M. Cover, P. GaÌcs, and R. M. Gray. Kolmogorovâs contributions to information theory and algorithmic complexity. Ann. Prob., pages 840â865, July
1989.
[115] T. M. Cover, A. A. El Gamal, and M. Salehi. Multiple access channels with
arbitrarily correlated sources. IEEE Trans. Inf. Theory, pages 648â657, Nov.
1980.
[116] T. M. Cover and P. E. Hart. Nearest neighbor pattern classiï¬cation. IEEE
Trans. Inf. Theory, pages 21â27, Jan. 1967.
[117] T. M. Cover and S. Pombra. Gaussian feedback capacity. IEEE Trans. Inf.
Theory, pages 37â43, January 1989.
[118] T. M. Cover and J. A. Thomas. Determinant inequalities via information
theory. SIAM J. Matrix Anal. and Its Applications, 9(3):384â392, July 1988.
[119] T. M. Cover. Broadcast channels. IEEE Trans. Inf. Theory, IT-18:2â14,
1972.
[120] T. M. Cover. Enumerative source encoding. IEEE Trans. Inf. Theory, IT19(1):73â77, Jan. 1973.
[121] T. M. Cover. An achievable rate region for the broadcast channel. IEEE
Trans. Inf. Theory, IT-21:399â404, 1975.
[122] T. M. Cover. A proof of the data compression theorem of Slepian and Wolf
for ergodic sources. IEEE Trans. Inf. Theory, IT-22:226â228, 1975.
[123] T. M. Cover. An algorithm for maximizing expected log investment return.
IEEE Trans. Inf. Theory, IT-30(2):369â373, 1984.
[124] T. M. Cover. Kolmogorov complexity, data compression and inference.
In J. Skwirzynski (Ed.), The Impact of Processing Techniques on Communications, Vol. 91 of Applied Sciences. Martinus-Nijhoff, Dordrecht, The
Netherlands, 1985.

696

BIBLIOGRAPHY

[125] T. M. Cover. On the competitive optimality of Huffman codes. IEEE Trans.
Inf. Theory, 37(1):172â174, Jan. 1991.
[126] T. M. Cover. Universal portfolios. Math. Finance, pages 1â29, Jan. 1991.
[127] T. M. Cover and A El Gamal. Capacity theorems for the relay channel.
IEEE Trans. Inf. Theory, IT-25:572â584, 1979.
[128] T. M. Cover and A. El Gamal. An information theoretic proof of Hadamardâs
inequality. IEEE Trans. Inf. Theory, IT-29(6):930â931, Nov. 1983.
[129] T. M. Cover, A. El Gamal, and M. Salehi. Multiple access channels with
arbitrarily correlated sources. IEEE Trans. Inf. Theory, IT-26:648â657,
1980.
[130] T. M. Cover. Pick the largest number, Open Problems in Communication
and Computation. Ed. by T. M. Cover and B. Gopinath, page 152, New
York, 1987.
[131] T. M. Cover and R. King. A convergent gambling estimate of the entropy
of English. IEEE Trans. Inf. Theory, IT-24:413â421, 1978.
[132] T. M. Cover and C. S. K. Leung. Some equivalences between Shannon entropy and Kolmogorov complexity. IEEE Trans. Inf. Theory, IT24:331â338, 1978.
[133] T. M. Cover and C. S. K. Leung. An achievable rate region for the multiple
access channel with feedback. IEEE Trans. Inf. Theory, IT-27:292â298,
1981.
[134] T. M. Cover, R. J. McEliece, and E. Posner. Asynchronous multiple access
channel capacity. IEEE Trans. Inf. Theory, IT-27:409â413, 1981.
[135] T. M. Cover and E. Ordentlich. Universal portfolios with side information.
IEEE Trans. Inf. Theory, IT-42:348â363, Mar. 1996.
[136] T. M. Cover and S. Pombra. Gaussian feedback capacity. IEEE Trans. Inf.
Theory, IT-35:37â43, 1989.
[137] H. Cramer. Mathematical Methods of Statistics. Princeton University Press,
Princeton, NJ, 1946.
[138] I. CsiszaÌr. Information type measures of difference of probability distributions and indirect observations. Stud. Sci. Math. Hung., 2:299â318,
1967.
[139] I CsiszaÌr. On the computation of rate distortion functions. IEEE Trans. Inf.
Theory, IT-20:122â124, 1974.
[140] I. CsiszaÌr. I-divergence geometry of probability distributions and minimization problems. Ann. Prob., pages 146â158, Feb. 1975.
[141] I CsiszaÌr. Sanov property, generalized I-projection and a conditional limit
theorem. Ann. Prob., 12:768â793, 1984.
[142] I. CsiszaÌr. Information theory and ergodic theory. Probl. Contr. Inf. Theory,
pages 3â27, 1987.
[143] I. CsiszaÌr. A geometric interpretation of Darroch and Ratcliffâs generalized
iterative scaling. Ann. Stat., pages 1409â1413, 1989.

BIBLIOGRAPHY

697

[144] I. CsiszaÌr. Why least squares and maximum entropy? An axiomatic approach
to inference for linear inverse problems. Ann. Stat., pages 2032â2066, Dec.
1991.
[145] I. CsiszaÌr. Arbitrarily varying channels with general alphabets and states.
IEEE Trans. Inf. Theory, pages 1725â1742, Nov. 1992.
[146] I. CsiszaÌr. The method of types. IEEE Trans. Inf. Theory, pages 2505â2523,
October 1998.
[147] I. CsiszaÌr, T. M. Cover, and B. S. Choi. Conditional limit theorems under
Markov conditioning. IEEE Trans. Inf. Theory, IT-33:788â801, 1987.
[148] I. CsiszaÌr and J. KoÌrner. Towards a general theory of source networks. IEEE
Trans. Inf. Theory, IT-26:155â165, 1980.
[149] I. CsiszaÌr and J. KoÌrner. Information Theory: Coding Theorems for Discrete
Memoryless Systems. Academic Press, New York, 1981.
[150] I. CsiszaÌr and J. KoÌrner. Feedback does not affect the reliability function of
a DMC at rates above capacity (corresp.). IEEE Trans. Inf. Theory, pages
92â93, Jan. 1982.
[151] I. CsiszaÌr and J. KoÌrner. Broadcast channels with conï¬dential messages.
IEEE Trans. Inf. Theory, pages 339â348, May 1978.
[152] I. CsiszaÌr and J. KoÌrner. Graph decomposition: a new key to coding
theorems. IEEE Trans. Inf. Theory, pages 5â12, Jan. 1981.
[153] I. CsiszaÌr and G. Longo. On the Error Exponent for Source Coding and
for Testing Simple Statistical Hypotheses. Hungarian Academy of Sciences,
Budapest, 1971.
[154] I. CsiszaÌr and P. Narayan. Capacity of the Gaussian arbitrarily varying channel. IEEE Trans. Inf. Theory, pages 18â26, Jan. 1991.
[155] I. CsiszaÌr and G. TusnaÌdy. Information geometry and alternating minimization procedures. Statistics and Decisions, Supplement Issue 1:205â237,
1984.
[156] G. B. Dantzig and D. R. Fulkerson. On the max-ï¬ow min-cut theorem of
networks. In H. W. Kuhn and A. W. Tucker (Eds.), Linear Inequalities and
Related Systems (Vol. 38 of Annals of Mathematics Study), pages 215â221.
Princeton University Press, Princeton, NJ, 1956.
[157] J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear
models. Ann. Math. Stat., pages 1470â1480, 1972.
[158] I. Daubechies. Ten Lectures on Wavelets. SIAM, Philadelphia, 1992.
[159] L. D. Davisson. Universal noiseless coding. IEEE Trans. Inf. Theory, IT19:783â795, 1973.
[160] L. D. Davisson. Minimax noiseless universal coding for Markov sources.
IEEE Trans. Inf. Theory, pages 211â215, Mar. 1983.
[161] L. D. Davisson, R. J. McEliece, M. B. Pursley, and M. S. Wallace. Efï¬cient
universal noiseless source codes. IEEE Trans. Inf. Theory, pages 269â279,
May 1981.

698

BIBLIOGRAPHY

[162] A. Dembo. Information Inequalities and Uncertainty Principles (Technical Report), Department of Statistics, Stanford University, Stanford, CA,
1990.
[163] A. Dembo. Information inequalities and concentration of measure. Ann.
Prob., pages 927â939, 1997.
[164] A. Dembo, T. M. Cover, and J. A. Thomas. Information theoretic inequalities. IEEE Trans. Inf. Theory, 37(6):1501â1518, Nov. 1991.
[165] A. Dembo and O. Zeitouni. Large Deviations Techniques and Applications.
Jones & Bartlett, Boston, 1993.
[166] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from
incomplete data via the EM algorithm. J. Roy. Stat. Soc. B, 39(1):1â38,
1977.
[167] L. Devroye and L. Gyorï¬. Nonparametric Density Estimation: The L1 View.
Wiley, New York, 1985.
[168] L. Devroye, L. Gyorï¬, and G. Lugosi. A Probabilistic Theory of Pattern
Recognition. Springer-Verlag, New York, 1996.
[169] D. P. DiVincenzo, P. W. Shor, and J. A. Smolin. Quantum-channel capacity
of very noisy channels. Phys. Rev. A, pages 830â839, 1998.
[170] R.L. Dobrushin. General formulation of Shannonâs main theorem of information theory. Usp. Math. Nauk, 14:3â104, 1959. Translated in Am. Math.
Soc. Trans., 33:323-438.
[171] R. L. Dobrushin. Survey of Soviet research in information theory. IEEE
Trans. Inf. Theory, pages 703â724, Nov. 1972.
[172] D. L. Donoho. De-noising by soft-thresholding. IEEE Trans. Inf. Theory,
pages 613â627, May 1995.
[173] R. O. Duda and P. E. Hart. Pattern Classiï¬cation and Scene Analysis. Wiley,
New York, 1973.
[174] G. Dueck. Maximal error capacity regions are smaller than average error
capacity regions for multi-user channels. Probl. Contr. Inf. Theory, pages
11â19, 1978.
[175] G. Dueck. The capacity region of the two-way channel can exceed the inner
bound. Inf. Control, 40:258â266, 1979.
[176] G. Dueck. Partial feedback for two-way and broadcast channels. Inf. Control,
46:1â15, 1980.
[177] G. Dueck and J. KoÌrner. Reliability function of a discrete memoryless channel at rates above capacity. IEEE Trans. Inf. Theory, IT-25:82â85, 1979.
[178] P. M. Ebert. The capacity of the Gaussian channel with feedback. Bell Syst.
Tech. J., 49:1705â1712, Oct. 1970.
[179] P. M. Ebert. The capacity of the Gaussian channel with feedback. Bell Syst.
Tech. J., pages 1705â1712, Oct. 1970.
[180] K. Eckschlager. Information Theory in Analytical Chemistry. Wiley, New
York, 1994.

BIBLIOGRAPHY

699

[181] M. Effros, K. Visweswariah, S. R. Kulkarni, and S. Verdu. Universal lossless source coding with the BurrowsâWheeler transform. IEEE Trans. Inf.
Theory, IT-48:1061â1081, May 2002.
[182] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman &
Hall, London, 1993.
[183] H. G. Eggleston. Convexity (Cambridge Tracts in Mathematics and Mathematical Physics, No. 47). Cambridge University Press, Cambridge, 1969.
[184] A. El Gamal. The feedback capacity of degraded broadcast channels. IEEE
Trans. Inf. Theory, IT-24:379â381, 1978.
[185] A. El Gamal. The capacity region of a class of broadcast channels. IEEE
Trans. Inf. Theory, IT-25:166â169, 1979.
[186] A. El Gamal and T. M. Cover. Multiple user information theory. Proc.
IEEE, 68:1466â1483, 1980.
[187] A. El Gamal and T. M. Cover. Achievable rates for multiple descriptions.
IEEE Trans. Inf. Theory, IT-28:851â857, 1982.
[188] A. El Gamal and E. C. Van der Meulen. A proof of Martonâs coding theorem
for the discrete memoryless broadcast channel. IEEE Trans. Inf. Theory,
IT-27:120â122, 1981.
[189] P. Elias. Error-free coding. IRE Trans. Inf. Theory, IT-4:29â37, 1954.
[190] P. Elias. Coding for noisy channels. IRE Conv. Rec., Pt. 4, pages 37â46,
1955.
[191] P. Elias. Networks of Gaussian channels with applications to feedback systems. IEEE Trans. Inf. Theory, pages 493â501, July 1967.
[192] P. Elias. The efï¬cient construction of an unbiased random sequence. Ann.
Math. Stat., pages 865â870, 1972.
[193] P. Elias. Universal codeword sets and representations of the integers. IEEE
Trans. Inf. Theory, pages 194â203, Mar. 1975.
[194] P. Elias. Interval and recency rank source coding: two on-line adaptive
variable-length schemes. IEEE Trans. Inf. Theory, pages 3â10, Jan. 1987.
[195] P. Elias, A. Feinstein, and C. E. Shannon. A note on the maximum ï¬ow
through a network. IEEE Trans. Inf. Theory, pages 117â119, December
1956.
[196] R. S. Ellis. Entropy, Large Deviations, and Statistical Mechanics. SpringerVerlag, New York, 1985.
[197] A. Ephremides and B. Hajek. Information theory and communication
networks: an unconsummated union. IEEE Trans. Inf. Theory, pages
2416â2434, Oct. 1998.
[198] W. H. R. Equitz and T. M. Cover. Successive reï¬nement of information.
IEEE Trans. Inf. Theory, pages 269â275, Mar. 1991.
[199] Ky Fan. On a theorem of Weyl concerning the eigenvalues of linear transformations II. Proc. Nat. Acad. Sci. USA, 36:31â35, 1950.

700

BIBLIOGRAPHY

[200] Ky Fan. Some inequalities concerning positive-deï¬nite matrices. Proc. Cambridge Philos. Soc., 51:414â421, 1955.
[201] R. M. Fano. Class notes for Transmission of Information, course 6.574
(Technical Report). MIT, Cambridge, MA, 1952.
[202] R. M. Fano. Transmission of Information: A Statistical Theory of Communication. Wiley, New York, 1961.
[203] M. Feder. A note on the competetive optimality of Huffman codes. IEEE
Trans. Inf. Theory, 38(2):436â439, Mar. 1992.
[204] M. Feder, N. Merhav, and M. Gutman. Universal prediction of individual
sequences. IEEE Trans. Inf. Theory, pages 1258â1270, July 1992.
[205] A. Feinstein. A new basic theorem of information theory. IRE Trans. Inf.
Theory, IT-4:2â22, 1954.
[206] A. Feinstein. Foundations of Information Theory. McGraw-Hill, New York,
1958.
[207] A. Feinstein. On the coding theorem and its converse for ï¬nite-memory
channels. Inf. Control, 2:25â44, 1959.
[208] W. Feller. An Introduction to Probability Theory and Its Applications, 2nd
ed., Vol. 1. Wiley, New York, 1957.
[209] R. A. Fisher. On the mathematical foundations of theoretical statistics. Philos. Trans. Roy. Soc., London A, 222:309â368, 1922.
[210] R. A. Fisher. Theory of statistical estimation. Proc. Cambridge Philos. Soc.,
22:700â725, 1925.
[211] B. M. Fitingof. Optimal encoding with unknown and variable message statistics. Probl. Inf. Transm. (USSR), pages 3â11, 1966.
[212] B. M. Fitingof. The compression of discrete information. Probl. Inf. Transm.
(USSR), pages 28â36, 1967.
[213] L. R. Ford and D. R. Fulkerson. Maximal ï¬ow through a network. Can. J.
Math., pages 399â404, 1956.
[214] L. R. Ford and D. R. Fulkerson. Flows in Networks. Princeton University
Press, Princeton, NJ, 1962.
[215] G. D. Forney. Exponential error bounds for erasure, list and decision feedback schemes. IEEE Trans. Inf. Theory, IT-14:549â557, 1968.
[216] G. D. Forney. Information Theory: unpublished course notes. Stanford
University, Stanford, CA, 1972.
[217] G. J. Foschini. Layered space-time architecture for wireless communication
in a fading environment when using multi-element antennas. Bell Syst. Tech.
J., 1(2):41â59, 1996.
[218] P. Franaszek, P. Tsoucas, and J. Thomas. Context allocation for multiple dictionary data compression. In Proc. IEEE Int. Symp. Inf. Theory, Trondheim,
Norway, page 12, 1994.
[219] P. A. Franaszek. On synchronous variable length coding for discrete noiseless channels. Inf. Control, 15:155â164, 1969.

BIBLIOGRAPHY

701

[220] T. Gaarder and J. K. Wolf. The capacity region of a multiple-access discrete
memoryless channel can increase with feedback. IEEE Trans. Inf. Theory,
IT-21:100â102, 1975.
[221] D. Gabor. Theory of communication. J. Inst. Elec. Engg., pages 429â457,
Sept. 1946.
[222] P. Gacs and J. KoÌrner. Common information is much less than mutual information. Probl. Contr. Inf. Theory, pages 149â162, 1973.
[223] R. G. Gallager. Source coding with side information and universal coding.
Unpublished manuscript, also presented at the Int. Symp. Inf. Theory, Oct.
1974.
[224] R. G. Gallager. A simple derivation of the coding theorem and some applications. IEEE Trans. Inf. Theory, IT-11:3â18, 1965.
[225] R. G. Gallager. Capacity and coding for degraded broadcast channels. Probl.
Peredachi Inf., 10(3):3â14, 1974.
[226] R. G. Gallager. Basic limits on protocol information in data communication networks. IEEE Trans. Inf. Theory, pages 385â398, July
1976.
[227] R. G. Gallager. A minimum delay routing algorithm using distributed computation. IEEE Trans. Commun., pages 73â85, Jan. 1977.
[228] R. G. Gallager. Variations on a theme by Huffman. IEEE Trans. Inf. Theory,
pages 668â674, Nov. 1978.
[229] R. G. Gallager. Source Coding with Side Information and Universal Coding
(Tech. Rept. LIDS-P-937). Laboratory for Information Decision Systems,
MIT, Cambridge, MA, 1979.
[230] R. G. Gallager. A perspective on multiaccess channels. IEEE Trans. Inf.
Theory, pages 124â142, Mar. 1985.
[231] R. G. Gallager. Low density parity check codes. IRE Trans. Inf. Theory,
IT-8:21â28, Jan. 1962.
[232] R. G. Gallager. Low Density Parity Check Codes. MIT Press, Cambridge,
MA, 1963.
[233] R. G. Gallager. Information Theory and Reliable Communication. Wiley,
New York, 1968.
[234] A. A. El Gamal and T. M. Cover. Achievable rates for multiple descriptions.
IEEE Trans. Inf. Theory, pages 851â857, November 1982.
[235] A. El Gamal. Broadcast channels with and without feedback. 11th Ann.
Asilomar Conf. Circuits, pages 180â183, Nov. 1977.
[236] A. El Gamal. Capacity of the product and sum of two unmatched broadcast
channels. Probl. Peredachi Inf., pages 3â23, Jan.âMar. 1980.
[237] A. A. El Gamal. The feedback capacity of degraded broadcast channels
(corresp.). IEEE Trans. Inf. Theory, pages 379â381, May 1978.
[238] A. A. El Gamal. The capacity of a class of broadcast channels. IEEE Trans.
Inf. Theory, pages 166â169, Mar. 1979.

702

BIBLIOGRAPHY

[239] A. A. El Gamal. The capacity of the physically degraded Gaussian broadcast
channel with feedback (corresp.). IEEE Trans. Inf. Theory, pages 508â511,
July 1981.
[240] A. A. El Gamal and E. C. van der Meulen. A proof of Martonâs coding
theorem for the discrete memoryless broadcast channel. IEEE Trans. Inf.
Theory, pages 120â122, Jan. 1981.
[241] I. M. Gelfand, A. N. Kolmogorov, and A. M. Yaglom. On the general
deï¬nition of mutual information. Rept. Acad. Sci. USSR, pages 745â748,
1956.
[242] S. I. Gelfand. Capacity of one broadcast channel. Probl. Peredachi Inf.,
pages 106â108, JulyâSept. 1977.
[243] S. I. Gelfand and M. S. Pinsker. Capacity of a broadcast channel with one
deterministic component. Probl. Peredachi Inf., pages 24â34, Jan.âMar.
1980.
[244] S. I. Gelfand and M. S. Pinsker. Coding for channel with random parameters.
Probl. Contr. Inf. Theory, pages 19â31, 1980.
[245] A. Gersho and R. M. Gray. Vector Quantization and Signal Compression.
Kluwer, Boston, 1992.
[246] G. G. Rayleigh and J. M. Ciofï¬. Spatio-temporal coding for wireless communication. IEEE Trans. Commun., 46:357â366, 1998.
[247] J. D. Gibson and J. L. Melsa. Introduction to Nonparametric Detection with
Applications. IEEE Press, New York, 1996.
[248] E. N. Gilbert. Codes based on inaccurate source probabilities. IEEE Trans.
Inf. Theory, pages 304â314, May 1971.
[249] E. N. Gilbert and E. F. Moore. Variable length binary encodings. Bell Syst.
Tech. J., 38:933â967, 1959.
[250] S. Goldman. Some fundamental considerations concerning noise reduction and range in radar and communication. Proc. Inst. Elec. Engg., pages
584â594, 1948.
[251] S. Goldman. Information Theory. Prentice-Hall, Englewood Cliffs, NJ,
1953.
[252] A. Goldsmith and M. Effros. The capacity region of Gaussian broadcast
channels with intersymbol interference and colored Gaussian noise. IEEE
Trans. Inf. Theory, 47:2â8, Jan. 2001.
[253] S. W. Golomb. Run-length encodings. IEEE Trans. Inf. Theory, pages
399â401, July 1966.
[254] S. W. Golomb, R. E. Peile, and R. A. Scholtz. Basic Concepts in Information
Theory and Coding: The Adventures of Secret Agent 00111 (Applications of
Communications Theory). Plenum Publishing, New York, 1994.
[255] A. J. Grant, B. Rimoldi, R. L. Urbanke, and P. A. Whiting. Rate-splitting
multiple access for discrete memoryless channels. IEEE Trans. Inf. Theory,
pages 873â890, Mar. 2001.

BIBLIOGRAPHY

703

[256] R. M. Gray. Source Coding Theory. Kluwer, Boston, 1990.
[257] R. M. Gray and L. D. Davisson, (Eds.). Ergodic and Information Theory.
Dowden, Hutchinson & Ross, Stroudsburg, PA, 1977.
[258] R. M. Gray and Lee D. Davisson. Source coding theorems without the
ergodic assumption. IEEE Trans. Inf. Theory, pages 502â516, July 1974.
[259] R. M. Gray. Sliding block source coding. IEEE Trans. Inf. Theory, IT21:357â368, 1975.
[260] R. M. Gray. Entropy and Information Theory. Springer-Verlag, New York,
1990.
[261] R. M. Gray and A. Wyner. Source coding for a simple network. Bell Syst.
Tech. J., 58:1681â1721, 1974.
[262] U. Grenander and G. Szego. Toeplitz Forms and Their Applications. University of California Press, Berkeley, CA, 1958.
[263] B. GruÌnbaum. Convex Polytopes. Interscience, New York, 1967.
[264] S. Guiasu. Information Theory with Applications. McGraw-Hill, New York,
1976.
[265] B. E. Hajek and M. B. Pursley. Evaluation of an achievable rate region for
the broadcast channel. IEEE Trans. Inf. Theory, pages 36â46, Jan. 1979.
[266] R. V. Hamming. Error detecting and error correcting codes. Bell Syst. Tech.
J., 29:147â160, 1950.
[267] T. S. Han. The capacity region for the deterministic broadcast channel with
a common message (corresp.). IEEE Trans. Inf. Theory, pages 122â125,
Jan. 1981.
[268] T. S. Han and S. I. Amari. Statistical inference under multiterminal data
compression. IEEE Trans. Inf. Theory, pages 2300â2324, Oct. 1998.
[269] T. S. Han and S. Verdu. New results in the theory of identiï¬cation via
channels. IEEE Trans. Inf. Theory, pages 14â25, Jan. 1992.
[270] T. S. Han. Nonnegative entropy measures of multivariate symmetric correlations. Inf. Control, 36(2):133â156, 1978.
[271] T. S. Han. The capacity region of a general multiple access channel with
certain correlated sources. Inf. Control, 40:37â60, 1979.
[272] T. S. Han. Information-Spectrum Methods in Information Theory. SpringerVerlag, New York, 2002.
[273] T. S. Han and M. H. M. Costa. Broadcast channels with arbitrarily correlated
sources. IEEE Trans. Inf. Theory, IT-33:641â650, 1987.
[274] T. S. Han and K. Kobayashi. A new achievable rate region for the interference channel. IEEE Trans. Inf. Theory, IT-27:49â60, 1981.
[275] R. V. Hartley. Transmission of information. Bell Syst. Tech. J., 7:535, 1928.
[276] C. W. Helstrom. Elements of Signal Detection and Estimation. Prentice-Hall,
Englewood Cliffs, NJ, 1995.
[277] Y. Hershkovits and J. Ziv. On sliding-window universal data compression
with limited memory. IEEE Trans. Inf. Theory, pages 66â78, Jan. 1998.

704

BIBLIOGRAPHY

[278] P. A. Hocquenghem. Codes correcteurs dâerreurs. Chiffres, 2:147â156,
1959.
[279] J. L. Holsinger. Digital Communication over Fixed Time-Continuous
Channels with Memory, with Special Application to Telephone Channels
(Technical Report). MIT, Cambridge, MA, 1964.
[280] M. L. Honig, U. Madhow, and S. Verdu. Blind adaptive multiuser detection.
IEEE Trans. Inf. Theory, pages 944â960, July 1995.
[281] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Formal
Languages and Computation. Addison-Wesley, Reading, MA, 1979.
[282] Y. Horibe. An improved bound for weight-balanced tree. Inf. Control,
34:148â151, 1977.
[283] D. A. Huffman. A method for the construction of minimum redundancy
codes. Proc. IRE, 40:1098â1101, 1952.
[284] J. Y. Hui. Switching an Trafï¬c Theory for Integrated Broadband Networks.
Kluwer, Boston, 1990.
[285] J. Y. N. Hui and P. A. Humblet. The capacity region of the totally asynchronous multiple-access channel. IEEE Trans. Inf. Theory, pages 207â216,
Mar. 1985.
[286] S. Ihara. On the capacity of channels with additive non-Gaussian noise. Inf.
Contr., pages 34â39, 1978.
[287] S. Ihara. Information Theory for Continuous Systems. World Scientiï¬c, Singapore, 1993.
[288] K. A. Schouhamer Immink, Paul H. Siegel, and Jack K. Wolf. Codes
for digital recorders. IEEE Trans. Inf. Theory, pages 2260â2299, Oct.
1998.
[289] N. S. Jayant (Ed.). Waveform Quantization and Coding. IEEE Press, New
York, 1976.
[290] N. S. Jayant and P. Noll. Digital Coding of Waveforms. Prentice-Hall, Englewood Cliffs, NJ, 1984.
[291] E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev.,
106:620, 1957.
[292] E. T. Jaynes. Information theory and statistical mechanics II. Phys. Rev.,
108:171, 1957.
[293] E. T. Jaynes. On the rationale of maximum entropy methods. Proc. IEEE,
70:939â952, 1982.
[294] E. T. Jaynes. Papers on Probability, Statistics and Statistical Physics. Reidel,
Dordrecht, The Netherlands, 1982.
[295] F. Jelinek. Buffer overï¬ow in variable length encoding of ï¬xed rate sources.
IEEE Trans. Inf. Theory, IT-14:490â501, 1968.
[296] F. Jelinek. Evaluation of expurgated error bounds. IEEE Trans. Inf. Theory,
IT-14:501â505, 1968.

BIBLIOGRAPHY

705

[297] F. Jelinek. Probabilistic Information Theory. McGraw-Hill, New York,
1968.
[298] F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA, 1998.
[299] R Jozsa and B. Schumacher. A new proof of the quantum noiseless coding
theorem. J Mod. Opt., pages 2343â2350, 1994.
[300] G. G. Langdon, Jr. A note on the ZivâLempel model for compressing
individual sequences. IEEE Trans. Inf. Theory, pages 284â287, Mar. 1983.
[301] J. Justesen. A class of constructive asymptotically good algebraic codes.
IEEE Trans. Inf. Theory, IT-18:652â656, 1972.
[302] M. Kac. On the notion of recurrence in discrete stochastic processes. Bull.
Am. Math. Soc., pages 1002â1010, Oct. 1947.
[303] T. Kailath and J. P. M. Schwalkwijk. A coding scheme for additive noise
channels with feedback. Part I: No bandwidth constraints. IEEE Trans. Inf.
Theory, IT-12:172â182, 1966.
[304] T. Kailath and H. V. Poor. Detection of stochastic processes. IEEE Trans.
Inf. Theory, pages 2230â2259, Oct. 1998.
[305] S. Karlin. Mathematical Methods and Theory in Games, Programming and
Economics, Vol. 2. Addison-Wesley, Reading, MA, 1959.
[306] J. Karush. A simple proof of an inequality of McMillan. IRE Trans. Inf.
Theory, IT-7:118, 1961.
[307] F. P. Kelly. Notes on effective bandwidth. Stochastic Networks Theory and
Applications, pages 141â168, 1996.
[308] J. Kelly. A new interpretation of information rate. Bell Syst. Tech. J,
35:917â926, July 1956.
[309] J. H. B. Kemperman. On the Optimum Rate of Transmitting Information
(Lecture Notes in Mathematics), pages 126â169. Springer Verlag, New York,
1967.
[310] M. Kendall and A. Stuart. The Advanced Theory of Statistics. Macmillan,
New York, 1977.
[311] A. Y. Khinchin. Mathematical Foundations of Information Theory. Dover,
New York, 1957.
[312] J. C. Kieffer. A simple proof of the MoyâPerez generalization of the ShannonâMcMillan theorem. Paciï¬c J. Math., 51:203â206, 1974.
[313] J. C. Kieffer. A survey of the theory of source coding with a ï¬delity criterion.
IEEE Trans. Inf. Theory, pages 1473â1490, Sept. 1993.
[314] Y. H. Kim. Feedback capacity of ï¬rst-order moving average Gaussian channel. Proc. IEEE Int. Symp. Information Theory, Adelaide, pages 416â420,
Sept. 2005.
[315] D. E. Knuth. Dynamic Huffman coding. J. Algorithms, pages 163â180,
1985.
[316] D. E. Knuth. Art of Computer Programming.

706

BIBLIOGRAPHY

[317] D. E. Knuth and A. C. Yao. The complexity of random number generation.
In J. F. Traub (Ed.), Algorithms and Complexity: Recent Results and New
Directions (Proceedings of the Symposium on New Directions and Recent
Results in Algorithms and Complexity, Carnegie-Mellon University, 1976),
pages 357â428. Academic Press, New York, 1976.
[318] A. N. Kolmogorov. A new metric invariant of transitive dynamical systems and automorphism in Lebesgue spaces. Dokl. Akad. Nauk SSSR, pages
861â864, 1958.
[319] A. N. Kolmogorov. On the Shannon theory of information transmission in
the case of continuous signals. IRE Trans. Inf. Theory, IT-2:102â108, Sept.
1956.
[320] A. N. Kolmogorov. A new invariant for transitive dynamical systems. Dokl.
Acad. Nauks SSR, 119:861â864, 1958.
[321] A. N. Kolmogorov. Three approaches to the quantitative deï¬nition of
information. Probl. Inf. Transm. (USSR), 1:4â7, 1965.
[322] A. N. Kolmogorov. Logical basis for information theory and probability
theory. IEEE Trans. Inf. Theory, IT-14:662â664, 1968.
[323] A. N. Kolmogorov. The theory of transmission of information. In Selected
Works of A. N. Kolmogorov, Vol. III: Information Theory and the Theory of
Algorithms, Session on scientiï¬c problems of automatization in industry,
Vol. 1, Plenary talks, Izd. Akad. Nauk SSSR, Moscow, 1957, pages 66â99.
Kluwer, Dordrecht, The Netherlands, 1993.
[324] J. KoÌrner and K. Marton. The comparison of two noisy channels. In
I. CsiszaÌr and P. Elias (Ed.), Topics in Information Theory (Coll. Math. Soc.
J. Bolyai, No. 16), pages 411â423. North-Holland, Amsterdam, 1977.
[325] J. KoÌrner and K. Marton. General broadcast channels with degraded message
sets. IEEE Trans. Inf. Theory, IT-23:60â64, 1977.
[326] J. KoÌrner and K. Marton. How to encode the modulo 2 sum of two binary
sources. IEEE Trans. Inf. Theory, IT-25:219â221, 1979.
[327] J. KoÌrner and A. Orlitsky. Zero error information theory. IEEE Trans. Inf.
Theory, IT-44:2207â2229, Oct. 1998.
[328] V. A. Kotelânikov. On the transmission capacity of âetherâ and wire in
electrocommunications. Izd. Red. Upr. Svyazi RKKA, 44, 1933.
[329] V. A. Kotelânikov. The Theory of Optimum Noise Immunity. McGraw-Hill,
New York, 1959.
[330] L. G. Kraft. A device for quantizing, grouping and coding amplitude modulated pulses. Masterâs thesis, Department of Electrical Engineering, MIT,
Cambridge, MA, 1949.
[331] R. E. Krichevsky. Laplaceâs law of succession and universal encoding. IEEE
Trans. Inf. Theory, pages 296â303, Jan. 1998.
[332] R. E. Krichevsky. Universal Compression and Retrieval. Kluwer, Dordrecht,
The Netherlands, 1994.

BIBLIOGRAPHY

707

[333] R. E. Krichevsky and V. K. Troï¬mov. The performance of universal encoding. IEEE Trans. Inf. Theory, pages 199â207, Mar. 1981.
[334] S. R. Kulkarni, G. Lugosi, and S. S. Venkatesh. Learning pattern classiï¬cation: a survey. IEEE Trans. Inf. Theory, pages 2178â2206, Oct. 1998.
[335] S. Kullback. Information Theory and Statistics. Wiley, New York, 1959.
[336] S. Kullback. A lower bound for discrimination in terms of variation. IEEE
Trans. Inf. Theory, IT-13:126â127, 1967.
[337] S. Kullback, J. C. Keegel, and J. H. Kullback. Topics in Statistical Information Theory. Springer-Verlag, Berlin, 1987.
[338] S. Kullback and M. A. Khairat. A note on minimum discrimination information. Ann. Math. Stat., pages 279â280, 1966.
[339] S. Kullback and R. A. Leibler. On information and sufï¬ciency. Ann. Math.
Stat., 22:79â86, 1951.
[340] H. J. Landau and H. O. Pollak. Prolate spheroidal wave functions, Fourier
analysis and uncertainty: Part II. Bell Syst. Tech. J., 40:65â84, 1961.
[341] H. J. Landau and H. O. Pollak. Prolate spheroidal wave functions, Fourier
analysis and uncertainty: Part III. Bell Syst. Tech. J., 41:1295â1336, 1962.
[342] G. G. Langdon. An introduction to arithmetic coding. IBM J. Res. Dev.,
28:135â149, 1984.
[343] G. G. Langdon and J. J. Rissanen. A simple general binary source code.
IEEE Trans. Inf. Theory, IT-28:800, 1982.
[344] A. Lapidoth and P. Narayan. Reliable communication under channel uncertainty. IEEE Trans. Inf. Theory, pages 2148â2177, Oct. 1998.
[345] A. Lapidoth and J. Ziv. On the universality of the LZ-based decoding algorithm. IEEE Trans. Inf. Theory, pages 1746â1755, Sept. 1998.
[346] H. A. LataneÌ. Criteria for choice among risky ventures. J. Polit. Econ.,
38:145â155, Apr. 1959.
[347] H. A. LataneÌ and D.L. Tuttle. Criteria for portfolio building. J. Finance,
22:359â373, Sept. 1967.
[348] E. A. Lee and D. G. Messerschmitt. Digital Communication, 2nd ed.
Kluwer, Boston, 1994.
[349] J. Leech and N. J. A. Sloane. Sphere packing and error-correcting codes.
Can. J. Math, pages 718â745, 1971.
[350] E. L. Lehmann and H. ScheffeÌ. Completeness, similar regions and unbiased
estimation. Sankhya, 10:305â340, 1950.
[351] A. Lempel and J. Ziv. On the complexity of ï¬nite sequences. IEEE Trans.
Inf. Theory, pages 75â81, Jan. 1976.
[352] L. A. Levin. On the notion of a random sequence. Sov. Math. Dokl.,
14:1413â1416, 1973.
[353] L. A. Levin and A. K. Zvonkin. The complexity of ï¬nite objects and the
development of the concepts of information and randomness by means of
the theory of algorithms. Russ. Math. Surv., 25/6:83â124, 1970.

708

BIBLIOGRAPHY

[354] M. Li and P. Vitanyi. An Introduction to Kolmogorov Complexity and Its
Applications, 2nd ed. Springer-Verlag, New York, 1997.
[355] H. Liao. Multiple access channels. Ph.D. thesis, Department of Electrical
Engineering, University of Hawaii, Honolulu, 1972.
[356] S. Lin and D. J. Costello, Jr. Error Control Coding: Fundamentals and
Applications. Prentice-Hall, Englewood Cliffs, NJ, 1983.
[357] D. Lind and B. Marcus. Symbolic Dynamics and Coding. Cambridge
University Press, Cambridge, 1995.
[358] Y. Linde, A. Buzo, and R. M. Gray. An algorithm for vector quantizer
design. IEEE Trans. Commun., COM-28:84â95, 1980.
[359] T. Linder, G. Lugosi, and K. Zeger. Rates of convergence in the source
coding theorem in empirical quantizer design. IEEE Trans. Inf. Theory,
pages 1728â1740, Nov. 1994.
[360] T. Linder, G. Lugosi, and K. Zeger. Fixed-rate universal lossy source coding
and rates of convergence for memoryless sources. IEEE Trans. Inf. Theory,
pages 665â676, May 1995.
[361] D. Lindley. Boltzmannâs Atom: The Great Debate That Launched A Revolution in Physics. Free Press, New York, 2001.
[362] A. Liversidge. Proï¬le of Claude Shannon. In N. J. A. Sloane and A. D.
Wyner (Eds.), Claude Elwood Shannon Collected Papers. IEEE Press, Piscataway, NJ, 1993 (Omni magazine, Aug. 1987.)
[363] S. P. Lloyd. Least Squares Quantization in PCM (Technical Report). Bell
Lab. Tech. Note, 1957.
[364] G. Louchard and Wojciech Szpankowski. On the average redundancy
rate of the LempelâZiv code. IEEE Trans. Inf. Theory, pages 2â8, Jan.
1997.
[365] L. Lovasz. On the Shannon capacity of a graph. IEEE Trans. Inf. Theory,
IT-25:1â7, 1979.
[366] R. W. Lucky. Silicon Dreams: Information, Man and Machine. St. Martinâs
Press, New York, 1989.
[367] D. J. C. Mackay. Information Theory, Inference, and Learning Algorithms.
Cambridge University Press, Cambridge, 2003.
[368] D. J. C. MacKay and R. M. Neal. Near Shannon limit performance of
low-density parity-check codes. Electron. Lett., pages 1645â1646, Mar.
1997.
[369] F. J. MacWilliams and N. J. A. Sloane. The Theory of Error-Correcting
Codes. North-Holland, Amsterdam, 1977.
[370] B. Marcus. Soï¬c systems and encoding data. IEEE Trans. Inf. Theory,
IT-31(3):366â377, May 1985.
[371] R. J. Marks. Introduction to Shannon Sampling and Interpolation Theory.
Springer-Verlag New York, 1991.

BIBLIOGRAPHY

709

[372] A. Marshall and I. Olkin. Inequalities: Theory of Majorization and Its Applications. Academic Press, New York, 1979.
[373] A. Marshall and I. Olkin. A convexity proof of Hadamardâs inequality. Am.
Math. Monthly, 89(9):687â688, 1982.
[374] P. Martin-LoÌf. The deï¬nition of random sequences. Inf. Control, 9:602â619,
1966.
[375] K. Marton. Information and information stability of ergodic sources. Probl.
Inf. Transm. (VSSR), pages 179â183, 1972.
[376] K. Marton. Error exponent for source coding with a ï¬delity criterion. IEEE
Trans. Inf. Theory, IT-20:197â199, 1974.
[377] K. Marton. A coding theorem for the discrete memoryless broadcast channel.
IEEE Trans. Inf. Theory, IT-25:306â311, 1979.
[378] J. L. Massey and P. Mathys. The collision channel without feedback. IEEE
Trans. Inf. Theory, pages 192â204, Mar. 1985.
[379] R. A. McDonald. Information rates of Gaussian signals under criteria constraining the error spectrum. D. Eng. dissertation, Yale University School
of Electrical Engineering, New Haven, CT, 1961.
[380] R. A. McDonald and P. M. Schultheiss. Information rates of Gaussian
signals under criteria constraining the error spectrum. Proc. IEEE, pages
415â416, 1964.
[381] R. A. McDonald and P. M. Schultheiss. Information rates of Gaussian signals
under criteria constraining the error spectrum. Proc. IEEE, 52:415â416,
1964.
[382] R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. Turbo decoding as an
instance of Pearlâs belief propagation algorithm. IEEE J. Sel. Areas Commun., pages 140â152, Feb. 1998.
[383] R. J. McEliece. The Theory of Information and Coding. Addison-Wesley,
Reading, MA, 1977.
[384] B. McMillan. The basic theorems of information theory. Ann. Math. Stat.,
24:196â219, 1953.
[385] B. McMillan. Two inequalities implied by unique decipherability. IEEE
Trans. Inf. Theory, IT-2:115â116, 1956.
[386] N. Merhav and M. Feder. Universal schemes for sequential decision from
individual data sequences. IEEE Trans. Inf. Theory, pages 1280â1292, July
1993.
[387] N. Merhav and M. Feder. A strong version of the redundancy-capacity theorem of universal coding. IEEE Trans. Inf. Theory, pages 714â722, May 1995.
[388] N. Merhav and M. Feder. Universal prediction. IEEE Trans. Inf. Theory,
pages 2124â2147, Oct. 1998.
[389] R. C. Merton and P. A. Samuelson. Fallacy of the log-normal approximation
to optimal portfolio decision-making over many periods. J. Finan. Econ.,
1:67â94, 1974.

710

BIBLIOGRAPHY

[390] H. Minkowski. DiskontinuitaÌtsbereich fuÌr arithmetische AÌquivalenz. J.
Math., 129:220â274, 1950.
[391] L. Mirsky. On a generalization of Hadamardâs determinantal inequality due
to Szasz. Arch. Math., VIII:274â275, 1957.
[392] S. C. Moy. Generalizations of the ShannonâMcMillan theorem. Paciï¬c J.
Math., pages 705â714, 1961.
[393] J. von Neumann and O. Morgenstern. Theory of Games and Economic
Behaviour. Princeton University Press, Princeton, NJ, 1980.
[394] J. Neyman and E. S. Pearson. On the problem of the most efï¬cient tests
of statistical hypotheses. Philos. Trans. Roy. Soc. London A, 231:289â337,
1933.
[395] M. Nielsen and I. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, Cambridge, 2000.
[396] H. Nyquist. Certain factors affecting telegraph speed. Bell Syst. Tech. J.,
3:324, 1924.
[397] H. Nyquist. Certain topics in telegraph transmission theory. AIEE Trans.,
pages 617â644, Apr. 1928.
[398] J. Omura. A coding theorem for discrete time sources. IEEE Trans. Inf.
Theory, IT-19:490â498, 1973.
[399] A. Oppenheim. Inequalities connected with deï¬nite Hermitian forms. J.
London Math. Soc., 5:114â119, 1930.
[400] E. Ordentlich. On the factor-of-two bound for Gaussian multiple-access
channels with feedback. IEEE Trans. Inf. Theory, pages 2231â2235, Nov.
1996.
[401] E. Ordentlich and T. Cover. The cost of achieving the best portfolio in
hindsight. Math. Operations Res., 23(4): 960â982, Nov. 1998.
[402] S. Orey.
On the ShannonâPerezâMoy theorem.
Contemp. Math.,
41:319â327, 1985.
[403] A. Orlitsky. Worst-case interactive communication. I: Two messages are
almost optimal. IEEE Trans. Inf. Theory, pages 1111â1126, Sept. 1990.
[404] A. Orlitsky. Worst-case interactive communication. II: Two messages are
not optimal. IEEE Trans. Inf. Theory, pages 995â1005, July 1991.
[405] A. Orlitsky. Average-case interactive communication. IEEE Trans. Inf. Theory, pages 1534â1547, Sept. 1992.
[406] A. Orlitsky and A. El Gamal. Average and randomized communication complexity. IEEE Trans. Inf. Theory, pages 3â16, Jan. 1990.
[407] D. S. Ornstein. Bernoulli shifts with the same entropy are isomorphic. Adv.
Math., pages 337â352, 1970.
[408] D. S. Ornstein and B. Weiss. Entropy and data compression schemes. IEEE
Trans. Inf. Theory, pages 78â83, Jan. 1993.
[409] D. S. Ornstein. Bernoulli shifts with the same entropy are isomorphic. Adv.
Math., 4:337â352, 1970.

BIBLIOGRAPHY

711

[410] L. H. Ozarow. The capacity of the white Gaussian multiple access channel
with feedback. IEEE Trans. Inf. Theory, IT-30:623â629, 1984.
[411] L. H. Ozarow and C. S. K. Leung. An achievable region and an outer bound
for the Gaussian broadcast channel with feedback. IEEE Trans. Inf. Theory,
IT-30:667â671, 1984.
[412] H. Pagels. The Dreams of Reason: the Computer and the Rise of the Sciences
of Complexity. Simon and Schuster, New York, 1988.
[413] C. Papadimitriou. Information theory and computational complexity: The
expanding interface. IEEE Inf. Theory Newslett. (Special Golden Jubilee
Issue), pages 12â13, June 1998.
[414] R. Pasco. Source coding algorithms for fast data compression. Ph.D. thesis,
Stanford University, Stanford, CA, 1976.
[415] A. J. Paulraj and C. B. Papadias. Space-time processing for wireless communications. IEEE Signal Processing Mag., pages 49â83, Nov. 1997.
[416] W. B. Pennebaker and J. L. Mitchell. JPEG Still Image Data Compression
Standard. Van Nostrand Reinhold, New York, 1988.
[417] A. Perez. Extensions of ShannonâMcMillanâs limit theorem to more general
stochastic processes. In Trans. Third Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, pages 545â574,
Czechoslovak Academy of Sciences, Prague, 1964.
[418] J. R. Pierce. The early days of information theory. IEEE Trans. Inf. Theory,
pages 3â8, Jan. 1973.
[419] J. R. Pierce. An Introduction to Information Theory: Symbols, Signals and
Noise, 2nd ed. Dover Publications, New York, 1980.
[420] J. T. Pinkston. An application of rate-distortion theory to a converse to the
coding theorem. IEEE Trans. Inf. Theory, IT-15:66â71, 1969.
[421] M. S. Pinsker. Talk at Soviet Information Theory meeting, 1969. No abstract
published.
[422] M. S. Pinsker. Information and Information Stability of Random Variables
and Processes. Holden-Day, San Francisco, CA, 1964. (Originally published
in Russian in 1960.)
[423] M. S. Pinsker. The capacity region of noiseless broadcast channels. Probl.
Inf. Transm. (USSR), 14(2):97â102, 1978.
[424] M. S. Pinsker and R. L. Dobrushin. Memory increases capacity. Probl. Inf.
Transm. (USSR), pages 94â95, Jan. 1969.
[425] M. S. Pinsker. Information and Stability of Random Variables and Processes.
Izd. Akad. Nauk, 1960. Translated by A. Feinstein, 1964.
[426] E. Plotnik, M. Weinberger, and J. Ziv. Upper bounds on the probability
of sequences emitted by ï¬nite-state sources and on the redundancy of the
LempelâZiv algorithm. IEEE Trans. Inf. Theory, IT-38(1):66â72, Jan. 1992.
[427] D. Pollard. Convergence of Stochastic Processes. Springer-Verlag, New
York, 1984.

712

BIBLIOGRAPHY

[428] G. S. Poltyrev. Carrying capacity for parallel broadcast channels with
degraded components. Probl. Peredachi Inf., pages 23â35, Apr.âJune 1977.
[429] S. Pombra and T. M. Cover. Nonwhite Gaussian multiple access channels
with feedback. IEEE Trans. Inf. Theory, pages 885â892, May 1994.
[430] H. V. Poor. An Introduction to Signal Detection and Estimation, 2nd ed.
Springer-Verlag, New York, 1994.
[431] F. Pratt. Secret and Urgent. Blue Ribbon Books, Garden City, NY, 1939.
[432] L. R. Rabiner. A tutorial on hidden Markov models and selected applications
in speech recognition. Proc. IEEE, pages 257â286, Feb. 1989.
[433] L. R. Rabiner and R. W. Schafer. Digital Processing of Speech Signals.
Prentice-Hall, Englewood Cliffs, NJ, 1978.
[434] R. Ahlswede and Z. Zhang. New directions in the theory of identiï¬cation
via channels. IEEE Trans. Inf. Theory, 41:1040â1050, 1995.
[435] C. R. Rao. Information and accuracy obtainable in the estimation of statistical
parameters. Bull. Calcutta Math. Soc., 37:81â91, 1945.
[436] I. S. Reed. 1982 Claude Shannon lecture: Application of transforms to coding
and related topics. IEEE Inf. Theory Newslett., pages 4â7, Dec. 1982.
[437] F. M. Reza. An Introduction to Information Theory. McGraw-Hill, New
York, 1961.
[438] S. O. Rice. Mathematical analysis of random noise. Bell Syst. Tech. J., pages
282â332, Jan. 1945.
[439] S. O. Rice. Communication in the presence of noise: probability of error for
two encoding schemes. Bell Syst. Tech. J., 29:60â93, 1950.
[440] B. E. Rimoldi and R. Urbanke. A rate-splitting approach to the Gaussian
multiple-access channel. IEEE Trans. Inf. Theory, pages 364â375, Mar.
1996.
[441] J. Rissanen. Generalized Kraft inequality and arithmetic coding. IBM J. Res.
Dev., 20:198, 1976.
[442] J. Rissanen.
Modelling by shortest data description.
Automatica,
14:465â471, 1978.
[443] J. Rissanen. A universal prior for integers and estimation by minimum
description length. Ann. Stat., 11:416â431, 1983.
[444] J. Rissanen. Universal coding, information, prediction and estimation. IEEE
Trans. Inf. Theory, IT-30:629â636, 1984.
[445] J. Rissanen. Stochastic complexity and modelling. Ann. Stat., 14:1080â1100,
1986.
[446] J. Rissanen. Stochastic complexity (with discussions). J. Roy. Stat. Soc.,
49:223â239, 252â265, 1987.
[447] J. Rissanen. Stochastic complexity in Statistical Inquiry. World Scientiï¬c,
Singapore, 1989.
[448] J. J. Rissanen. Complexity of strings in the class of Markov sources. IEEE
Trans. Inf. Theory, pages 526â532, July 1986.

BIBLIOGRAPHY

713

[449] J. J. Rissanen and G. G. Langdon, Jr. Universal modeling and coding. IEEE
Trans. Inf. Theory, pages 12â23, Jan. 1981.
[450] B. Y. Ryabko. Encoding a source with unknown but ordered probabilities.
Probl. Inf. Transm., pages 134â139, Oct. 1979.
[451] B. Y. Ryabko. A fast on-line adaptive code. IEEE Trans. Inf. Theory, pages
1400â1404, July 1992.
[452] P. A. Samuelson. Lifetime portfolio selection by dynamic stochastic programming. Rev. Econ. Stat., pages 236â239, 1969.
[453] P. A. Samuelson. The âfallacyâ of maximizing the geometric mean in long
sequences of investing or gambling. Proc. Natl. Acad. Sci. USA, 68:214â224,
Oct. 1971.
[454] P. A. Samuelson. Why we should not make mean log of wealth big though
years to act are long. J. Banking and Finance, 3:305â307, 1979.
[455] I. N. Sanov. On the probability of large deviations of random variables.
Mat. Sbornik, 42:11â44, 1957. English translation in Sel. Transl. Math. Stat.
Prob., Vol. 1, pp. 213-244, 1961.
[456] A. A. Sardinas and G.W. Patterson. A necessary and sufï¬cient condition for
the unique decomposition of coded messages. IRE Conv. Rec., Pt. 8, pages
104â108, 1953.
[457] H. Sato. On the capacity region of a discrete two-user channel for strong
interference. IEEE Trans. Inf. Theory, IT-24:377â379, 1978.
[458] H. Sato. The capacity of the Gaussian interference channel under strong
interference. IEEE Trans. Inf. Theory, IT-27:786â788, 1981.
[459] H. Sato and M. Tanabe. A discrete two-user channel with strong interference.
Trans. IECE Jap., 61:880â884, 1978.
[460] S. A. Savari. Redundancy of the LempelâZiv incremental parsing rule. IEEE
Trans. Inf. Theory, pages 9â21, January 1997.
[461] S. A. Savari and R. G. Gallager. Generalized Tunstall codes for sources
with memory. IEEE Trans. Inf. Theory, pages 658â668, Mar. 1997.
[462] K. Sayood. Introduction to Data Compression. Morgan Kaufmann, San
Francisco, CA, 1996.
[463] J. P. M. Schalkwijk. A coding scheme for additive noise channels with
feedback. II: Bandlimited signals. IEEE Trans. Inf. Theory, pages 183â189,
Apr. 1966.
[464] J. P. M. Schalkwijk. The binary multiplying channel: a coding scheme
that operates beyond Shannonâs inner bound. IEEE Trans. Inf. Theory, IT28:107â110, 1982.
[465] J. P. M. Schalkwijk. On an extension of an achievable rate region for
the binary multiplying channel. IEEE Trans. Inf. Theory, IT-29:445â448,
1983.
[466] C. P. Schnorr. A uniï¬ed approach to the deï¬nition of random sequences.
Math. Syst. Theory, 5:246â258, 1971.

714

BIBLIOGRAPHY

[467] C. P. Schnorr. Process, complexity and effective random tests. J. Comput.
Syst. Sci., 7:376â388, 1973.
[468] C. P. Schnorr. A surview on the theory of random sequences. In R. Butts and
J. Hinitikka (Eds.), Logic, Methodology and Philosophy of Science. Reidel,
Dordrecht, The Netherlands, 1977.
[469] G. Schwarz. Estimating the dimension of a model. Ann. Stat., 6:461â464,
1978.
[470] S. Shamai and S. Verdu. The empirical distribution of good codes. IEEE
Trans. Inf. Theory, pages 836â846, May 1997.
[471] C. E. Shannon. A Mathematical Theory of Cryptography (Tech. Rept. MM
45-110-02). Bell Lab. Tech. Memo., Sept. 1, 1945.
[472] C. E. Shannon. A mathematical theory of communication. Bell Syst. Tech.
J., 27:379â423,623â656, 1948.
[473] C. E. Shannon. Some geometrical results in channel capacity. Verh. Dtsch.
Elektrotechnik. Fachber., pages 13â15, 1956.
[474] C. E. Shannon. The zero-error capacity of a noisy channel. IRE Trans. Inf.
Theory, IT-2:8â19, 1956.
[475] C. E. Shannon. Channels with side information at the transmitter. IBM J.
Res. Dev., pages 289â293, 1958.
[476] C. E. Shannon. Probability of error for optimal codes in a Gaussian channel.
Bell Syst. Tech. J., pages 611â656, May 1959.
[477] C. E. Shannon. Two-way communication channels. Proc. 4th. Berkeley
Symp. Mathematical Statistics and Probability (June 20âJuly 30, 1960),
pages 611â644, 1961.
[478] C. E. Shannon. The wonderful world of feedback. IEEE Int. Symp. Infor.
Theory, ser. First Shannon Lecture, Ashkelon, Israel, 1973.
[479] C. E. Shannon. The mind reading machine. In Shannonâs Collected Papers,
pages 688â689, 1993.
[480] C. E. Shannon. Communication in the presence of noise. Proc. IRE,
37:10â21, January 1949.
[481] C. E. Shannon. Communication theory of secrecy systems. Bell Syst. Tech.
J., 28:656â715, 1949.
[482] C. E. Shannon. Prediction and entropy of printed English. Bell Syst. Tech.
J., 30:50â64, January 1951.
[483] C. E. Shannon. Certain results in coding theory for noisy channels. Infor.
Control, 1:6â25, 1957.
[484] C. E. Shannon. Channels with side information at the transmitter. IBM J.
Res. Dev., 2:289â293, 1958.
[485] C. E. Shannon. Coding theorems for a discrete source with a ï¬delity criterion. IRE Nat. Conv. Rec., Pt. 4, pages 142â163, 1959.

BIBLIOGRAPHY

715

[486] C. E. Shannon. Two-way communication channels. In Proc. 4th Berkeley
Symp. Math. Stat. Prob., Vol. 1, pages 611â644. University of California
Press, Berkeley, CA, 1961.
[487] C. E. Shannon, R. G. Gallager, and E. R. Berlekamp. Lower bounds to
error probability for coding in discrete memoryless channels. I. Inf. Control,
10:65â103, 1967.
[488] C. E. Shannon, R. G. Gallager, and E. R. Berlekamp. Lower bounds to error
probability for coding in discrete memoryless channels. II. Inf. Control,
10:522â552, 1967.
[489] C. E. Shannon and W. W. Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, IL, 1949.
[490] C. E. Shannon. General treatment of the problem of coding. IEEE Trans.
Inf. Theory, pages 102â104, February 1953.
[491] W. F. Sharpe. Investments, 3rd ed. Prentice-Hall, Englewood Cliffs, NJ,
1985.
[492] P. C. Shields. Universal redundancy rates do not exist. IEEE Trans. Inf.
Theory, pages 520â524, Mar. 1993.
[493] P. C. Shields. The interactions between ergodic theory and information
theory. IEEE Trans. Inf. Theory, pages 2079â2093, Oct. 1998.
[494] P. C. Shields and B. Weiss. Universal redundancy rates for the class of
B-processes do not exist. IEEE Trans. Inf. Theory, pages 508â512, Mar.
1995.
[495] J. E. Shore and R. W. Johnson. Axiomatic derivation of the principle of
maximum entropy and the principle of minimum cross-entropy. IEEE Trans.
Inf. Theory, IT-26:26â37, 1980.
[496] Y. M. Shtarkov. Universal sequential coding of single messages. Probl. Inf.
Transm. (USSR), 23(3):3â17, JulyâSept. 1987.
[497] A. Shwartz and A. Weiss. Large Deviations for Performance Analysis,
Queues, Communication and Computing. Chapman & Hall, London, 1995.
[498] D. Slepian. Key Papers in the Development of Information Theory. IEEE
Press, New York, 1974.
[499] D. Slepian. On bandwidth. Proc. IEEE, pages 292â300, Mar. 1976.
[500] D. Slepian and H. O. Pollak. Prolate spheroidal wave functions, Fourier
analysis and uncertainty: Part I. Bell Syst. Tech. J., 40:43â64, 1961.
[501] D. Slepian and J. K. Wolf. A coding theorem for multiple access channels
with correlated sources. Bell Syst. Tech. J., 52:1037â1076, 1973.
[502] D. Slepian and J. K. Wolf. Noiseless coding of correlated information
sources. IEEE Trans. Inf. Theory, IT-19:471â480, 1973.
[503] D. S. Slepian. Information theory in the ï¬fties. IEEE Trans. Inf. Theory,
pages 145â148, Mar. 1973.

716

BIBLIOGRAPHY

[504] R. J. Solomonoff. A formal theory of inductive inference. Inf. Control,
7:1â22,224â254, 1964.
[505] A. Stam. Some inequalities satisï¬ed by the quantities of information of
Fisher and Shannon. Inf. Control, 2:101â112, June 1959.
[506] A. Steane. Quantum computing. Rept. Progr. Phys., pages 117â173, Feb.
1998.
[507] J. A. Storer and T. G. Szymanski. Data compression via textual substitution.
J. ACM, 29(4):928â951, 1982.
[508] W. Szpankowski. Asymptotic properties of data compression and sufï¬x
trees. IEEE Trans. Inf. Theory, pages 1647â1659, Sept. 1993.
[509] W. Szpankowski. Average Case Analysis of Algorithms on Sequences. WileyInterscience, New York, 2001.
[510] D. L. Tang and L. R. Bahl. Block codes for a class of constrained noiseless
channels. Inf. Control, 17:436â461, 1970.
[511] I. E. Teletar and R. G. Gallager. Combining queueing theory with information theory for multiaccess. IEEE J. Sel. Areas Commun., pages 963â969,
Aug. 1995.
[512] E. Teletar. Capacity of multiple antenna Gaussian channels. Eur. Trans.
Telecommun., 10(6):585â595, 1999.
[513] J. A. Thomas. Feedback can at most double Gaussian multiple access channel capacity. IEEE Trans. Inf. Theory, pages 711â716, Sept. 1987.
[514] T. J. Tjalkens and F. M. J. Willems. A universal variable-to-ï¬xed length
source code based on Lawrenceâs algorithm. IEEE Trans. Inf. Theory, pages
247â253, Mar. 1992.
[515] T. J. Tjalkens and F. M. J. Willems. Variable- to ï¬xed-length codes for
Markov sources. IEEE Trans. Inf. Theory, pages 246â257, Mar. 1987.
[516] S. C. Tornay. Ockham: Studies and Selections (chapter âCommentarium in
Sententias,â I, 27). Open Court Publishers, La Salle, IL, 1938.
[517] H. L. Van Trees. Detection, Estimation, and Modulation Theory, Part I.
Wiley, New York, 1968.
[518] B. S. Tsybakov. Capacity of a discrete-time Gaussian channel with a ï¬lter.
Probl. Inf. Transm., pages 253â256, JulyâSept. 1970.
[519] B. P. Tunstall. Synthesis of noiseless compression codes. Ph.D. dissertation,
Georgia Institute of Technology, Atlanta, GA, Sept. 1967.
[520] G. Ungerboeck. Channel coding with multilevel/phase signals. IEEE Trans.
Inf. Theory, pages 55â67, January 1982.
[521] G. Ungerboeck. Trellis-coded modulation with redundant signal sets part I:
Introduction. IEEE Commun. Mag., pages 5â11, Feb. 1987.
[522] G. Ungerboeck. Trellis-coded modulation with redundant signal sets part II:
State of the art. IEEE Commun. Mag., pages 12â21, Feb. 1987.
[523] I. Vajda. Theory of Statistical Inference and Information. Kluwer, Dordrecht,
The Netherlands, 1989.

BIBLIOGRAPHY

717

[524] L. G. Valiant. A theory of the learnable. Commun. ACM, pages 1134â1142,
1984.
[525] J. M. Van Campenhout and T. M. Cover. Maximum entropy and conditional
probability. IEEE Trans. Inf. Theory, IT-27:483â489, 1981.
[526] E. Van der Meulen. Random coding theorems for the general discrete memoryless broadcast channel. IEEE Trans. Inf. Theory, IT-21:180â190, 1975.
[527] E. C. van der Meulen. Some reï¬ections on the interference channel. In R.
E. Blahut, D. J. Costello, U. Maurer, and T. Mittelholzer, (Eds.), Communications and Cryptography: Two Sides of One Tapestry. Kluwer, Boston,
1994.
[528] E. C. Van der Meulen. A survey of multi-way channels in information theory. IEEE Trans. Inf. Theory, IT-23:1â37, 1977.
[529] E. C. Van der Meulen. Recent coding theorems for multi-way channels.
Part I: The broadcast channel (1976-1980). In J. K. Skwyrzinsky (Ed.), New
Concepts in Multi-user Communication (NATO Advanced Study Institute
Series), pages 15â51. Sijthoff & Noordhoff, Amsterdam, 1981.
[530] E. C. Van der Meulen. Recent coding theorems and converses for multiway channels. Part II: The multiple access channel (1976â1985) (Technical
Report). Department Wiskunde, Katholieke Universiteit Leuven, 1985.
[531] V. N. Vapnik. Estimation of Dependencies Based on Empirical Data.
Springer-Verlag, New York, 1982.
[532] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag,
New York, 1991.
[533] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies to their probabilities. Theory Prob. Appl., pages 264â280,
1971.
[534] V. N. Vapnik and A. Y. Chervonenkis. Necessary and sufï¬cient conditions
for the uniform convergence of means to their expectations. Theory Prob.
Appl., pages 532â553, 1981.
[535] S. Verdu. The capacity region of the symbol-asynchronous Gaussian
multiple-access channel. IEEE Trans. Inf. Theory, pages 733â751, July 1989.
[536] S. Verdu. Recent Progress in Multiuser Detection (Advances in Communication and Signal Processing), Springer-Verlag, Berlin, 1989. [Reprinted
in N. Abramson (Ed.), Multiple Access Communications, IEEE Press, New
York, 1993.]
[537] S. Verdu. The exponential distribution in information theory. Probl. Inf.
Transm. (USSR), pages 86â95, Jan.âMar. 1996.
[538] S. Verdu. Fifty years of Shannon theory. IEEE Trans. Inf. Theory, pages
2057â2078, Oct. 1998.
[539] S. Verdu. Multiuser Detection. Cambridge University Press, New York,
1998.
[540] S. Verdu and T. S. Han. A general formula for channel capacity. IEEE
Trans. Inf. Theory, pages 1147â1157, July 1994.

718

BIBLIOGRAPHY

[541] S. Verdu and T. S. Han. The role of the asymptotic equipartition property
in noiseless source coding. IEEE Trans. Inf. Theory, pages 847â857, May
1997.
[542] S. Verdu and S. W. McLaughlin (Eds.). Information Theory: 50 Years of
Discovery. WileyâIEEE Press, New York, 1999.
[543] S. Verdu and V. K. W. Wei. Explicit construction of optimal constant-weight
codes for identiï¬cation via channels. IEEE Trans. Inf. Theory, pages 30â36,
Jan. 1993.
[544] A. C. G. Verdugo Lazo and P. N. Rathie. On the entropy of continuous probability distributions. IEEE Trans. Inf. Theory, IT-24:120â122,
1978.
[545] M. Vidyasagar. A Theory of Learning and Generalization. Springer-Verlag,
New York, 1997.
[546] K. Visweswariah, S. R. Kulkarni, and S. Verdu. Source codes as random
number generators. IEEE Trans. Inf. Theory, pages 462â471, Mar. 1998.
[547] A. J. Viterbi and J. K. Omura. Principles of Digital Communication and
Coding. McGraw-Hill, New York, 1979.
[548] J. S. Vitter. Dynamic Huffman coding. ACM Trans. Math. Software, pages
158â167, June 1989.
[549] V. V. Vâyugin. On the defect of randomness of a ï¬nite object with
respect to measures with given complexity bounds. Theory Prob. Appl.,
32(3):508â512, 1987.
[550] A. Wald. Sequential Analysis. Wiley, New York, 1947.
[551] A. Wald. Note on the consistency of the maximum likelihood estimate. Ann.
Math. Stat., pages 595â601, 1949.
[552] M. J. Weinberger, N. Merhav, and M. Feder. Optimal sequential probability assignment for individual sequences. IEEE Trans. Inf. Theory, pages
384â396, Mar. 1994.
[553] N. Weiner. Cybernetics. MIT Press, Cambridge, MA, and Wiley, New York,
1948.
[554] T. A. Welch. A technique for high-performance data compression. Computer,
17(1):8â19, Jan. 1984.
[555] N. Wiener. Extrapolation, Interpolation and Smoothing of Stationary Time
Series. MIT Press, Cambridge, MA, and Wiley, New York, 1949.
[556] H. J. Wilcox and D. L. Myers. An Introduction to Lebesgue Integration and
Fourier Series. R.E. Krieger, Huntington, NY, 1978.
[557] F. M. J. Willems. The feedback capacity of a class of discrete memoryless
multiple access channels. IEEE Trans. Inf. Theory, IT-28:93â95, 1982.
[558] F. M. J. Willems and A. P. Hekstra. Dependence balance bounds for singleoutput two-way channels. IEEE Trans. Inf. Theory, IT-35:44â53, 1989.
[559] F. M. J. Willems. Universal data compression and repetition times. IEEE
Trans. Inf. Theory, pages 54â58, Jan. 1989.

BIBLIOGRAPHY

719

[560] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. The context-tree
weighting method: basic properties. IEEE Trans. Inf. Theory, pages 653â664,
May 1995.
[561] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. Context weighting for
general ï¬nite-context sources. IEEE Trans. Inf. Theory, pages 1514â1520,
Sept. 1996.
[562] H. S. Witsenhausen. The zero-error side information problem and chromatic
numbers. IEEE Trans. Inf. Theory, pages 592â593, Sept. 1976.
[563] H. S. Witsenhausen. Some aspects of convexity useful in information theory.
IEEE Trans. Inf. Theory, pages 265â271, May 1980.
[564] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data
compression. Commun. ACM, 30(6):520â540, June 1987.
[565] J. Wolfowitz. The coding of messages subject to chance errors. Ill. J. Math.,
1:591â606, 1957.
[566] J. Wolfowitz. Coding Theorems of Information Theory. Springer-Verlag,
Berlin, and Prentice-Hall, Englewood Cliffs, NJ, 1978.
[567] P. M. Woodward. Probability and Information Theory with Applications to
Radar. McGraw-Hill, New York, 1953.
[568] J. Wozencraft and B. Reiffen. Sequential Decoding. MIT Press, Cambridge,
MA, 1961.
[569] J. M. Wozencraft and I. M. Jacobs. Principles of Communication Engineering. Wiley, New York, 1965.
[570] A. Wyner. A theorem on the entropy of certain binary sequences and applications II. IEEE Trans. Inf. Theory, IT-19:772â777, 1973.
[571] A. Wyner. The common information of two dependent random variables.
IEEE Trans. Inf. Theory, IT-21:163â179, 1975.
[572] A. Wyner. On source coding with side information at the decoder. IEEE
Trans. Inf. Theory, IT-21:294â300, 1975.
[573] A. Wyner and J. Ziv. A theorem on the entropy of certain binary sequences
and applications I. IEEE Trans. Inf. Theory, IT-19:769â771, 1973.
[574] A. Wyner and J. Ziv. The rate distortion function for source coding with
side information at the receiver. IEEE Trans. Inf. Theory, IT-22:1â11, 1976.
[575] A. Wyner and J. Ziv. On entropy and data compression. IEEE Trans. Inf.
Theory, 1991.
[576] A. D. Wyner. Capacity of the the band-limited Gaussian channel. Bell Syst.
Tech. J., 45:359â395, Mar. 1966.
[577] A. D. Wyner. Communication of analog data from a Gaussian source over
a noisy channel. Bell Syst. Tech. J., pages 801â812, MayâJune 1968.
[578] A. D. Wyner. Recent results in the Shannon theory. IEEE Trans. Inf. Theory,
pages 2â10, Jan. 1974.
[579] A. D. Wyner. The wiretap channel. Bell Syst. Tech. J., pages 1355â1387,
1975.

720

BIBLIOGRAPHY

[580] A. D. Wyner. The rate-distortion function for source coding with side information at the decoder. II: General sources. Inf. Control, pages 60â80, 1978.
[581] A. D. Wyner. Shannon-theoretic approach to a Gaussian cellular multipleaccess channel. IEEE Trans. Inf. Theory, pages 1713â1727, Nov. 1994.
[582] A. D. Wyner and A. J. Wyner. Improved redundancy of a version of the
LempelâZiv algorithm. IEEE Trans. Inf. Theory, pages 723â731, May 1995.
[583] A. D. Wyner and J. Ziv. Bounds on the rate-distortion function for stationary
sources with memory. IEEE Trans. Inf. Theory, pages 508â513, Sept. 1971.
[584] A. D. Wyner and J. Ziv. The rate-distortion function for source coding with
side information at the decoder. IEEE Trans. Inf. Theory, pages 1â10, Jan.
1976.
[585] A. D. Wyner and J. Ziv. Some asymptotic properties of the entropy of a
stationary ergodic data source with applications to data compression. IEEE
Trans. Inf. Theory, pages 1250â1258, Nov. 1989.
[586] A. D. Wyner and J. Ziv. Classiï¬cation with ï¬nite memory. IEEE Trans. Inf.
Theory, pages 337â347, Mar. 1996.
[587] A. D. Wyner, J. Ziv, and A. J. Wyner. On the role of pattern matching in
information theory. IEEE Trans. Inf. Theory, pages 2045â2056, Oct. 1998.
[588] A. J. Wyner. The redundancy and distribution of the phrase lengths of
the ï¬xed-database LempelâZiv algorithm. IEEE Trans. Inf. Theory, pages
1452â1464, Sept. 1997.
[589] A. D. Wyner. The capacity of the band-limited Gaussian channel. Bell Syst.
Tech. J., 45:359â371, 1965.
[590] A. D. Wyner and N. J. A. Sloane (Eds.) Claude E. Shannon: Collected
Papers. WileyâIEEE Press, New York, 1993.
[591] A. D. Wyner and J. Ziv. The sliding window LempelâZiv algorithm is
asymptotically optimal. Proc. IEEE, 82(6):872â877, 1994.
[592] E.-H. Yang and J. C. Kieffer. On the performance of data compression
algorithms based upon string matching. IEEE Trans. Inf. Theory, pages
47â65, Jan. 1998.
[593] R. Yeung. A First Course in Information Theory. Kluwer Academic, Boston,
2002.
[594] H. P. Yockey. Information Theory and Molecular Biology. Cambridge University Press, New York, 1992.
[595] Z. Zhang, T. Berger, and J. P. M. Schalkwijk. New outer bounds to capacity
regions of two-way channels. IEEE Trans. Inf. Theory, pages 383â386, May
1986.
[596] Z. Zhang, T. Berger, and J. P. M. Schalkwijk. New outer bounds to capacity regions of two-way channels. IEEE Trans. Inf. Theory, IT-32:383â386,
1986.
[597] J. Ziv. Coding of sources with unknown statistics. II: Distortion relative to
a ï¬delity criterion. IEEE Trans. Inf. Theory, IT-18:389â394, 1972.

BIBLIOGRAPHY

721

[598] J. Ziv. Coding of sources with unknown statistics. II: Distortion relative to
a ï¬delity criterion. IEEE Trans. Inf. Theory, pages 389â394, May 1972.
[599] J. Ziv. Coding theorems for individual sequences. IEEE Trans. Inf. Theory,
pages 405â412, July 1978.
[600] J. Ziv. Distortion-rate theory for individual sequences. IEEE Trans. Inf.
Theory, pages 137â143, March 1980.
[601] J. Ziv. Universal decoding for ï¬nite-state channels. IEEE Trans. Inf. Theory,
pages 453â460, July 1985.
[602] J. Ziv. Variable-to-ï¬xed length codes are better than ï¬xed-to-variable length
codes for Markov sources. IEEE Trans. Inf. Theory, pages 861â863, July
1990.
[603] J. Ziv and A. Lempel. A universal algorithm for sequential data compression.
IEEE Trans. Inf. Theory, IT-23:337â343, 1977.
[604] J. Ziv and A. Lempel. Compression of individual sequences by variable rate
coding. IEEE Trans. Inf. Theory, IT-24:530â536, 1978.
[605] J. Ziv and N. Merhav. A measure of relative entropy between individual
sequences with application to universal classiï¬cation. IEEE Trans. Inf. Theory, pages 1270â1279, July 1993.
[606] W. H. Zurek. Algorithmic randomness and physical entropy. Phys. Rev. A,
40:4731â4751, Oct. 15 1989.
[607] W. H. Zurek. Thermodynamic cost of computation, algorithmic complexity
and the information metric. Nature, 341(6238):119â124, Sept. 1989.
[608] W. H. Zurek (Ed.) Complexity, Entropy and the Physics of Information
(Proceedings of the 1988 Workshop on the Complexity, Entropy and the
Physics of Information). Addison-Wesley, Reading, MA, 1990.

LIST OF SYMBOLS

X, 14
p(x), 14
pX (x), 14
X, 14
H (X), 14
H (p), 14
Hb (X), 14
Ep g(X), 14
Eg(X), 14
def
==, 15
p(x, y), 17
H (X, Y ), 17
H (Y |X), 17
D(p||q), 20
I (X; Y ), 21
I (X; Y |Z), 24
D(p(y|x)||q(y|x)), 25
|X|, 30
X â Y â Z, 35
N(Âµ, Ï 2 ), 37
T (X), 38
fÎ¸ (x), 38
Pe , 39
H (p), 45
{0, 1}â , 55
2ân(H Â±) , 58
A(n)
 , 59
x, 60
X n , 60

x n , 61
BÎ´(n) , 62
.
=, 63
Z n , 65
H (X), 71
Pij , 72
H  (X), 75
C(x), 103
l(x), 103
C, 103
L(C), 104
D, 104
C â , 105
li , 107
lmax , 108
L, 110
Lâ , 111
HD (X), 111
x, 113
F (x), 127
F (x), 128
sgn(t), 132
(j )
pi , 138
pi , 159
oi , 159
bi , 160
b(i), 160
Sn , 160
S(X), 160

Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

723

724

LIST OF SYMBOLS

W (b, p), 160
b, 160
p, 160
W â (p), 161
W , 165
W â (X|Y ), 165
C, 184
WÌ , 193
(X n , p(y n |x n ), Y n ), 193
Î»i , 194
Î»(n) , 194
Pe(n) , 194
R,
 195

( 2nR , n), 195
C, 200
E, 202
Ei , 203
âª, 203
Câ , 204
CFB , 216
â, 224
F (x), 243
f (x), 243
h(X), 243
S, 243
h(f ), 243
Ï(x), 244
Vol(A), 245
X  , 247
h(X|Y ), 249
Nn (Âµ, K), 249
|K|, 249
D(f ||g), 250
X, 253
P , 261
W , 270
F (Ï), 271
sinc(t), 271
N0 , 272
x + , 276
KX , 278

KZ , 278
tr(KX ), 278
B, 282
KV , 282
Cn,F B , 283
{XÌ(w)}, 303
XÌ, 304
R+ , 304
dmax , 304
D, 306
XÌ n (w), 306
R(D), 306
D(R), 306
R (I ) (D), 307
A(n)
d, , 319
D, 321
K(x n , xÌ n ), 322
N (a|x n ), 326
Aâ(n)
 , 326
N (a, b|x n , y n ), 326
Ï(D), 337
Vy n |x n (b|a), 342
TV (x n ), 342
n
Aâ(n)
 (Y |x ), 343
x, 347
X, 347
Px , 348
Px n , 348
Pn , 348
T (P ), 348
Qn (x n ), 349
TQ , 356
Qn (E), 361
P â , 362
L1 , 369
|| ||1 , 369
St , 372
D â , 372
Î±, 375
Î², 376
a â , 376

LIST OF SYMBOLS

bâ , 376
An , 376
Bn , 376
ÏA (), 376
PÎ» , 380
Î»â , 380
C(P1 , P2 ), 386
Ï(s), 392
T (X1 , X2 , . . . , Xn ), 393
V , 394
J (Î¸ ), 394
bT (Î¸ ), 396
Jij (Î¸ ), 397
R(k), 415
S(Î»), 415
RÌ(k), 415
K (n) , 416
2
Ïâ
, 417
Kp , 420
(u), 422
pÎ¸ , 428
R(pÎ¸ , q), 429
R â , 429
qÏ , 430
A(n, k), 434
q 1 (x n ), 436
2
FU (u), 437
Rn (X0 , X1 , . . . , Xnâ1 ), 444
Qu (i), 445
Aj k , 445
c(n), 450
nk , 450
cls , 452
{Xi }â
ââ , 455
AD , 460
U, 466
U(p), 466
p, 466
KU (x), 466
KU (x|l(x)), 467
K(x|l(x)), 468

logâ n, 469
H0 (p), 470
K(n), 471
PU (x), 481
, 484
n , 484
Kk (x n |n), 496
k â , 497
p â , 497
S â , 497
S ââ , 497
p ââ , 497
C(P /N ), 514
S, 520
X(S), 520
A(n)
 .(S), 521
an = 2n(bÂ±) , 521
A(n)
 (S1 |s2 ), 523
Eij , 531
Q, 534
CI , 535
R(S), 543
S c , 543
Î² â p1 , 569
b, 613
S, 613
W (b, F ), 615
W â (F ), 615
bâ , 615
Snâ , 615
W â , 615
B, 617
W , 622
â
Wâ
, 624
â
U , 628
Snâ (xn ), 630
SÌ n (xn ), 630
Vn , 631
j n , 634
w(j n ), 634
K, 636

725

726

LIST OF SYMBOLS

(m), 638
B(Î»1 , Î»2 ), 642
(, B, P ), 644
X(Ï), 644
H k , 645
H â , 645
pÌn , 660
(z), 662
Ï(z), 662
Î³ , 662
X(S), 668
h(n)
k , 668
(n)
tk , 669
gk(n) , 669
fk(n) , 671

J (X), 671
gt (y), 672
V (A), 675
f â g, 676
Lr , 676
||f ||r , 676
Cp , 676
hr (X), 676
Vr (X), 677
Pk , 680
Sk(n) , 680
K(i1 , i2 , . . . , ik ), 680
Qk , 681
Ïi2 , 681
Kk , 681

INDEX

Abrahams, J., 689
Abramson, N.M., xxiii, 689
Abu-Mostafa, Y.S., 689
acceptance region, 376, 383
achievable rate, 195, 268, 306, 538, 546,
550, 598
achievable rate distortion pair, 306
achievable rate region, 514, 550
AczeÌl, J., 690
Adams, K., xxiii
adaptive dictionary compression algorithms,
441
additive channel, 229
additive noise channel, 280, 287, 296
additive white Gaussian noise (AWGN),
289
Adler, R.L., 158, 689
AEP, xix, xx, 6, 12, 57, 58, 64, 69, 77,
101, 168, 219, 220, 222, 223, 347,
356, 381, 382, 409, 460, 531, 554,
560, 566, 644, 645, 649, 656, 686, see
also Shannon-McMillan-Breiman
theorem
continuous random variables, 245
discrete random variables, 58â62
distortion typical, 319
growth rate, 650
joint, 196, 223
products, 66
relative entropy, 380
sandwich proof, 644â649
stationary ergodic processes, 644â649
Ahlswede, R., 11, 609, 610, 689, 690, 712
Akaike, H., 690
Algoet, P., xxiii, 69, 626, 645, 656, 690

algorithm, 442
alternating minimization, 332
arithmetic coding, see arithmetic coding
Blahut-Arimoto, see Blahut-Arimoto
algorithm
Durbin, 419
Frank-Wolfe, 191
gradient search, 191
greedy, 127
Huffman coding, 127, 154, see Huffman
coding
iterative, 332
Lempel-Ziv, see Lempel-Ziv coding
Levinson, 419
online, 427
algorithmic complexity, 1, 3, 463, 464, 466,
507, 508, see Kolmogorov
complexity
algorithmically random, 477, 486, 502,
507
almost sure, 58, 69
alphabet, 13, 347
discrete, 13
effective size, 46, 256
effective support set, 256
input, 183
output, 183
alphabetic codes, 121
Altria, 643
Amari, S.I., 55, 690, 703
Anantharam, V., 690
antenna, 292, 611
Arimoto, S., 191, 240, 335, 346, 690, see
also Blahut-Arimoto algorithm
arbitrarily varying channel, 689, 690,
697

Elements of Information Theory, Second Edition, By Thomas M. Cover and Joy A. Thomas
Copyright ï 2006 John Wiley & Sons, Inc.

727

728

INDEX

arithmetic coding, 130, 158, 171, 427, 428,
435â440, 461
ï¬nite precision arithmetic, 439
arithmetic mean geometric mean inequality,
669
ASCII, 466
Ash, R.B., 690
asymmetric distortion, 337
asymptotic equipartition property, see AEP
asymptotic optimality,
log-optimal portfolio, 619
ATM networks, 218
atmosphere, 412
atom, 137â140, 257, 404
autocorrelation, 415, 420
autoregressive process, 416
auxiliary random variable, 565, 569
average codeword length, 124, 129, 148
average description length, 103
average distortion, 318, 324, 325, 329, 344
average power, 261, 295, 296, 547
average probability of error, 199, 201, 202,
204, 207, 231, 297, 532, 554
AWGN (Additive white Gaussian noise),
289
axiomatic deï¬nition of entropy, 14, 54
Baer, M., xxi
Bahl, L.R., xxiii, 690, 716
bandlimited, 272
bandpass ï¬lter, 270
bandwidth, 272â274, 289, 515, 547, 606
Barron, A.R., xxi, xxiii, 69, 420, 508, 656,
674, 691, 694
base of logarithm, 14
baseball, 389
Baum, E.B., 691
Bayesian error exponent, 388
Bayesian hypothesis testing, 384
Bayesian posterior probability, 435
Bayesian probability of error, 385, 399
BCH (Bose-Chaudhuri-Hocquenghem)
codes, 214
Beckenbach, E.F., 484
Beckner, W., 691
Bell, R., 182, 656, 691
Bell, T.C., 439, 691
Bellâs inequality, 56
Bellman, R., 691

Bennett, C.H., 241, 691
Bentley, J., 691
Benzel, R., 692
Berger, T., xxiii, 325, 345, 610, 611, 692,
720
Bergmans, P., 609, 692
BergstrÃ¸mâs inequality, 684
Berlekamp, E.R., 692, 715
Bernoulli, J., 182
Bernoulli distribution, 434
Bernoulli process, 237, 361, 437, 476, 484,
488
Bernoulli random variable, 63
Bernoulli source, 307, 338
rate distortion, 307
Berrou, C., 692
Berryâs paradox, 483
Bertsekas, D., 692
beta distribution, 436, 661
beta function, 642
betting, 162, 164, 167, 173, 174, 178, 487,
626
horse race, 160
proportional, 162, 626
bias, 393, 402
Bierbaum, M., 692
Biglieri, E., 692
binary entropy function, 15, 49
graph, 15, 16
binary erasure channel, 188, 189, 218, 227,
232, 457
binary multiplier channel, 234, 602
binary rate distortion function, 307
binary source, 308, 337
binary symmetric channel (BSC), 8,
187â189, 210, 215, 222, 224, 225,
227â229, 231, 232, 237, 238, 262,
308, 568, 601
capacity, 187
binning, 609
random, 551, 585
bioinformatics, xv
bird, 97
Birkhoffâs ergodic theorem, 644
bishop, 80
bit, 14
Blachman, N., 674, 687, 692
Blackwell, D., 692

INDEX

Blahut, R.E., 191, 240, 335, 346, 692, see
also Blahut-Arimoto algorithm
Blahut-Arimoto algorithm, 191, 334
block code, 226
block length, 195, 204
block Markov encoding, 573
Boltzmann, L., 11, 55, 693, see
also Maxwell-Boltzmann distribution
bone, 93
bookie, 163
Borel-Cantelli lemma, 357, 621, 649
Bose, R.C., 214, 693
bottleneck, 47, 48
bounded convergence theorem, 396, 647
bounded distortion, 307, 321
brain, 465
Brascamp, H.J., 693
Brassard, G., 691
Breiman, L., 69, 655, 656, 692, 693, see
also Shannon-McMillan-Breiman
theorem
Brillouin, L., 56, 693
broadcast channel, 11, 100, 515, 518, 533,
563, 560â571, 593, 595, 599, 601,
604, 607, 609, 610
capacity region, 564
convexity, 598
common information, 563
converse, 599
degraded, 599, 609
achievablity, 565
converse, 599
with feedback, 610
Gaussian, 515, 610
physically degraded, 564
stochastically degraded, 564
Brunn-Minkowski inequality, xx, 657,
674â676, 678, 679, 687
BSC, see binary symmetric channel (BSC)
Bucklew, J.A., 693
Burg, J.P., 416, 425, 693
Burgâs algorithm, 416
Burgâs maximum entropy theorem, 417
Burrows, M., 462, 693
burst error correcting code, 215
Buzo, A., 708
Caire, G., 693
cake, 65

729

calculus, 103, 111, 410
Calderbank, A.R., 693
Canada, 82
capacity, 223, 428
channel, see channel capacity
capacity region, 11, 509â610
degraded broadcast channel, 565
multiple access channel, 526
capacity theorem, xviii, 215
CAPM (Capital Asset Pricing Model), 614
CaratheÌodoryâs theorem, 538
cardinality, 245, 538, 542, 565, 569
cards, 55, 84, 167, 608
Carleial, A.B., 610, 693
cascade, 225
cascade of channels, 568
Castelli, V., xxiii
Cauchy distribution, 661
Cauchy-Schwarz inequality, 393, 395
causal, 516, 623
causal investment strategy, 619, 623, 635
causal portfolio, 620, 621, 624, 629â631,
635, 639, 641, 643
universal, 651
CCITT, 462
CDMA (Code Division Multiple Access),
548
central limit theorem, xviii, 261, 361
centroid, 303, 312
CesaÌro mean, 76, 625, 682
chain rule, xvii, 35, 43, 49, 287, 418, 540,
541, 555, 578, 584, 590, 624, 667
differential entropy, 253
entropy, 23, 31, 43, 76
growth rate, 650
mutual information, 24, 25, 43
relative entropy, 44, 81
Chaitin, G.J., 3, 4, 484, 507, 508, 693, 694
Chang, C.S., xxi, 694
channel,
binary erasure, 188, 222
binary symmetric, see binary symmetric
channel (BSC)
broadcast, see broadcast channel
cascade, 225, 568
discrete memoryless, see discrete
memoryless channel
exponential noise, 291
extension, 193

730

INDEX

channel (continued )
feedback, 216
Gaussian, see Gaussian channel
interference, see interference channel
multiple access, see multiple access
channel
noiseless binary, 7, 184
parallel, 224, 238
relay, see relay channel
symmetric, 189, 190, 222
time-varying, 226, 294
two-way, see two-way channel
union, 236
weakly symmetric, 190
with memory, 224, 240
Z-channel, 225
channel capacity, xv, xviii, xix, 1, 3, 7â9,
11, 38, 183â241, 291, 297, 298, 307,
333, 427, 432, 458, 461, 544â546,
548, 592, 604, 608, 686
achievability, 200
computation, 332
feedback, 223
information, 184, 263
operational deï¬nition, 184
properties, 222
zero-error, 226
channel capacity theorem, 38
channel code, 193, 220
channel coding, 207, 219, 220, 318, 324,
325, 344
achievability, 195
channel coding theorem, 9, 199, 207, 210,
223, 230, 321, 324, 347
achievability, 200
converse, 207
channel transition matrix, 190, 234, 428,
433, 509
channels with memory, 224, 277
characteristic function, 422
Chellappa, R., 694
Cheng, J.F., 709
Chernoff, H., 694
Chernoff bound, 392
Chernoff information, 380, 384, 386, 388,
399
Chernoff-Stein lemma, 347, 376, 380, 383,
399
Chervonenkis, A.Y., 717

chessboard, 80, 97
Ï 2 distance, 665
Ï 2 statistic, 400
Chi-squared distribution, 661
Chiang, M.S., xxi, 610, 695
chicken, 301
Choi, B.S., 425, 694, 697
Chomsky, N., 694
Chou, P.A., 694
Chuang, I., 241, 710
Chung, K.L., 69, 694
Churchâs thesis, 465
Ciofï¬, J.M., 611, 702
cipher, 170
substitution, 170
Clarke, B.S., 694
classroom, 561
Cleary, J.G., 691, 719
closed system, 11
closure, 363
Cocke, J., 690
cocktail party, 515
code, 10, 20, 61, 62, 98, 104â158, 172,
194â232, 264â281, 302â339, 357,
360, 428, 429, 434, 436, 438, 440,
443, 444, 456, 460â463, 492,
525â602
arithmetic, see arithmetic coding
block, see block code
channel, see channel code
distributed source, see distributed source
coding
error correcting, see error correcting code
extension, 105, 126
Gaussian channel, 264
Hamming, see Hamming code
Huffman, see Huffman code
instantaneous, 103, 106, 107, 110, 118,
124, 142, 143, 146, 152, see
also code, preï¬x
minimum distance, 212
minimum weight, 212
Morse, see Morse code
nonsingular, 105, 152
optimal, 124, 144
preï¬x, 106
random, 199, 201
self-punctuating, 105, 106
Shannon, see Shannon code

INDEX

source, see source code
uniquely decodable, 105, 110, 116â118,
127, 131, 141â143, 147, 148, 150,
152, 157, 158
zero-error, 205
codebook, 204, 206, 212, 266, 268, 321,
322, 327, 328, 513â519, 530, 534,
565, 574, 580
codelength, 114, 115, 122
code points, 302
codeword, 109, 122, 131
optimal length, 113
Cohn, M., xxi
coin ï¬ips, 44, 103, 155
coin tosses, 37, 134, 137, 139, 225, 375
coin weighing, 45
coins,
bent, 48
biased, 96, 375, 407
fair, 134
large deviations, 365
colored noise, 288
coloring,
random, 558
comma, 105, 468
common information, 563, 564, 567,
568
communication channel, 1, 7, 187, 223,
261, 560, 663
communication networks, 509
communication system, 8, 184, 192
communication theory, 1
compact disc, 3, 215
compact set, 432, 538
company mergers, 149
competitive optimality,
log-optimal portfolio, 627
Shannon code, 130
competitively optimal, 103, 613
composition class, 348
compression, see data compression
computable, 466, 479, 482, 484, 491
computable probability distribution, 482
computable statistical tests, 479
computation, 4, 5, 12, 68, 190, 438, 463,
466, 594
channel capacity, 191, 332
halting, see halting computation
models of, 464

731

physical limits, 56
rate distortion, 332
computer science, xvii, 1, 463, 483
computer source code, 360
computers, 4, 442, 464, 504
concatenation, 105, 116
concavity, 27, 31, 33, 222, 453, 474, 616,
see also convexity
conditional entropy, 17, 38â40, 75, 77, 88,
89, 206, 417, 669, 670
conditional limit theorem, 366, 371, 375,
389, 398
conditional mutual information, 24, 589
conditional rate distortion,
convexity, 583, 585
conditional rate distortion function, 584,
585
conditional relative entropy, 25
conditional type, 342, 366
conditionally typical set, 327, 342
conditioning reduces entropy, 33, 39, 85,
311, 313, 318, 418, 578, 584, 682
constant rebalanced portfolio, 615, 630
constrained sequences, 94, 101
continuous alphabet, 261, 305
continuous channel, 263
continuous random variable, 21, 37, 243,
245, 248, 256, 301, 304, 338, 500, see
also differential entropy, rate
distortion theory
quantization, see quantization
continuous time, 270
convergence of random variables,
convergence in mean square, 58
convergence in probability, 58
convergence with probability, 1, 58
converse,
broadcast channel, 599
discrete memoryless channel, 206
with feedback, 216
general multiterminal network, 589
multiple access channel, 538
rate distortion, 315
Slepian-Wolf coding, 555

732

INDEX

convex closure, 538
convex families, 655
convex hull, 526, 530, 532, 534â536, 538,
543, 544, 565, 591, 594, 595
convex set, 330, 534
convexiï¬cation, 598
convexity, 26, 28, 32, 42, 432, 616, 657,
see also concavity
capacity region,
broadcast channel, 598
multiple access channel, 534
rate distortion function, 316
strict, 26
convolution, 270, 674
convolutional code, 215
interleaved, 215
cookie-cutting, 240
copper, 274
Coppersmith, D., 689
correlated random variables,
coding, see Slepian-Wolf coding
correlation, 46, 252, 258, 294, 295, 593
Costa, M.H.M., 593, 694, 703
Costello, D.J., 708
counterfeit, 45
covariance matrix, 249, 277, 279, 280, 284,
292, 397, 416, 417, 681
Cover, T.M., xxi, 69, 158, 182, 240, 299,
425, 508, 575, 593, 609â611, 626,
656, 687, 690, 691, 694â699, 701,
712, 717
CPU, 465
CrameÌr, H., 696
CrameÌr-Rao inequality, 392, 395, 396, 399
with bias, 402
crosstalk, 273
cryptography, 171
CsiszaÌr, I., 55, 325, 332, 334, 335, 346,
347, 358, 408, 461, 610, 696, 697
CsiszaÌr-TusnaÌdy algorithm, 335
cumulative distribution function, 127, 128,
130, 243, 437, 439
cut set, 512, 589, 597
D-adic distribution, 112
D-ary alphabet, 103, 109
Dantzig, G.B., 697
Darroch, J.N., 697
DaroÌczy, Z., 690

data compression, xv, xvii, xix, 1, 3, 5, 11,
103, 156, 163, 172, 173, 184, 218,
221, 301, 427, 442, 457, 549, 656, 686
data processing inequality, 35, 39, 44, 47,
371, 687
data transmission, 5, 686
Daubechies, I., 697
Davisson, L.D., 461, 697, 702, 703
de Bruijnâs identity, 672
decision function, 375
decision theory, see hypothesis testing
decoder, 194
decoding, 194
robust, 296
decoding delay, 148
decoding function, 194, 219, 264, 552, 571,
583
degraded, 515, 516, 533, 565â570, 595,
599, 601, 604, 609
broadcast channel, see broadcast channel,
degraded
physically degraded, 564
relay channel, see relay channel,
degraded
stochastically degraded, 564
Dembo, A., xxiii, 687, 697, 698
demodulation, 3
Dempster, A.P., 698
density, 243
descriptive complexity, 463
determinant, 249, 279, 679, 681
determinant inequalities, xviii, 679â685
deterministic, 53, 173, 204, 511, 607, 609
deterministic decoding rule, 194
deterministic function, 339, 456
Devroye, L., 698
dice, 364, 375, 411, 412
dictionary, 442
differential entropy, xix, 243â674
bound, 258
conditional, 249
Gaussian distribution, 244
relationship to discrete entropy, 247
table of, 661
Diggavi, S., xxi
digital, 273, 274, 465
dimension, 211, 675
Dirichlet distribution, 436, 641
Dirichlet partition, 303

INDEX

discrete, 184
discrete channel, 263, 264, 268
discrete entropy, 244, 259, 660
discrete memoryless channel, 183,
184â241, 280, 318, 344, 536
discrete memoryless source, 92
discrete random variable, 14, 49, 54, 134,
243, 245, 249, 251, 252, 258, 347,
371, 658
discrete time, 261
discrimination, 55
distance, 20, 301, 325, 332, 369, 432
between probability distributions, 13
Euclidean, 297, 332, 367
relative entropy, 356, 366
variational, 370
distinguishable inputs, 10, 222
distinguishable signals, 183
distortion, 10, 301â341, 580â582, 586,
587, see also rate distortion theory
asymmetric, 337
Hamming, see Hamming distortion
squared error, see squared error
distortion
distortion function, 304, 307, 309, 312,
336, 340, 344, 345
distortion measure, 301, 302, 304, 305,
307, 314, 319, 321
bounded, 304
Hamming distortion, 304,
Itakura-Saito, 305
squared error distortion, 305
distortion rate function, 306, 341
distortion typical, 319, 321, 322, 328
distributed source coding, 509, 511, 550,
556, 586, see also Slepian-Wolf
coding
distribution, 427, 428, 456
two mass point, 28
divergence, 55
DiVincenzo, D.P., 691, 698
DMC, see discrete memoryless channel
Dobrushin, R.L., 698, 711
dog, 93
Donoho, D.L., 698
doubling rate, xvii, 9, 11, 12, 160,
163â167, 175, 179, 180
doubly stochastic matrix, 88
DSL, 274

733

duality, 610
data compression and data transmission,
184
data compression and gambling, 173
growth rate and entropy rate, 159, 613
multiple access channel and
Slepian-Wolf coding, 558
rate distortion and channel capacity, 311
source coding and generation of random
variables, 134
Duda, R.O., 698
Dueck, G., 609, 610, 690, 698
Durbin algorithm, 419
Dutch, 561, 562, 606
Dutch book, 164, 180
DVD, 3
dyadic, 129, 130, 132, 137â139
dyadic distribution, 151

ear, 219
Ebert, P.M., 299, 698
echoes, 273
Eckschlager, K., 698
economics, 4
edge process, 98
effectively computable, 465
efï¬cient estimator, 396
efï¬cient frontier, 614
Effros, M., 462, 694, 698, 702
Efron, B., 699
Eggleston, H.G., 538, 699
eigenvalue, 95, 279, 282, 315, 336
Einstein, A., xvii
Ekroot, L., xxiii
El Gamal, A., xxiii, 575, 609â611,
694â696, 699, 701, 702, 710
elephant, 301
Elias, P., 158, 699
Ellis, R.S., 699
EM algorithm, 335
empirical, 68, 381, 409, 470, 474, 542, 660
empirical distribution, 68, 168, 209, 347,
356, 366, 630
convergence, 68
empirical entropy, 195
empirical frequency, 474
encoder, 173, 231, 304, 321, 357, 360, 443,
458, 459, 549, 553, 557, 574, 585, 588

734

INDEX

encoding function, 193, 264, 305, 359, 571,
583, 599
encrypted text, 506
energy, 261, 265, 272, 273, 294, 424
England, 82
English, 104, 168â171, 174, 175, 182, 360,
470, 506
entropy rate, 159, 182
models of, 168
entanglement, 56
entropy, xvii, 3, 4, 13â56, 87, 659, 671,
686
average, 49
axiomatic deï¬nition, 14, 54
base of logarithm, 14, 15
bounds, 663
chain rule, 23
concavity, 33, 34
conditional, 16, 51, see conditional
entropy
conditioning, 42
cross entropy, 55
differential, see differential entropy
discrete, 14
encoded bits, 156
functions, 45
grouping, 50
independence bound, 31
inï¬nite, 49
joint, 16, 47, see joint entropy
mixing increase, 51
mixture, 46
and mutual information, 21
properties of, 42
relative, see relative entropy
Renyi, 676
sum, 47
thermodynamics, 14
entropy and relative entropy, 12, 28
entropy power, xviii, 674, 675, 678, 679,
687
entropy power inequality, xx, 298, 657,
674â676, 678, 679, 687
entropy rate, 4, 74, 71â101, 114, 115, 134,
151, 156, 159, 163, 167, 168, 171,
175, 182, 221, 223, 259, 417, 419,
420, 423â425, 428â462, 613, 624,
645, 667, 669
differential, 416

English, 168, 170, 174, 175
Gaussian process, 416
Hidden Markov model, 86
Markov chain, 77
subsets, 667
envelopes, 182
Ephremides, A., 611, 699
Epimenides liar paradox, 483
equalization, 611
Equitz, W., xxiii, 699
erasure, 188, 226, 227, 232, 235, 527, 529,
594
erasure channel, 219, 235, 433
ergodic, 69, 96, 167, 168, 175, 297, 360,
443, 444, 455, 462, 557, 613, 626,
644, 646, 647, 651
ergodic process, xx, 11, 77, 168, 444, 446,
451, 453, 644
ergodic source, 428, 644
ergodic theorem, 644
ergodic theory, 11
Erkip, E., xxi, xxiii
Erlang distribution, 661
error correcting code, 205
error detecting code, 211
error exponent, 4, 376, 380, 384, 385, 388,
399, 403
estimation, xviii, 255, 347, 392, 425, 508
spectrum, 415
estimator, 39, 40, 52, 255, 392, 393,
395â397, 401, 402, 407, 417, 500, 663
bias, 393
biased, 401
consistent in probability, 393
domination, 393
efï¬cient, 396
unbiased, 392, 393, 395â397, 399, 401,
402, 407
Euclidean distance, 514
Euclidean geometry, 378
Euclidean space, 538
Eulerâs constant, 153, 662
exchangeable stocks, 653
expectation, 14, 167, 281, 306, 321, 328,
393, 447, 479, 617, 645, 647, 669, 670
expected length, 104
exponential distribution, 256, 661
extension of channel, 193
extension of code, 105

INDEX

F-distribution, 661
face vase illusion, 505
factorial, 351, 353
Stirlingâs approximation, 405
fading, 611
fading channel, 291
Fahn, P., xxi
fair odds, 159, 164, 487, 488
fair randomization, 627, 629
Fan, K., 679, 699
Fano, R.M., 56, 158, 240, 699, 700, see
also Shannon-Fano-Elias code
Fanoâs inequality, 13, 38, 39, 41, 44, 52,
56, 206, 208, 221, 255, 268, 283,
539â541, 555, 576, 578, 590, 663
FAX, 130
FDMA (Frequency Division Multiple
Access), 547, 548, 606
Feder, M., 158, 462, 700, 709, 718
Feder, T., 461
feedback, xix, 189, 193, 216, 218, 238,
280â284, 286â290, 509, 519, 593,
594, 610, 611
discrete memoryless channel, 216
Gaussian channel, xv, 280â289
Feinstein, A., 240, 699, 700
Feller, W., 182, 700
Fermatâs last theorem, 486
ï¬ngers, 143
ï¬nite alphabet, 220, 318, 344, 473, 474, 645
ï¬nitely often, 649
ï¬nitely refutable, 486
ï¬rst order in the expononent, 63
Fisher, R.A., 56, 700
Fisher information, xviii, xx, 247, 347, 392,
394, 395, 397, 399, 401, 407, 657,
671, 673, 674
examples, 401
multiparameter, 397
Fitingof, B.M., 461, 700
ï¬xed rate block code, 357
ï¬ag, 61, 442, 460
ï¬ow of information, 588, 589
ï¬ow of time, 89
ï¬ow of water, 511
football, 390, 391
Ford, L.R., 700
Ford-Fulkerson theorem, 511, 512
Forney, G.D., 240, 700

735

Foschini, G.J., 611, 700
Fourier transform, 271, 415
fractal, 471
Franaszek, P.A., xxi, xxiii, 158, 700
Frank-Wolfe algorithm, 191
French, 606
frequency, 168â170, 270, 274, 315, 404,
547
Friedman, J.H., 693
Fulkerson, D.R., 697, 700
function,
concave, 26
convex, 26
functional, 161, 276, 313, 330
future, 93
Gaarder, T., 593, 609, 700
Gabor, D., 701
GaÌcs, P., 695, 701
Gadsby, 168
Gallager, R.G., xxiii, 215, 240, 299, 430,
461, 609, 692, 701, 713, 715, 716
Galois ï¬eld theory, 214
gambling, xviii, xx, 11, 13, 159, 171â173,
175, 178, 181, 182, 488, 507, 629
universal, 487
gambling and data compression, 171
game, 181, 298, 391, 631
20 questions, 6, 120, 121, 143, 145, 157,
237
Hi-Lo, 147
mutual information, 298
red and black, 167, 177
Shannon guessing, 174
stock market, 630
game theory, 132
fundamental theorem, 432
game-theoretic optimality, 132, 619
Î³ (Eulerâs constant), 153, 662
Gamma distribution, 661
gas, 34, 409, 411, 412
Gaussâs law, 548
Gauss-Markov process, 417â420
Gaussian, 252, 255, 258, 378, 389, 684, 685
Gaussian channel, xv, xix, 205, 261â299,
324, 513, 514, 519, 520, 544, 546, 686
achievability, 266
AWGN (additive white Gaussian noise),
289

736

INDEX

Gaussian channel (continued )
bandlimited, 270â274
broadcast, see broadcast channel,
Gaussian
capacity, 264
colored noise, 277
converse, 268
feedback, 280â289
interference, see interference channel,
Gaussian
with memory, 277, 280
multiple access, see also multiple access
channel, Gaussian
parallel, 274â280, 292
relay, see also relay channel, Gaussian
Gaussian distribution, see normal
distribution
Gaussian process, 272, 279, 417
Gaussian source, 311, 336
rate distortion function, 311
Gaussian stochastic process, 315, 416, 417,
423
Gelfand, I.M., 702
Gelfand, S.I., 609, 610, 702
Gemelos, G., xxi
general multiterminal network, 587
general theory of relativity, 490
generalized Lloyd algorithm, 303
generation of random variables, 134
geodesic, 380
geometric distribution, 405, 444
geometry, 9, 301, 367
Euclidean, 378
geophysical applications, 415
Gersho, A., 702
Gibson, J.D., 702
GIF, 443, 462
Gilbert, E.N., 158, 702
Gill, J., xxiii
Glavieux, A., 692
GoÌdelâs incompleteness theorem, 483
Goldbachâs conjecture, 486
Goldberg, M., xxiii
Goldman, S., 702
Goldsmith, A., 702
Golomb, S.W., 702
Goodell, K., xxiii
Gopinath, R., xxi
Gotham, 470, 550

gradient search, 191
grammar, 171
Grant, A.J., 702
graph, 73, 78, 79, 97
graph coloring, 557
gravestone, 55
gravitation, 490
Gray, R.M., 610, 694, 695, 702, 703, 708
greetings telegrams, 441
Grenander, U., 703
grouping rule, 50
growth rate, xix, 4, 159, 178, 180, 182,
615, 613â656, 686
chain rule, 624, 650
competitive optimality, 628
convexity, 616, 650
optimal, 615
side information, 622, 650
growth rate optimal, 162, 613
GruÌnbaum, B., 538, 703
Guiasu, S., 703
Gupta, V., xxi
Gutman, M., 462, 700
Gyorï¬, L., 698
gzip, 442
Hadamardâs inequality, 279, 680, 681
Hajek, B., 611, 699, 703
halting, 484
halting computation, 466, 486
halting problem, 483
halting program, 473
Hamming codes, 205, 212â214
Hamming distortion, 307, 308, 336, 337
Hamming, R.V., 210, 703
Han, T.S., xxi, 593, 609, 610, 668, 670,
687, 689, 703, 717, 718
handwriting, 87
Hart, P.E., 695, 698
Hartley, R.V., 55, 703
Hassanpour, N., xxi
Hassibi, B., 693
Hassner, M., 689
HDTV, 560
Hekstra, A.P., 609, 718
Helstrom, C.W., 703
Hershkovits, Y., 703
Hewlett-Packard, 643
hidden Markov model (HMM), 87, 101

INDEX

high probability set, 62
histogram, 174
historical notes, xv
HMM, see hidden Markov model (HMM)
Hochwald, B.M., 693
Hocquenghem, P.A., 214, 703
Holsinger, J.L., 704
Honig, M.L., 704
Hopcroft, J.E., 704
Horibe, Y., 704
horse race, 5, 6, 11, 159â182, 622, 626
Huffman code, 103, 118â127, 129â131,
137, 142, 145, 146, 149, 151, 155,
157, 357, 427, 436, 460, 491, 492
competitive optimality, 158
dyadic distribution, 151
Huffman, D.A., 158, 704
Hui, J.Y., 704
Humblet, P.A., 704
hypothesis testing, 1, 4, 11, 355, 375, 380,
384, 389
Bayesian, 384
optimal, see Neyman-Pearson lemma
i.i.d. (independent and identically
distributed) source, 307, 318, 344, 357
identiï¬cation capacity, 610
Ihara, S., 704
image, 305
distortion measure, 305
entropy rate, 171
Kolmogorov complexity, 499, 505, 506
Immink, K.A.S., 704
incompressible sequence, 477, 479
independence bound on entropy, 31
India, 441
indicator function, 194, 219, 486, 497, 503
induction, 95, 123, 127, 674
inequalities, xviiiâxx, 53, 207, 418,
657â687
inequality,
arithmetic mean geometric mean, 669
Brunn-Minkowski,
see Brunn-Minkowski inequality
Cauchy-Schwarz, 393
Chebyshevâs, 64
data processing, see data processing
inequality
determinant, see determinant inequalities

737

entropy power, see entropy power
inequality
Fanoâs, see Fanoâs inequality
Hadamardâs, see Hadamardâs inequality
information, 29, 410, 659
Jensenâs, see Jensenâs inequality
Kraft, see Kraft inequality
log sum, see log sum inequality
Markovâs, see Markovâs inequality
McMillanâs, see McMillanâs inequality
subset, see subset inequalities
Youngâs, 676
Zivâs, 450
inference, 1, 3, 4, 463, 484
inï¬nite bandwidth, 273
inï¬nitely often, 621
information, see also Fisher information,
mutual information, self information
information capacity, 207, 263, 274, 277
information channel capacity, 184
information divergence, 55, see
also relative entropy
information for discrimination, 55, see
also relative entropy
information rate distortion function, 306,
307, 329
innovations, 282
input alphabet, 183, 209, 268
input distribution, 188, 227, 228, 278, 335,
430, 431, 532, 544, 546, 591
instantaneous code, see code, instantaneous
integer,
binary representation, 469
descriptive complexity, 469
integrability, 248
interference, xix, 3, 11, 273, 509, 511, 515,
518, 519, 527, 547, 588, 610
interference channel, 510, 518, 519, 610
degraded, 610
Gaussian, 518, 519, 610
high interference, 518
strong interference, 610
interleaving, 611
internet, 218
intersymbol interference, 94
intrinsic complexity, 464
investment, 4, 9, 11, 159, 614, 619, 623,
636, 655, 656
investor, 619, 623, 627, 629, 633, 635

738

INDEX

irreducible Markov chain, see Markov
chain, irreducible
Itakura-Saito distance, 305
iterative decoding, 215
Iyengar, G., xxi
Jacobs, I.M., 719
Jayant, N.S., 704
Jaynes, E.T., 56, 416, 425, 704
Jelinek, F., xxiii, 158, 690, 704, 705
Jensenâs inequality, 28, 32, 41, 42, 44, 49,
252, 253, 270, 318, 447, 453, 474,
585, 618, 622, 657
Johnson, R.W., 715
joint AEP, 202, 203, 267, 329, 520
joint density, 249
joint distribution, 16, 23, 34, 51, 52, 71,
228, 268, 307, 308, 323, 328, 343,
365, 402, 537, 539, 542, 550, 564,
565, 578, 586, 595, 600, 602, 608
joint entropy, 16
joint source channel coding theorem, 218
joint type, 499
joint typicality, 195, 222, 240
jointly typical, 198â203, 227â230, 240,
266, 267, 319, 327â329, 341, 343,
365, 366, 520, 553, 557, 559, 560,
575, 580
jointly typical sequences, 520
jointly typical set, 227, 228, 319, 327
Jozsa, R, 705
JPEG, 130
Julian, D., xxi
Justesen, J., 215, 705
Kac, M., 443, 705
Kacâs lemma, 444
Kailath, T., 705
Karlin, S., 705
Karush, J., 158, 705
Kaul, A., xxiii
Kawabata, B., xxiii
Keegel, J.C., 707
Kelly, J., 182, 655, 705
Kelly, F.P., 705
Kelly gambling, 182, 626
Kemperman, J.H.B., 408, 705
Kendall, M., 705
keyboard, 480, 482

Khairat, M.A., 707
Khinchin, A.Y., 705
Kieffer, J.C., 69, 705, 720
Kim, Y.H., xxi, 299, 705
Kimber, D., xxiii
kinetic energy, 409
King, R., 182, 696
Knuth, D.E., 153, 705
Kobayashi, K., 610, 703
Kolmogorov, A.N., 3, 345, 417, 463, 507,
702, 706
Kolmogorov complexity, xv, xviii, xix, 1,
3, 4, 10â12, 428, 466, 463â508, 686
conditional, 467
and entropy, 473, 502
of integers, 475
lower bound, 469, 502
universal probability, 490
upper bound, 501
Kolmogorov structure function, 496, 503,
507
Kolmogorov sufï¬cient statistic, 496, 497,
508
Kolmogorovâs inequality, 626
Kontoyiannis, Y., xxi
KoÌrner, J., 241, 325, 347, 358, 408, 609,
610, 690, 697, 698, 701, 706
Kotelânikov, V.A., 706
Kraft, L.G., 158, 706
Kraft inequality, 103, 107â110, 112, 113,
116â118, 127, 138, 141, 143, 158,
473, 484, 494
Krichevsky, R.E., 706
Kuhn-Tucker conditions, 164, 177, 191,
314, 331, 617, 618, 621, 622
Kulkarni, S.R., 698, 707, 718
Kullback, J.H., 707
Kullback, S., xix, 55, 408, 707
Kullback Leibler distance, 20, 55, 251, see
also relative entropy
L1 distance, 369
Lagrange multipliers, 110, 153, 161, 276,
313, 330, 334, 335, 421
Laird, N.M., 698
Lamping, J., xxi
Landau, H.J., 272, 299, 707
Landauer, R., 56, 691
Langdon, G.G., 705, 707, 713

INDEX

Lapidoth, A., xxi, 707
Laplace, P.S., 488, 489
Laplace distribution, 257, 661
Laplace estimate, 488
large deviation theory, 4, 12, 357, 360
LataneÌ, H.A., 182, 655, 707
Lavenberg, S., xxiii
law of large numbers, 57, 199, 245, 267,
319, 326, 355â357, 361, 403, 477,
479, 520, 522, 615
incompressible sequences, 477, 502
method of types, 355
weak law, 57, 58, 65, 196, 245, 361,
380, 479
lecturer, 561
Lee, E.A., 707
Leech, J., 707
Lehmann, E.L., 56, 707
Leibler, R.A., 55, 707
Lempel, A., 428, 442, 462, 707, 721, see
also Lempel-Ziv coding
Lempel-Ziv,
ï¬xed database, 459
inï¬nite dictionary, 458
sliding window, 443
tree structured, 448
Lempel-Ziv algorithm, xxiii, 441
Lempel-Ziv coding, 440â456
Lempel-Ziv compression, 360
Lempel-Ziv parsing, 427
letter, 105, 168â171, 174, 175, 209, 210,
224, 226, 233
Leung, C.S.K., 593, 609, 610, 696, 711
Levin, L.A., 507, 707
Levinson algorithm, 419
Levyâs martingale convergence theorem,
647
lexicographic order, 327, 472
Li, M., 508, 707
Liao, H., 10, 609, 708
liar paradox, 483
Lieb, E.J., 693
likelihood, 20, 365, 377, 404, 482, 508
likelihood ratio, 482
likelihood ratio test, 377, 378, 385,
389
Lin, S., 708
Lind, D., 708
Linde, Y., 708

739

Linder, T., 708
Lindley, D., 708
linear algebra, 211
linear code, 214
linear inequalities, 534
linear predictive coding, 416
list decoding, 517, 575
Liversidge, A., 708
Lloyd, S.P., 708
Lloyd aglorithm, 303
local realism, 56
logarithm,
base of, 14
lognormal distribution, 662
log likelihood, 65, 67, 405
log-optimal portfolio, 616â624, 626â629,
649, 653, 654, 656
competitive optimality, 627, 651
log sum inequality, 31â33, 44
Longo, G., 697
Lotto, 178
Louchard, G., 708
Lovasz, L., 226, 241, 708
low density parity check (LDPC) codes,
215
Lucky, R.W., 170, 171, 708
Lugosi, G., 698, 707, 708
LZ77, 441
LZ78, 441
MacKay, D.J.C., 215, 708, 709
macrostate, 55, 409, 411, 412
MacWilliams, F.J., 708
Madhow, U., 704
magnetic recording, 94, 101, 105, 158
Malone, D., 175
Mandelbrot set, 471
Marcus, B., 158, 708
margin, 181
marginal distribution, 297, 333
Markov approximation, 169, 646
Markov chain, 35, 36, 39, 40, 47, 52,
71â100, 144, 206, 258, 294, 295, 423,
458, 470, 497, 499, 578â580, 584,
659, 687
aperiodic, 72, 78
functions of, 84
irreducible, 72, 78, 98
stationary distribution, 73

740

INDEX

Markov chain (continued )
time invariant, 72
time-reversible, 81
Markov ï¬elds, 35
Markov lemma, 586
Markov process, 87, 100, 144, 422, 428,
437, see also Gauss-Markov process
Markovâs inequality, 49, 64, 157, 238, 392,
460, 621, 627, 648, 649
Markowitz, H., 614
Marks, R.J., 708
Marshall, A., 708, 709
Martian, 143
Martin-LoÌf, P., 507, 709
martingale, 647
martingale convergence theorem, 626
Marton, K., 609, 610, 706, 709
Marzetta, T.L., 693
Massey, J.L., 709
mathematics, xvi
Mathis, C., xxi
Mathys, P., 709
matrix, 88, 95, 99, 200, 212, 239, 337, 338,
340, 342, 397, 432, 458, 657, 681,
682, 687
channel transition, 190
doubly stochastic, 190
parity check, 211
permutation, 88
probability transition, 72
trace, 278
transition, 77, 88
matrix inequalities, 687
max-ï¬ow min-cut, 512
maximal probability of error, 204, 207,
264, 268
maximum a posteriori, 388
maximum entropy, xviii, 51, 92, 96, 255,
258, 263, 282, 289, 375, 409,
412â415, 417, 420â425, 451
conditional limit theorem, 371
prediction error, 423
spectral density, 419, 421
maximum entropy distribution, 30, 364,
375, 409, 410, 412â414
maximum entropy graph, 97
maximum entropy process, 419, 422
maximum likelihood, 201, 231, 500
maximum likelihood decoding, 231

maximum likelihood estimation, 404
Maxwell-Boltzmann distribution, 409, 662
Maxwellâs demon, 507
maze, 97
Mazo, J., xxiii
McDonald, R.A., 345, 709
McEliece, R.J., 696, 697, 709
McLaughlin, S.W., 718
McMillan, B., 69, 158, 709, see
also Shannon-McMillan-Breiman
theorem
McMillanâs inequality, 141
MDL (minimum description length), 501
mean value theorem, 247
mean-variance theory, 614
measure theory, xx
median, 257
medical testing, 375
Melsa, J.L., 702
memoryless, 184, 216, 280, 513, 563, 572,
588, 593, 610, see also channel,
discrete memoryless
merges, 149
Merhav, N., 461, 462, 700, 709, 718, 721
Merton, R.C., 709
Messerschmitt, D.G., 707
method of types, xv, 347, 357, 361, 665
metric, 46
microprocessor, 468
microstate, 55, 409, 411
MIMO (multiple-input multiple-output),
611
minimal sufï¬cient statistic, 38
minimax redundancy, 456
minimum description length, 3, 501, 508
minimum distance, 213, 325, 332
between convex sets, 332
relative entropy, 367
minimum variance, 396
minimum weight, 212
Minkowski, H., 710
Mirsky, L., 710
Mitchell, J.L., 711
mixed strategy, 391
mobile telephone, 607
models of computation, 464
modem, 273, 442
modulation, 3, 263
modulo 2 arithmetic, 211, 308, 596

INDEX

molecules, 409
moments, 255, 414, 614
Mona Lisa, 471, 499
money, 160, 164, 171, 172, 176â178, 487,
631, 634, see also wealth
monkey, 480, 482, 504
Moore, E.F., 158, 702
Morgenstern, O., 710
Morrell, M., xxiii
Morse code, 103, 104
Moy, S.C., 69, 710
multipath, 292, 611
multiple access channel, 10, 518, 524, 589,
594, 609
achievability, 530
binary erasure channel, 527
binary erasure multiple access channel,
594
binary multiplier channel, 527
capacity region, 526
convexity, 534
converse, 538
cooperative capacity, 596
correlated source, 593
duality with Slepian-Wolf coding, 558
erasure channel, 529
feedback, 594
Gaussian, 514, 598, 607
independent BSCâs, 526
multiplexing, 273, 515, 547
multi-user information theory, see network
information theory
multivariate distributions, 411
multivariate normal, 249, 254, 287, 305,
315, 413, 417, 679
music, 1, 428
mutual fund, 653
mutual information, xvii, 12, 20, 159, 252,
656, 686
chain rule, 24
conditional, 45, 49
continuous random variables, 251
non-negativity, 29
properties, 43
Myers, D.L., 718
Nagaoka, H., 690
Nahamoo, D., xxiii
Narayan, P., 697, 707

741

nats, 14, 244, 255, 313
Nayak, P.P., xxi
Neal, R.M., 215, 708, 719
nearest neighbor, 303
nearest neighbor decoding, 3
neighborhood, 361, 638
Nelson, R., xxi
network, 11, 270, 273, 274, 509â511, 519,
520, 587, 588, 592, 594
network information theory, xv, xix, 3, 10,
11, 509â611
feedback, 593
Neumann, J.von, 710
Newton, I., xvii, 4
Newtonian physics, 490
Neyman, J., 710
Neyman-Pearson lemma, 376, 398
Nielsen, M., 241, 710
Nobel, A., xxiii
noise, xvii, xix, 1, 3, 11, 183, 224, 234,
237, 257, 261, 265, 272â274,
276â281, 289, 291â293, 297â299,
324, 509, 513â516, 519, 520, 533,
546, 548, 588
colored, 277
noiseless channel, 8, 558
noisy typewriter, 186
Noll, P., 704
nonnegative deï¬nite matrix, 284, 285
nonnegativity,
entropy, 15
mutual information, 29
relative entropy, 20, 29
nonsense, 464, 482, 504
norm, 297
Euclidean, 297
normal distribution, 38, 254, 269, 311, 411,
414, 662, 675, see also Gaussian
channel, Gaussian source
generalized, 662
maximum entropy property, 254
null space, 211
Nyquist, H., 270, 272, 710
Occamâs Razor, 1, 4, 463, 481, 488, 490,
500
odds, 11, 67, 159, 162â164, 176â180, 626,
645
even, 159

742

INDEX

odds (continued )
fair, 159, 167, 176
subfair, 164, 176
superfair, 164
uniform, 172
uniform fair, 163
Olkin, I., 708, 709
Olshen, R.A., 693
, xix, 484, 502
Omura, J.K., 718, 710
onion-peeling, 546
Oppenheim, A., 710
optical channel, 101
optimal code length, 148, 149
optimal decoding, 231, 514
optimal doubling rate, 162, 165, 166
optimal portfolio, 613, 626, 629, 652
oracle, 485
Ordentlich, E., xxi, xxiii, 656, 696, 710
Orey, S., 69, 656, 710
Orlitsky, A., xxi, xxiii, 241, 706, 710
Ornstein, D.S., 710
Oslick, M., xxiii
output alphabet, 143, 183
Ozarow, L.H., 594, 609, 610, 711
Pagels, H., 508, 711
Papadias, C.B., 711
Papadimitriou, C., 711
paradox, 482
Berryâs, 483
Epimenides liar, 483
St. Petersburg, 181
parallel channels, 277, 293
parallel Gaussian source, 314
Pareto distribution, 662
parity, 212â214
parity check code, 211, 214
parity check matrix, 211
parsing, 441, 448â450, 452, 455, 456, 458,
459
partial recursive functions, 466
partition, 251
Pasco, R., 158, 711
past, 93
Patterson, G.W., 149, 713
Paulraj, A.J., 711
Pearson, E.S., 710
Peile, R.E., 702

Pennebaker, W.B., 711
Perez, A., 69, 711
perihelion of Mercury, 490
periodogram, 415
permutation, 84, 190, 258
permutation matrix, 88
perpendicular bisector, 378
perturbation, 674
Phamdo, N., xxi
philosopherâs stone, 484
philosophy of science, 4
photographic ï¬lm, 293
phrase, 441â443, 448, 452
physically degraded, 564, 568, 571, 573,
610
physics, xvi, xvii, 1, 4, 56, 409, 463, 481
Ï, 4
picture on cover, 471
Pierce, J.R., 711
pigeon, 233
Pinkston, J.T., 337, 711
Pinsker, M.S., 299, 609, 610, 702, 711
pitfalls, 483
pixels, 471
pkzip, 442
Plotnik, E., 711
Poisson distribution, 293
Pollak, H.O., 272, 299, 707, 715
Pollard, D., 711
Poltyrev, G.S., 712
Polyaâs urn model, 90
polynomial number of types, 355, 357, 373
Pombra, S., xxi, 299, 695, 696, 712
Poor, H.V., 705, 712
portfolio, 182, 613â654, 656
portfolio strategy, 620, 629â631, 634, 636,
643
portfolio theory, xv, 613
positive deï¬nite matrix, 279, 686, 687
Posner, E., 696
power, 84, 116, 142, 273, 293, 295, 297,
298, 320, 324, 357, 415, 513â515,
517, 518, 546â548, 606, 607, 610,
674
power constraint, 261, 262â264, 266, 268,
270, 274, 277, 278, 281, 289, 291,
292, 296, 513, 547
power spectral density, 272, 289, 415
Pratt, F., 712

INDEX

prediction, 11
prediction error, 423
preï¬x, 106, 109, 110, 118, 124, 149, 150,
443, 473
preï¬x code, 109, 110, 118, 148, 150
principal minor, 680, 681
prior, 385, 388, 389, 435, 436
Bayesian, 384
Proakis, J., 692
probability density, 243, 250, 420, 425
probability mass function, 5
probability of error,
Bayesian, 385
maximal, 195, 204
probability simplex, 348, 359, 362,
378â380, 385, 386, 391, 408
probability theory, 1, 12
probability transition matrix, 7, 72, 73, 226,
524
process, 183
program length, 3, 463
prolate spheroidal functions, 272
proportional betting, 487
proportional gambling, 162â164, 173, 182,
619, 645
punctuation, 168
Pursley, M.B., 697, 703
Pythagorean theorem, 367, 368
quantization, 247, 248, 251, 263, 301â303,
312, 363
quantum channel capacity, 56
quantum data compression, 56
quantum information theory, 11, 241
quantum mechanics, 11, 56, 241
queen, 80
Rabiner, L.R., 712
race, see horse race
radio, 261, 270, 547, 560
radium, 257
random box size, 67
random coding, 3, 201, 204, 230, 324, 565
random number generation, 134
random process,
Bernoulli, 98
random questions, 53
random variable, 5, 6, 13, 14, 103
Bernoulli, 53, 63

743

generation, 134, 155
random walk, 78
randomization, 627
rank, 211, 393
Rao, C.R., 712
Ratcliff, D., 697
rate,
achievable, see achievable rate
entropy, see entropy rate
rate distortion, xv, 301â347, 582, 585, 586,
596, 610, 686
achievability, 306, 318
Bernoulli source, 307, 336
computation, 332
converse, 316
erasure distortion, 338
Gaussian source, 310, 311, 325, 336
inï¬nite distortion, 336
multivariate Gaussian source, 336
operational deï¬nition, 307
parallel Gaussian source, 314
Shannon lower bound, 337
with side information, 580, 596
squared error distortion, 310, 338
rate distortion code, 305, 316, 321, 324,
325, 329, 341, 583
optimal, 339
rate distortion function, 306â308, 310, 311,
313â316, 321, 327, 333, 334,
337â340, 344, 596, 610
convexity, 316
information, 307
rate distortion region, 306, 586
rate distortion theorem, 307, 310, 324, 325,
336, 341, 583, 585
rate distortion theory, 10, 301, 303, 307,
357
rate region, 535, 536, 557, 569, 592, 593,
602â605, 608
Rathie, P.N., 662, 718
Raviv, J., 690
Ray-Chaudhuri, D.K., 214, 693
Rayleigh, G.G., 611, 702
Rayleigh distribution, 662
rebalanced portfolio, 613, 629â632, 634,
636, 638, 639, 643
receiver, 183
recurrence, 91, 457, 459, 460
recurrence time, 444, 445

744

INDEX

recursion, 90, 95, 123, 469
redistribution of wealth, 82
redundancy, 148, 171, 184, 210, 429, 430,
435, 436, 456, 461, 462, 631
minimax, 429
Reed, I.S., 214, 712
Reed-Solomon codes, 214, 215
Reiffen, B., 666, 719
reinvest, 181, 615
relative entropy, xvii, xix, 4, 9, 11, 12, 20,
25, 30, 43, 52, 68, 81, 87, 112, 115,
151, 252, 259, 305, 332, 333, 362,
366, 368, 369, 378â384, 401, 421,
427, 429, 545, 658â660, 665, 686
Ï 2 bound, 400
asymmetry, 52
bounds, 663
chain rule, 25
convexity, 33
and Fisher information, 401
L1 bound, 398
non-negativity, 29, 50
properties, 43
relative entropy distance, 82, 356, 433
relative entropy neighborhood, 361
relay channel, 510, 516, 571, 572, 591,
595, 610
achievability, 573
capacity, 573
converse, 572
degraded, 571, 573, 591, 610
feedback, 591
Gaussian, 516
physically degraded, 571, 573, 591
reversely degraded, 575
Renyi entropy, 676, 677
Renyi entropy power, 677
reproduction points, 302
reverse water-ï¬lling, 315, 336, 345
Reza, F.M., 712
Rice, S.O., 712
Riemann integrability, 248
Rimoldi, B., 702, 712
risk-free asset, 614
Rissanen, J., 158, 420, 462, 508, 691, 707,
712, 713
Roche, J., xxi, xxiii
rook, 80
Roy, B., xxi

Rubin, D.B., 698
run length coding, 49
Ryabko, B.Ya., 430, 461, 713
saddlepoint, 298
Salehi, M., xxiii, 695, 696
Salz, J., xxiii
sample correlation, 415
sampling theorem, 272
Samuelson, P.A., 656, 709, 713
sandwich argument, 69, 644, 648
Sanov, I.N., 408, 713
Sanovâs theorem, 362, 378, 386, 391, 398,
403
Sardinas, A.A., 149, 713
Sardinas-Patterson test, 149
satellite, 215, 261, 509, 515, 565
Sato, H., 610, 713
Savari, S.A., 713
Sayood, K., 713
Schafer, R.W., 712
Schalkwijk, J.P.M., 609, 713, 720
ScheffeÌ, H., 56, 707
Schnorr, C.P., 507, 713, 714
Scholtz, R.A., 702
SchroÌdingerâs wave equation, xvii
Schultheiss, P.M., 345, 709
Schumacher, B., 705
Schwalkwijk, J.P.M., 705
Schwarz, G., 714
score function, 393, 394
second law of thermodynamics, xviii, 4, 11,
55, 81, 87, 507, see also statistical
mechanics
concavity, 100
self-information, 13, 22
self-punctuating, 468
self-reference, 483
sequence length, 55
sequential projection, 400
set sum, 675
sgn function, 132
Shakespeare, 482
Shamai, S., 692, 714
Shannon code, 115, 122, 131, 132, 142,
145, 463, 470, 613
competitive optimality, 130, 132, 142,
158
Shannon guessing game, 174

INDEX

Shannon lower bound, 337
Shannonâs ï¬rst theorem (source coding
theorem), 115
Shannonâs second theorem (channel coding
theorem), 189, 192
Shannonâs third theorem (rate distortion
theorem), 307
Shannon, C.E., xv, xviii, 55, 69, 100, 157,
171, 174, 182, 205, 240, 270, 299,
345, 609, 656, 687, 699, 714, 715, see
also Shannon code,
Shannon-Fano-Elias code,
Shannon-McMiIlan-Breiman theorem
Shannon-Fano code, 158, 491, see
also Shannon code
Shannon-Fano-Elias code, 127, 130, 428
Shannon-McMillan-Breiman theorem, 69,
644â649
Shannon-Nyquist sampling theorem, 272
Sharpe, W.F., 614, 715
Sharpe-Markowitz theory, 614
Shields, P.C., 462, 715
Shimizu, M., xxiii
Shor, P.W., 241, 691, 693, 698
Shore, J.E., 715
short selling, 181
Shtarkov, Y.M., 631, 656, 719
Shtarkov, Y.V., 715
shufï¬e, 84, 89
Shwartz, A., 715
side information,
and source coding, 575
side information, xvii, 12, 159, 165, 166,
180, 255, 574, 576, 580â583, 596,
610, 623, 652
and doubling rate, 165, 622
Siegel, P.H., 704
Ï algebra, 644
signal, 1, 171, 192, 199, 234, 258,
262â299, 513, 517, 519, 533, 544,
561, 607
Sigurjonsson, S., xxi
Silicon Dreams, 170
silver iodide crystals, 293
simplex, 348, 378, 380, 385, 386, 391, 408,
617, 618
sinc function, 271
Sleator, D., 691
Slepian, D., 272, 299, 549, 609, 715

745

Slepian-Wolf coding, 10, 549â560, 575,
581, 586, 592, 593, 595, 598,
603â605, 608â610
achievability, 551
converse, 555
duality with multiple access channels,
558
slice code, 122
slice questions, 121
sliding window Lempel-Ziv, 441
Sloane, N.J.A., 707, 708, 720
smallest probable set, 64
Smolin, J., 691, 698
SNR (Signal to Noise Ratio), 273, 514, 516
Solomon, G., 214
Solomonoff, R.J., 3, 4, 507, 716
source, 103, 337
binary, 307
Gaussian, 310
source channel coding theorem, 218, 223
source channel separation, 218, 318, 344,
592, 593
source code, 103, 123, 552, 631
source coding, 60, 134, 447, 473, 511
and channel capacity, 430
with side information, 575, 595
source coding theorem, 144, 158
space-time coding, 611
Spanish, 561, 562, 606
spectral representation theorem, 315
spectrum, 271, 279, 280, 315, 415, 417,
419, 421
spectrum estimation, 415
speech, 1, 87, 101, 171, 218, 305, 416
sphere, 265, 297, 324, 675
sphere covering, 324
sphere packing, 10, 324, 325
squared error, 302, 393, 423, 683
squared error distortion, 336
St. Petersburg paradox, 181, 182
Stam, A., 674, 687, 716
state diagram, 95
state transition, 73, 465
stationary, 4, 69, 114, 168, 220, 221, 279,
297, 415â417, 423, 428, 444, 446,
451, 453, 455, 458, 462, 613, 625,
626, 646, 647, 651, 659, 681
stationary distribution, 73, 77â79, 96
stationary ergodic processes, 69

746

INDEX

stationary ergodic source, 219
stationary market, 624
stationary process, 71, 142, 644
statistic, 36, 38, 400
Kolmogorov sufï¬cient, 496
minimal sufï¬cient, 38
sufï¬cient, 38
statistical mechanics, 4, 6, 11, 55, 56, 425
statistics, xviâxix, 1, 4, 12, 13, 20, 36â38,
169, 347, 375, 497, 499
Steane, A., 716
Steinâs lemma, 399
stereo, 604
Stirlingâs approximation, 351, 353, 405,
411, 666
stochastic process, 71, 72, 74, 75, 77, 78,
87, 88, 91, 93, 94, 97, 98, 100, 114,
166, 219, 220, 223, 279, 415, 417,
420, 423â425, 455, 625, 626, 646
ergodic, see ergodic process
function of, 93
Gaussian, 315
without entropy rate, 75
stock, 9, 613â615, 619, 624, 626, 627,
629â634, 636, 637, 639â641, 652,
653
stock market, xix, 4, 9, 159, 613, 614â617,
619â622, 627, 629â631, 634, 636,
649, 652, 653, 655, 656
Stone, C.J., 693
stopping time, 55
Storer, J.A., 441, 459, 716
strategy, 160, 163, 164, 166, 178, 391, 392,
487
investment, 620
strong converse, 208, 240
strongly jointly typical, 327, 328
strongly typical, 326, 327, 357, 579, 580
strongly typical set, 342, 357
Stuart, A., 705
Studentâs t distribution, 662
subfair odds, 164
submatrix, 680
subset, xx, 8, 66, 71, 183, 192, 211, 222,
319, 347, 520, 644, 657, 668â670
subset inequalities, 668â671
subsets, 505
entropy rate, 667
subspace, 211

sufï¬cient statistic, 13, 36, 37, 38, 44, 56,
209, 497â499
minimal, 38, 56
sufï¬x code, 145
superfair odds, 164, 180
supermartingale, 625
superposition coding, 609
support set, 29, 243, 244, 249, 251, 252,
256, 409, 676â678
surface area, 247
Sutivong, A., xxi
Sweetkind-Singer, J., xxi
symbol, 103
symmetric channel, 187, 190
synchronization, 94
Szaszâs inequality, 680
SzegoÌ, G., 703
Szpankowski, W., 716, 708
Szymanski, T.G., 441, 459, 716
Tanabe, M., 713
Tang, D.L., 716
Tarjan, R., 691
TDMA (Time Division Multiple Access),
547, 548
Telatar, I.E., 716
telegraph, 441
telephone, 261, 270, 273, 274
channel capacity, 273
Teletar, E., 611, 716
temperature, 409, 411
ternary, 119, 145, 157, 239, 439, 504, 527
ternary alphabet, 349
ternary channel, 239
ternary code, 145, 152, 157
text, 428
thermodynamics, 1, 4, see also second law
of thermodynamics
Thitimajshima, P., 692
Thomas, J.A., xxi, 687, 694, 695, 698, 700,
716
Thomasian, A.J., 692
Tibshirani, R., 699
time symmetry, 100
timesharing, 527, 532â534, 538, 562, 598,
600
Tjalkens, T.J., 716, 719
Toeplitz matrix, 416, 681
Tornay, S.C., 716

INDEX

trace, 279, 547
transition matrix, 77, 92, 98, 144, 190
doubly stochastic, 83, 88, 190
transmitter, 266, 294, 296, 299, 515,
517â519, 546, 573, 574, 588, 601,
611
Treasury bonds, 614
tree,
code, 107
Huffman, 124
random, 89
tree structured Lempel-Ziv, 441, 442
triangle inequality, 20, 369
triangular distribution, 662
trigram model, 171
Troï¬mov, V.K., 706
Trott, M., xxiii
Tseng, C.W., xxiii
Tsoucas, P., 700
Tsybakov, B.S., 716
Tunstall, B.P., 716
Tunstall coding, 460
turbo codes, 3, 205, 215
Turing, A., 465
Turing machine, xix, 465, 466
TusnaÌdy, G., 332, 335, 346, 697
Tuttle, D.L., 182, 707
TV, 509, 560, 561
twin, 171
two envelope problem, 179
two level signalling, 262
two stage description, 496
two-way channel, 510, 519, 594, 602, 609
type, 342, 347, 348â350, 353â356, 358,
360, 361, 366, 367, 371, 373, 374,
378, 391, 408, 474, 490, 499, 570, 666
type class, 348â351, 353â356, 666
typewriter, 74, 192, 224, 235, 482
typical sequence, 11, 12, 57, 63, 245, 381,
522
typical set, 57, 59, 61, 62, 64, 68, 77, 196,
220, 227, 245, 247, 258, 319, 321,
356, 381, 382, 384, 524, 551
conditionally typical, 341
data compression, 60
distortion typical, 319
properties, 59, 64, 245
strongly typical, 326
volume, 245

747

Ullman, J.D., 704
uncertainty, 5, 6, 11, 13, 15, 20, 22, 24, 31,
53, 83, 89, 170, 517, 518, 593
Ungerboeck, G., 716
uniform distribution, 5, 30, 43, 83, 88, 148,
163, 190, 195, 202, 204, 209, 210,
228, 268, 338, 375, 408, 411, 412,
434, 436, 437, 553, 662, 663
uniform fair odds, 163, 166, 176, 626
uniquely decodable code, see code,
uniquely decodable
universal computer, 465, 501
universal data compression, 333, 457
universal gambling, 487, 488, 507
universal portfolios, 629â643, 651
ï¬nite horizon, 631
horizon free, 638
universal probability, 481, 487, 489â491,
502, 503, 507, 686
universal probability mass function, 481
universal source, 358
universal source code, 357, 360, 461
universal source coding, xv, 355, 427â462
error exponent, 400
universal Turing machine, 465, 480
Unix, 443
Urbanke, R., 702, 712
V.90, 273
Vâyugin, V.V., 507, 718
Vajda, I., 716
Valiant, L.G., 717
Van Campenhout, J.M., 717
Van der Meulen, E., xxiii, 609â611, 699,
702, 717
Van Trees, H.L., 716
Vapnik, V.N., 717
variable-to-ï¬xed length coding, 460
variance, xx, 36, 37, 64, 65, 255, 261, 265,
272, 292, 315, 325, 389, 393, 394,
396, 513, 516, 520, 544, 614, 655,
681, 685, see also covariance matrix
variational distance, 370
vector quantization, 303, 306
Venkata, R., xxi
Venkatesh, S.S., 707
Venn diagram, 23, 47, 50, 213
Verdu, S., 690, 698, 703, 704, 714, 717,
718

748

INDEX

Verdugo Lazo, A.C.G., 662, 718
video, 218
video compression, xv
Vidyasagar, M., 718
Visweswariah, K., 698, 718
vitamins, xx
Vitanyi, P., 508, 707
Viterbi, A.J., 718
Vitter, J.S., 718
vocabulary, 561
volume, 67, 149, 244â247, 249, 265, 324,
675, 676, 679
Von Neumann, J., 11
Voronoi partition, 303
waiting time, 99
Wald, A., 718
Wallace, M.S., 697
Wallmeier, H.M., 692
Washington, 551
Watanabe, Y., xxiii
water-ï¬lling, 164, 177, 277, 279, 282, 289,
315
waveform, 270, 305, 519
weak, 342
weakly typical, see typical
wealth, 175
wealth relative, 613, 619
weather, 470, 471, 550, 551
Weaver, W.W., 715
web search, xv
Wei, V.K.W., 718, 691
Weibull distribution, 662
Weinberger, M.J., 711, 718
Weiner, N., 718
Weiss, A., 715
Weiss, B., 462, 710, 715
Welch, T.A., 462, 718
Wheeler, D.J., 462, 693
white noise, 270, 272, 280, see
also Gaussian channel
Whiting, P.A., 702
Wiener, N., 718
Wiesner, S.J., 691
Wilcox, H.J., 718

Willems, F.M.J., xxi, 461, 594, 609, 716,
718, 719
window, 442
wine, 153
wireless, xv, 215, 611
Witsenhausen, H.S., 719
Witten, I.H., 691, 719
Wolf, J.K., 549, 593, 609, 700, 704, 715
Wolfowitz, J., 240, 408, 719
Woodward, P.M., 719
Wootters, W.K., 691
World Series, 48
Wozencraft, J.M., 666, 719
Wright, E.V., 168
wrong distribution, 115
Wyner, A.D., xxiii, 299, 462, 581, 586,
610, 703, 719, 720
Wyner, A.J., 720
Yaglom, A.M., 702
Yamamoto, H., xxiii
Yang, E.-H., 690, 720
Yao, A.C., 705
Yard, J., xxi
Yeung, R.W., xxi, xxiii, 610, 692, 720
Yockey, H.P., 720
Youngâs inequality, 676, 677
Yu, Bin, 691
Yule-Walker equations, 418, 419
Zeevi, A., xxi
Zeger, K., 708
Zeitouni, O., 698
zero-error, 205, 206, 210, 226, 301
zero-error capacity, 210, 226
zero-sum game, 131
Zhang, Z., 609, 690, 712, 720
Ziv, J., 428, 442, 462, 581, 586, 610, 703,
707, 711, 719â721, see
also Lempel-Ziv coding
Zivâs inequality, 449, 450, 453, 455,
456
Zurek, W.H., 507, 721
Zvonkin, A.K., 507, 707

